id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://root.cern/doc/master/classTMVA_1_1BinarySearchTreeNode.html:8338,Modifiability,variab,variable,8338,nts TMVA::Node.; Definition at line 72 of file BinarySearchTreeNode.h. ◆ DeclFileName(). static const char * TMVA::BinarySearchTreeNode::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 118 of file BinarySearchTreeNode.h. ◆ EqualsMe(). Bool_t TMVA::BinarySearchTreeNode::EqualsMe ; (; const Event & ; e); const. virtual . check if the event fed into the node actually equals the event that forms the node (in case of a search tree) ; Definition at line 139 of file BinarySearchTreeNode.cxx. ◆ GetClass(). UInt_t TMVA::BinarySearchTreeNode::GetClass ; (; ); const. inline . Definition at line 90 of file BinarySearchTreeNode.h. ◆ GetEventV(). const std::vector< Float_t > & TMVA::BinarySearchTreeNode::GetEventV ; (; ); const. inline . Definition at line 88 of file BinarySearchTreeNode.h. ◆ GetSelector(). Short_t TMVA::BinarySearchTreeNode::GetSelector ; (; ); const. inline . return index of variable used for discrimination at this node ; Definition at line 86 of file BinarySearchTreeNode.h. ◆ GetTargets(). const std::vector< Float_t > & TMVA::BinarySearchTreeNode::GetTargets ; (; ); const. inline . Definition at line 92 of file BinarySearchTreeNode.h. ◆ GetWeight(). Float_t TMVA::BinarySearchTreeNode::GetWeight ; (; ); const. inline . Definition at line 89 of file BinarySearchTreeNode.h. ◆ GoesLeft(). Bool_t TMVA::BinarySearchTreeNode::GoesLeft ; (; const Event & ; e); const. virtual . check if the event fed into the node goes/descends to the left daughter ; Implements TMVA::Node.; Definition at line 129 of file BinarySearchTreeNode.cxx. ◆ GoesRight(). Bool_t TMVA::BinarySearchTreeNode::GoesRight ; (; const Event & ; e); const. virtual . check if the event fed into the node goes/descends to the right daughter ; Implements TMVA::Node.; Definition at line 120 of file BinarySearchTreeNode.cxx. ◆ IsA(). virtual TClass * TMVA::BinarySearchTreeNode::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current o,MatchSource.WIKI,doc/master/classTMVA_1_1BinarySearchTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1BinarySearchTreeNode.html
https://root.cern/doc/master/classTMVA_1_1BinarySearchTreeNode.html:10730,Modifiability,variab,variable,10730,"ively print the node and its daughters (--> print the 'tree') ; Implements TMVA::Node.; Definition at line 172 of file BinarySearchTreeNode.cxx. ◆ ReadAttributes(). void TMVA::BinarySearchTreeNode::ReadAttributes ; (; void * ; node, . UInt_t ; tmva_Version_Code = 262657 . ). virtual . read attributes from XML ; Implements TMVA::Node.; Definition at line 225 of file BinarySearchTreeNode.cxx. ◆ ReadContent(). void TMVA::BinarySearchTreeNode::ReadContent ; (; std::stringstream & ; s). virtual . read events from node ; Implements TMVA::Node.; Definition at line 268 of file BinarySearchTreeNode.cxx. ◆ ReadDataRecord(). Bool_t TMVA::BinarySearchTreeNode::ReadDataRecord ; (; std::istream & ; is, . UInt_t ; tmva_Version_Code = 262657 . ). virtual . Read the data block. ; Implements TMVA::Node.; Definition at line 189 of file BinarySearchTreeNode.cxx. ◆ SetSelector(). void TMVA::BinarySearchTreeNode::SetSelector ; (; Short_t ; i). inline . set index of variable used for discrimination at this node ; Definition at line 84 of file BinarySearchTreeNode.h. ◆ Streamer(). virtual void TMVA::BinarySearchTreeNode::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::Node. ◆ StreamerNVirtual(). void TMVA::BinarySearchTreeNode::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 118 of file BinarySearchTreeNode.h. Member Data Documentation. ◆ fClass. UInt_t TMVA::BinarySearchTreeNode::fClass. private . Definition at line 114 of file BinarySearchTreeNode.h. ◆ fEventV. std::vector<Float_t> TMVA::BinarySearchTreeNode::fEventV. private . Definition at line 110 of file BinarySearchTreeNode.h. ◆ fSelector. Short_t TMVA::BinarySearchTreeNode::fSelector. private . index of variable used in node selection (decision tree) ; Definition at line 116 of file BinarySearchTreeNode.h. ◆ fTargets. std::vector<Float_t> TMVA::BinarySearchTreeNode::fTargets. private . Definition at line 111 of file BinarySearchTreeNode.h. ◆ fWeight. Float_t TMVA::BinarySea",MatchSource.WIKI,doc/master/classTMVA_1_1BinarySearchTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1BinarySearchTreeNode.html
https://root.cern/doc/master/classTMVA_1_1BinarySearchTreeNode.html:11498,Modifiability,variab,variable,11498,"SearchTreeNode::ReadContent ; (; std::stringstream & ; s). virtual . read events from node ; Implements TMVA::Node.; Definition at line 268 of file BinarySearchTreeNode.cxx. ◆ ReadDataRecord(). Bool_t TMVA::BinarySearchTreeNode::ReadDataRecord ; (; std::istream & ; is, . UInt_t ; tmva_Version_Code = 262657 . ). virtual . Read the data block. ; Implements TMVA::Node.; Definition at line 189 of file BinarySearchTreeNode.cxx. ◆ SetSelector(). void TMVA::BinarySearchTreeNode::SetSelector ; (; Short_t ; i). inline . set index of variable used for discrimination at this node ; Definition at line 84 of file BinarySearchTreeNode.h. ◆ Streamer(). virtual void TMVA::BinarySearchTreeNode::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::Node. ◆ StreamerNVirtual(). void TMVA::BinarySearchTreeNode::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 118 of file BinarySearchTreeNode.h. Member Data Documentation. ◆ fClass. UInt_t TMVA::BinarySearchTreeNode::fClass. private . Definition at line 114 of file BinarySearchTreeNode.h. ◆ fEventV. std::vector<Float_t> TMVA::BinarySearchTreeNode::fEventV. private . Definition at line 110 of file BinarySearchTreeNode.h. ◆ fSelector. Short_t TMVA::BinarySearchTreeNode::fSelector. private . index of variable used in node selection (decision tree) ; Definition at line 116 of file BinarySearchTreeNode.h. ◆ fTargets. std::vector<Float_t> TMVA::BinarySearchTreeNode::fTargets. private . Definition at line 111 of file BinarySearchTreeNode.h. ◆ fWeight. Float_t TMVA::BinarySearchTreeNode::fWeight. private . Definition at line 113 of file BinarySearchTreeNode.h. Libraries for TMVA::BinarySearchTreeNode:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/BinarySearchTreeNode.h; tmva/tmva/src/BinarySearchTreeNode.cxx. TMVABinarySearchTreeNode. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1BinarySearchTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1BinarySearchTreeNode.html
https://root.cern/doc/master/classTMVA_1_1BinaryTree.html:6435,Integrability,depend,depends,6435,"::GetRoot ; (; ); const. inlinevirtual . Reimplemented in TMVA::DecisionTree.; Definition at line 83 of file BinaryTree.h. ◆ GetTotalTreeDepth(). UInt_t TMVA::BinaryTree::GetTotalTreeDepth ; (; ); const. inline . Definition at line 93 of file BinaryTree.h. ◆ IsA(). virtual TClass * TMVA::BinaryTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented in TMVA::BinarySearchTree, and TMVA::DecisionTree.; Definition at line 122 of file BinaryTree.h. ◆ Log(). TMVA::MsgLogger & TMVA::BinaryTree::Log ; (; ); const. protected . Definition at line 235 of file BinaryTree.cxx. ◆ Print(). void TMVA::BinaryTree::Print ; (; std::ostream & ; os); const. virtual . recursively print the tree ; Definition at line 125 of file BinaryTree.cxx. ◆ Read(). void TMVA::BinaryTree::Read ; (; std::istream & ; istr, . UInt_t ; tmva_Version_Code = 262657 . ). virtual . Read the binary tree from an input stream. ; The input stream format depends on the tree type, it is defined be the node of the tree ; Definition at line 169 of file BinaryTree.cxx. ◆ ReadXML(). void TMVA::BinaryTree::ReadXML ; (; void * ; node, . UInt_t ; tmva_Version_Code = 262657 . ). virtual . read attributes from XML ; Definition at line 144 of file BinaryTree.cxx. ◆ SetRoot(). void TMVA::BinaryTree::SetRoot ; (; Node * ; r). inline . Definition at line 80 of file BinaryTree.h. ◆ SetTotalTreeDepth() [1/2]. void TMVA::BinaryTree::SetTotalTreeDepth ; (; Int_t ; depth). inline . Definition at line 95 of file BinaryTree.h. ◆ SetTotalTreeDepth() [2/2]. void TMVA::BinaryTree::SetTotalTreeDepth ; (; Node * ; n = nullptr). descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ; Definition at line 213 of file BinaryTree.cxx. ◆ Streamer(). virtual void TMVA::BinaryTree::Streamer ; (; TBuffer & ; ). virtual . Reimplemented in TMVA::BinarySearchTree, and TMVA::DecisionTree. ◆ StreamerNVirtual(). void TMVA::BinaryTree::StreamerNVirtual ; (; TBuffer & ; ClassDe",MatchSource.WIKI,doc/master/classTMVA_1_1BinaryTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1BinaryTree.html
https://root.cern/doc/master/classTMVA_1_1CCPruner.html:1336,Security,validat,validation,1336,". ; A helper class to prune a decision tree using the Cost Complexity method (see Classification and Regression Trees by Leo Breiman et al) . Some definitions:. \( T_{max} \) - the initial, usually highly overtrained tree, that is to be pruned back; \( R(T) \) - quality index (Gini, misclassification rate, or other) of a tree \( T \); \( \sim T \) - set of terminal nodes in \( T \); \( T' \) - the pruned subtree of \( T_max \) that has the best quality index \( R(T') \); \( \alpha \) - the prune strength parameter in Cost Complexity pruning \( (R_{\alpha}(T) = R(T) + \alpha*|\sim T|) \). There are two running modes in CCPruner: (i) one may select a prune strength and prune back the tree \( T_{max}\) until the criterion: ; \[; \alpha < \frac{R(T) - R(t)}{|\sim T_t| - 1}; \]. is true for all nodes t in \( T \), or (ii) the algorithm finds the sequence of critical points \( \alpha_k < \alpha_{k+1} ... < \alpha_K \) such that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CCPruner.h. Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCPruner (DecisionTree *t_max, const DataSet *validationSample, SeparationBase *qualityIndex=nullptr);  constructor ;  ;  CCPruner (DecisionTree *t_max, const EventList *validationSample, SeparationBase *qualityIndex=nullptr);  constructor ;  ;  ~CCPruner ();  ; std::vector< TMVA::DecisionTreeNode * > GetOptimalPruneSequence () const;  return the prune strength (=alpha) corresponding to the prune sequence ;  ; Float_t GetOptimalPruneStrength () const;  ; Float_t GetOptimalQualityIndex () const;  ; void Optimize ();  determine the pruning sequence ;  ; void SetPruneStrength (Float_t alpha=-1.0);  . Private Attributes; Float_t fAlpha;  ! regularization parameter in CC pruning ;  ; Bool_t fDebug;  ! debug flag ;  ; Int_t fOptimalK;  ! index of the optimal tree in the pruned",MatchSource.WIKI,doc/master/classTMVA_1_1CCPruner.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCPruner.html
https://root.cern/doc/master/classTMVA_1_1CCPruner.html:1530,Security,validat,validationSample,1530,"ength parameter in Cost Complexity pruning \( (R_{\alpha}(T) = R(T) + \alpha*|\sim T|) \). There are two running modes in CCPruner: (i) one may select a prune strength and prune back the tree \( T_{max}\) until the criterion: ; \[; \alpha < \frac{R(T) - R(t)}{|\sim T_t| - 1}; \]. is true for all nodes t in \( T \), or (ii) the algorithm finds the sequence of critical points \( \alpha_k < \alpha_{k+1} ... < \alpha_K \) such that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CCPruner.h. Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCPruner (DecisionTree *t_max, const DataSet *validationSample, SeparationBase *qualityIndex=nullptr);  constructor ;  ;  CCPruner (DecisionTree *t_max, const EventList *validationSample, SeparationBase *qualityIndex=nullptr);  constructor ;  ;  ~CCPruner ();  ; std::vector< TMVA::DecisionTreeNode * > GetOptimalPruneSequence () const;  return the prune strength (=alpha) corresponding to the prune sequence ;  ; Float_t GetOptimalPruneStrength () const;  ; Float_t GetOptimalQualityIndex () const;  ; void Optimize ();  determine the pruning sequence ;  ; void SetPruneStrength (Float_t alpha=-1.0);  . Private Attributes; Float_t fAlpha;  ! regularization parameter in CC pruning ;  ; Bool_t fDebug;  ! debug flag ;  ; Int_t fOptimalK;  ! index of the optimal tree in the pruned tree sequence ;  ; Bool_t fOwnQIndex;  ! flag indicates if fQualityIndex is owned by this ;  ; std::vector< TMVA::DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Float_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; SeparationBase * fQualityIndex;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  ; std::vector< Float_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; DecisionTre",MatchSource.WIKI,doc/master/classTMVA_1_1CCPruner.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCPruner.html
https://root.cern/doc/master/classTMVA_1_1CCPruner.html:1654,Security,validat,validationSample,1654,"ength parameter in Cost Complexity pruning \( (R_{\alpha}(T) = R(T) + \alpha*|\sim T|) \). There are two running modes in CCPruner: (i) one may select a prune strength and prune back the tree \( T_{max}\) until the criterion: ; \[; \alpha < \frac{R(T) - R(t)}{|\sim T_t| - 1}; \]. is true for all nodes t in \( T \), or (ii) the algorithm finds the sequence of critical points \( \alpha_k < \alpha_{k+1} ... < \alpha_K \) such that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CCPruner.h. Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCPruner (DecisionTree *t_max, const DataSet *validationSample, SeparationBase *qualityIndex=nullptr);  constructor ;  ;  CCPruner (DecisionTree *t_max, const EventList *validationSample, SeparationBase *qualityIndex=nullptr);  constructor ;  ;  ~CCPruner ();  ; std::vector< TMVA::DecisionTreeNode * > GetOptimalPruneSequence () const;  return the prune strength (=alpha) corresponding to the prune sequence ;  ; Float_t GetOptimalPruneStrength () const;  ; Float_t GetOptimalQualityIndex () const;  ; void Optimize ();  determine the pruning sequence ;  ; void SetPruneStrength (Float_t alpha=-1.0);  . Private Attributes; Float_t fAlpha;  ! regularization parameter in CC pruning ;  ; Bool_t fDebug;  ! debug flag ;  ; Int_t fOptimalK;  ! index of the optimal tree in the pruned tree sequence ;  ; Bool_t fOwnQIndex;  ! flag indicates if fQualityIndex is owned by this ;  ; std::vector< TMVA::DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Float_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; SeparationBase * fQualityIndex;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  ; std::vector< Float_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; DecisionTre",MatchSource.WIKI,doc/master/classTMVA_1_1CCPruner.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCPruner.html
https://root.cern/doc/master/classTMVA_1_1CCPruner.html:3305,Security,validat,validationSample,3305," ! flag indicates if fQualityIndex is owned by this ;  ; std::vector< TMVA::DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Float_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; SeparationBase * fQualityIndex;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  ; std::vector< Float_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; DecisionTree * fTree;  ! (pruned) decision tree ;  ; const DataSet * fValidationDataSet;  ! the event sample to select the optimally-pruned tree ;  ; const EventList * fValidationSample;  ! the event sample to select the optimally-pruned tree ;  . #include <TMVA/CCPruner.h>; Member Typedef Documentation. ◆ EventList. typedef std::vector<Event*> TMVA::CCPruner::EventList. Definition at line 64 of file CCPruner.h. Constructor & Destructor Documentation. ◆ CCPruner() [1/2]. CCPruner::CCPruner ; (; DecisionTree * ; t_max, . const EventList * ; validationSample, . SeparationBase * ; qualityIndex = nullptr . ). constructor ; Definition at line 69 of file CCPruner.cxx. ◆ CCPruner() [2/2]. CCPruner::CCPruner ; (; DecisionTree * ; t_max, . const DataSet * ; validationSample, . SeparationBase * ; qualityIndex = nullptr . ). constructor ; Definition at line 92 of file CCPruner.cxx. ◆ ~CCPruner(). CCPruner::~CCPruner ; (; ). Definition at line 115 of file CCPruner.cxx. Member Function Documentation. ◆ GetOptimalPruneSequence(). std::vector< DecisionTreeNode * > CCPruner::GetOptimalPruneSequence ; (; ); const. return the prune strength (=alpha) corresponding to the prune sequence ; Definition at line 240 of file CCPruner.cxx. ◆ GetOptimalPruneStrength(). Float_t TMVA::CCPruner::GetOptimalPruneStrength ; (; ); const. inline . Definition at line 89 of file CCPruner.h. ◆ GetOptimalQualityIndex(). Float_t TMVA::CCPruner::GetOptimalQualityIndex ; (; ); const. inline . Definition at line 85 of file CCPruner.h. ◆ Optimize(). void CCPruner:",MatchSource.WIKI,doc/master/classTMVA_1_1CCPruner.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCPruner.html
https://root.cern/doc/master/classTMVA_1_1CCPruner.html:3518,Security,validat,validationSample,3518,"StrengthList;  ! map of alpha -> pruning index ;  ; SeparationBase * fQualityIndex;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  ; std::vector< Float_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; DecisionTree * fTree;  ! (pruned) decision tree ;  ; const DataSet * fValidationDataSet;  ! the event sample to select the optimally-pruned tree ;  ; const EventList * fValidationSample;  ! the event sample to select the optimally-pruned tree ;  . #include <TMVA/CCPruner.h>; Member Typedef Documentation. ◆ EventList. typedef std::vector<Event*> TMVA::CCPruner::EventList. Definition at line 64 of file CCPruner.h. Constructor & Destructor Documentation. ◆ CCPruner() [1/2]. CCPruner::CCPruner ; (; DecisionTree * ; t_max, . const EventList * ; validationSample, . SeparationBase * ; qualityIndex = nullptr . ). constructor ; Definition at line 69 of file CCPruner.cxx. ◆ CCPruner() [2/2]. CCPruner::CCPruner ; (; DecisionTree * ; t_max, . const DataSet * ; validationSample, . SeparationBase * ; qualityIndex = nullptr . ). constructor ; Definition at line 92 of file CCPruner.cxx. ◆ ~CCPruner(). CCPruner::~CCPruner ; (; ). Definition at line 115 of file CCPruner.cxx. Member Function Documentation. ◆ GetOptimalPruneSequence(). std::vector< DecisionTreeNode * > CCPruner::GetOptimalPruneSequence ; (; ); const. return the prune strength (=alpha) corresponding to the prune sequence ; Definition at line 240 of file CCPruner.cxx. ◆ GetOptimalPruneStrength(). Float_t TMVA::CCPruner::GetOptimalPruneStrength ; (; ); const. inline . Definition at line 89 of file CCPruner.h. ◆ GetOptimalQualityIndex(). Float_t TMVA::CCPruner::GetOptimalQualityIndex ; (; ); const. inline . Definition at line 85 of file CCPruner.h. ◆ Optimize(). void CCPruner::Optimize ; (; ). determine the pruning sequence ; Definition at line 124 of file CCPruner.cxx. ◆ SetPruneStrength(). void TMVA::CCPruner::SetPruneStrength ; (; Float_t ; alpha = -1.0). inline . Definition at line ",MatchSource.WIKI,doc/master/classTMVA_1_1CCPruner.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCPruner.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:1429,Integrability,wrap,wrapped,1429,"r.h. Classes; class  CCTreeNode;  . Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCTreeWrapper (DecisionTree *T, SeparationBase *qualityIndex);  constructor ;  ;  ~CCTreeWrapper ();  destructor ;  ; Double_t CheckEvent (const TMVA::Event &e, Bool_t useYesNoLeaf=false);  return the decision tree output for an event ;  ; CCTreeNode * GetRoot ();  ; void InitTree (CCTreeNode *t);  initialize the node t and all its descendants ;  ; void PruneNode (CCTreeNode *t);  remove the branch rooted at node t ;  ; Double_t TestTreeQuality (const DataSet *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using the DataSet ;  ; Double_t TestTreeQuality (const EventList *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using an EventList ;  . Private Attributes; DecisionTree * fDTParent;  ! pointer to underlying DecisionTree ;  ; SeparationBase * fQualityIndex;  ! pointer to the used quality index calculator ;  ; CCTreeNode * fRoot;  ! the root node of the (wrapped) decision Tree ;  . #include <TMVA/CCTreeWrapper.h>; Member Typedef Documentation. ◆ EventList. typedef std::vector<Event*> TMVA::CCTreeWrapper::EventList. Definition at line 42 of file CCTreeWrapper.h. Constructor & Destructor Documentation. ◆ CCTreeWrapper(). TMVA::CCTreeWrapper::CCTreeWrapper ; (; DecisionTree * ; T, . SeparationBase * ; qualityIndex . ). constructor ; Definition at line 104 of file CCTreeWrapper.cxx. ◆ ~CCTreeWrapper(). TMVA::CCTreeWrapper::~CCTreeWrapper ; (; ). destructor ; Definition at line 116 of file CCTreeWrapper.cxx. Member Function Documentation. ◆ CheckEvent(). Double_t TMVA::CCTreeWrapper::CheckEvent ; (; const TMVA::Event & ; e, . Bool_t ; useYesNoLeaf = false . ). return the decision tree output for an event ; Definition at line 226 of file CCTreeWrapper.cxx. ◆ GetRoot(). CCTreeNode * TMVA::CCTreeWrapper::GetRoot ; (; ). inline . Definition at line 1",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:3671,Integrability,wrap,wrapped,3671,"eWrapper::CheckEvent ; (; const TMVA::Event & ; e, . Bool_t ; useYesNoLeaf = false . ). return the decision tree output for an event ; Definition at line 226 of file CCTreeWrapper.cxx. ◆ GetRoot(). CCTreeNode * TMVA::CCTreeWrapper::GetRoot ; (; ). inline . Definition at line 148 of file CCTreeWrapper.h. ◆ InitTree(). void TMVA::CCTreeWrapper::InitTree ; (; CCTreeNode * ; t). initialize the node t and all its descendants ; Definition at line 123 of file CCTreeWrapper.cxx. ◆ PruneNode(). void TMVA::CCTreeWrapper::PruneNode ; (; CCTreeNode * ; t). remove the branch rooted at node t ; Definition at line 160 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [1/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const DataSet * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using the DataSet ; Definition at line 203 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [2/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const EventList * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using an EventList ; Definition at line 183 of file CCTreeWrapper.cxx. Member Data Documentation. ◆ fDTParent. DecisionTree* TMVA::CCTreeWrapper::fDTParent. private . ! pointer to underlying DecisionTree ; Definition at line 151 of file CCTreeWrapper.h. ◆ fQualityIndex. SeparationBase* TMVA::CCTreeWrapper::fQualityIndex. private . ! pointer to the used quality index calculator ; Definition at line 150 of file CCTreeWrapper.h. ◆ fRoot. CCTreeNode* TMVA::CCTreeWrapper::fRoot. private . ! the root node of the (wrapped) decision Tree ; Definition at line 152 of file CCTreeWrapper.h. Libraries for TMVA::CCTreeWrapper:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/CCTreeWrapper.h; tmva/tmva/src/CCTreeWrapper.cxx. TMVACCTreeWrapper. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:918,Security,validat,validationSample,918,". ROOT: TMVA::CCTreeWrapper Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::CCTreeWrapper Class ReferenceTMVA. . Definition at line 38 of file CCTreeWrapper.h. Classes; class  CCTreeNode;  . Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCTreeWrapper (DecisionTree *T, SeparationBase *qualityIndex);  constructor ;  ;  ~CCTreeWrapper ();  destructor ;  ; Double_t CheckEvent (const TMVA::Event &e, Bool_t useYesNoLeaf=false);  return the decision tree output for an event ;  ; CCTreeNode * GetRoot ();  ; void InitTree (CCTreeNode *t);  initialize the node t and all its descendants ;  ; void PruneNode (CCTreeNode *t);  remove the branch rooted at node t ;  ; Double_t TestTreeQuality (const DataSet *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using the DataSet ;  ; Double_t TestTreeQuality (const EventList *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using an EventList ;  . Private Attributes; DecisionTree * fDTParent;  ! pointer to underlying DecisionTree ;  ; SeparationBase * fQualityIndex;  ! pointer to the used quality index calculator ;  ; CCTreeNode * fRoot;  ! the root node of the (wrapped) decision Tree ;  . #include <TMVA/CCTreeWrapper.h>; Member Typedef Documentation. ◆ EventList. typedef std::vector<Event*> TMVA::CCTreeWrapper::EventList. Definition at line 42 of file CCTreeWrapper.h. Constructor & Destructor Documentation. ◆ CCTreeWrapper(). TMVA::CCTreeWrapper::CCTreeWrapper ; (; DecisionTree * ; T, . SeparationBase * ; qualityIndex . ). constructor ; Definition at line 104 of file CCTreeWrapper.cxx. ◆ ~CCTreeWrapper(). TMVA::CCTreeWrapper::~CCTreeWrapper ; (; ). destructor ; Definition at line 116 of file CCTreeWrapper.cxx. Member Funct",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:995,Security,validat,validation,995,". ROOT: TMVA::CCTreeWrapper Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::CCTreeWrapper Class ReferenceTMVA. . Definition at line 38 of file CCTreeWrapper.h. Classes; class  CCTreeNode;  . Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCTreeWrapper (DecisionTree *T, SeparationBase *qualityIndex);  constructor ;  ;  ~CCTreeWrapper ();  destructor ;  ; Double_t CheckEvent (const TMVA::Event &e, Bool_t useYesNoLeaf=false);  return the decision tree output for an event ;  ; CCTreeNode * GetRoot ();  ; void InitTree (CCTreeNode *t);  initialize the node t and all its descendants ;  ; void PruneNode (CCTreeNode *t);  remove the branch rooted at node t ;  ; Double_t TestTreeQuality (const DataSet *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using the DataSet ;  ; Double_t TestTreeQuality (const EventList *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using an EventList ;  . Private Attributes; DecisionTree * fDTParent;  ! pointer to underlying DecisionTree ;  ; SeparationBase * fQualityIndex;  ! pointer to the used quality index calculator ;  ; CCTreeNode * fRoot;  ! the root node of the (wrapped) decision Tree ;  . #include <TMVA/CCTreeWrapper.h>; Member Typedef Documentation. ◆ EventList. typedef std::vector<Event*> TMVA::CCTreeWrapper::EventList. Definition at line 42 of file CCTreeWrapper.h. Constructor & Destructor Documentation. ◆ CCTreeWrapper(). TMVA::CCTreeWrapper::CCTreeWrapper ; (; DecisionTree * ; T, . SeparationBase * ; qualityIndex . ). constructor ; Definition at line 104 of file CCTreeWrapper.cxx. ◆ ~CCTreeWrapper(). TMVA::CCTreeWrapper::~CCTreeWrapper ; (; ). destructor ; Definition at line 116 of file CCTreeWrapper.cxx. Member Funct",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:1085,Security,validat,validationSample,1085,". ROOT: TMVA::CCTreeWrapper Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::CCTreeWrapper Class ReferenceTMVA. . Definition at line 38 of file CCTreeWrapper.h. Classes; class  CCTreeNode;  . Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCTreeWrapper (DecisionTree *T, SeparationBase *qualityIndex);  constructor ;  ;  ~CCTreeWrapper ();  destructor ;  ; Double_t CheckEvent (const TMVA::Event &e, Bool_t useYesNoLeaf=false);  return the decision tree output for an event ;  ; CCTreeNode * GetRoot ();  ; void InitTree (CCTreeNode *t);  initialize the node t and all its descendants ;  ; void PruneNode (CCTreeNode *t);  remove the branch rooted at node t ;  ; Double_t TestTreeQuality (const DataSet *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using the DataSet ;  ; Double_t TestTreeQuality (const EventList *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using an EventList ;  . Private Attributes; DecisionTree * fDTParent;  ! pointer to underlying DecisionTree ;  ; SeparationBase * fQualityIndex;  ! pointer to the used quality index calculator ;  ; CCTreeNode * fRoot;  ! the root node of the (wrapped) decision Tree ;  . #include <TMVA/CCTreeWrapper.h>; Member Typedef Documentation. ◆ EventList. typedef std::vector<Event*> TMVA::CCTreeWrapper::EventList. Definition at line 42 of file CCTreeWrapper.h. Constructor & Destructor Documentation. ◆ CCTreeWrapper(). TMVA::CCTreeWrapper::CCTreeWrapper ; (; DecisionTree * ; T, . SeparationBase * ; qualityIndex . ). constructor ; Definition at line 104 of file CCTreeWrapper.cxx. ◆ ~CCTreeWrapper(). TMVA::CCTreeWrapper::~CCTreeWrapper ; (; ). destructor ; Definition at line 116 of file CCTreeWrapper.cxx. Member Funct",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:1162,Security,validat,validation,1162,". ROOT: TMVA::CCTreeWrapper Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::CCTreeWrapper Class ReferenceTMVA. . Definition at line 38 of file CCTreeWrapper.h. Classes; class  CCTreeNode;  . Public Types; typedef std::vector< Event * > EventList;  . Public Member Functions;  CCTreeWrapper (DecisionTree *T, SeparationBase *qualityIndex);  constructor ;  ;  ~CCTreeWrapper ();  destructor ;  ; Double_t CheckEvent (const TMVA::Event &e, Bool_t useYesNoLeaf=false);  return the decision tree output for an event ;  ; CCTreeNode * GetRoot ();  ; void InitTree (CCTreeNode *t);  initialize the node t and all its descendants ;  ; void PruneNode (CCTreeNode *t);  remove the branch rooted at node t ;  ; Double_t TestTreeQuality (const DataSet *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using the DataSet ;  ; Double_t TestTreeQuality (const EventList *validationSample);  return the misclassification rate of a pruned tree for a validation event sample using an EventList ;  . Private Attributes; DecisionTree * fDTParent;  ! pointer to underlying DecisionTree ;  ; SeparationBase * fQualityIndex;  ! pointer to the used quality index calculator ;  ; CCTreeNode * fRoot;  ! the root node of the (wrapped) decision Tree ;  . #include <TMVA/CCTreeWrapper.h>; Member Typedef Documentation. ◆ EventList. typedef std::vector<Event*> TMVA::CCTreeWrapper::EventList. Definition at line 42 of file CCTreeWrapper.h. Constructor & Destructor Documentation. ◆ CCTreeWrapper(). TMVA::CCTreeWrapper::CCTreeWrapper ; (; DecisionTree * ; T, . SeparationBase * ; qualityIndex . ). constructor ; Definition at line 104 of file CCTreeWrapper.cxx. ◆ ~CCTreeWrapper(). TMVA::CCTreeWrapper::~CCTreeWrapper ; (; ). destructor ; Definition at line 116 of file CCTreeWrapper.cxx. Member Funct",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:2790,Security,validat,validationSample,2790,"arationBase * ; qualityIndex . ). constructor ; Definition at line 104 of file CCTreeWrapper.cxx. ◆ ~CCTreeWrapper(). TMVA::CCTreeWrapper::~CCTreeWrapper ; (; ). destructor ; Definition at line 116 of file CCTreeWrapper.cxx. Member Function Documentation. ◆ CheckEvent(). Double_t TMVA::CCTreeWrapper::CheckEvent ; (; const TMVA::Event & ; e, . Bool_t ; useYesNoLeaf = false . ). return the decision tree output for an event ; Definition at line 226 of file CCTreeWrapper.cxx. ◆ GetRoot(). CCTreeNode * TMVA::CCTreeWrapper::GetRoot ; (; ). inline . Definition at line 148 of file CCTreeWrapper.h. ◆ InitTree(). void TMVA::CCTreeWrapper::InitTree ; (; CCTreeNode * ; t). initialize the node t and all its descendants ; Definition at line 123 of file CCTreeWrapper.cxx. ◆ PruneNode(). void TMVA::CCTreeWrapper::PruneNode ; (; CCTreeNode * ; t). remove the branch rooted at node t ; Definition at line 160 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [1/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const DataSet * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using the DataSet ; Definition at line 203 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [2/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const EventList * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using an EventList ; Definition at line 183 of file CCTreeWrapper.cxx. Member Data Documentation. ◆ fDTParent. DecisionTree* TMVA::CCTreeWrapper::fDTParent. private . ! pointer to underlying DecisionTree ; Definition at line 151 of file CCTreeWrapper.h. ◆ fQualityIndex. SeparationBase* TMVA::CCTreeWrapper::fQualityIndex. private . ! pointer to the used quality index calculator ; Definition at line 150 of file CCTreeWrapper.h. ◆ fRoot. CCTreeNode* TMVA::CCTreeWrapper::fRoot. private . ! the root node of the (wrapped) decision Tree ; Definition at line 152 of file CCTreeWrapper.h. Libraries for TMVA::",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:2866,Security,validat,validation,2866," TMVA::CCTreeWrapper::~CCTreeWrapper ; (; ). destructor ; Definition at line 116 of file CCTreeWrapper.cxx. Member Function Documentation. ◆ CheckEvent(). Double_t TMVA::CCTreeWrapper::CheckEvent ; (; const TMVA::Event & ; e, . Bool_t ; useYesNoLeaf = false . ). return the decision tree output for an event ; Definition at line 226 of file CCTreeWrapper.cxx. ◆ GetRoot(). CCTreeNode * TMVA::CCTreeWrapper::GetRoot ; (; ). inline . Definition at line 148 of file CCTreeWrapper.h. ◆ InitTree(). void TMVA::CCTreeWrapper::InitTree ; (; CCTreeNode * ; t). initialize the node t and all its descendants ; Definition at line 123 of file CCTreeWrapper.cxx. ◆ PruneNode(). void TMVA::CCTreeWrapper::PruneNode ; (; CCTreeNode * ; t). remove the branch rooted at node t ; Definition at line 160 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [1/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const DataSet * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using the DataSet ; Definition at line 203 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [2/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const EventList * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using an EventList ; Definition at line 183 of file CCTreeWrapper.cxx. Member Data Documentation. ◆ fDTParent. DecisionTree* TMVA::CCTreeWrapper::fDTParent. private . ! pointer to underlying DecisionTree ; Definition at line 151 of file CCTreeWrapper.h. ◆ fQualityIndex. SeparationBase* TMVA::CCTreeWrapper::fQualityIndex. private . ! pointer to the used quality index calculator ; Definition at line 150 of file CCTreeWrapper.h. ◆ fRoot. CCTreeNode* TMVA::CCTreeWrapper::fRoot. private . ! the root node of the (wrapped) decision Tree ; Definition at line 152 of file CCTreeWrapper.h. Libraries for TMVA::CCTreeWrapper:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:3058,Security,validat,validationSample,3058,"t(). Double_t TMVA::CCTreeWrapper::CheckEvent ; (; const TMVA::Event & ; e, . Bool_t ; useYesNoLeaf = false . ). return the decision tree output for an event ; Definition at line 226 of file CCTreeWrapper.cxx. ◆ GetRoot(). CCTreeNode * TMVA::CCTreeWrapper::GetRoot ; (; ). inline . Definition at line 148 of file CCTreeWrapper.h. ◆ InitTree(). void TMVA::CCTreeWrapper::InitTree ; (; CCTreeNode * ; t). initialize the node t and all its descendants ; Definition at line 123 of file CCTreeWrapper.cxx. ◆ PruneNode(). void TMVA::CCTreeWrapper::PruneNode ; (; CCTreeNode * ; t). remove the branch rooted at node t ; Definition at line 160 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [1/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const DataSet * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using the DataSet ; Definition at line 203 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [2/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const EventList * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using an EventList ; Definition at line 183 of file CCTreeWrapper.cxx. Member Data Documentation. ◆ fDTParent. DecisionTree* TMVA::CCTreeWrapper::fDTParent. private . ! pointer to underlying DecisionTree ; Definition at line 151 of file CCTreeWrapper.h. ◆ fQualityIndex. SeparationBase* TMVA::CCTreeWrapper::fQualityIndex. private . ! pointer to the used quality index calculator ; Definition at line 150 of file CCTreeWrapper.h. ◆ fRoot. CCTreeNode* TMVA::CCTreeWrapper::fRoot. private . ! the root node of the (wrapped) decision Tree ; Definition at line 152 of file CCTreeWrapper.h. Libraries for TMVA::CCTreeWrapper:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/CCTreeWrapper.h; tmva/tmva/src/CCTreeWrapper.cxx. TMVACCTreeWrapper. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) u",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html:3134,Security,validat,validation,3134,"eWrapper::CheckEvent ; (; const TMVA::Event & ; e, . Bool_t ; useYesNoLeaf = false . ). return the decision tree output for an event ; Definition at line 226 of file CCTreeWrapper.cxx. ◆ GetRoot(). CCTreeNode * TMVA::CCTreeWrapper::GetRoot ; (; ). inline . Definition at line 148 of file CCTreeWrapper.h. ◆ InitTree(). void TMVA::CCTreeWrapper::InitTree ; (; CCTreeNode * ; t). initialize the node t and all its descendants ; Definition at line 123 of file CCTreeWrapper.cxx. ◆ PruneNode(). void TMVA::CCTreeWrapper::PruneNode ; (; CCTreeNode * ; t). remove the branch rooted at node t ; Definition at line 160 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [1/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const DataSet * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using the DataSet ; Definition at line 203 of file CCTreeWrapper.cxx. ◆ TestTreeQuality() [2/2]. Double_t TMVA::CCTreeWrapper::TestTreeQuality ; (; const EventList * ; validationSample). return the misclassification rate of a pruned tree for a validation event sample using an EventList ; Definition at line 183 of file CCTreeWrapper.cxx. Member Data Documentation. ◆ fDTParent. DecisionTree* TMVA::CCTreeWrapper::fDTParent. private . ! pointer to underlying DecisionTree ; Definition at line 151 of file CCTreeWrapper.h. ◆ fQualityIndex. SeparationBase* TMVA::CCTreeWrapper::fQualityIndex. private . ! pointer to the used quality index calculator ; Definition at line 150 of file CCTreeWrapper.h. ◆ fRoot. CCTreeNode* TMVA::CCTreeWrapper::fRoot. private . ! the root node of the (wrapped) decision Tree ; Definition at line 152 of file CCTreeWrapper.h. Libraries for TMVA::CCTreeWrapper:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/CCTreeWrapper.h; tmva/tmva/src/CCTreeWrapper.cxx. TMVACCTreeWrapper. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1CCTreeWrapper.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CCTreeWrapper.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:351,Modifiability,config,configure,351,". ROOT: Classification Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:365,Modifiability,variab,variables,365,". ROOT: Classification Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:4822,Modifiability,variab,variable,4822," gSystemDefinition TSystem.h:561; TCanvasThe Canvas class.Definition TCanvas.h:23; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::Experimental::Classification::GetResultsstd::vector< ClassificationResult > & GetResults()Return the vector of TMVA::Experimental::ClassificationResult objects.Definition Classification.cxx:945; TMVA::Experimental::Classification::Evaluatevirtual void Evaluate()Method to perform Train/Test over all ml method booked.Definition Classification.cxx:248; TMVA::Tools::Instancestatic Tools & Instance(",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:5052,Modifiability,variab,variable,5052,"Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::Experimental::Classification::GetResultsstd::vector< ClassificationResult > & GetResults()Return the vector of TMVA::Experimental::ClassificationResult objects.Definition Classification.cxx:945; TMVA::Experimental::Classification::Evaluatevirtual void Evaluate()Method to perform Train/Test over all ml method booked.Definition Classification.cxx:248; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.D",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:168,Performance,perform,perform,168,". ROOT: Classification Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:4298,Performance,cache,cacheDir,4298,"x:101; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; Double_tdouble Double_tDefinition RtypesCore.h:59; inputOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void inputDefinition TGWin32VirtualXProxy.cxx:142; Formchar * Form(const char *fmt,...)Formats a string in a circular formatting buffer.Definition TString.cxx:2489; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCanvasThe Canvas class.Definition TCanvas.h:23; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Exper",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:4413,Performance,cache,cache,4413,"x:101; UInt_tunsigned int UInt_tDefinition RtypesCore.h:46; Double_tdouble Double_tDefinition RtypesCore.h:59; inputOption_t Option_t TPoint TPoint const char GetTextMagnitude GetFillStyle GetLineColor GetLineWidth GetMarkerStyle GetTextAlign GetTextColor GetTextSize void inputDefinition TGWin32VirtualXProxy.cxx:142; Formchar * Form(const char *fmt,...)Formats a string in a circular formatting buffer.Definition TString.cxx:2489; gSystemR__EXTERN TSystem * gSystemDefinition TSystem.h:561; TCanvasThe Canvas class.Definition TCanvas.h:23; TFileA ROOT file is an on-disk file, usually with extension .root, that stores objects in a file-system-li...Definition TFile.h:53; TFile::Openstatic TFile * Open(const char *name, Option_t *option="""", const char *ftitle="""", Int_t compress=ROOT::RCompressionSetting::EDefaults::kUseCompiledDefault, Int_t netopt=0)Create / open a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Exper",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:5264,Performance,perform,perform,5264,"en a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::Experimental::Classification::GetResultsstd::vector< ClassificationResult > & GetResults()Return the vector of TMVA::Experimental::ClassificationResult objects.Definition Classification.cxx:945; TMVA::Experimental::Classification::Evaluatevirtual void Evaluate()Method to perform Train/Test over all ml method booked.Definition Classification.cxx:248; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a fil",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:5660,Performance,perform,perform,5660,"=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::Experimental::Classification::GetResultsstd::vector< ClassificationResult > & GetResults()Return the vector of TMVA::Experimental::ClassificationResult objects.Definition Classification.cxx:945; TMVA::Experimental::Classification::Evaluatevirtual void Evaluate()Method to perform Train/Test over all ml method booked.Definition Classification.cxx:248; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; classificationvoid classification(UInt_t jobs=4)Definition classification.C:14; unsigned int; ; #include <TMVA/Classification.h>; The documentation for this class was generated from the following file:; tmva/tmva/inc/TMVA/Classification.h. Classification. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:6181,Security,access,access,6181,"ata set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::Experimental::Classification::GetResultsstd::vector< ClassificationResult > & GetResults()Return the vector of TMVA::Experimental::ClassificationResult objects.Definition Classification.cxx:945; TMVA::Experimental::Classification::Evaluatevirtual void Evaluate()Method to perform Train/Test over all ml method booked.Definition Classification.cxx:248; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; classificationvoid classification(UInt_t jobs=4)Definition classification.C:14; unsigned int; ; #include <TMVA/Classification.h>; The documentation for this class was generated from the following file:; tmva/tmva/inc/TMVA/Classification.h. Classification. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:35 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:6215,Security,access,access,6215,"ata set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::Experimental::Classification::GetResultsstd::vector< ClassificationResult > & GetResults()Return the vector of TMVA::Experimental::ClassificationResult objects.Definition Classification.cxx:945; TMVA::Experimental::Classification::Evaluatevirtual void Evaluate()Method to perform Train/Test over all ml method booked.Definition Classification.cxx:248; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a file using the specified access mode.Definition TSystem.cxx:1296; TTreeA TTree represents a columnar dataset.Definition TTree.h:79; classificationvoid classification(UInt_t jobs=4)Definition classification.C:14; unsigned int; ; #include <TMVA/Classification.h>; The documentation for this class was generated from the following file:; tmva/tmva/inc/TMVA/Classification.h. Classification. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:41:35 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:409,Testability,test,test,409,". ROOT: Classification Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:835,Testability,test,test,835,". ROOT: Classification Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:901,Testability,test,testing,901,". ROOT: Classification Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:1014,Testability,test,test,1014,"ence. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4"", ""Variable 4"", ""units"", 'F');; ;",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:1597,Testability,test,test,1597,"s bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4"", ""Variable 4"", ""units"", 'F');; ; dataloader->AddSpectator(""spec1 := var1*2"", ""Spectator 1"", ""units"", 'F');; dataloader->AddSpectator(""spec2 := var1*3"", ""Spectator 2"", ""units"", 'F');; ; // global event weights per tree (see below for setting event-wise weights); Double_t signalWeight = 1.0;; Double_t backgroundWeight = 1.0;; ; dataloader->SetBackgroundWeightExpression(""weight"");; ; TMVA::Experimental::Classification *cl = new TMVA::Experimental::Classification(dataloader, Form(""Jobs=%d"", jobs));; ; cl->BookMethod(TMVA::Types::kBDT, ""BDTG"", ""!H:!V:NTrees=2000:MinNodeSize=2.5%:BoostType=Grad:Shrinkage=0.10:""; ""UseBaggedBoost:BaggedSampleFraction=0.5:nCuts=20:MaxDepth=2"");; cl->BookMethod(TMVA::Types::kSVM, ""SVM"", ""Gamma=0.25:Tol=0.001:VarTransform=Norm"");; ; cl->Evaluate(); // Train and Test all methods; ; auto &results = cl->GetResults();; ; TCanvas *c = new",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:864,Usability,progress bar,progress bar,864,". ROOT: Classification Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. Classification Class ReferenceTMVA. ; Class to perform two class classification. ; The first step before any analysis is to prepare the data, to do that you need to create an object of TMVA::DataLoader, in this object you need to configure the variables and the number of events to train/test. The class TMVA::Experimental::Classification needs a TMVA::DataLoader object, optional a TFile object to save the results and some extra options in a string like ""V:Color:Transformations=I;D;P;U;G:Silent:DrawProgressBar:ModelPersistence:Jobs=2"" where: V = verbose output Color = coloured screen output Silent = batch mode: boolean silent flag inhibiting any output from TMVA Transformations = list of transformations to test. DrawProgressBar = draw progress bar to display training and testing. ModelPersistence = to save the trained model in xml or serialized files. Jobs = number of ml methods to test/train in parallel using MultiProc, requires to call Evaluate method. Basic example. void classification(UInt_t jobs = 2); {; TMVA::Tools::Instance();; ; TFile *input(0);; TString fname = ""./tmva_class_example.root"";; if (!gSystem->AccessPathName(fname)) {; input = TFile::Open(fname); // check if file in local directory exists; } else {; TFile::SetCacheFileDir(""."");; input = TFile::Open(""http://root.cern/files/tmva_class_example.root"", ""CACHEREAD"");; }; if (!input) {; std::cout << ""ERROR: could not open data file"" << std::endl;; exit(1);; }; ; // Register the training and test trees; ; TTree *signalTree = (TTree *)input->Get(""TreeS"");; TTree *background = (TTree *)input->Get(""TreeB"");; ; TMVA::DataLoader *dataloader = new TMVA::DataLoader(""dataset"");; ; dataloader->AddVariable(""myvar1 := var1+var2"", 'F');; dataloader->AddVariable(""myvar2 := var1-var2"", ""Expression 2"", """", 'F');; dataloader->AddVariable(""var3"", ""Variable 3"", ""units"", 'F');; dataloader->AddVariable(""var4",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1Classification.html:5245,Usability,learn,learning,5245,"en a file.Definition TFile.cxx:4089; TFile::SetCacheFileDirstatic Bool_t SetCacheFileDir(std::string_view cacheDir, Bool_t operateDisconnected=kTRUE, Bool_t forceCacheread=kFALSE)Sets the directory where to locally stage/cache remote files.Definition TFile.cxx:4626; TMVA::DataLoaderDefinition DataLoader.h:50; TMVA::DataLoader::AddSpectatorvoid AddSpectator(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)user inserts target in data set infoDefinition DataLoader.cxx:524; TMVA::DataLoader::SetBackgroundWeightExpressionvoid SetBackgroundWeightExpression(const TString &variable)Definition DataLoader.cxx:556; TMVA::DataLoader::AddVariablevoid AddVariable(const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0)user inserts discriminating variable in data set infoDefinition DataLoader.cxx:485; TMVA::Envelope::BookMethodvirtual void BookMethod(TString methodname, TString methodtitle, TString options="""")Method to book the machine learning method to perform the algorithm.Definition Envelope.cxx:163; TMVA::Experimental::ClassificationDefinition Classification.h:162; TMVA::Experimental::Classification::GetResultsstd::vector< ClassificationResult > & GetResults()Return the vector of TMVA::Experimental::ClassificationResult objects.Definition Classification.cxx:945; TMVA::Experimental::Classification::Evaluatevirtual void Evaluate()Method to perform Train/Test over all ml method booked.Definition Classification.cxx:248; TMVA::Tools::Instancestatic Tools & Instance()Definition Tools.cxx:71; TMVA::Types::kBDT@ kBDTDefinition Types.h:86; TMVA::Types::kSVM@ kSVMDefinition Types.h:89; TMultiGraphA TMultiGraph is a collection of TGraph (or derived) objects.Definition TMultiGraph.h:34; TStringBasic string class.Definition TString.h:139; TSystem::AccessPathNamevirtual Bool_t AccessPathName(const char *path, EAccessMode mode=kFileExists)Returns FALSE if one can access a fil",MatchSource.WIKI,doc/master/classTMVA_1_1Classification.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Classification.html
https://root.cern/doc/master/classTMVA_1_1ClassifierFactory.html:1614,Security,access,access,1614,"t TString &option);  . Public Member Functions; IMethod * Create (const std::string &name, const TString &job, const TString &title, DataSetInfo &dsi, const TString &option);  creates the method if needed based on the method name using the creator function the factory has stored ;  ; IMethod * Create (const std::string &name, DataSetInfo &dsi, const TString &weightfile="""");  creates the method if needed based on the method name using the creator function the factory has stored ;  ; const std::vector< std::string > List () const;  returns a vector of the method type names of registered methods ;  ; void Print () const;  prints the registered method type names ;  ; Bool_t Register (const std::string &name, Creator creator);  registers a classifier creator function under the method type name ;  ; Bool_t Unregister (const std::string &name);  unregisters a classifier type name ;  . Static Public Member Functions; static void DestroyInstance ();  destroy the singleton instance ;  ; static ClassifierFactory & Instance ();  access to the ClassifierFactory singleton creates the instance if needed ;  . Private Types; typedef std::map< std::string, Creator > CallMap;  . Private Member Functions;  ClassifierFactory ();  ;  ClassifierFactory (const ClassifierFactory &);  ;  ~ClassifierFactory ();  ; const ClassifierFactory & operator= (const ClassifierFactory &);  . Private Attributes; CallMap fCalls;  . Static Private Attributes; static ClassifierFactory * fgInstance = 0;  Initialize static singleton pointer. ;  . #include <TMVA/ClassifierFactory.h>; Member Typedef Documentation. ◆ CallMap. typedef std::map<std::string, Creator> TMVA::ClassifierFactory::CallMap. private . Definition at line 98 of file ClassifierFactory.h. ◆ Creator. typedef IMethod *(* TMVA::ClassifierFactory::Creator) (const TString &job, const TString &title, DataSetInfo &dsi, const TString &option). Definition at line 60 of file ClassifierFactory.h. Constructor & Destructor Documentation. ◆ ClassifierFactor",MatchSource.WIKI,doc/master/classTMVA_1_1ClassifierFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassifierFactory.html
https://root.cern/doc/master/classTMVA_1_1ClassifierFactory.html:3951,Security,access,access,3951," [1/2]. TMVA::IMethod * TMVA::ClassifierFactory::Create ; (; const std::string & ; name, . const TString & ; job, . const TString & ; title, . DataSetInfo & ; dsi, . const TString & ; option . ). creates the method if needed based on the method name using the creator function the factory has stored ; Definition at line 89 of file ClassifierFactory.cxx. ◆ Create() [2/2]. TMVA::IMethod * TMVA::ClassifierFactory::Create ; (; const std::string & ; name, . DataSetInfo & ; dsi, . const TString & ; weightfile = """" . ). creates the method if needed based on the method name using the creator function the factory has stored ; Definition at line 114 of file ClassifierFactory.cxx. ◆ DestroyInstance(). void TMVA::ClassifierFactory::DestroyInstance ; (; ). static . destroy the singleton instance ; Definition at line 58 of file ClassifierFactory.cxx. ◆ Instance(). TMVA::ClassifierFactory & TMVA::ClassifierFactory::Instance ; (; ). static . access to the ClassifierFactory singleton creates the instance if needed ; Definition at line 48 of file ClassifierFactory.cxx. ◆ List(). const std::vector< std::string > TMVA::ClassifierFactory::List ; (; ); const. returns a vector of the method type names of registered methods ; Definition at line 136 of file ClassifierFactory.cxx. ◆ operator=(). const ClassifierFactory & TMVA::ClassifierFactory::operator= ; (; const ClassifierFactory & ; ). private . ◆ Print(). void TMVA::ClassifierFactory::Print ; (; ); const. prints the registered method type names ; Definition at line 149 of file ClassifierFactory.cxx. ◆ Register(). Bool_t TMVA::ClassifierFactory::Register ; (; const std::string & ; name, . Creator ; creator . ). registers a classifier creator function under the method type name ; Definition at line 66 of file ClassifierFactory.cxx. ◆ Unregister(). Bool_t TMVA::ClassifierFactory::Unregister ; (; const std::string & ; name). unregisters a classifier type name ; Definition at line 80 of file ClassifierFactory.cxx. Member Data Documentation. ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassifierFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassifierFactory.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:4250,Availability,error,error,4250,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:4339,Availability,error,error,4339,"ecord whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:4494,Availability,error,error,4494,"Name () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:4791,Availability,error,error,4791,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:8875,Availability,error,error,8875,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:4256,Integrability,message,message,4256,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:4797,Integrability,message,message,4797,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:5787,Integrability,message,message,5787,"ssue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class shou",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:8881,Integrability,message,message,8881,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:9254,Integrability,message,message,9254," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  G",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:10663,Integrability,message,message,10663," DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; MsgLogger & Log () const;  . Private Attributes; TMatrixD * fCorrMatrix;  Correlation matrix for this class. ;  ; TCut fCut;  pre-training cut for the class ;  ; MsgLogger * fLogger;  ! message logger ;  ; UInt_t fNumber;  index in of this class in vectors ;  ; TString fWeight;  the input formula string that is the weight for the class ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:14961,Integrability,message,message,14961,nition at line 80 of file ClassInfo.h. ◆ Log(). MsgLogger & TMVA::ClassInfo::Log ; (; ); const. inlineprivate . Definition at line 77 of file ClassInfo.h. ◆ SetCorrelationMatrix(). void TMVA::ClassInfo::SetCorrelationMatrix ; (; TMatrixD * ; matrix). inline . Definition at line 60 of file ClassInfo.h. ◆ SetCut(). void TMVA::ClassInfo::SetCut ; (; const TCut & ; cut). inline . Definition at line 58 of file ClassInfo.h. ◆ SetNumber(). void TMVA::ClassInfo::SetNumber ; (; const UInt_t ; index). inline . Definition at line 59 of file ClassInfo.h. ◆ SetWeight(). void TMVA::ClassInfo::SetWeight ; (; const TString & ; weight). inline . Definition at line 57 of file ClassInfo.h. ◆ Streamer(). virtual void TMVA::ClassInfo::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::ClassInfo::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 80 of file ClassInfo.h. Member Data Documentation. ◆ fCorrMatrix. TMatrixD* TMVA::ClassInfo::fCorrMatrix. private . Correlation matrix for this class. ; Definition at line 74 of file ClassInfo.h. ◆ fCut. TCut TMVA::ClassInfo::fCut. private . pre-training cut for the class ; Definition at line 71 of file ClassInfo.h. ◆ fLogger. MsgLogger* TMVA::ClassInfo::fLogger. mutableprivate . ! message logger ; Definition at line 76 of file ClassInfo.h. ◆ fNumber. UInt_t TMVA::ClassInfo::fNumber. private . index in of this class in vectors ; Definition at line 72 of file ClassInfo.h. ◆ fWeight. TString TMVA::ClassInfo::fWeight. private . the input formula string that is the weight for the class ; Definition at line 70 of file ClassInfo.h. Libraries for TMVA::ClassInfo:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/ClassInfo.h; tmva/tmva/src/ClassInfo.cxx. TMVAClassInfo. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:1053,Modifiability,inherit,inherited,1053,"ide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::ClassInfo Class ReferenceTMVA. ; Class that contains all the information of a class. ; Definition at line 49 of file ClassInfo.h. Public Member Functions;  ClassInfo (const TString &name=""default"");  constructor ;  ;  ~ClassInfo ();  destructor ;  ; const TMatrixD * GetCorrelationMatrix () const;  ; const TCut & GetCut () const;  ; UInt_t GetNumber () const;  ; const TString & GetWeight () const;  ; virtual TClass * IsA () const;  ; void SetCorrelationMatrix (TMatrixD *matrix);  ; void SetCut (const TCut &cut);  ; void SetNumber (const UInt_t index);  ; void SetWeight (const TString &weight);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:2793,Modifiability,inherit,inherited,2793,"ide;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:3886,Modifiability,inherit,inheritance,3886,"  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:5885,Modifiability,inherit,inherits,5885,"e) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:6002,Modifiability,inherit,inherits,6002,"t be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is necessary). ;  ; virtual Bool_t Notify ();  This method must be overridden to handle object notification (the base ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:9755,Modifiability,inherit,inherited,9755,"stem error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; MsgLogger & Log () const;  . Private Attributes; TMatrixD * fCorrMatrix;  Correlation matrix for this class. ;  ; TCut fCut;  pre-training cut for the class ;  ; MsgLogger * fLogger;  ! message logger ;  ; UInt_t fNumber;  index in of this class in vectors ;  ; TString fWeight;  the input formula string that is the weight for the class ;  . Additional Inherited Members;  Public Types inherit",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:9965,Modifiability,inherit,inherited,9965,"stem error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; MsgLogger & Log () const;  . Private Attributes; TMatrixD * fCorrMatrix;  Correlation matrix for this class. ;  ; TCut fCut;  pre-training cut for the class ;  ; MsgLogger * fLogger;  ! message logger ;  ; UInt_t fNumber;  index in of this class in vectors ;  ; TString fWeight;  the input formula string that is the weight for the class ;  . Additional Inherited Members;  Public Types inherit",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:10864,Modifiability,inherit,inherited,10864,"();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; MsgLogger & Log () const;  . Private Attributes; TMatrixD * fCorrMatrix;  Correlation matrix for this class. ;  ; TCut fCut;  pre-training cut for the class ;  ; MsgLogger * fLogger;  ! message logger ;  ; UInt_t fNumber;  index in of this class in vectors ;  ; TString fWeight;  the input formula string that is the weight for the class ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/ClassInfo.h>. Inheritance diagram for TMVA:",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:11505,Modifiability,inherit,inherited,11505," for the class ;  ; MsgLogger * fLogger;  ! message logger ;  ; UInt_t fNumber;  index in of this class in vectors ;  ; TString fWeight;  the input formula string that is the weight for the class ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/ClassInfo.h>. Inheritance diagram for TMVA::ClassInfo:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ ClassInfo(). TMVA::ClassInfo::ClassInfo ; (; const TString & ; name = ""default""). constructor ; Definition at line 48 of file ClassInfo.cxx. ◆ ~ClassInfo(). TMVA::ClassInfo::~ClassInfo ; (; ). destructor ; Definition at line 61 of file ClassInfo.cxx. Member Function Documentation. ◆ Class(). static TClass * TMVA::ClassInfo::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::ClassInfo::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:11604,Modifiability,inherit,inherited,11604," for the class ;  ; MsgLogger * fLogger;  ! message logger ;  ; UInt_t fNumber;  index in of this class in vectors ;  ; TString fWeight;  the input formula string that is the weight for the class ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/ClassInfo.h>. Inheritance diagram for TMVA::ClassInfo:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ ClassInfo(). TMVA::ClassInfo::ClassInfo ; (; const TString & ; name = ""default""). constructor ; Definition at line 48 of file ClassInfo.cxx. ◆ ~ClassInfo(). TMVA::ClassInfo::~ClassInfo ; (; ). destructor ; Definition at line 61 of file ClassInfo.cxx. Member Function Documentation. ◆ Class(). static TClass * TMVA::ClassInfo::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::ClassInfo::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t ",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:11809,Modifiability,inherit,inherited,11809,"s ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/ClassInfo.h>. Inheritance diagram for TMVA::ClassInfo:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ ClassInfo(). TMVA::ClassInfo::ClassInfo ; (; const TString & ; name = ""default""). constructor ; Definition at line 48 of file ClassInfo.cxx. ◆ ~ClassInfo(). TMVA::ClassInfo::~ClassInfo ; (; ). destructor ; Definition at line 61 of file ClassInfo.cxx. Member Function Documentation. ◆ Class(). static TClass * TMVA::ClassInfo::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::ClassInfo::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::ClassInfo::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 80 of file ClassInfo.h. ◆ DeclFileName(). static const char * TMVA::ClassInfo::D",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:1921,Security,hash,hash,1921,"Buffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:10671,Testability,log,logger,10671," DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; MsgLogger & Log () const;  . Private Attributes; TMatrixD * fCorrMatrix;  Correlation matrix for this class. ;  ; TCut fCut;  pre-training cut for the class ;  ; MsgLogger * fLogger;  ! message logger ;  ; UInt_t fNumber;  index in of this class in vectors ;  ; TString fWeight;  the input formula string that is the weight for the class ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va",MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1ClassInfo.html:14969,Testability,log,logger,14969,nition at line 80 of file ClassInfo.h. ◆ Log(). MsgLogger & TMVA::ClassInfo::Log ; (; ); const. inlineprivate . Definition at line 77 of file ClassInfo.h. ◆ SetCorrelationMatrix(). void TMVA::ClassInfo::SetCorrelationMatrix ; (; TMatrixD * ; matrix). inline . Definition at line 60 of file ClassInfo.h. ◆ SetCut(). void TMVA::ClassInfo::SetCut ; (; const TCut & ; cut). inline . Definition at line 58 of file ClassInfo.h. ◆ SetNumber(). void TMVA::ClassInfo::SetNumber ; (; const UInt_t ; index). inline . Definition at line 59 of file ClassInfo.h. ◆ SetWeight(). void TMVA::ClassInfo::SetWeight ; (; const TString & ; weight). inline . Definition at line 57 of file ClassInfo.h. ◆ Streamer(). virtual void TMVA::ClassInfo::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::ClassInfo::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 80 of file ClassInfo.h. Member Data Documentation. ◆ fCorrMatrix. TMatrixD* TMVA::ClassInfo::fCorrMatrix. private . Correlation matrix for this class. ; Definition at line 74 of file ClassInfo.h. ◆ fCut. TCut TMVA::ClassInfo::fCut. private . pre-training cut for the class ; Definition at line 71 of file ClassInfo.h. ◆ fLogger. MsgLogger* TMVA::ClassInfo::fLogger. mutableprivate . ! message logger ; Definition at line 76 of file ClassInfo.h. ◆ fNumber. UInt_t TMVA::ClassInfo::fNumber. private . index in of this class in vectors ; Definition at line 72 of file ClassInfo.h. ◆ fWeight. TString TMVA::ClassInfo::fWeight. private . the input formula string that is the weight for the class ; Definition at line 70 of file ClassInfo.h. Libraries for TMVA::ClassInfo:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/ClassInfo.h; tmva/tmva/src/ClassInfo.cxx. TMVAClassInfo. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1ClassInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ClassInfo.html
https://root.cern/doc/master/classTMVA_1_1Config.html:423,Deployability,configurat,configuration,423,. ROOT: TMVA::Config Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Static Public Member Functions |; Public Attributes |; Protected Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::Config Class ReferenceTMVA. ; Singleton class for global configuration settings used by TMVA. ; Definition at line 49 of file Config.h. Classes; class  IONames;  ; class  VariablePlotting;  . Public Member Functions; void DisableMT ();  Force disabling MT running and release the thread pool by using instead seriaql execution. ;  ; Bool_t DrawProgressBar () const;  ; void EnableMT (int numthreads=0);  Enable MT in TMVA (by default is on when ROOT::EnableImplicitMT() is set. ;  ; IONames & GetIONames ();  ; ROOT::TThreadExecutor & GetMultiThreadExecutor ();  ; UInt_t GetNCpu ();  ; UInt_t GetNumWorkers () const;  ; Executor & GetThreadExecutor ();  Get executor class for multi-thread usage In case when MT is not enabled will return a serial executor. ;  ; VariablePlotting & GetVariablePlotting ();  ; virtual TClass * IsA () const;  ; Bool_t IsMTEnabled () const;  Check if IMT is enabled. ;  ; Bool_t IsSilent () const;  ; void SetDrawProgressBar (Bool_t d);  ; void SetNumWorkers (UInt_t n);  ; void SetSilent (Bool_t s);  ; void SetUseColor (Bool_t uc);  ; void SetWriteOptionsReference (Bool_t w);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UseColor () const;  ; Bool_t WriteOptionsReference () const;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static void DestroyInstance ();  static function: destroy TMVA instance ;  ; static Config & Instance ();  static function: returns TMVA instance ;  . Public At,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:634,Deployability,release,release,634,. ROOT: TMVA::Config Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Static Public Member Functions |; Public Attributes |; Protected Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::Config Class ReferenceTMVA. ; Singleton class for global configuration settings used by TMVA. ; Definition at line 49 of file Config.h. Classes; class  IONames;  ; class  VariablePlotting;  . Public Member Functions; void DisableMT ();  Force disabling MT running and release the thread pool by using instead seriaql execution. ;  ; Bool_t DrawProgressBar () const;  ; void EnableMT (int numthreads=0);  Enable MT in TMVA (by default is on when ROOT::EnableImplicitMT() is set. ;  ; IONames & GetIONames ();  ; ROOT::TThreadExecutor & GetMultiThreadExecutor ();  ; UInt_t GetNCpu ();  ; UInt_t GetNumWorkers () const;  ; Executor & GetThreadExecutor ();  Get executor class for multi-thread usage In case when MT is not enabled will return a serial executor. ;  ; VariablePlotting & GetVariablePlotting ();  ; virtual TClass * IsA () const;  ; Bool_t IsMTEnabled () const;  Check if IMT is enabled. ;  ; Bool_t IsSilent () const;  ; void SetDrawProgressBar (Bool_t d);  ; void SetNumWorkers (UInt_t n);  ; void SetSilent (Bool_t s);  ; void SetUseColor (Bool_t uc);  ; void SetWriteOptionsReference (Bool_t w);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UseColor () const;  ; Bool_t WriteOptionsReference () const;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static void DestroyInstance ();  static function: destroy TMVA instance ;  ; static Config & Instance ();  static function: returns TMVA instance ;  . Public At,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:4172,Deployability,release,release,4172,g & ; ). private . ◆ ~Config(). TMVA::Config::~Config ; (; ). privatevirtual . destructor ; Definition at line 82 of file Config.cxx. Member Function Documentation. ◆ Class(). static TClass * TMVA::Config::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::Config::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::Config::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 148 of file Config.h. ◆ DeclFileName(). static const char * TMVA::Config::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 148 of file Config.h. ◆ DestroyInstance(). void TMVA::Config::DestroyInstance ; (; ). static . static function: destroy TMVA instance ; Definition at line 90 of file Config.cxx. ◆ DisableMT(). void TMVA::Config::DisableMT ; (; ). inline . Force disabling MT running and release the thread pool by using instead seriaql execution. ; Definition at line 87 of file Config.h. ◆ DrawProgressBar(). Bool_t TMVA::Config::DrawProgressBar ; (; ); const. inline . Definition at line 68 of file Config.h. ◆ EnableMT(). void TMVA::Config::EnableMT ; (; int ; numthreads = 0). inline . Enable MT in TMVA (by default is on when ROOT::EnableImplicitMT() is set. ; Definition at line 84 of file Config.h. ◆ GetIONames(). IONames & TMVA::Config::GetIONames ; (; ). inline . Definition at line 98 of file Config.h. ◆ GetMultiThreadExecutor(). ROOT::TThreadExecutor & TMVA::Config::GetMultiThreadExecutor ; (; ). inline . Definition at line 76 of file Config.h. ◆ GetNCpu(). UInt_t TMVA::Config::GetNCpu ; (; ). inline . Definition at line 70 of file Config.h. ◆ GetNumWorkers(). UInt_t TMVA::Config::GetNumWorkers ; (; ); const. inline . Definition at line 72 of file Config.h. ◆ GetThreadExecutor(). Executor & TMVA::Config::GetThreadExecutor ; (; ). inline . Get executor class for multi-thread usage,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:2501,Integrability,message,message,2501,rtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UseColor () const;  ; Bool_t WriteOptionsReference () const;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static void DestroyInstance ();  static function: destroy TMVA instance ;  ; static Config & Instance ();  static function: returns TMVA instance ;  . Public Attributes; class TMVA::Config::IONames fIONames;  ; class TMVA::Config::VariablePlotting fVariablePlotting;  . Protected Attributes; Executor fExecutor;  . Private Member Functions;  Config ();  constructor - set defaults ;  ;  Config (const Config &);  ; virtual ~Config ();  destructor ;  ; MsgLogger & Log () const;  ; Config & operator= (const Config &);  . Private Attributes; std::atomic< Bool_t > fDrawProgressBar;  draw progress bar to indicate training evolution ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::atomic< UInt_t > fNWorkers;  Default number of workers for multi-process jobs. ;  ; std::atomic< Bool_t > fSilent;  no output at all ;  ; std::atomic< Bool_t > fUseColoredConsole;  coloured standard output ;  ; std::atomic< Bool_t > fWriteOptionsReference;  if set true: Configurable objects write file with option reference ;  . Static Private Attributes; static std::atomic< Config * > fgConfigPtr { 0 };  . #include <TMVA/Config.h>; Constructor & Destructor Documentation. ◆ Config() [1/2]. TMVA::Config::Config ; (; ). private . constructor - set defaults ; Definition at line 51 of file Config.cxx. ◆ Config() [2/2]. TMVA::Config::Config ; (; const Config & ; ). private . ◆ ~Config(). TMVA::Config::~Config ; (; ). privatevirtual . destructor ; Definition at line 82 of file Config.cxx. Member Function Documentation. ◆ Class(). static TClass * TMVA::Config::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const ch,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:7835,Integrability,message,message,7835, ). virtual . ◆ StreamerNVirtual(). void TMVA::Config::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 148 of file Config.h. ◆ UseColor(). Bool_t TMVA::Config::UseColor ; (; ); const. inline . Definition at line 59 of file Config.h. ◆ WriteOptionsReference(). Bool_t TMVA::Config::WriteOptionsReference ; (; ); const. inline . Definition at line 65 of file Config.h. Member Data Documentation. ◆ fDrawProgressBar. std::atomic<Bool_t> TMVA::Config::fDrawProgressBar. private . draw progress bar to indicate training evolution ; Definition at line 140 of file Config.h. ◆ fExecutor. Executor TMVA::Config::fExecutor. protected . Definition at line 52 of file Config.h. ◆ fgConfigPtr. std::atomic< TMVA::Config * > TMVA::Config::fgConfigPtr { 0 }. staticprivate . Definition at line 137 of file Config.h. ◆ fIONames. class TMVA::Config::IONames TMVA::Config::fIONames. ◆ fLogger. MsgLogger* TMVA::Config::fLogger. mutableprivate . ! message logger ; Definition at line 145 of file Config.h. ◆ fNWorkers. std::atomic<UInt_t> TMVA::Config::fNWorkers. private . Default number of workers for multi-process jobs. ; Definition at line 141 of file Config.h. ◆ fSilent. std::atomic<Bool_t> TMVA::Config::fSilent. private . no output at all ; Definition at line 143 of file Config.h. ◆ fUseColoredConsole. std::atomic<Bool_t> TMVA::Config::fUseColoredConsole. private . coloured standard output ; Definition at line 142 of file Config.h. ◆ fVariablePlotting. class TMVA::Config::VariablePlotting TMVA::Config::fVariablePlotting. ◆ fWriteOptionsReference. std::atomic<Bool_t> TMVA::Config::fWriteOptionsReference. private . if set true: Configurable objects write file with option reference ; Definition at line 144 of file Config.h. Libraries for TMVA::Config:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/Config.h; tmva/tmva/src/Config.cxx. TMVAConfig. ROOT master - Reference Guide Generated on Tue Nov 5 2,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:423,Modifiability,config,configuration,423,. ROOT: TMVA::Config Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Static Public Member Functions |; Public Attributes |; Protected Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::Config Class ReferenceTMVA. ; Singleton class for global configuration settings used by TMVA. ; Definition at line 49 of file Config.h. Classes; class  IONames;  ; class  VariablePlotting;  . Public Member Functions; void DisableMT ();  Force disabling MT running and release the thread pool by using instead seriaql execution. ;  ; Bool_t DrawProgressBar () const;  ; void EnableMT (int numthreads=0);  Enable MT in TMVA (by default is on when ROOT::EnableImplicitMT() is set. ;  ; IONames & GetIONames ();  ; ROOT::TThreadExecutor & GetMultiThreadExecutor ();  ; UInt_t GetNCpu ();  ; UInt_t GetNumWorkers () const;  ; Executor & GetThreadExecutor ();  Get executor class for multi-thread usage In case when MT is not enabled will return a serial executor. ;  ; VariablePlotting & GetVariablePlotting ();  ; virtual TClass * IsA () const;  ; Bool_t IsMTEnabled () const;  Check if IMT is enabled. ;  ; Bool_t IsSilent () const;  ; void SetDrawProgressBar (Bool_t d);  ; void SetNumWorkers (UInt_t n);  ; void SetSilent (Bool_t s);  ; void SetUseColor (Bool_t uc);  ; void SetWriteOptionsReference (Bool_t w);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UseColor () const;  ; Bool_t WriteOptionsReference () const;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static void DestroyInstance ();  static function: destroy TMVA instance ;  ; static Config & Instance ();  static function: returns TMVA instance ;  . Public At,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:1044,Performance,multi-thread,multi-thread,1044,. ROOT: TMVA::Config Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Static Public Member Functions |; Public Attributes |; Protected Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::Config Class ReferenceTMVA. ; Singleton class for global configuration settings used by TMVA. ; Definition at line 49 of file Config.h. Classes; class  IONames;  ; class  VariablePlotting;  . Public Member Functions; void DisableMT ();  Force disabling MT running and release the thread pool by using instead seriaql execution. ;  ; Bool_t DrawProgressBar () const;  ; void EnableMT (int numthreads=0);  Enable MT in TMVA (by default is on when ROOT::EnableImplicitMT() is set. ;  ; IONames & GetIONames ();  ; ROOT::TThreadExecutor & GetMultiThreadExecutor ();  ; UInt_t GetNCpu ();  ; UInt_t GetNumWorkers () const;  ; Executor & GetThreadExecutor ();  Get executor class for multi-thread usage In case when MT is not enabled will return a serial executor. ;  ; VariablePlotting & GetVariablePlotting ();  ; virtual TClass * IsA () const;  ; Bool_t IsMTEnabled () const;  Check if IMT is enabled. ;  ; Bool_t IsSilent () const;  ; void SetDrawProgressBar (Bool_t d);  ; void SetNumWorkers (UInt_t n);  ; void SetSilent (Bool_t s);  ; void SetUseColor (Bool_t uc);  ; void SetWriteOptionsReference (Bool_t w);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UseColor () const;  ; Bool_t WriteOptionsReference () const;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static void DestroyInstance ();  static function: destroy TMVA instance ;  ; static Config & Instance ();  static function: returns TMVA instance ;  . Public At,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:5168,Performance,multi-thread,multi-thread,5168,by using instead seriaql execution. ; Definition at line 87 of file Config.h. ◆ DrawProgressBar(). Bool_t TMVA::Config::DrawProgressBar ; (; ); const. inline . Definition at line 68 of file Config.h. ◆ EnableMT(). void TMVA::Config::EnableMT ; (; int ; numthreads = 0). inline . Enable MT in TMVA (by default is on when ROOT::EnableImplicitMT() is set. ; Definition at line 84 of file Config.h. ◆ GetIONames(). IONames & TMVA::Config::GetIONames ; (; ). inline . Definition at line 98 of file Config.h. ◆ GetMultiThreadExecutor(). ROOT::TThreadExecutor & TMVA::Config::GetMultiThreadExecutor ; (; ). inline . Definition at line 76 of file Config.h. ◆ GetNCpu(). UInt_t TMVA::Config::GetNCpu ; (; ). inline . Definition at line 70 of file Config.h. ◆ GetNumWorkers(). UInt_t TMVA::Config::GetNumWorkers ; (; ); const. inline . Definition at line 72 of file Config.h. ◆ GetThreadExecutor(). Executor & TMVA::Config::GetThreadExecutor ; (; ). inline . Get executor class for multi-thread usage In case when MT is not enabled will return a serial executor. ; Definition at line 81 of file Config.h. ◆ GetVariablePlotting(). VariablePlotting & TMVA::Config::GetVariablePlotting ; (; ). inline . Definition at line 97 of file Config.h. ◆ Instance(). TMVA::Config & TMVA::Config::Instance ; (; ). static . static function: returns TMVA instance ; Definition at line 98 of file Config.cxx. ◆ IsA(). virtual TClass * TMVA::Config::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Definition at line 148 of file Config.h. ◆ IsMTEnabled(). Bool_t TMVA::Config::IsMTEnabled ; (; ); const. inline . Check if IMT is enabled. ; Definition at line 90 of file Config.h. ◆ IsSilent(). Bool_t TMVA::Config::IsSilent ; (; ); const. inline . Definition at line 62 of file Config.h. ◆ Log(). MsgLogger & TMVA::Config::Log ; (; ); const. inlineprivate . Definition at line 146 of file Config.h. ◆ operator=(). Config & TMVA::Config::operator= ; (; const Config & ; ). private . ◆ SetDrawProgress,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:2509,Testability,log,logger,2509,rtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UseColor () const;  ; Bool_t WriteOptionsReference () const;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static void DestroyInstance ();  static function: destroy TMVA instance ;  ; static Config & Instance ();  static function: returns TMVA instance ;  . Public Attributes; class TMVA::Config::IONames fIONames;  ; class TMVA::Config::VariablePlotting fVariablePlotting;  . Protected Attributes; Executor fExecutor;  . Private Member Functions;  Config ();  constructor - set defaults ;  ;  Config (const Config &);  ; virtual ~Config ();  destructor ;  ; MsgLogger & Log () const;  ; Config & operator= (const Config &);  . Private Attributes; std::atomic< Bool_t > fDrawProgressBar;  draw progress bar to indicate training evolution ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::atomic< UInt_t > fNWorkers;  Default number of workers for multi-process jobs. ;  ; std::atomic< Bool_t > fSilent;  no output at all ;  ; std::atomic< Bool_t > fUseColoredConsole;  coloured standard output ;  ; std::atomic< Bool_t > fWriteOptionsReference;  if set true: Configurable objects write file with option reference ;  . Static Private Attributes; static std::atomic< Config * > fgConfigPtr { 0 };  . #include <TMVA/Config.h>; Constructor & Destructor Documentation. ◆ Config() [1/2]. TMVA::Config::Config ; (; ). private . constructor - set defaults ; Definition at line 51 of file Config.cxx. ◆ Config() [2/2]. TMVA::Config::Config ; (; const Config & ; ). private . ◆ ~Config(). TMVA::Config::~Config ; (; ). privatevirtual . destructor ; Definition at line 82 of file Config.cxx. Member Function Documentation. ◆ Class(). static TClass * TMVA::Config::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const ch,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:7843,Testability,log,logger,7843, ). virtual . ◆ StreamerNVirtual(). void TMVA::Config::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 148 of file Config.h. ◆ UseColor(). Bool_t TMVA::Config::UseColor ; (; ); const. inline . Definition at line 59 of file Config.h. ◆ WriteOptionsReference(). Bool_t TMVA::Config::WriteOptionsReference ; (; ); const. inline . Definition at line 65 of file Config.h. Member Data Documentation. ◆ fDrawProgressBar. std::atomic<Bool_t> TMVA::Config::fDrawProgressBar. private . draw progress bar to indicate training evolution ; Definition at line 140 of file Config.h. ◆ fExecutor. Executor TMVA::Config::fExecutor. protected . Definition at line 52 of file Config.h. ◆ fgConfigPtr. std::atomic< TMVA::Config * > TMVA::Config::fgConfigPtr { 0 }. staticprivate . Definition at line 137 of file Config.h. ◆ fIONames. class TMVA::Config::IONames TMVA::Config::fIONames. ◆ fLogger. MsgLogger* TMVA::Config::fLogger. mutableprivate . ! message logger ; Definition at line 145 of file Config.h. ◆ fNWorkers. std::atomic<UInt_t> TMVA::Config::fNWorkers. private . Default number of workers for multi-process jobs. ; Definition at line 141 of file Config.h. ◆ fSilent. std::atomic<Bool_t> TMVA::Config::fSilent. private . no output at all ; Definition at line 143 of file Config.h. ◆ fUseColoredConsole. std::atomic<Bool_t> TMVA::Config::fUseColoredConsole. private . coloured standard output ; Definition at line 142 of file Config.h. ◆ fVariablePlotting. class TMVA::Config::VariablePlotting TMVA::Config::fVariablePlotting. ◆ fWriteOptionsReference. std::atomic<Bool_t> TMVA::Config::fWriteOptionsReference. private . if set true: Configurable objects write file with option reference ; Definition at line 144 of file Config.h. Libraries for TMVA::Config:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/Config.h; tmva/tmva/src/Config.cxx. TMVAConfig. ROOT master - Reference Guide Generated on Tue Nov 5 2,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:2428,Usability,progress bar,progress bar,2428,rtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UseColor () const;  ; Bool_t WriteOptionsReference () const;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static void DestroyInstance ();  static function: destroy TMVA instance ;  ; static Config & Instance ();  static function: returns TMVA instance ;  . Public Attributes; class TMVA::Config::IONames fIONames;  ; class TMVA::Config::VariablePlotting fVariablePlotting;  . Protected Attributes; Executor fExecutor;  . Private Member Functions;  Config ();  constructor - set defaults ;  ;  Config (const Config &);  ; virtual ~Config ();  destructor ;  ; MsgLogger & Log () const;  ; Config & operator= (const Config &);  . Private Attributes; std::atomic< Bool_t > fDrawProgressBar;  draw progress bar to indicate training evolution ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::atomic< UInt_t > fNWorkers;  Default number of workers for multi-process jobs. ;  ; std::atomic< Bool_t > fSilent;  no output at all ;  ; std::atomic< Bool_t > fUseColoredConsole;  coloured standard output ;  ; std::atomic< Bool_t > fWriteOptionsReference;  if set true: Configurable objects write file with option reference ;  . Static Private Attributes; static std::atomic< Config * > fgConfigPtr { 0 };  . #include <TMVA/Config.h>; Constructor & Destructor Documentation. ◆ Config() [1/2]. TMVA::Config::Config ; (; ). private . constructor - set defaults ; Definition at line 51 of file Config.cxx. ◆ Config() [2/2]. TMVA::Config::Config ; (; const Config & ; ). private . ◆ ~Config(). TMVA::Config::~Config ; (; ). privatevirtual . destructor ; Definition at line 82 of file Config.cxx. Member Function Documentation. ◆ Class(). static TClass * TMVA::Config::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const ch,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Config.html:7386,Usability,progress bar,progress bar,7386,. ◆ SetSilent(). void TMVA::Config::SetSilent ; (; Bool_t ; s). inline . Definition at line 63 of file Config.h. ◆ SetUseColor(). void TMVA::Config::SetUseColor ; (; Bool_t ; uc). inline . Definition at line 60 of file Config.h. ◆ SetWriteOptionsReference(). void TMVA::Config::SetWriteOptionsReference ; (; Bool_t ; w). inline . Definition at line 66 of file Config.h. ◆ Streamer(). virtual void TMVA::Config::Streamer ; (; TBuffer & ; ). virtual . ◆ StreamerNVirtual(). void TMVA::Config::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 148 of file Config.h. ◆ UseColor(). Bool_t TMVA::Config::UseColor ; (; ); const. inline . Definition at line 59 of file Config.h. ◆ WriteOptionsReference(). Bool_t TMVA::Config::WriteOptionsReference ; (; ); const. inline . Definition at line 65 of file Config.h. Member Data Documentation. ◆ fDrawProgressBar. std::atomic<Bool_t> TMVA::Config::fDrawProgressBar. private . draw progress bar to indicate training evolution ; Definition at line 140 of file Config.h. ◆ fExecutor. Executor TMVA::Config::fExecutor. protected . Definition at line 52 of file Config.h. ◆ fgConfigPtr. std::atomic< TMVA::Config * > TMVA::Config::fgConfigPtr { 0 }. staticprivate . Definition at line 137 of file Config.h. ◆ fIONames. class TMVA::Config::IONames TMVA::Config::fIONames. ◆ fLogger. MsgLogger* TMVA::Config::fLogger. mutableprivate . ! message logger ; Definition at line 145 of file Config.h. ◆ fNWorkers. std::atomic<UInt_t> TMVA::Config::fNWorkers. private . Default number of workers for multi-process jobs. ; Definition at line 141 of file Config.h. ◆ fSilent. std::atomic<Bool_t> TMVA::Config::fSilent. private . no output at all ; Definition at line 143 of file Config.h. ◆ fUseColoredConsole. std::atomic<Bool_t> TMVA::Config::fUseColoredConsole. private . coloured standard output ; Definition at line 142 of file Config.h. ◆ fVariablePlotting. class TMVA::Config::VariablePlotting TMVA::Config::fVariablePlotting. ◆ ,MatchSource.WIKI,doc/master/classTMVA_1_1Config.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Config.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:5465,Availability,error,error,5465,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:5554,Availability,error,error,5554,"ecord whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:5709,Availability,error,error,5709,"Name () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:6006,Availability,error,error,6006,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:10090,Availability,error,error,10090,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:5471,Integrability,message,message,5471,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:6012,Integrability,message,message,6012,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:7002,Integrability,message,message,7002,"ssue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class shou",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:10096,Integrability,message,message,10096,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:10469,Integrability,message,message,10469," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  G",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:12295,Integrability,message,message,12295,"tic const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Private Member Functions; template<class T > ; void AssignOpt (const TString &name, T &valAssign) const;  ; void SplitOptions (const TString &theOpt, TList &loo) const;  splits the option string at ':' and fills the list 'loo' with the primitive strings ;  . Private Attributes; TString fConfigDescription;  description of this configurable ;  ; OptionBase * fLastDeclaredOption;  ! last declared option ;  ; TList fListOfOptions;  option list ;  ; Bool_t fLooseOptionCheckingEnabled;  checker for option string ;  ; TString fOptions;  options string ;  ; TString fReferenceFile;  reference file for options writing ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelet",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:22765,Integrability,message,message,22765,"olds. ◆ StreamerNVirtual(). void TMVA::Configurable::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 138 of file Configurable.h. ◆ WriteOptionsReferenceToFile(). void TMVA::Configurable::WriteOptionsReferenceToFile ; (; ). protected . write complete options to output stream ; Definition at line 409 of file Configurable.cxx. ◆ WriteOptionsToStream(). void TMVA::Configurable::WriteOptionsToStream ; (; std::ostream & ; o, . const TString & ; prefix . ); const. write options to output stream (e.g. in writing the MVA weight files ; Definition at line 333 of file Configurable.cxx. Member Data Documentation. ◆ fConfigDescription. TString TMVA::Configurable::fConfigDescription. private . description of this configurable ; Definition at line 116 of file Configurable.h. ◆ fLastDeclaredOption. OptionBase* TMVA::Configurable::fLastDeclaredOption. private . ! last declared option ; Definition at line 113 of file Configurable.h. ◆ fListOfOptions. TList TMVA::Configurable::fListOfOptions. private . option list ; Definition at line 114 of file Configurable.h. ◆ fLogger. MsgLogger* TMVA::Configurable::fLogger. mutableprotected . ! message logger ; Definition at line 128 of file Configurable.h. ◆ fLooseOptionCheckingEnabled. Bool_t TMVA::Configurable::fLooseOptionCheckingEnabled. private . checker for option string ; Definition at line 110 of file Configurable.h. ◆ fOptions. TString TMVA::Configurable::fOptions. private . options string ; Definition at line 109 of file Configurable.h. ◆ fReferenceFile. TString TMVA::Configurable::fReferenceFile. private . reference file for options writing ; Definition at line 117 of file Configurable.h. Libraries for TMVA::Configurable:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/Configurable.h; tmva/tmva/src/Configurable.cxx. TMVAConfigurable. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:2268,Modifiability,inherit,inherited,2268,"g &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; virtual TClass * IsA () const;  ; MsgLogger & Log () const;  ; virtual void ParseOptions ();  options parser ;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed na",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:4008,Modifiability,inherit,inherited,4008,"ide;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:5101,Modifiability,inherit,inheritance,5101,"  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:7100,Modifiability,inherit,inherits,7100,"e) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:7217,Modifiability,inherit,inherits,7217,"t be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is necessary). ;  ; virtual Bool_t Notify ();  This method must be overridden to handle object notification (the base ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:10970,Modifiability,inherit,inherited,10970,"stem error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObje",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:11180,Modifiability,inherit,inherited,11180,"stem error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObje",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:12066,Modifiability,inherit,inherited,12066,"tic Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Private Member Functions; template<class T > ; void AssignOpt (const TString &name, T &valAssign) const;  ; void SplitOptions (const TString &theOpt, TList &loo) const;  splits the option string at ':' and fills the list 'loo' with the primitive strings ;  . Private Attributes; TString fConfigDescription;  description of this configurable ;  ; OptionBase * fLastDeclaredOption;  ! last declared option ;  ; TList fListOfOptions;  option list ;  ; Bool_t fLooseOptionCheckingEnabled;  checker for option string ;  ; TString fOptions;  options st",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:12337,Modifiability,inherit,inherited,12337,"tic const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Private Member Functions; template<class T > ; void AssignOpt (const TString &name, T &valAssign) const;  ; void SplitOptions (const TString &theOpt, TList &loo) const;  splits the option string at ':' and fills the list 'loo' with the primitive strings ;  . Private Attributes; TString fConfigDescription;  description of this configurable ;  ; OptionBase * fLastDeclaredOption;  ! last declared option ;  ; TList fListOfOptions;  option list ;  ; Bool_t fLooseOptionCheckingEnabled;  checker for option string ;  ; TString fOptions;  options string ;  ; TString fReferenceFile;  reference file for options writing ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelet",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:12725,Modifiability,config,configurable,12725," ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Private Member Functions; template<class T > ; void AssignOpt (const TString &name, T &valAssign) const;  ; void SplitOptions (const TString &theOpt, TList &loo) const;  splits the option string at ':' and fills the list 'loo' with the primitive strings ;  . Private Attributes; TString fConfigDescription;  description of this configurable ;  ; OptionBase * fLastDeclaredOption;  ! last declared option ;  ; TList fListOfOptions;  option list ;  ; Bool_t fLooseOptionCheckingEnabled;  checker for option string ;  ; TString fOptions;  options string ;  ; TString fReferenceFile;  reference file for options writing ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  . #include <TMVA/Configurable.h>. Inheritance diagram for TMVA::",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:13062,Modifiability,inherit,inherited,13062,"onst char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Private Member Functions; template<class T > ; void AssignOpt (const TString &name, T &valAssign) const;  ; void SplitOptions (const TString &theOpt, TList &loo) const;  splits the option string at ':' and fills the list 'loo' with the primitive strings ;  . Private Attributes; TString fConfigDescription;  description of this configurable ;  ; OptionBase * fLastDeclaredOption;  ! last declared option ;  ; TList fListOfOptions;  option list ;  ; Bool_t fLooseOptionCheckingEnabled;  checker for option string ;  ; TString fOptions;  options string ;  ; TString fReferenceFile;  reference file for options writing ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  . #include <TMVA/Configurable.h>. Inheritance diagram for TMVA::Configurable:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ Configurable(). TMVA::Configurable::Configurable ; (; const TString & ; theOption = """"). constructor ; Definition at line 66 of file Con",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:13703,Modifiability,inherit,inherited,13703,"onfigurable ;  ; OptionBase * fLastDeclaredOption;  ! last declared option ;  ; TList fListOfOptions;  option list ;  ; Bool_t fLooseOptionCheckingEnabled;  checker for option string ;  ; TString fOptions;  options string ;  ; TString fReferenceFile;  reference file for options writing ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  . #include <TMVA/Configurable.h>. Inheritance diagram for TMVA::Configurable:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ Configurable(). TMVA::Configurable::Configurable ; (; const TString & ; theOption = """"). constructor ; Definition at line 66 of file Configurable.cxx. ◆ ~Configurable(). TMVA::Configurable::~Configurable ; (; ). virtual . default destructor ; Definition at line 84 of file Configurable.cxx. Member Function Documentation. ◆ AddOptionsXMLTo(). void TMVA::Configurable::AddOptionsXMLTo ; (; void * ; parent); const. write options to XML file ; Definition at line 349 of file Configurable.cxx. ◆ AddPreDefVal() [1/2]. template<class T > . void TMVA::Configurable::AddPreDefVal ; (; const T & ; val). Definition at line 168 of file Configurable.h. ◆ AddPreDefVal() [2/2]. template<class T > . void TMVA::Configurable::AddPreDefVal ; (; const",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:22342,Modifiability,config,configurable,22342,"odMLP, TMVA::MethodPDEFoam, TMVA::MethodPDERS, TMVA::MethodRuleFit, TMVA::MethodSVM, TMVA::MethodTMlpANN, TMVA::MinuitFitter, TMVA::PDF, TMVA::Reader, TMVA::SimulatedAnnealingFitter, TMVA::VariableImportance, and TMVA::CvSplitKFolds. ◆ StreamerNVirtual(). void TMVA::Configurable::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 138 of file Configurable.h. ◆ WriteOptionsReferenceToFile(). void TMVA::Configurable::WriteOptionsReferenceToFile ; (; ). protected . write complete options to output stream ; Definition at line 409 of file Configurable.cxx. ◆ WriteOptionsToStream(). void TMVA::Configurable::WriteOptionsToStream ; (; std::ostream & ; o, . const TString & ; prefix . ); const. write options to output stream (e.g. in writing the MVA weight files ; Definition at line 333 of file Configurable.cxx. Member Data Documentation. ◆ fConfigDescription. TString TMVA::Configurable::fConfigDescription. private . description of this configurable ; Definition at line 116 of file Configurable.h. ◆ fLastDeclaredOption. OptionBase* TMVA::Configurable::fLastDeclaredOption. private . ! last declared option ; Definition at line 113 of file Configurable.h. ◆ fListOfOptions. TList TMVA::Configurable::fListOfOptions. private . option list ; Definition at line 114 of file Configurable.h. ◆ fLogger. MsgLogger* TMVA::Configurable::fLogger. mutableprotected . ! message logger ; Definition at line 128 of file Configurable.h. ◆ fLooseOptionCheckingEnabled. Bool_t TMVA::Configurable::fLooseOptionCheckingEnabled. private . checker for option string ; Definition at line 110 of file Configurable.h. ◆ fOptions. TString TMVA::Configurable::fOptions. private . options string ; Definition at line 109 of file Configurable.h. ◆ fReferenceFile. TString TMVA::Configurable::fReferenceFile. private . reference file for options writing ; Definition at line 117 of file Configurable.h. Libraries for TMVA::Configurable:. [legend]; The documentation for this class wa",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:3136,Security,hash,hash,3136,"ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:12303,Testability,log,logger,12303,"tic const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Private Member Functions; template<class T > ; void AssignOpt (const TString &name, T &valAssign) const;  ; void SplitOptions (const TString &theOpt, TList &loo) const;  splits the option string at ':' and fills the list 'loo' with the primitive strings ;  . Private Attributes; TString fConfigDescription;  description of this configurable ;  ; OptionBase * fLastDeclaredOption;  ! last declared option ;  ; TList fListOfOptions;  option list ;  ; Bool_t fLooseOptionCheckingEnabled;  checker for option string ;  ; TString fOptions;  options string ;  ; TString fReferenceFile;  reference file for options writing ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelet",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1Configurable.html:22773,Testability,log,logger,22773,"olds. ◆ StreamerNVirtual(). void TMVA::Configurable::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 138 of file Configurable.h. ◆ WriteOptionsReferenceToFile(). void TMVA::Configurable::WriteOptionsReferenceToFile ; (; ). protected . write complete options to output stream ; Definition at line 409 of file Configurable.cxx. ◆ WriteOptionsToStream(). void TMVA::Configurable::WriteOptionsToStream ; (; std::ostream & ; o, . const TString & ; prefix . ); const. write options to output stream (e.g. in writing the MVA weight files ; Definition at line 333 of file Configurable.cxx. Member Data Documentation. ◆ fConfigDescription. TString TMVA::Configurable::fConfigDescription. private . description of this configurable ; Definition at line 116 of file Configurable.h. ◆ fLastDeclaredOption. OptionBase* TMVA::Configurable::fLastDeclaredOption. private . ! last declared option ; Definition at line 113 of file Configurable.h. ◆ fListOfOptions. TList TMVA::Configurable::fListOfOptions. private . option list ; Definition at line 114 of file Configurable.h. ◆ fLogger. MsgLogger* TMVA::Configurable::fLogger. mutableprotected . ! message logger ; Definition at line 128 of file Configurable.h. ◆ fLooseOptionCheckingEnabled. Bool_t TMVA::Configurable::fLooseOptionCheckingEnabled. private . checker for option string ; Definition at line 110 of file Configurable.h. ◆ fOptions. TString TMVA::Configurable::fOptions. private . options string ; Definition at line 109 of file Configurable.h. ◆ fReferenceFile. TString TMVA::Configurable::fReferenceFile. private . reference file for options writing ; Definition at line 117 of file Configurable.h. Libraries for TMVA::Configurable:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/Configurable.h; tmva/tmva/src/Configurable.cxx. TMVAConfigurable. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1Configurable.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1Configurable.html
https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html:3838,Energy Efficiency,reduce,reduce,3838,") to 1 (finished) ; Definition at line 91 of file ConvergenceTest.cxx. ◆ ResetConvergenceCounter(). void TMVA::ConvergenceTest::ResetConvergenceCounter ; (; ). inline . Definition at line 58 of file ConvergenceTest.h. ◆ SetConvergenceParameters(). void TMVA::ConvergenceTest::SetConvergenceParameters ; (; Int_t ; steps, . Double_t ; improvement . ). inline . Definition at line 54 of file ConvergenceTest.h. ◆ SetCurrentValue(). void TMVA::ConvergenceTest::SetCurrentValue ; (; Float_t ; value). inline . Definition at line 56 of file ConvergenceTest.h. ◆ SpeedControl(). Float_t TMVA::ConvergenceTest::SpeedControl ; (; UInt_t ; ofSteps). this function provides the ability to change the learning rate according to the success of the last generations. ; Parameters:. int ofSteps : = if OF the number of STEPS given in this variable (ofSteps) the rate of improvement has to be calculated. using this function one can increase the stepSize of the mutation when we have good success (to pass fast through the easy phase-space) and reduce the learning rate if we are in a difficult ""territory"" of the phase-space. ; Definition at line 112 of file ConvergenceTest.cxx. Member Data Documentation. ◆ fBestResult. Float_t TMVA::ConvergenceTest::fBestResult. private . Definition at line 81 of file ConvergenceTest.h. ◆ fConvValue. Float_t TMVA::ConvergenceTest::fConvValue. private . ! the best ""fitness"" value ; Definition at line 76 of file ConvergenceTest.h. ◆ fCounter. Int_t TMVA::ConvergenceTest::fCounter. private . ! counts the number of steps without improvement ; Definition at line 75 of file ConvergenceTest.h. ◆ fCurrentValue. Float_t TMVA::ConvergenceTest::fCurrentValue. protected . ! current value ; Definition at line 68 of file ConvergenceTest.h. ◆ fImprovement. Float_t TMVA::ConvergenceTest::fImprovement. protected . ! minimum improvement which counts as improvement ; Definition at line 70 of file ConvergenceTest.h. ◆ fLastResult. Float_t TMVA::ConvergenceTest::fLastResult. private ",MatchSource.WIKI,doc/master/classTMVA_1_1ConvergenceTest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html
https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html:3633,Modifiability,variab,variable,3633,"t is to be expected. ; Definition at line 66 of file ConvergenceTest.cxx. ◆ Progress(). Float_t TMVA::ConvergenceTest::Progress ; (; ). returns a float from 0 (just started) to 1 (finished) ; Definition at line 91 of file ConvergenceTest.cxx. ◆ ResetConvergenceCounter(). void TMVA::ConvergenceTest::ResetConvergenceCounter ; (; ). inline . Definition at line 58 of file ConvergenceTest.h. ◆ SetConvergenceParameters(). void TMVA::ConvergenceTest::SetConvergenceParameters ; (; Int_t ; steps, . Double_t ; improvement . ). inline . Definition at line 54 of file ConvergenceTest.h. ◆ SetCurrentValue(). void TMVA::ConvergenceTest::SetCurrentValue ; (; Float_t ; value). inline . Definition at line 56 of file ConvergenceTest.h. ◆ SpeedControl(). Float_t TMVA::ConvergenceTest::SpeedControl ; (; UInt_t ; ofSteps). this function provides the ability to change the learning rate according to the success of the last generations. ; Parameters:. int ofSteps : = if OF the number of STEPS given in this variable (ofSteps) the rate of improvement has to be calculated. using this function one can increase the stepSize of the mutation when we have good success (to pass fast through the easy phase-space) and reduce the learning rate if we are in a difficult ""territory"" of the phase-space. ; Definition at line 112 of file ConvergenceTest.cxx. Member Data Documentation. ◆ fBestResult. Float_t TMVA::ConvergenceTest::fBestResult. private . Definition at line 81 of file ConvergenceTest.h. ◆ fConvValue. Float_t TMVA::ConvergenceTest::fConvValue. private . ! the best ""fitness"" value ; Definition at line 76 of file ConvergenceTest.h. ◆ fCounter. Int_t TMVA::ConvergenceTest::fCounter. private . ! counts the number of steps without improvement ; Definition at line 75 of file ConvergenceTest.h. ◆ fCurrentValue. Float_t TMVA::ConvergenceTest::fCurrentValue. protected . ! current value ; Definition at line 68 of file ConvergenceTest.h. ◆ fImprovement. Float_t TMVA::ConvergenceTest::fImprovement. protected",MatchSource.WIKI,doc/master/classTMVA_1_1ConvergenceTest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html
https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html:1015,Usability,learn,learning,1015,". ROOT: TMVA::ConvergenceTest Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Protected Attributes |; Private Attributes |; List of all members ; TMVA::ConvergenceTest Class ReferenceTMVA. ; Check for convergence. ; Definition at line 46 of file ConvergenceTest.h. Public Member Functions;  ConvergenceTest ();  constructor ;  ;  ~ConvergenceTest ();  destructor ;  ; Float_t GetCurrentValue ();  ; Bool_t HasConverged (Bool_t withinConvergenceBand=kFALSE);  gives back true if the last ""steps"" steps have lead to an improvement of the ""fitness"" of the ""individuals"" of at least ""improvement"" ;  ; Float_t Progress ();  returns a float from 0 (just started) to 1 (finished) ;  ; void ResetConvergenceCounter ();  ; void SetConvergenceParameters (Int_t steps, Double_t improvement);  ; void SetCurrentValue (Float_t value);  ; Float_t SpeedControl (UInt_t ofSteps);  this function provides the ability to change the learning rate according to the success of the last generations. ;  . Protected Attributes; Float_t fCurrentValue;  ! current value ;  ; Float_t fImprovement;  ! minimum improvement which counts as improvement ;  ; Int_t fSteps;  ! number of steps without improvement required for convergence ;  . Private Attributes; Float_t fBestResult;  ; Float_t fConvValue;  ! the best ""fitness"" value ;  ; Int_t fCounter;  ! counts the number of steps without improvement ;  ; Float_t fLastResult;  ; Int_t fMaxCounter;  ! maximum value for the counter so far ;  ; std::deque< Short_t > fSuccessList;  to calculate the improvement-speed ;  . #include <TMVA/ConvergenceTest.h>. Inheritance diagram for TMVA::ConvergenceTest:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ ConvergenceTest(). TMVA::ConvergenceTest::ConvergenceTest ; (; ). constructor ; Definition at line 40 of file ConvergenceTest.cxx. ◆ ~C",MatchSource.WIKI,doc/master/classTMVA_1_1ConvergenceTest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html
https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html:2553,Usability,simpl,simple,2553,"imum value for the counter so far ;  ; std::deque< Short_t > fSuccessList;  to calculate the improvement-speed ;  . #include <TMVA/ConvergenceTest.h>. Inheritance diagram for TMVA::ConvergenceTest:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ ConvergenceTest(). TMVA::ConvergenceTest::ConvergenceTest ; (; ). constructor ; Definition at line 40 of file ConvergenceTest.cxx. ◆ ~ConvergenceTest(). TMVA::ConvergenceTest::~ConvergenceTest ; (; ). destructor ; Definition at line 55 of file ConvergenceTest.cxx. Member Function Documentation. ◆ GetCurrentValue(). Float_t TMVA::ConvergenceTest::GetCurrentValue ; (; ). inline . Definition at line 57 of file ConvergenceTest.h. ◆ HasConverged(). Bool_t TMVA::ConvergenceTest::HasConverged ; (; Bool_t ; withinConvergenceBand = kFALSE). gives back true if the last ""steps"" steps have lead to an improvement of the ""fitness"" of the ""individuals"" of at least ""improvement"" ; this gives a simple measure of if the estimator of the MLP is converging and no major improvement is to be expected. ; Definition at line 66 of file ConvergenceTest.cxx. ◆ Progress(). Float_t TMVA::ConvergenceTest::Progress ; (; ). returns a float from 0 (just started) to 1 (finished) ; Definition at line 91 of file ConvergenceTest.cxx. ◆ ResetConvergenceCounter(). void TMVA::ConvergenceTest::ResetConvergenceCounter ; (; ). inline . Definition at line 58 of file ConvergenceTest.h. ◆ SetConvergenceParameters(). void TMVA::ConvergenceTest::SetConvergenceParameters ; (; Int_t ; steps, . Double_t ; improvement . ). inline . Definition at line 54 of file ConvergenceTest.h. ◆ SetCurrentValue(). void TMVA::ConvergenceTest::SetCurrentValue ; (; Float_t ; value). inline . Definition at line 56 of file ConvergenceTest.h. ◆ SpeedControl(). Float_t TMVA::ConvergenceTest::SpeedControl ; (; UInt_t ; ofSteps). this function provides the ability to change the learning rate according to the s",MatchSource.WIKI,doc/master/classTMVA_1_1ConvergenceTest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html
https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html:3498,Usability,learn,learning,3498,"iduals"" of at least ""improvement"" ; this gives a simple measure of if the estimator of the MLP is converging and no major improvement is to be expected. ; Definition at line 66 of file ConvergenceTest.cxx. ◆ Progress(). Float_t TMVA::ConvergenceTest::Progress ; (; ). returns a float from 0 (just started) to 1 (finished) ; Definition at line 91 of file ConvergenceTest.cxx. ◆ ResetConvergenceCounter(). void TMVA::ConvergenceTest::ResetConvergenceCounter ; (; ). inline . Definition at line 58 of file ConvergenceTest.h. ◆ SetConvergenceParameters(). void TMVA::ConvergenceTest::SetConvergenceParameters ; (; Int_t ; steps, . Double_t ; improvement . ). inline . Definition at line 54 of file ConvergenceTest.h. ◆ SetCurrentValue(). void TMVA::ConvergenceTest::SetCurrentValue ; (; Float_t ; value). inline . Definition at line 56 of file ConvergenceTest.h. ◆ SpeedControl(). Float_t TMVA::ConvergenceTest::SpeedControl ; (; UInt_t ; ofSteps). this function provides the ability to change the learning rate according to the success of the last generations. ; Parameters:. int ofSteps : = if OF the number of STEPS given in this variable (ofSteps) the rate of improvement has to be calculated. using this function one can increase the stepSize of the mutation when we have good success (to pass fast through the easy phase-space) and reduce the learning rate if we are in a difficult ""territory"" of the phase-space. ; Definition at line 112 of file ConvergenceTest.cxx. Member Data Documentation. ◆ fBestResult. Float_t TMVA::ConvergenceTest::fBestResult. private . Definition at line 81 of file ConvergenceTest.h. ◆ fConvValue. Float_t TMVA::ConvergenceTest::fConvValue. private . ! the best ""fitness"" value ; Definition at line 76 of file ConvergenceTest.h. ◆ fCounter. Int_t TMVA::ConvergenceTest::fCounter. private . ! counts the number of steps without improvement ; Definition at line 75 of file ConvergenceTest.h. ◆ fCurrentValue. Float_t TMVA::ConvergenceTest::fCurrentValue. protected . ! cur",MatchSource.WIKI,doc/master/classTMVA_1_1ConvergenceTest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html
https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html:3849,Usability,learn,learning,3849,") to 1 (finished) ; Definition at line 91 of file ConvergenceTest.cxx. ◆ ResetConvergenceCounter(). void TMVA::ConvergenceTest::ResetConvergenceCounter ; (; ). inline . Definition at line 58 of file ConvergenceTest.h. ◆ SetConvergenceParameters(). void TMVA::ConvergenceTest::SetConvergenceParameters ; (; Int_t ; steps, . Double_t ; improvement . ). inline . Definition at line 54 of file ConvergenceTest.h. ◆ SetCurrentValue(). void TMVA::ConvergenceTest::SetCurrentValue ; (; Float_t ; value). inline . Definition at line 56 of file ConvergenceTest.h. ◆ SpeedControl(). Float_t TMVA::ConvergenceTest::SpeedControl ; (; UInt_t ; ofSteps). this function provides the ability to change the learning rate according to the success of the last generations. ; Parameters:. int ofSteps : = if OF the number of STEPS given in this variable (ofSteps) the rate of improvement has to be calculated. using this function one can increase the stepSize of the mutation when we have good success (to pass fast through the easy phase-space) and reduce the learning rate if we are in a difficult ""territory"" of the phase-space. ; Definition at line 112 of file ConvergenceTest.cxx. Member Data Documentation. ◆ fBestResult. Float_t TMVA::ConvergenceTest::fBestResult. private . Definition at line 81 of file ConvergenceTest.h. ◆ fConvValue. Float_t TMVA::ConvergenceTest::fConvValue. private . ! the best ""fitness"" value ; Definition at line 76 of file ConvergenceTest.h. ◆ fCounter. Int_t TMVA::ConvergenceTest::fCounter. private . ! counts the number of steps without improvement ; Definition at line 75 of file ConvergenceTest.h. ◆ fCurrentValue. Float_t TMVA::ConvergenceTest::fCurrentValue. protected . ! current value ; Definition at line 68 of file ConvergenceTest.h. ◆ fImprovement. Float_t TMVA::ConvergenceTest::fImprovement. protected . ! minimum improvement which counts as improvement ; Definition at line 70 of file ConvergenceTest.h. ◆ fLastResult. Float_t TMVA::ConvergenceTest::fLastResult. private ",MatchSource.WIKI,doc/master/classTMVA_1_1ConvergenceTest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1ConvergenceTest.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:2318,Availability,down,down,2318,"that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CostComplexityPruneTool.h. Public Member Functions;  CostComplexityPruneTool (SeparationBase *qualityIndex=nullptr);  the constructor for the cost complexity pruning ;  ; virtual ~CostComplexityPruneTool ();  the destructor for the cost complexity pruning ;  ; virtual PruningInfo * CalculatePruningInfo (DecisionTree *dt, const IPruneTool::EventSample *testEvents=nullptr, Bool_t isAutomatic=kFALSE);  the routine that basically ""steers"" the pruning process. ;  ;  Public Member Functions inherited from TMVA::IPruneTool;  IPruneTool ();  ; virtual ~IPruneTool ();  ; Double_t GetPruneStrength () const;  ; Bool_t IsAutomatic () const;  ; void SetAutomatic ();  ; void SetPruneStrength (Double_t alpha);  . Private Member Functions; void InitTreePruningMetaData (DecisionTreeNode *n);  initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ;  ; MsgLogger & Log () const;  output stream to save logging information ;  ; void Optimize (DecisionTree *dt, Double_t weights);  after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ;  . Private Attributes; MsgLogger * fLogger;  ; Int_t fOptimalK;  ! the optimal index of the prune sequence ;  ; std::vector< DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Double_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; std::vector< Double_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; SeparationBase * fQualityIndexTool;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  . Additional Inherited Members;  Public Types inherited from TMVA::",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:4838,Availability,down,down,4838,"lityIndex = nullptr). the constructor for the cost complexity pruning ; Definition at line 68 of file CostComplexityPruneTool.cxx. ◆ ~CostComplexityPruneTool(). CostComplexityPruneTool::~CostComplexityPruneTool ; (; ). virtual . the destructor for the cost complexity pruning ; Definition at line 89 of file CostComplexityPruneTool.cxx. Member Function Documentation. ◆ CalculatePruningInfo(). PruningInfo * CostComplexityPruneTool::CalculatePruningInfo ; (; DecisionTree * ; dt, . const IPruneTool::EventSample * ; validationSample = nullptr, . Bool_t ; isAutomatic = kFALSE . ). virtual . the routine that basically ""steers"" the pruning process. ; Call the calculation of the pruning sequence, the tree quality and alike.. ; Implements TMVA::IPruneTool.; Definition at line 98 of file CostComplexityPruneTool.cxx. ◆ InitTreePruningMetaData(). void CostComplexityPruneTool::InitTreePruningMetaData ; (; DecisionTreeNode * ; n). private . initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ; Definition at line 181 of file CostComplexityPruneTool.cxx. ◆ Log(). MsgLogger & TMVA::CostComplexityPruneTool::Log ; (; ); const. inlineprivate . output stream to save logging information ; Definition at line 87 of file CostComplexityPruneTool.h. ◆ Optimize(). void CostComplexityPruneTool::Optimize ; (; DecisionTree * ; dt, . Double_t ; weights . ). private . after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ; find the value of \( \alpha \) for which the test sample gives minimal error, on the tree with all nodes pruned that have \( \alpha_{critical} < \alpha \), fixed parameter pruning ; Definition at line 236 of file CostComplexityPruneTool.cxx. Member Data Documentation. ◆ fLogger. MsgLogger* TMVA::CostComplexityPruneTool::fLogger. mutableprivate . Definition at line 86 of file Cost",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:5484,Availability,error,error,5484,"all the calculation of the pruning sequence, the tree quality and alike.. ; Implements TMVA::IPruneTool.; Definition at line 98 of file CostComplexityPruneTool.cxx. ◆ InitTreePruningMetaData(). void CostComplexityPruneTool::InitTreePruningMetaData ; (; DecisionTreeNode * ; n). private . initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ; Definition at line 181 of file CostComplexityPruneTool.cxx. ◆ Log(). MsgLogger & TMVA::CostComplexityPruneTool::Log ; (; ); const. inlineprivate . output stream to save logging information ; Definition at line 87 of file CostComplexityPruneTool.h. ◆ Optimize(). void CostComplexityPruneTool::Optimize ; (; DecisionTree * ; dt, . Double_t ; weights . ). private . after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ; find the value of \( \alpha \) for which the test sample gives minimal error, on the tree with all nodes pruned that have \( \alpha_{critical} < \alpha \), fixed parameter pruning ; Definition at line 236 of file CostComplexityPruneTool.cxx. Member Data Documentation. ◆ fLogger. MsgLogger* TMVA::CostComplexityPruneTool::fLogger. mutableprivate . Definition at line 86 of file CostComplexityPruneTool.h. ◆ fOptimalK. Int_t TMVA::CostComplexityPruneTool::fOptimalK. private . ! the optimal index of the prune sequence ; Definition at line 77 of file CostComplexityPruneTool.h. ◆ fPruneSequence. std::vector<DecisionTreeNode*> TMVA::CostComplexityPruneTool::fPruneSequence. private . ! map of weakest links (i.e., branches to prune) -> pruning index ; Definition at line 73 of file CostComplexityPruneTool.h. ◆ fPruneStrengthList. std::vector<Double_t> TMVA::CostComplexityPruneTool::fPruneStrengthList. private . ! map of alpha -> pruning index ; Definition at line 74 of file CostComplexityPruneTool.h. ◆ fQualityIndexList. std::ve",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:1833,Integrability,rout,routine,1833,"nal nodes in \( T \); \( T' \) - the pruned subtree of \( T_max \) that has the best quality index \( R(T') \); \( \alpha \) - the prune strength parameter in Cost Complexity pruning \( (R_{\alpha}(T) = R(T) + \alpha*|\sim T|) \). There are two running modes in CCPruner: (i) one may select a prune strength and prune back the tree \( T_{max}\) until the criterion: ; \[; \alpha < \frac{R(T) - R(t)}{|\sim T_t| - 1}; \]. is true for all nodes t in \( T \), or (ii) the algorithm finds the sequence of critical points \( \alpha_k < \alpha_{k+1} ... < \alpha_K \) such that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CostComplexityPruneTool.h. Public Member Functions;  CostComplexityPruneTool (SeparationBase *qualityIndex=nullptr);  the constructor for the cost complexity pruning ;  ; virtual ~CostComplexityPruneTool ();  the destructor for the cost complexity pruning ;  ; virtual PruningInfo * CalculatePruningInfo (DecisionTree *dt, const IPruneTool::EventSample *testEvents=nullptr, Bool_t isAutomatic=kFALSE);  the routine that basically ""steers"" the pruning process. ;  ;  Public Member Functions inherited from TMVA::IPruneTool;  IPruneTool ();  ; virtual ~IPruneTool ();  ; Double_t GetPruneStrength () const;  ; Bool_t IsAutomatic () const;  ; void SetAutomatic ();  ; void SetPruneStrength (Double_t alpha);  . Private Member Functions; void InitTreePruningMetaData (DecisionTreeNode *n);  initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ;  ; MsgLogger & Log () const;  output stream to save logging information ;  ; void Optimize (DecisionTree *dt, Double_t weights);  after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ;  . Privat",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:4389,Integrability,rout,routine,4389," parameter in pruning ;  ; Double_t S;  . #include <TMVA/CostComplexityPruneTool.h>. Inheritance diagram for TMVA::CostComplexityPruneTool:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CostComplexityPruneTool(). CostComplexityPruneTool::CostComplexityPruneTool ; (; SeparationBase * ; qualityIndex = nullptr). the constructor for the cost complexity pruning ; Definition at line 68 of file CostComplexityPruneTool.cxx. ◆ ~CostComplexityPruneTool(). CostComplexityPruneTool::~CostComplexityPruneTool ; (; ). virtual . the destructor for the cost complexity pruning ; Definition at line 89 of file CostComplexityPruneTool.cxx. Member Function Documentation. ◆ CalculatePruningInfo(). PruningInfo * CostComplexityPruneTool::CalculatePruningInfo ; (; DecisionTree * ; dt, . const IPruneTool::EventSample * ; validationSample = nullptr, . Bool_t ; isAutomatic = kFALSE . ). virtual . the routine that basically ""steers"" the pruning process. ; Call the calculation of the pruning sequence, the tree quality and alike.. ; Implements TMVA::IPruneTool.; Definition at line 98 of file CostComplexityPruneTool.cxx. ◆ InitTreePruningMetaData(). void CostComplexityPruneTool::InitTreePruningMetaData ; (; DecisionTreeNode * ; n). private . initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ; Definition at line 181 of file CostComplexityPruneTool.cxx. ◆ Log(). MsgLogger & TMVA::CostComplexityPruneTool::Log ; (; ); const. inlineprivate . output stream to save logging information ; Definition at line 87 of file CostComplexityPruneTool.h. ◆ Optimize(). void CostComplexityPruneTool::Optimize ; (; DecisionTree * ; dt, . Double_t ; weights . ). private . after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ; ",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:1916,Modifiability,inherit,inherited,1916,"}\) until the criterion: ; \[; \alpha < \frac{R(T) - R(t)}{|\sim T_t| - 1}; \]. is true for all nodes t in \( T \), or (ii) the algorithm finds the sequence of critical points \( \alpha_k < \alpha_{k+1} ... < \alpha_K \) such that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CostComplexityPruneTool.h. Public Member Functions;  CostComplexityPruneTool (SeparationBase *qualityIndex=nullptr);  the constructor for the cost complexity pruning ;  ; virtual ~CostComplexityPruneTool ();  the destructor for the cost complexity pruning ;  ; virtual PruningInfo * CalculatePruningInfo (DecisionTree *dt, const IPruneTool::EventSample *testEvents=nullptr, Bool_t isAutomatic=kFALSE);  the routine that basically ""steers"" the pruning process. ;  ;  Public Member Functions inherited from TMVA::IPruneTool;  IPruneTool ();  ; virtual ~IPruneTool ();  ; Double_t GetPruneStrength () const;  ; Bool_t IsAutomatic () const;  ; void SetAutomatic ();  ; void SetPruneStrength (Double_t alpha);  . Private Member Functions; void InitTreePruningMetaData (DecisionTreeNode *n);  initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ;  ; MsgLogger & Log () const;  output stream to save logging information ;  ; void Optimize (DecisionTree *dt, Double_t weights);  after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ;  . Private Attributes; MsgLogger * fLogger;  ; Int_t fOptimalK;  ! the optimal index of the prune sequence ;  ; std::vector< DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Double_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; std::vector< Double_t > fQualityI",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:3214,Modifiability,inherit,inherited,3214," alpha down the tree, etc... for each node!! ;  ; MsgLogger & Log () const;  output stream to save logging information ;  ; void Optimize (DecisionTree *dt, Double_t weights);  after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ;  . Private Attributes; MsgLogger * fLogger;  ; Int_t fOptimalK;  ! the optimal index of the prune sequence ;  ; std::vector< DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Double_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; std::vector< Double_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; SeparationBase * fQualityIndexTool;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  . Additional Inherited Members;  Public Types inherited from TMVA::IPruneTool; typedef std::vector< const Event * > EventSample;  ;  Protected Attributes inherited from TMVA::IPruneTool; Double_t B;  ; Double_t fPruneStrength;  ! regularization parameter in pruning ;  ; Double_t S;  . #include <TMVA/CostComplexityPruneTool.h>. Inheritance diagram for TMVA::CostComplexityPruneTool:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CostComplexityPruneTool(). CostComplexityPruneTool::CostComplexityPruneTool ; (; SeparationBase * ; qualityIndex = nullptr). the constructor for the cost complexity pruning ; Definition at line 68 of file CostComplexityPruneTool.cxx. ◆ ~CostComplexityPruneTool(). CostComplexityPruneTool::~CostComplexityPruneTool ; (; ). virtual . the destructor for the cost complexity pruning ; Definition at line 89 of file CostComplexityPruneTool.cxx. Member Function Documentation. ◆ CalculatePruningInfo(). PruningInfo * CostComplexityPruneTool::CalculatePruningInfo ; (; DecisionTree * ; dt, . const IPruneTool::EventSample * ; v",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:3322,Modifiability,inherit,inherited,3322," alpha down the tree, etc... for each node!! ;  ; MsgLogger & Log () const;  output stream to save logging information ;  ; void Optimize (DecisionTree *dt, Double_t weights);  after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ;  . Private Attributes; MsgLogger * fLogger;  ; Int_t fOptimalK;  ! the optimal index of the prune sequence ;  ; std::vector< DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Double_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; std::vector< Double_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; SeparationBase * fQualityIndexTool;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  . Additional Inherited Members;  Public Types inherited from TMVA::IPruneTool; typedef std::vector< const Event * > EventSample;  ;  Protected Attributes inherited from TMVA::IPruneTool; Double_t B;  ; Double_t fPruneStrength;  ! regularization parameter in pruning ;  ; Double_t S;  . #include <TMVA/CostComplexityPruneTool.h>. Inheritance diagram for TMVA::CostComplexityPruneTool:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CostComplexityPruneTool(). CostComplexityPruneTool::CostComplexityPruneTool ; (; SeparationBase * ; qualityIndex = nullptr). the constructor for the cost complexity pruning ; Definition at line 68 of file CostComplexityPruneTool.cxx. ◆ ~CostComplexityPruneTool(). CostComplexityPruneTool::~CostComplexityPruneTool ; (; ). virtual . the destructor for the cost complexity pruning ; Definition at line 89 of file CostComplexityPruneTool.cxx. Member Function Documentation. ◆ CalculatePruningInfo(). PruningInfo * CostComplexityPruneTool::CalculatePruningInfo ; (; DecisionTree * ; dt, . const IPruneTool::EventSample * ; v",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:1375,Security,validat,validation,1375,"MVA. ; A class to prune a decision tree using the Cost Complexity method. ; (see ""Classification and Regression Trees"" by Leo Breiman et al). Some definitions:. \( T_{max} \) - the initial, usually highly overtrained tree, that is to be pruned back; \( R(T) \) - quality index (Gini, misclassification rate, or other) of a tree \( T \); \( \sim T \) - set of terminal nodes in \( T \); \( T' \) - the pruned subtree of \( T_max \) that has the best quality index \( R(T') \); \( \alpha \) - the prune strength parameter in Cost Complexity pruning \( (R_{\alpha}(T) = R(T) + \alpha*|\sim T|) \). There are two running modes in CCPruner: (i) one may select a prune strength and prune back the tree \( T_{max}\) until the criterion: ; \[; \alpha < \frac{R(T) - R(t)}{|\sim T_t| - 1}; \]. is true for all nodes t in \( T \), or (ii) the algorithm finds the sequence of critical points \( \alpha_k < \alpha_{k+1} ... < \alpha_K \) such that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CostComplexityPruneTool.h. Public Member Functions;  CostComplexityPruneTool (SeparationBase *qualityIndex=nullptr);  the constructor for the cost complexity pruning ;  ; virtual ~CostComplexityPruneTool ();  the destructor for the cost complexity pruning ;  ; virtual PruningInfo * CalculatePruningInfo (DecisionTree *dt, const IPruneTool::EventSample *testEvents=nullptr, Bool_t isAutomatic=kFALSE);  the routine that basically ""steers"" the pruning process. ;  ;  Public Member Functions inherited from TMVA::IPruneTool;  IPruneTool ();  ; virtual ~IPruneTool ();  ; Double_t GetPruneStrength () const;  ; Bool_t IsAutomatic () const;  ; void SetAutomatic ();  ; void SetPruneStrength (Double_t alpha);  . Private Member Functions; void InitTreePruningMetaData (DecisionTreeNode *n);  initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the ",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:4310,Security,validat,validationSample,4310,"ted Attributes inherited from TMVA::IPruneTool; Double_t B;  ; Double_t fPruneStrength;  ! regularization parameter in pruning ;  ; Double_t S;  . #include <TMVA/CostComplexityPruneTool.h>. Inheritance diagram for TMVA::CostComplexityPruneTool:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CostComplexityPruneTool(). CostComplexityPruneTool::CostComplexityPruneTool ; (; SeparationBase * ; qualityIndex = nullptr). the constructor for the cost complexity pruning ; Definition at line 68 of file CostComplexityPruneTool.cxx. ◆ ~CostComplexityPruneTool(). CostComplexityPruneTool::~CostComplexityPruneTool ; (; ). virtual . the destructor for the cost complexity pruning ; Definition at line 89 of file CostComplexityPruneTool.cxx. Member Function Documentation. ◆ CalculatePruningInfo(). PruningInfo * CostComplexityPruneTool::CalculatePruningInfo ; (; DecisionTree * ; dt, . const IPruneTool::EventSample * ; validationSample = nullptr, . Bool_t ; isAutomatic = kFALSE . ). virtual . the routine that basically ""steers"" the pruning process. ; Call the calculation of the pruning sequence, the tree quality and alike.. ; Implements TMVA::IPruneTool.; Definition at line 98 of file CostComplexityPruneTool.cxx. ◆ InitTreePruningMetaData(). void CostComplexityPruneTool::InitTreePruningMetaData ; (; DecisionTreeNode * ; n). private . initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ; Definition at line 181 of file CostComplexityPruneTool.cxx. ◆ Log(). MsgLogger & TMVA::CostComplexityPruneTool::Log ; (; ); const. inlineprivate . output stream to save logging information ; Definition at line 87 of file CostComplexityPruneTool.h. ◆ Optimize(). void CostComplexityPruneTool::Optimize ; (; DecisionTree * ; dt, . Double_t ; weights . ). private . after the critical \( \alpha \) values (at which the corresponding",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:1780,Testability,test,testEvents,1780,"nal nodes in \( T \); \( T' \) - the pruned subtree of \( T_max \) that has the best quality index \( R(T') \); \( \alpha \) - the prune strength parameter in Cost Complexity pruning \( (R_{\alpha}(T) = R(T) + \alpha*|\sim T|) \). There are two running modes in CCPruner: (i) one may select a prune strength and prune back the tree \( T_{max}\) until the criterion: ; \[; \alpha < \frac{R(T) - R(t)}{|\sim T_t| - 1}; \]. is true for all nodes t in \( T \), or (ii) the algorithm finds the sequence of critical points \( \alpha_k < \alpha_{k+1} ... < \alpha_K \) such that \( T_K = root(T_{max}) \) and then selects the optimally-pruned subtree, defined to be the subtree with the best quality index for the validation sample. ; Definition at line 62 of file CostComplexityPruneTool.h. Public Member Functions;  CostComplexityPruneTool (SeparationBase *qualityIndex=nullptr);  the constructor for the cost complexity pruning ;  ; virtual ~CostComplexityPruneTool ();  the destructor for the cost complexity pruning ;  ; virtual PruningInfo * CalculatePruningInfo (DecisionTree *dt, const IPruneTool::EventSample *testEvents=nullptr, Bool_t isAutomatic=kFALSE);  the routine that basically ""steers"" the pruning process. ;  ;  Public Member Functions inherited from TMVA::IPruneTool;  IPruneTool ();  ; virtual ~IPruneTool ();  ; Double_t GetPruneStrength () const;  ; Bool_t IsAutomatic () const;  ; void SetAutomatic ();  ; void SetPruneStrength (Double_t alpha);  . Private Member Functions; void InitTreePruningMetaData (DecisionTreeNode *n);  initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ;  ; MsgLogger & Log () const;  output stream to save logging information ;  ; void Optimize (DecisionTree *dt, Double_t weights);  after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ;  . Privat",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:2410,Testability,log,logging,2410,"ol (SeparationBase *qualityIndex=nullptr);  the constructor for the cost complexity pruning ;  ; virtual ~CostComplexityPruneTool ();  the destructor for the cost complexity pruning ;  ; virtual PruningInfo * CalculatePruningInfo (DecisionTree *dt, const IPruneTool::EventSample *testEvents=nullptr, Bool_t isAutomatic=kFALSE);  the routine that basically ""steers"" the pruning process. ;  ;  Public Member Functions inherited from TMVA::IPruneTool;  IPruneTool ();  ; virtual ~IPruneTool ();  ; Double_t GetPruneStrength () const;  ; Bool_t IsAutomatic () const;  ; void SetAutomatic ();  ; void SetPruneStrength (Double_t alpha);  . Private Member Functions; void InitTreePruningMetaData (DecisionTreeNode *n);  initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ;  ; MsgLogger & Log () const;  output stream to save logging information ;  ; void Optimize (DecisionTree *dt, Double_t weights);  after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ;  . Private Attributes; MsgLogger * fLogger;  ; Int_t fOptimalK;  ! the optimal index of the prune sequence ;  ; std::vector< DecisionTreeNode * > fPruneSequence;  ! map of weakest links (i.e., branches to prune) -> pruning index ;  ; std::vector< Double_t > fPruneStrengthList;  ! map of alpha -> pruning index ;  ; std::vector< Double_t > fQualityIndexList;  ! map of R(T) -> pruning index ;  ; SeparationBase * fQualityIndexTool;  ! the quality index used to calculate R(t), R(T) = sum[t in ~T]{ R(t) } ;  . Additional Inherited Members;  Public Types inherited from TMVA::IPruneTool; typedef std::vector< const Event * > EventSample;  ;  Protected Attributes inherited from TMVA::IPruneTool; Double_t B;  ; Double_t fPruneStrength;  ! regularization parameter in pruning ;  ; Double_t S;  . #include <TMVA/CostComplexityPruneTool.h>. Inhe",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:5047,Testability,log,logging,5047,"efinition at line 89 of file CostComplexityPruneTool.cxx. Member Function Documentation. ◆ CalculatePruningInfo(). PruningInfo * CostComplexityPruneTool::CalculatePruningInfo ; (; DecisionTree * ; dt, . const IPruneTool::EventSample * ; validationSample = nullptr, . Bool_t ; isAutomatic = kFALSE . ). virtual . the routine that basically ""steers"" the pruning process. ; Call the calculation of the pruning sequence, the tree quality and alike.. ; Implements TMVA::IPruneTool.; Definition at line 98 of file CostComplexityPruneTool.cxx. ◆ InitTreePruningMetaData(). void CostComplexityPruneTool::InitTreePruningMetaData ; (; DecisionTreeNode * ; n). private . initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ; Definition at line 181 of file CostComplexityPruneTool.cxx. ◆ Log(). MsgLogger & TMVA::CostComplexityPruneTool::Log ; (; ); const. inlineprivate . output stream to save logging information ; Definition at line 87 of file CostComplexityPruneTool.h. ◆ Optimize(). void CostComplexityPruneTool::Optimize ; (; DecisionTree * ; dt, . Double_t ; weights . ). private . after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ; find the value of \( \alpha \) for which the test sample gives minimal error, on the tree with all nodes pruned that have \( \alpha_{critical} < \alpha \), fixed parameter pruning ; Definition at line 236 of file CostComplexityPruneTool.cxx. Member Data Documentation. ◆ fLogger. MsgLogger* TMVA::CostComplexityPruneTool::fLogger. mutableprivate . Definition at line 86 of file CostComplexityPruneTool.h. ◆ fOptimalK. Int_t TMVA::CostComplexityPruneTool::fOptimalK. private . ! the optimal index of the prune sequence ; Definition at line 77 of file CostComplexityPruneTool.h. ◆ fPruneSequence. std::vector<DecisionTreeNode*> TMVA::CostComplexityPruneTool::fPru",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html:5458,Testability,test,test,5458,"all the calculation of the pruning sequence, the tree quality and alike.. ; Implements TMVA::IPruneTool.; Definition at line 98 of file CostComplexityPruneTool.cxx. ◆ InitTreePruningMetaData(). void CostComplexityPruneTool::InitTreePruningMetaData ; (; DecisionTreeNode * ; n). private . initialise ""meta data"" for the pruning, like the ""costcomplexity"", the critical alpha, the minimal alpha down the tree, etc... for each node!! ; Definition at line 181 of file CostComplexityPruneTool.cxx. ◆ Log(). MsgLogger & TMVA::CostComplexityPruneTool::Log ; (; ); const. inlineprivate . output stream to save logging information ; Definition at line 87 of file CostComplexityPruneTool.h. ◆ Optimize(). void CostComplexityPruneTool::Optimize ; (; DecisionTree * ; dt, . Double_t ; weights . ). private . after the critical \( \alpha \) values (at which the corresponding nodes would be pruned away) had been established in the ""InitMetaData"" we need now: automatic pruning: ; find the value of \( \alpha \) for which the test sample gives minimal error, on the tree with all nodes pruned that have \( \alpha_{critical} < \alpha \), fixed parameter pruning ; Definition at line 236 of file CostComplexityPruneTool.cxx. Member Data Documentation. ◆ fLogger. MsgLogger* TMVA::CostComplexityPruneTool::fLogger. mutableprivate . Definition at line 86 of file CostComplexityPruneTool.h. ◆ fOptimalK. Int_t TMVA::CostComplexityPruneTool::fOptimalK. private . ! the optimal index of the prune sequence ; Definition at line 77 of file CostComplexityPruneTool.h. ◆ fPruneSequence. std::vector<DecisionTreeNode*> TMVA::CostComplexityPruneTool::fPruneSequence. private . ! map of weakest links (i.e., branches to prune) -> pruning index ; Definition at line 73 of file CostComplexityPruneTool.h. ◆ fPruneStrengthList. std::vector<Double_t> TMVA::CostComplexityPruneTool::fPruneStrengthList. private . ! map of alpha -> pruning index ; Definition at line 74 of file CostComplexityPruneTool.h. ◆ fQualityIndexList. std::ve",MatchSource.WIKI,doc/master/classTMVA_1_1CostComplexityPruneTool.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CostComplexityPruneTool.html
https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html:804,Modifiability,inherit,inherited,804,". ROOT: TMVA::CrossEntropy Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; List of all members ; TMVA::CrossEntropy Class ReferenceTMVA. ; Implementation of the CrossEntropy as separation criterion. ; -p log (p) - (1-p)log(1-p); p=purity. Definition at line 43 of file CrossEntropy.h. Public Member Functions;  CrossEntropy ();  ;  CrossEntropy (const CrossEntropy &g);  ; virtual ~CrossEntropy ();  ; virtual Double_t GetSeparationIndex (const Double_t s, const Double_t b);  Cross Entropy defined as: ;  ; virtual TClass * IsA () const;  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::SeparationBase;  SeparationBase ();  Constructor. ;  ;  SeparationBase (const SeparationBase &s);  Copy constructor. ;  ; virtual ~SeparationBase ();  ; const TString & GetName ();  ; virtual Double_t GetSeparationGain (const Double_t nSelS, const Double_t nSelB, const Double_t nTotS, const Double_t nTotB);  Separation Gain: the measure of how the quality of separation of the sample increases by splitting the sample e.g. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::SeparationBase; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Additional Inherited Members;  Protected Attributes inherited from TMVA::SeparationBase; TString fName;  ; Double_t fPrecisionCut;  . #include <TMVA/CrossEntropy.h>. Inheritance diagram for TMVA::CrossEntropy:. This browser is not able to show SVG: try Firefox, Chrome, ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossEntropy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html
https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html:1539,Modifiability,inherit,inherited,1539,"ropy ();  ; virtual Double_t GetSeparationIndex (const Double_t s, const Double_t b);  Cross Entropy defined as: ;  ; virtual TClass * IsA () const;  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::SeparationBase;  SeparationBase ();  Constructor. ;  ;  SeparationBase (const SeparationBase &s);  Copy constructor. ;  ; virtual ~SeparationBase ();  ; const TString & GetName ();  ; virtual Double_t GetSeparationGain (const Double_t nSelS, const Double_t nSelB, const Double_t nTotS, const Double_t nTotB);  Separation Gain: the measure of how the quality of separation of the sample increases by splitting the sample e.g. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::SeparationBase; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Additional Inherited Members;  Protected Attributes inherited from TMVA::SeparationBase; TString fName;  ; Double_t fPrecisionCut;  . #include <TMVA/CrossEntropy.h>. Inheritance diagram for TMVA::CrossEntropy:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossEntropy() [1/2]. TMVA::CrossEntropy::CrossEntropy ; (; ). inline . Definition at line 48 of file CrossEntropy.h. ◆ CrossEntropy() [2/2]. TMVA::CrossEntropy::CrossEntropy ; (; const CrossEntropy & ; g). inline . Definition at line 51 of file CrossEntropy.h. ◆ ~CrossEntropy(). virtual TMVA::CrossEntropy::~CrossEntropy ; (; ). inlinevirtual . Definition at line 54 of file CrossEntropy.h. Member Function Documentation. ◆ Class(). static TCl",MatchSource.WIKI,doc/master/classTMVA_1_1CrossEntropy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html
https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html:1783,Modifiability,inherit,inherited,1783,"ctions inherited from TMVA::SeparationBase;  SeparationBase ();  Constructor. ;  ;  SeparationBase (const SeparationBase &s);  Copy constructor. ;  ; virtual ~SeparationBase ();  ; const TString & GetName ();  ; virtual Double_t GetSeparationGain (const Double_t nSelS, const Double_t nSelB, const Double_t nTotS, const Double_t nTotB);  Separation Gain: the measure of how the quality of separation of the sample increases by splitting the sample e.g. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::SeparationBase; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Additional Inherited Members;  Protected Attributes inherited from TMVA::SeparationBase; TString fName;  ; Double_t fPrecisionCut;  . #include <TMVA/CrossEntropy.h>. Inheritance diagram for TMVA::CrossEntropy:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossEntropy() [1/2]. TMVA::CrossEntropy::CrossEntropy ; (; ). inline . Definition at line 48 of file CrossEntropy.h. ◆ CrossEntropy() [2/2]. TMVA::CrossEntropy::CrossEntropy ; (; const CrossEntropy & ; g). inline . Definition at line 51 of file CrossEntropy.h. ◆ ~CrossEntropy(). virtual TMVA::CrossEntropy::~CrossEntropy ; (; ). inlinevirtual . Definition at line 54 of file CrossEntropy.h. Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossEntropy::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CrossEntropy::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::CrossEntropy::Class_V",MatchSource.WIKI,doc/master/classTMVA_1_1CrossEntropy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html
https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html:338,Testability,log,log,338,". ROOT: TMVA::CrossEntropy Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; List of all members ; TMVA::CrossEntropy Class ReferenceTMVA. ; Implementation of the CrossEntropy as separation criterion. ; -p log (p) - (1-p)log(1-p); p=purity. Definition at line 43 of file CrossEntropy.h. Public Member Functions;  CrossEntropy ();  ;  CrossEntropy (const CrossEntropy &g);  ; virtual ~CrossEntropy ();  ; virtual Double_t GetSeparationIndex (const Double_t s, const Double_t b);  Cross Entropy defined as: ;  ; virtual TClass * IsA () const;  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::SeparationBase;  SeparationBase ();  Constructor. ;  ;  SeparationBase (const SeparationBase &s);  Copy constructor. ;  ; virtual ~SeparationBase ();  ; const TString & GetName ();  ; virtual Double_t GetSeparationGain (const Double_t nSelS, const Double_t nSelB, const Double_t nTotS, const Double_t nTotB);  Separation Gain: the measure of how the quality of separation of the sample increases by splitting the sample e.g. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::SeparationBase; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Additional Inherited Members;  Protected Attributes inherited from TMVA::SeparationBase; TString fName;  ; Double_t fPrecisionCut;  . #include <TMVA/CrossEntropy.h>. Inheritance diagram for TMVA::CrossEntropy:. This browser is not able to show SVG: try Firefox, Chrome, ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossEntropy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html
https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html:353,Testability,log,log,353,". ROOT: TMVA::CrossEntropy Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; List of all members ; TMVA::CrossEntropy Class ReferenceTMVA. ; Implementation of the CrossEntropy as separation criterion. ; -p log (p) - (1-p)log(1-p); p=purity. Definition at line 43 of file CrossEntropy.h. Public Member Functions;  CrossEntropy ();  ;  CrossEntropy (const CrossEntropy &g);  ; virtual ~CrossEntropy ();  ; virtual Double_t GetSeparationIndex (const Double_t s, const Double_t b);  Cross Entropy defined as: ;  ; virtual TClass * IsA () const;  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::SeparationBase;  SeparationBase ();  Constructor. ;  ;  SeparationBase (const SeparationBase &s);  Copy constructor. ;  ; virtual ~SeparationBase ();  ; const TString & GetName ();  ; virtual Double_t GetSeparationGain (const Double_t nSelS, const Double_t nSelB, const Double_t nTotS, const Double_t nTotB);  Separation Gain: the measure of how the quality of separation of the sample increases by splitting the sample e.g. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::SeparationBase; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Additional Inherited Members;  Protected Attributes inherited from TMVA::SeparationBase; TString fName;  ; Double_t fPrecisionCut;  . #include <TMVA/CrossEntropy.h>. Inheritance diagram for TMVA::CrossEntropy:. This browser is not able to show SVG: try Firefox, Chrome, ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossEntropy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html
https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html:3276,Testability,log,log,3276,"2/2]. TMVA::CrossEntropy::CrossEntropy ; (; const CrossEntropy & ; g). inline . Definition at line 51 of file CrossEntropy.h. ◆ ~CrossEntropy(). virtual TMVA::CrossEntropy::~CrossEntropy ; (; ). inlinevirtual . Definition at line 54 of file CrossEntropy.h. Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossEntropy::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CrossEntropy::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::CrossEntropy::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 61 of file CrossEntropy.h. ◆ DeclFileName(). static const char * TMVA::CrossEntropy::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 61 of file CrossEntropy.h. ◆ GetSeparationIndex(). Double_t TMVA::CrossEntropy::GetSeparationIndex ; (; const Double_t ; s, . const Double_t ; b . ). virtual . Cross Entropy defined as: ; -p log (p) - (1-p)log(1-p); p=purity = s/(s+b) . Implements TMVA::SeparationBase.; Definition at line 47 of file CrossEntropy.cxx. ◆ IsA(). virtual TClass * TMVA::CrossEntropy::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::SeparationBase.; Definition at line 61 of file CrossEntropy.h. ◆ Streamer(). virtual void TMVA::CrossEntropy::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::SeparationBase. ◆ StreamerNVirtual(). void TMVA::CrossEntropy::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 61 of file CrossEntropy.h. Libraries for TMVA::CrossEntropy:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/CrossEntropy.h; tmva/tmva/src/CrossEntropy.cxx. TMVACrossEntropy. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossEntropy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html
https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html:3291,Testability,log,log,3291,"2/2]. TMVA::CrossEntropy::CrossEntropy ; (; const CrossEntropy & ; g). inline . Definition at line 51 of file CrossEntropy.h. ◆ ~CrossEntropy(). virtual TMVA::CrossEntropy::~CrossEntropy ; (; ). inlinevirtual . Definition at line 54 of file CrossEntropy.h. Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossEntropy::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CrossEntropy::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::CrossEntropy::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 61 of file CrossEntropy.h. ◆ DeclFileName(). static const char * TMVA::CrossEntropy::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 61 of file CrossEntropy.h. ◆ GetSeparationIndex(). Double_t TMVA::CrossEntropy::GetSeparationIndex ; (; const Double_t ; s, . const Double_t ; b . ). virtual . Cross Entropy defined as: ; -p log (p) - (1-p)log(1-p); p=purity = s/(s+b) . Implements TMVA::SeparationBase.; Definition at line 47 of file CrossEntropy.cxx. ◆ IsA(). virtual TClass * TMVA::CrossEntropy::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::SeparationBase.; Definition at line 61 of file CrossEntropy.h. ◆ Streamer(). virtual void TMVA::CrossEntropy::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::SeparationBase. ◆ StreamerNVirtual(). void TMVA::CrossEntropy::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 61 of file CrossEntropy.h. Libraries for TMVA::CrossEntropy:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/CrossEntropy.h; tmva/tmva/src/CrossEntropy.cxx. TMVACrossEntropy. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossEntropy.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossEntropy.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:2931,Availability,avail,available,2931,". ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; DataLoader * GetDataLoader ();  Method to get the pointer to TMVA::DataLoader object. ;  ; TFile * GetFile ();  Method to get the pointer to TFile object. ;  ; std::vector< OptionMap > & GetMethods ();  Method get the Booked methods in a option map object. ;  ; Bool_t HasMethod (TString methodname, TString methodtitle);  function to check methods booked ;  ; Bool_t IsModelPersistence ();  Method to see if the algorithm model is saved in xml or serialized files. ;  ; Bool_t IsSilentFile ();  Method to see if a file is available to save results. ;  ; Bool_t IsVerbose ();  Method to see if the algorithm should print extra information. ;  ; void SetDataLoader (DataLoader *dalaloader);  Method to set the pointer to TMVA::DataLoader object. ;  ; void SetFile (TFile *file);  Method to set the pointer to TFile object, with a writable file. ;  ; void SetModelPersistence (Bool_t status=kTRUE);  Method enable model persistence, then algorithms model is saved in xml or serialized files. ;  ; void SetVerbose (Bool_t status);  Method enable print extra information in the algorithms. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const T",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:8484,Availability,error,error,8484,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:8573,Availability,error,error,8573,"ecord whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:8728,Availability,error,error,8728,"Name () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:9025,Availability,error,error,9025,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:13109,Availability,error,error,13109,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Envelope; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr V",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:8490,Integrability,message,message,8490,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:9031,Integrability,message,message,9031,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:10021,Integrability,message,message,10021,"ssue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class shou",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:13115,Integrability,message,message,13115,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Envelope; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr V",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:13488,Integrability,message,message,13488," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Envelope; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ;",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:19324,Integrability,message,message,19324,"tions are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CrossValidation.h>. Inheritance diagram for TMVA::CrossValidation:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossValidation() [1/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TString ; options . ). explicit . Definition at line 308 of file CrossValidation.cxx. ◆ CrossValidation() [2/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TFile * ; outputFile, . TString ; options . ). explicit . Definition at line 277 of file CrossValidation.cxx. ◆ ~CrossValidation(). TMVA::CrossValidation::~CrossValidation ; (; ). default . Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossValida",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:2012,Modifiability,inherit,inherited,2012,"ee( """", """", ""nTest_cls1=1:nTest_cls2=1"" );. Split Expression; See CVSplit documentation? ; Definition at line 124 of file CrossValidation.h. Public Member Functions;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; DataLoader * GetDataLoader ();  Method to get the pointer to TMVA::DataLoader object. ;  ; TFile * GetFile ();  Method to get the pointer to TFile object. ;  ; std::vector< OptionMap > & GetMethods ();  Method get the Booked methods in a option map object. ;  ; Bool_t HasMethod (TString methodname, TString methodtitle);  function to check methods booked ;  ; Bool_t IsModelPersistence ();  Method to see if the algorithm model is saved in xml or serialized files. ;  ; Bool_t IsSilentFile ();  Method to see if a file is available to save results. ;  ; Bool_t IsVerbose ();  Method to se",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:3589,Modifiability,inherit,inherited,3589,"del is saved in xml or serialized files. ;  ; void SetVerbose (Bool_t status);  Method enable print extra information in the algorithms. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc="""");  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:5287,Modifiability,inherit,inherited,5287,"_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed na",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:7027,Modifiability,inherit,inherited,7027,"ide;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:8120,Modifiability,inherit,inheritance,8120,"  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:10119,Modifiability,inherit,inherits,10119,"e) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:10236,Modifiability,inherit,inherits,10236,"t be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is necessary). ;  ; virtual Bool_t Notify ();  This method must be overridden to handle object notification (the base ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:13989,Modifiability,inherit,inherited,13989,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Envelope; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; CrossValidationFoldResult ProcessFold (UInt_t iFold, const OptionMap &methodInfo);  Evaluates each fold in turn. ;  . Private Attributes; Types::EAnalysisType fAnalysisType; ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:14207,Modifiability,inherit,inherited,14207,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Envelope; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; CrossValidationFoldResult ProcessFold (UInt_t iFold, const OptionMap &methodInfo);  Evaluates each fold in turn. ;  . Private Attributes; Types::EAnalysisType fAnalysisType; ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:14429,Modifiability,inherit,inherited,14429,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Envelope; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; CrossValidationFoldResult ProcessFold (UInt_t iFold, const OptionMap &methodInfo);  Evaluates each fold in turn. ;  . Private Attributes; Types::EAnalysisType fAnalysisType; ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:14639,Modifiability,inherit,inherited,14639,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Envelope; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; CrossValidationFoldResult ProcessFold (UInt_t iFold, const OptionMap &methodInfo);  Evaluates each fold in turn. ;  . Private Attributes; Types::EAnalysisType fAnalysisType; ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:16288,Modifiability,inherit,inherited,16288,"tr;  ; Bool_t fCorrelations;  ; TString fCvFactoryOptions;  ; Bool_t fDrawProgressBar;  ; std::unique_ptr< Factory > fFactory;  ; std::unique_ptr< Factory > fFoldFactory;  ; Bool_t fFoldFileOutput;  ! If true: generate output file for each fold ;  ; Bool_t fFoldStatus;  ! If true: dataset is prepared ;  ; TString fJobName;  ; UInt_t fNumFolds;  ! Number of folds to prepare ;  ; UInt_t fNumWorkerProcs;  ! Number of processes to use for fold evaluation. (Default, no parallel evaluation) ;  ; TString fOutputEnsembling;  ! How to combine output of individual folds ;  ; TString fOutputFactoryOptions;  ; TFile * fOutputFile;  ; std::vector< CrossValidationResult > fResults;  ! ;  ; Bool_t fROC;  ; Bool_t fSilent;  ; std::unique_ptr< CvSplitKFolds > fSplit;  ; TString fSplitExprString;  ; TString fSplitTypeStr;  ; TString fTransformations;  ; Bool_t fVerbose;  ; TString fVerboseLevel;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Envelope;  Envelope (const TString &name, DataLoader *dataloader=nullptr, TFile *file=nullptr, const TString options="""");  Constructor for the initialization of Envelopes, differents Envelopes may needs differents constructors then this is a generic one protected. ;  ; DataInputHandler & GetDataLoader",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:16929,Modifiability,inherit,inherited,16929,";  ; TString fSplitExprString;  ; TString fSplitTypeStr;  ; TString fTransformations;  ; Bool_t fVerbose;  ; TString fVerboseLevel;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Envelope;  Envelope (const TString &name, DataLoader *dataloader=nullptr, TFile *file=nullptr, const TString options="""");  Constructor for the initialization of Envelopes, differents Envelopes may needs differents constructors then this is a generic one protected. ;  ; DataInputHandler & GetDataLoaderDataInput ();  Utility method to get TMVA::DataInputHandler reference from the DataLoader. ;  ; DataSetInfo & GetDataLoaderDataSetInfo ();  Utility method to get TMVA::DataSetInfo reference from the DataLoader. ;  ; DataSetManager * GetDataLoaderDataSetManager ();  Utility method to get TMVA::DataSetManager pointer from the DataLoader. ;  ; TDirectory * RootBaseDir ();  Utility method to get base dir directory from current file. ;  ; void WriteDataInformation (TMVA::DataSetInfo &fDataSetInfo, TMVA::Types::EAnalysisType fAnalysisType);  method to save Train/Test information into the output file. ;  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const; ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:17028,Modifiability,inherit,inherited,17028,";  ; TString fSplitExprString;  ; TString fSplitTypeStr;  ; TString fTransformations;  ; Bool_t fVerbose;  ; TString fVerboseLevel;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Envelope;  Envelope (const TString &name, DataLoader *dataloader=nullptr, TFile *file=nullptr, const TString options="""");  Constructor for the initialization of Envelopes, differents Envelopes may needs differents constructors then this is a generic one protected. ;  ; DataInputHandler & GetDataLoaderDataInput ();  Utility method to get TMVA::DataInputHandler reference from the DataLoader. ;  ; DataSetInfo & GetDataLoaderDataSetInfo ();  Utility method to get TMVA::DataSetInfo reference from the DataLoader. ;  ; DataSetManager * GetDataLoaderDataSetManager ();  Utility method to get TMVA::DataSetManager pointer from the DataLoader. ;  ; TDirectory * RootBaseDir ();  Utility method to get base dir directory from current file. ;  ; void WriteDataInformation (TMVA::DataSetInfo &fDataSetInfo, TMVA::Types::EAnalysisType fAnalysisType);  method to save Train/Test information into the output file. ;  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const; ",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:17986,Modifiability,inherit,inherited,17986,"ds differents constructors then this is a generic one protected. ;  ; DataInputHandler & GetDataLoaderDataInput ();  Utility method to get TMVA::DataInputHandler reference from the DataLoader. ;  ; DataSetInfo & GetDataLoaderDataSetInfo ();  Utility method to get TMVA::DataSetInfo reference from the DataLoader. ;  ; DataSetManager * GetDataLoaderDataSetManager ();  Utility method to get TMVA::DataSetManager pointer from the DataLoader. ;  ; TDirectory * RootBaseDir ();  Utility method to get base dir directory from current file. ;  ; void WriteDataInformation (TMVA::DataSetInfo &fDataSetInfo, TMVA::Types::EAnalysisType fAnalysisType);  method to save Train/Test information into the output file. ;  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protec",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:18393,Modifiability,inherit,inherited,18393,"ds differents constructors then this is a generic one protected. ;  ; DataInputHandler & GetDataLoaderDataInput ();  Utility method to get TMVA::DataInputHandler reference from the DataLoader. ;  ; DataSetInfo & GetDataLoaderDataSetInfo ();  Utility method to get TMVA::DataSetInfo reference from the DataLoader. ;  ; DataSetManager * GetDataLoaderDataSetManager ();  Utility method to get TMVA::DataSetManager pointer from the DataLoader. ;  ; TDirectory * RootBaseDir ();  Utility method to get base dir directory from current file. ;  ; void WriteDataInformation (TMVA::DataSetInfo &fDataSetInfo, TMVA::Types::EAnalysisType fAnalysisType);  method to save Train/Test information into the output file. ;  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protec",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:18598,Modifiability,inherit,inherited,18598,"ion (TMVA::DataSetInfo &fDataSetInfo, TMVA::Types::EAnalysisType fAnalysisType);  method to save Train/Test information into the output file. ;  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CrossValidation.h>. Inheritance diagram for TMVA::CrossValidation:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossValidation() [1/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TString ; options . ). explicit . Defin",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:19265,Modifiability,inherit,inherited,19265,"tions are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CrossValidation.h>. Inheritance diagram for TMVA::CrossValidation:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossValidation() [1/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TString ; options . ). explicit . Definition at line 308 of file CrossValidation.cxx. ◆ CrossValidation() [2/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TFile * ; outputFile, . TString ; options . ). explicit . Definition at line 277 of file CrossValidation.cxx. ◆ ~CrossValidation(). TMVA::CrossValidation::~CrossValidation ; (; ). default . Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossValida",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:19366,Modifiability,inherit,inherited,19366,"tions are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CrossValidation.h>. Inheritance diagram for TMVA::CrossValidation:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossValidation() [1/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TString ; options . ). explicit . Definition at line 308 of file CrossValidation.cxx. ◆ CrossValidation() [2/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TFile * ; outputFile, . TString ; options . ). explicit . Definition at line 277 of file CrossValidation.cxx. ◆ ~CrossValidation(). TMVA::CrossValidation::~CrossValidation ; (; ). default . Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossValida",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:338,Performance,perform,perform,338,". ROOT: TMVA::CrossValidation Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CrossValidation Class ReferenceTMVA. ; Class to perform cross validation, splitting the dataloader into folds. ; Use html for explicit line breaking; Markdown links? class reference?; ce->BookMethod(dataloader, options);; ce->Evaluate();; Cross-evaluation will generate a new training and a test set dynamically from from K folds. These K folds are generated by splitting the input training set. The input test set is currently ignored.; This means that when you specify your DataSet you should include all events in your training set. One way of doing this would be the following:; dataloader->AddTree( signalTree, ""cls1"" );; dataloader->AddTree( background, ""cls2"" );; dataloader->PrepareTrainingAndTestTree( """", """", ""nTest_cls1=1:nTest_cls2=1"" );. Split Expression; See CVSplit documentation? ; Definition at line 124 of file CrossValidation.h. Public Member Functions;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:1437,Performance,perform,performance,1437," ReferenceTMVA. ; Class to perform cross validation, splitting the dataloader into folds. ; Use html for explicit line breaking; Markdown links? class reference?; ce->BookMethod(dataloader, options);; ce->Evaluate();; Cross-evaluation will generate a new training and a test set dynamically from from K folds. These K folds are generated by splitting the input training set. The input test set is currently ignored.; This means that when you specify your DataSet you should include all events in your training set. One way of doing this would be the following:; dataloader->AddTree( signalTree, ""cls1"" );; dataloader->AddTree( background, ""cls2"" );; dataloader->PrepareTrainingAndTestTree( """", """", ""nTest_cls1=1:nTest_cls2=1"" );. Split Expression; See CVSplit documentation? ; Definition at line 124 of file CrossValidation.h. Public Member Functions;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle,",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:2218,Performance,perform,perform,2218,"ns;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; DataLoader * GetDataLoader ();  Method to get the pointer to TMVA::DataLoader object. ;  ; TFile * GetFile ();  Method to get the pointer to TFile object. ;  ; std::vector< OptionMap > & GetMethods ();  Method get the Booked methods in a option map object. ;  ; Bool_t HasMethod (TString methodname, TString methodtitle);  function to check methods booked ;  ; Bool_t IsModelPersistence ();  Method to see if the algorithm model is saved in xml or serialized files. ;  ; Bool_t IsSilentFile ();  Method to see if a file is available to save results. ;  ; Bool_t IsVerbose ();  Method to see if the algorithm should print extra information. ;  ; void SetDataLoader (DataLoader *dalaloader);  Method to set the pointer to TMVA::DataLoader object. ;  ; v",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:2380,Performance,perform,perform,2380,"dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; DataLoader * GetDataLoader ();  Method to get the pointer to TMVA::DataLoader object. ;  ; TFile * GetFile ();  Method to get the pointer to TFile object. ;  ; std::vector< OptionMap > & GetMethods ();  Method get the Booked methods in a option map object. ;  ; Bool_t HasMethod (TString methodname, TString methodtitle);  function to check methods booked ;  ; Bool_t IsModelPersistence ();  Method to see if the algorithm model is saved in xml or serialized files. ;  ; Bool_t IsSilentFile ();  Method to see if a file is available to save results. ;  ; Bool_t IsVerbose ();  Method to see if the algorithm should print extra information. ;  ; void SetDataLoader (DataLoader *dalaloader);  Method to set the pointer to TMVA::DataLoader object. ;  ; void SetFile (TFile *file);  Method to set the pointer to TFile object, with a writable file. ;  ; void SetModelPersistence (Bool_t status=kTRUE);  Method enable m",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:20942,Performance,perform,performance,20942,", . TMVA::DataLoader * ; dataloader, . TFile * ; outputFile, . TString ; options . ). explicit . Definition at line 277 of file CrossValidation.cxx. ◆ ~CrossValidation(). TMVA::CrossValidation::~CrossValidation ; (; ). default . Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossValidation::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CrossValidation::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::CrossValidation::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 175 of file CrossValidation.h. ◆ DeclFileName(). static const char * TMVA::CrossValidation::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 175 of file CrossValidation.h. ◆ Evaluate(). void TMVA::CrossValidation::Evaluate ; (; ). virtual . Does training, test set evaluation and performance evaluation of using cross-evalution. ; Implements TMVA::Envelope.; Definition at line 587 of file CrossValidation.cxx. ◆ GetFactory(). Factory & TMVA::CrossValidation::GetFactory ; (; ). inline . Definition at line 140 of file CrossValidation.h. ◆ GetNumFolds(). UInt_t TMVA::CrossValidation::GetNumFolds ; (; ). inline . Definition at line 137 of file CrossValidation.h. ◆ GetResults(). const std::vector< TMVA::CrossValidationResult > & TMVA::CrossValidation::GetResults ; (; ); const. Definition at line 699 of file CrossValidation.cxx. ◆ GetSplitExpr(). TString TMVA::CrossValidation::GetSplitExpr ; (; ). inline . Definition at line 138 of file CrossValidation.h. ◆ InitOptions(). void TMVA::CrossValidation::InitOptions ; (; ). Definition at line 321 of file CrossValidation.cxx. ◆ IsA(). virtual TClass * TMVA::CrossValidation::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Envelope.; Definition at line 175 of file CrossValidati",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:352,Security,validat,validation,352,". ROOT: TMVA::CrossValidation Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CrossValidation Class ReferenceTMVA. ; Class to perform cross validation, splitting the dataloader into folds. ; Use html for explicit line breaking; Markdown links? class reference?; ce->BookMethod(dataloader, options);; ce->Evaluate();; Cross-evaluation will generate a new training and a test set dynamically from from K folds. These K folds are generated by splitting the input training set. The input test set is currently ignored.; This means that when you specify your DataSet you should include all events in your training set. One way of doing this would be the following:; dataloader->AddTree( signalTree, ""cls1"" );; dataloader->AddTree( background, ""cls2"" );; dataloader->PrepareTrainingAndTestTree( """", """", ""nTest_cls1=1:nTest_cls2=1"" );. Split Expression; See CVSplit documentation? ; Definition at line 124 of file CrossValidation.h. Public Member Functions;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:6155,Security,hash,hash,6155,"ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:581,Testability,test,test,581,". ROOT: TMVA::CrossValidation Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CrossValidation Class ReferenceTMVA. ; Class to perform cross validation, splitting the dataloader into folds. ; Use html for explicit line breaking; Markdown links? class reference?; ce->BookMethod(dataloader, options);; ce->Evaluate();; Cross-evaluation will generate a new training and a test set dynamically from from K folds. These K folds are generated by splitting the input training set. The input test set is currently ignored.; This means that when you specify your DataSet you should include all events in your training set. One way of doing this would be the following:; dataloader->AddTree( signalTree, ""cls1"" );; dataloader->AddTree( background, ""cls2"" );; dataloader->PrepareTrainingAndTestTree( """", """", ""nTest_cls1=1:nTest_cls2=1"" );. Split Expression; See CVSplit documentation? ; Definition at line 124 of file CrossValidation.h. Public Member Functions;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:696,Testability,test,test,696,". ROOT: TMVA::CrossValidation Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CrossValidation Class ReferenceTMVA. ; Class to perform cross validation, splitting the dataloader into folds. ; Use html for explicit line breaking; Markdown links? class reference?; ce->BookMethod(dataloader, options);; ce->Evaluate();; Cross-evaluation will generate a new training and a test set dynamically from from K folds. These K folds are generated by splitting the input training set. The input test set is currently ignored.; This means that when you specify your DataSet you should include all events in your training set. One way of doing this would be the following:; dataloader->AddTree( signalTree, ""cls1"" );; dataloader->AddTree( background, ""cls2"" );; dataloader->PrepareTrainingAndTestTree( """", """", ""nTest_cls1=1:nTest_cls2=1"" );. Split Expression; See CVSplit documentation? ; Definition at line 124 of file CrossValidation.h. Public Member Functions;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:1413,Testability,test,test,1413," ReferenceTMVA. ; Class to perform cross validation, splitting the dataloader into folds. ; Use html for explicit line breaking; Markdown links? class reference?; ce->BookMethod(dataloader, options);; ce->Evaluate();; Cross-evaluation will generate a new training and a test set dynamically from from K folds. These K folds are generated by splitting the input training set. The input test set is currently ignored.; This means that when you specify your DataSet you should include all events in your training set. One way of doing this would be the following:; dataloader->AddTree( signalTree, ""cls1"" );; dataloader->AddTree( background, ""cls2"" );; dataloader->PrepareTrainingAndTestTree( """", """", ""nTest_cls1=1:nTest_cls2=1"" );. Split Expression; See CVSplit documentation? ; Definition at line 124 of file CrossValidation.h. Public Member Functions;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle,",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:19137,Testability,test,test,19137,"tions are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CrossValidation.h>. Inheritance diagram for TMVA::CrossValidation:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossValidation() [1/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TString ; options . ). explicit . Definition at line 308 of file CrossValidation.cxx. ◆ CrossValidation() [2/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TFile * ; outputFile, . TString ; options . ). explicit . Definition at line 277 of file CrossValidation.cxx. ◆ ~CrossValidation(). TMVA::CrossValidation::~CrossValidation ; (; ). default . Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossValida",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:19332,Testability,log,logger,19332,"tions are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Envelope; std::shared_ptr< DataLoader > fDataLoader;  ! data ;  ; std::shared_ptr< TFile > fFile;  ! file to save the results ;  ; UInt_t fJobs;  ! number of jobs to run some high level algorithm in parallel ;  ; std::vector< OptionMap > fMethods;  ! Booked method information ;  ; Bool_t fModelPersistence;  ! flag to save the trained model ;  ; Bool_t fSilentFile;  ! if true dont produce file output ;  ; TStopwatch fTimer;  ! timer to measure the time. ;  ; TString fTransformations;  ! List of transformations to test ;  ; Bool_t fVerbose;  ! flag for extra information ;  ; TProcPool fWorkers;  ! procpool object ;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CrossValidation.h>. Inheritance diagram for TMVA::CrossValidation:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CrossValidation() [1/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TString ; options . ). explicit . Definition at line 308 of file CrossValidation.cxx. ◆ CrossValidation() [2/2]. TMVA::CrossValidation::CrossValidation ; (; TString ; jobName, . TMVA::DataLoader * ; dataloader, . TFile * ; outputFile, . TString ; options . ). explicit . Definition at line 277 of file CrossValidation.cxx. ◆ ~CrossValidation(). TMVA::CrossValidation::~CrossValidation ; (; ). default . Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossValida",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:20918,Testability,test,test,20918,", . TMVA::DataLoader * ; dataloader, . TFile * ; outputFile, . TString ; options . ). explicit . Definition at line 277 of file CrossValidation.cxx. ◆ ~CrossValidation(). TMVA::CrossValidation::~CrossValidation ; (; ). default . Member Function Documentation. ◆ Class(). static TClass * TMVA::CrossValidation::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CrossValidation::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::CrossValidation::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 175 of file CrossValidation.h. ◆ DeclFileName(). static const char * TMVA::CrossValidation::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 175 of file CrossValidation.h. ◆ Evaluate(). void TMVA::CrossValidation::Evaluate ; (; ). virtual . Does training, test set evaluation and performance evaluation of using cross-evalution. ; Implements TMVA::Envelope.; Definition at line 587 of file CrossValidation.cxx. ◆ GetFactory(). Factory & TMVA::CrossValidation::GetFactory ; (; ). inline . Definition at line 140 of file CrossValidation.h. ◆ GetNumFolds(). UInt_t TMVA::CrossValidation::GetNumFolds ; (; ). inline . Definition at line 137 of file CrossValidation.h. ◆ GetResults(). const std::vector< TMVA::CrossValidationResult > & TMVA::CrossValidation::GetResults ; (; ); const. Definition at line 699 of file CrossValidation.cxx. ◆ GetSplitExpr(). TString TMVA::CrossValidation::GetSplitExpr ; (; ). inline . Definition at line 138 of file CrossValidation.h. ◆ InitOptions(). void TMVA::CrossValidation::InitOptions ; (; ). Definition at line 321 of file CrossValidation.cxx. ◆ IsA(). virtual TClass * TMVA::CrossValidation::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Envelope.; Definition at line 175 of file CrossValidati",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:22366,Testability,test,test,22366,"CrossValidation::GetResults ; (; ); const. Definition at line 699 of file CrossValidation.cxx. ◆ GetSplitExpr(). TString TMVA::CrossValidation::GetSplitExpr ; (; ). inline . Definition at line 138 of file CrossValidation.h. ◆ InitOptions(). void TMVA::CrossValidation::InitOptions ; (; ). Definition at line 321 of file CrossValidation.cxx. ◆ IsA(). virtual TClass * TMVA::CrossValidation::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Envelope.; Definition at line 175 of file CrossValidation.h. ◆ ParseOptions(). void TMVA::CrossValidation::ParseOptions ; (; ). virtual . Method to parse the internal option string. ; Reimplemented from TMVA::Envelope.; Definition at line 378 of file CrossValidation.cxx. ◆ ProcessFold(). TMVA::CrossValidationFoldResult TMVA::CrossValidation::ProcessFold ; (; UInt_t ; iFold, . const OptionMap & ; methodInfo . ). private . Evaluates each fold in turn. . Prepares train and test data sets; Trains method; Evalutes on test set; Stores the evaluation internally. Parameters. iFoldfold to evaluate ; methodInfomethod metadata . Definition at line 506 of file CrossValidation.cxx. ◆ SetNumFolds(). void TMVA::CrossValidation::SetNumFolds ; (; UInt_t ; i). Definition at line 472 of file CrossValidation.cxx. ◆ SetSplitExpr(). void TMVA::CrossValidation::SetSplitExpr ; (; TString ; splitExpr). Definition at line 485 of file CrossValidation.cxx. ◆ Streamer(). virtual void TMVA::CrossValidation::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Envelope. ◆ StreamerNVirtual(). void TMVA::CrossValidation::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 175 of file CrossValidation.h. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::CrossValidation::fAnalysisType. private . Definition at line 149 of file CrossValidation.h. ◆ fAnalysisTypeStr. TString TMVA::CrossValidation::fAnalysisType",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:22409,Testability,test,test,22409,"CrossValidation::GetResults ; (; ); const. Definition at line 699 of file CrossValidation.cxx. ◆ GetSplitExpr(). TString TMVA::CrossValidation::GetSplitExpr ; (; ). inline . Definition at line 138 of file CrossValidation.h. ◆ InitOptions(). void TMVA::CrossValidation::InitOptions ; (; ). Definition at line 321 of file CrossValidation.cxx. ◆ IsA(). virtual TClass * TMVA::CrossValidation::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Envelope.; Definition at line 175 of file CrossValidation.h. ◆ ParseOptions(). void TMVA::CrossValidation::ParseOptions ; (; ). virtual . Method to parse the internal option string. ; Reimplemented from TMVA::Envelope.; Definition at line 378 of file CrossValidation.cxx. ◆ ProcessFold(). TMVA::CrossValidationFoldResult TMVA::CrossValidation::ProcessFold ; (; UInt_t ; iFold, . const OptionMap & ; methodInfo . ). private . Evaluates each fold in turn. . Prepares train and test data sets; Trains method; Evalutes on test set; Stores the evaluation internally. Parameters. iFoldfold to evaluate ; methodInfomethod metadata . Definition at line 506 of file CrossValidation.cxx. ◆ SetNumFolds(). void TMVA::CrossValidation::SetNumFolds ; (; UInt_t ; i). Definition at line 472 of file CrossValidation.cxx. ◆ SetSplitExpr(). void TMVA::CrossValidation::SetSplitExpr ; (; TString ; splitExpr). Definition at line 485 of file CrossValidation.cxx. ◆ Streamer(). virtual void TMVA::CrossValidation::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Envelope. ◆ StreamerNVirtual(). void TMVA::CrossValidation::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 175 of file CrossValidation.h. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::CrossValidation::fAnalysisType. private . Definition at line 149 of file CrossValidation.h. ◆ fAnalysisTypeStr. TString TMVA::CrossValidation::fAnalysisType",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:2199,Usability,learn,learning,2199,"ns;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TFile *outputFile, TString options);  ;  CrossValidation (TString jobName, TMVA::DataLoader *dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; DataLoader * GetDataLoader ();  Method to get the pointer to TMVA::DataLoader object. ;  ; TFile * GetFile ();  Method to get the pointer to TFile object. ;  ; std::vector< OptionMap > & GetMethods ();  Method get the Booked methods in a option map object. ;  ; Bool_t HasMethod (TString methodname, TString methodtitle);  function to check methods booked ;  ; Bool_t IsModelPersistence ();  Method to see if the algorithm model is saved in xml or serialized files. ;  ; Bool_t IsSilentFile ();  Method to see if a file is available to save results. ;  ; Bool_t IsVerbose ();  Method to see if the algorithm should print extra information. ;  ; void SetDataLoader (DataLoader *dalaloader);  Method to set the pointer to TMVA::DataLoader object. ;  ; v",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidation.html:2361,Usability,learn,learning,2361,"dataloader, TString options);  ;  ~CrossValidation ();  ; void Evaluate ();  Does training, test set evaluation and performance evaluation of using cross-evalution. ;  ; Factory & GetFactory ();  ; UInt_t GetNumFolds ();  ; const std::vector< CrossValidationResult > & GetResults () const;  ; TString GetSplitExpr ();  ; void InitOptions ();  ; virtual TClass * IsA () const;  ; void ParseOptions ();  Method to parse the internal option string. ;  ; void SetNumFolds (UInt_t i);  ; void SetSplitExpr (TString splitExpr);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Envelope;  ~Envelope ();  Default destructor. ;  ; virtual void BookMethod (TString methodname, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; virtual void BookMethod (Types::EMVA method, TString methodtitle, TString options="""");  Method to book the machine learning method to perform the algorithm. ;  ; DataLoader * GetDataLoader ();  Method to get the pointer to TMVA::DataLoader object. ;  ; TFile * GetFile ();  Method to get the pointer to TFile object. ;  ; std::vector< OptionMap > & GetMethods ();  Method get the Booked methods in a option map object. ;  ; Bool_t HasMethod (TString methodname, TString methodtitle);  function to check methods booked ;  ; Bool_t IsModelPersistence ();  Method to see if the algorithm model is saved in xml or serialized files. ;  ; Bool_t IsSilentFile ();  Method to see if a file is available to save results. ;  ; Bool_t IsVerbose ();  Method to see if the algorithm should print extra information. ;  ; void SetDataLoader (DataLoader *dalaloader);  Method to set the pointer to TMVA::DataLoader object. ;  ; void SetFile (TFile *file);  Method to set the pointer to TFile object, with a writable file. ;  ; void SetModelPersistence (Bool_t status=kTRUE);  Method enable m",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidation.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidation.html
https://root.cern/doc/master/classTMVA_1_1CrossValidationResult.html:353,Security,validat,validation,353,". ROOT: TMVA::CrossValidationResult Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Private Member Functions |; Private Attributes |; Friends |; List of all members ; TMVA::CrossValidationResult Class ReferenceTMVA. ; Class to save the results of cross validation, the metric for the classification ins ROC and you can ROC curves ROC integrals, ROC average and ROC standard deviation. ; Definition at line 78 of file CrossValidation.h. Public Member Functions;  CrossValidationResult (const CrossValidationResult &);  ;  CrossValidationResult (UInt_t numFolds);  ;  ~CrossValidationResult ();  ; TCanvas * Draw (const TString name=""CrossValidation"") const;  ; TCanvas * DrawAvgROCCurve (Bool_t drawFolds=kFALSE, TString title="""") const;  ; TGraph * GetAvgROCCurve (UInt_t numSamples=100) const;  Generates a multigraph that contains an average ROC Curve. ;  ; std::vector< Double_t > GetEff01Values () const;  ; std::vector< Double_t > GetEff10Values () const;  ; std::vector< Double_t > GetEff30Values () const;  ; std::vector< Double_t > GetEffAreaValues () const;  ; Float_t GetROCAverage () const;  ; TMultiGraph * GetROCCurves (Bool_t fLegend=kTRUE);  ; Float_t GetROCStandardDeviation () const;  ; std::map< UInt_t, Float_t > GetROCValues () const;  ; std::vector< Double_t > GetSepValues () const;  ; std::vector< Double_t > GetSigValues () const;  ; std::vector< Double_t > GetTrainEff01Values () const;  ; std::vector< Double_t > GetTrainEff10Values () const;  ; std::vector< Double_t > GetTrainEff30Values () const;  ; void Print () const;  . Private Member Functions; void Fill (CrossValidationFoldResult const &fr);  . Private Attributes; std::vector< Double_t > fEff01s;  ; std::vector< Double_t > fEff10s;  ; std::vector< Double_t > fEff30s;  ; std::vector< Double_t > fEffAreas;  ; std::shared_ptr< TMultiGraph > fROCCurves;  ; std::map< UInt_t, Float_t > fROCs;  ; std::vector< Double_",MatchSource.WIKI,doc/master/classTMVA_1_1CrossValidationResult.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CrossValidationResult.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:5923,Availability,error,error,5923,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:6012,Availability,error,error,6012,"ecord whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:6167,Availability,error,error,6167,"Name () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:6464,Availability,error,error,6464,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:10548,Availability,error,error,10548,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constex",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:5929,Integrability,message,message,5929,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:6470,Integrability,message,message,6470,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:7460,Integrability,message,message,7460,"ssue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class shou",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:10554,Integrability,message,message,10554,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constex",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:10927,Integrability,message,message,10927," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:12619,Integrability,message,message,12619,"atic constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configu",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:977,Modifiability,inherit,inherited,977,"pes::ETreeType tt=Types::kTraining);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc="""");  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; virtual void ParseOptions ();  options parser ;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:2726,Modifiability,inherit,inherited,2726,"  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; virtual void ParseOptions ();  options parser ;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed na",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:4466,Modifiability,inherit,inherited,4466,"ide;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:5559,Modifiability,inherit,inheritance,5559,"  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:7558,Modifiability,inherit,inherits,7558,"e) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:7675,Modifiability,inherit,inherits,7675,"t be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is necessary). ;  ; virtual Bool_t Notify ();  This method must be overridden to handle object notification (the base ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:11428,Modifiability,inherit,inherited,11428," ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attri",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:11650,Modifiability,inherit,inherited,11650," ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attri",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:11860,Modifiability,inherit,inherited,11860," ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attri",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:12560,Modifiability,inherit,inherited,12560,"atic constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configu",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:12661,Modifiability,inherit,inherited,12661,"atic constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configu",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:12765,Modifiability,inherit,inherited,12765,"  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceT",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:13406,Modifiability,inherit,inherited,13406,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplit:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplit(). TMVA::CvSplit::CvSplit ; (; UInt_t ; numFolds). Definition at line 38 of file CvSplit.cxx. ◆ ~CvSplit(). virtual TMVA::CvSplit::~CvSplit ; (; ). inlinevirtual . Definition at line 40 of file CvSplit.h. Member Function Documentation. ◆ Class(). static TClass * TMVA::CvSplit::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CvSplit::Class_Name ; (; ). static ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:13505,Modifiability,inherit,inherited,13505,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplit:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplit(). TMVA::CvSplit::CvSplit ; (; UInt_t ; numFolds). Definition at line 38 of file CvSplit.cxx. ◆ ~CvSplit(). virtual TMVA::CvSplit::~CvSplit ; (; ). inlinevirtual . Definition at line 40 of file CvSplit.h. Member Function Documentation. ◆ Class(). static TClass * TMVA::CvSplit::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CvSplit::Class_Name ; (; ). static ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:13912,Modifiability,inherit,inherited,13912,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplit:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplit(). TMVA::CvSplit::CvSplit ; (; UInt_t ; numFolds). Definition at line 38 of file CvSplit.cxx. ◆ ~CvSplit(). virtual TMVA::CvSplit::~CvSplit ; (; ). inlinevirtual . Definition at line 40 of file CvSplit.h. Member Function Documentation. ◆ Class(). static TClass * TMVA::CvSplit::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CvSplit::Class_Name ; (; ). static ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:3594,Security,hash,hash,3594,"ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:664,Testability,test,test,664,". ROOT: TMVA::CvSplit Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Protected Attributes |; List of all members ; TMVA::CvSplit Class Referenceabstract. . Definition at line 37 of file CvSplit.h. Public Member Functions;  CvSplit (UInt_t numFolds);  ; virtual ~CvSplit ();  ; UInt_t GetNumFolds ();  ; virtual TClass * IsA () const;  ; virtual void MakeKFoldDataSet (DataSetInfo &dsi)=0;  ; Bool_t NeedsRebuild ();  ; virtual void PrepareFoldDataSet (DataSetInfo &dsi, UInt_t foldNumber, Types::ETreeType tt);  Set training and test set vectors of dataset described by dsi. ;  ; virtual void RecombineKFoldDataSet (DataSetInfo &dsi, Types::ETreeType tt=Types::kTraining);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc="""");  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOption",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:12627,Testability,log,logger,12627,"atic constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Attributes; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configu",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:15893,Testability,test,test,15893,"line 57 of file CvSplit.h. ◆ DeclFileName(). static const char * TMVA::CvSplit::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 57 of file CvSplit.h. ◆ GetNumFolds(). UInt_t TMVA::CvSplit::GetNumFolds ; (; ). inline . Definition at line 46 of file CvSplit.h. ◆ IsA(). virtual TClass * TMVA::CvSplit::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Configurable.; Reimplemented in TMVA::CvSplitKFolds.; Definition at line 57 of file CvSplit.h. ◆ MakeKFoldDataSet(). virtual void TMVA::CvSplit::MakeKFoldDataSet ; (; DataSetInfo & ; dsi). pure virtual . Implemented in TMVA::CvSplitKFolds. ◆ NeedsRebuild(). Bool_t TMVA::CvSplit::NeedsRebuild ; (; ). inline . Definition at line 47 of file CvSplit.h. ◆ PrepareFoldDataSet(). void TMVA::CvSplit::PrepareFoldDataSet ; (; DataSetInfo & ; dsi, . UInt_t ; foldNumber, . Types::ETreeType ; tt . ). virtual . Set training and test set vectors of dataset described by dsi. ; Parameters. [in]dsiDataSetInfo for data set to be split ; [in]foldNumberOrdinal of fold to prepare ; [in]ttThe set used to prepare fold. If equal to Types::kTraining splitting will be based off the original train set. If instead equal to Types::kTesting the test set will be used. The original training/test set is the set as defined by DataLoader::PrepareTrainingAndTestSet. Sets the training and test set vectors of the DataSet described by dsi as defined by the split. If tt is eqal to Types::kTraining the split will be based off of the original training set.; Note: Requires MakeKFoldDataSet to have been called first. ; Definition at line 57 of file CvSplit.cxx. ◆ RecombineKFoldDataSet(). void TMVA::CvSplit::RecombineKFoldDataSet ; (; DataSetInfo & ; dsi, . Types::ETreeType ; tt = Types::kTraining . ). virtual . Definition at line 114 of file CvSplit.cxx. ◆ Streamer(). virtual void TMVA::CvSplit::Streamer ; (; TBuffer & ; R__b). virtual . Stream an objec",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:16199,Testability,test,test,16199," Definition at line 46 of file CvSplit.h. ◆ IsA(). virtual TClass * TMVA::CvSplit::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Configurable.; Reimplemented in TMVA::CvSplitKFolds.; Definition at line 57 of file CvSplit.h. ◆ MakeKFoldDataSet(). virtual void TMVA::CvSplit::MakeKFoldDataSet ; (; DataSetInfo & ; dsi). pure virtual . Implemented in TMVA::CvSplitKFolds. ◆ NeedsRebuild(). Bool_t TMVA::CvSplit::NeedsRebuild ; (; ). inline . Definition at line 47 of file CvSplit.h. ◆ PrepareFoldDataSet(). void TMVA::CvSplit::PrepareFoldDataSet ; (; DataSetInfo & ; dsi, . UInt_t ; foldNumber, . Types::ETreeType ; tt . ). virtual . Set training and test set vectors of dataset described by dsi. ; Parameters. [in]dsiDataSetInfo for data set to be split ; [in]foldNumberOrdinal of fold to prepare ; [in]ttThe set used to prepare fold. If equal to Types::kTraining splitting will be based off the original train set. If instead equal to Types::kTesting the test set will be used. The original training/test set is the set as defined by DataLoader::PrepareTrainingAndTestSet. Sets the training and test set vectors of the DataSet described by dsi as defined by the split. If tt is eqal to Types::kTraining the split will be based off of the original training set.; Note: Requires MakeKFoldDataSet to have been called first. ; Definition at line 57 of file CvSplit.cxx. ◆ RecombineKFoldDataSet(). void TMVA::CvSplit::RecombineKFoldDataSet ; (; DataSetInfo & ; dsi, . Types::ETreeType ; tt = Types::kTraining . ). virtual . Definition at line 114 of file CvSplit.cxx. ◆ Streamer(). virtual void TMVA::CvSplit::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable.; Reimplemented in TMVA::CvSplitKFolds. ◆ StreamerNVirtual(). void TMVA::CvSplit::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 57 of file CvSplit.h. Member Data Documentation. ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:16244,Testability,test,test,16244,"it::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Configurable.; Reimplemented in TMVA::CvSplitKFolds.; Definition at line 57 of file CvSplit.h. ◆ MakeKFoldDataSet(). virtual void TMVA::CvSplit::MakeKFoldDataSet ; (; DataSetInfo & ; dsi). pure virtual . Implemented in TMVA::CvSplitKFolds. ◆ NeedsRebuild(). Bool_t TMVA::CvSplit::NeedsRebuild ; (; ). inline . Definition at line 47 of file CvSplit.h. ◆ PrepareFoldDataSet(). void TMVA::CvSplit::PrepareFoldDataSet ; (; DataSetInfo & ; dsi, . UInt_t ; foldNumber, . Types::ETreeType ; tt . ). virtual . Set training and test set vectors of dataset described by dsi. ; Parameters. [in]dsiDataSetInfo for data set to be split ; [in]foldNumberOrdinal of fold to prepare ; [in]ttThe set used to prepare fold. If equal to Types::kTraining splitting will be based off the original train set. If instead equal to Types::kTesting the test set will be used. The original training/test set is the set as defined by DataLoader::PrepareTrainingAndTestSet. Sets the training and test set vectors of the DataSet described by dsi as defined by the split. If tt is eqal to Types::kTraining the split will be based off of the original training set.; Note: Requires MakeKFoldDataSet to have been called first. ; Definition at line 57 of file CvSplit.cxx. ◆ RecombineKFoldDataSet(). void TMVA::CvSplit::RecombineKFoldDataSet ; (; DataSetInfo & ; dsi, . Types::ETreeType ; tt = Types::kTraining . ). virtual . Definition at line 114 of file CvSplit.cxx. ◆ Streamer(). virtual void TMVA::CvSplit::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable.; Reimplemented in TMVA::CvSplitKFolds. ◆ StreamerNVirtual(). void TMVA::CvSplit::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 57 of file CvSplit.h. Member Data Documentation. ◆ fMakeFoldDataSet. Bool_t TMVA::CvSplit::fMakeFoldDataSet. protected . Definit",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplit.html:16339,Testability,test,test,16339,"rom TMVA::Configurable.; Reimplemented in TMVA::CvSplitKFolds.; Definition at line 57 of file CvSplit.h. ◆ MakeKFoldDataSet(). virtual void TMVA::CvSplit::MakeKFoldDataSet ; (; DataSetInfo & ; dsi). pure virtual . Implemented in TMVA::CvSplitKFolds. ◆ NeedsRebuild(). Bool_t TMVA::CvSplit::NeedsRebuild ; (; ). inline . Definition at line 47 of file CvSplit.h. ◆ PrepareFoldDataSet(). void TMVA::CvSplit::PrepareFoldDataSet ; (; DataSetInfo & ; dsi, . UInt_t ; foldNumber, . Types::ETreeType ; tt . ). virtual . Set training and test set vectors of dataset described by dsi. ; Parameters. [in]dsiDataSetInfo for data set to be split ; [in]foldNumberOrdinal of fold to prepare ; [in]ttThe set used to prepare fold. If equal to Types::kTraining splitting will be based off the original train set. If instead equal to Types::kTesting the test set will be used. The original training/test set is the set as defined by DataLoader::PrepareTrainingAndTestSet. Sets the training and test set vectors of the DataSet described by dsi as defined by the split. If tt is eqal to Types::kTraining the split will be based off of the original training set.; Note: Requires MakeKFoldDataSet to have been called first. ; Definition at line 57 of file CvSplit.cxx. ◆ RecombineKFoldDataSet(). void TMVA::CvSplit::RecombineKFoldDataSet ; (; DataSetInfo & ; dsi, . Types::ETreeType ; tt = Types::kTraining . ). virtual . Definition at line 114 of file CvSplit.cxx. ◆ Streamer(). virtual void TMVA::CvSplit::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable.; Reimplemented in TMVA::CvSplitKFolds. ◆ StreamerNVirtual(). void TMVA::CvSplit::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 57 of file CvSplit.h. Member Data Documentation. ◆ fMakeFoldDataSet. Bool_t TMVA::CvSplit::fMakeFoldDataSet. protected . Definition at line 51 of file CvSplit.h. ◆ fNumFolds. UInt_t TMVA::CvSplit::fNumFolds. protected . Def",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplit.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplit.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:6320,Availability,error,error,6320,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:6409,Availability,error,error,6409,"ecord whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:6564,Availability,error,error,6564,"Name () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:6861,Availability,error,error,6861,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:10945,Availability,error,error,10945,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::CvSplit; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Ve",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:6326,Integrability,message,message,6326,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:6867,Integrability,message,message,6867,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:7857,Integrability,message,message,7857,"ssue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class shou",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:10951,Integrability,message,message,10951,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::CvSplit; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Ve",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:11324,Integrability,message,message,11324," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::CvSplit; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:15367,Integrability,message,message,15367,"nnotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds(). TMVA::CvSplitKFolds::CvSplitKFolds ; (; UInt_t ; numFolds, . TString ; splitExpr = """", . Bool_t ; stratified = kTRUE, . UInt_t ; seed = 100 . ). Splits a dataset into k folds, ready for use in cross validation. ; Parameters. [in]numFoldsNumber of folds to split data into ; [in]stratifiedIf true, use stratified splitting, balancing the number of events across classes and folds. If false, no such balancing is done. For ; [in]splitExprExpression used to split data into folds. If """" a random assignment will be done. Otherwise the expression is fed into ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:889,Modifiability,inherit,inherited,889,". ROOT: TMVA::CvSplitKFolds Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CvSplitKFolds Class Reference. . Definition at line 92 of file CvSplit.h. Public Member Functions;  CvSplitKFolds (UInt_t numFolds, TString splitExpr="""", Bool_t stratified=kTRUE, UInt_t seed=100);  Splits a dataset into k folds, ready for use in cross validation. ;  ;  ~CvSplitKFolds () override;  ; TClass * IsA () const override;  ; void MakeKFoldDataSet (DataSetInfo &dsi) override;  Prepares a DataSet for cross validation. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::CvSplit;  CvSplit (UInt_t numFolds);  ; virtual ~CvSplit ();  ; UInt_t GetNumFolds ();  ; Bool_t NeedsRebuild ();  ; virtual void PrepareFoldDataSet (DataSetInfo &dsi, UInt_t foldNumber, Types::ETreeType tt);  Set training and test set vectors of dataset described by dsi. ;  ; virtual void RecombineKFoldDataSet (DataSetInfo &dsi, Types::ETreeType tt=Types::kTraining);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TStri",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:1374,Modifiability,inherit,inherited,1374,"Set training and test set vectors of dataset described by dsi. ;  ; virtual void RecombineKFoldDataSet (DataSetInfo &dsi, Types::ETreeType tt=Types::kTraining);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc="""");  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; virtual void ParseOptions ();  options parser ;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functio",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:3123,Modifiability,inherit,inherited,3123,"  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; virtual void ParseOptions ();  options parser ;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed na",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:4863,Modifiability,inherit,inherited,4863,"ide;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:5956,Modifiability,inherit,inheritance,5956,"  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:7955,Modifiability,inherit,inherits,7955,"e) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:8072,Modifiability,inherit,inherits,8072,"t be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is necessary). ;  ; virtual Bool_t Notify ();  This method must be overridden to handle object notification (the base ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:11825,Modifiability,inherit,inherited,11825,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::CvSplit; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; std::vector< UInt_t > GetEventIndexToFoldMapping (UInt_t nEntries, UInt_t numFolds, UInt_t seed=100);  Generates a vector of fold assignments. ;  ; std::vector< std::vector< ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:12042,Modifiability,inherit,inherited,12042,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::CvSplit; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; std::vector< UInt_t > GetEventIndexToFoldMapping (UInt_t nEntries, UInt_t numFolds, UInt_t seed=100);  Generates a vector of fold assignments. ;  ; std::vector< std::vector< ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:12264,Modifiability,inherit,inherited,12264,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::CvSplit; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; std::vector< UInt_t > GetEventIndexToFoldMapping (UInt_t nEntries, UInt_t numFolds, UInt_t seed=100);  Generates a vector of fold assignments. ;  ; std::vector< std::vector< ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:12474,Modifiability,inherit,inherited,12474,"ither TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::CvSplit; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; std::vector< UInt_t > GetEventIndexToFoldMapping (UInt_t nEntries, UInt_t numFolds, UInt_t seed=100);  Generates a vector of fold assignments. ;  ; std::vector< std::vector< ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:13734,Modifiability,inherit,inherited,13734,"c void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; std::vector< UInt_t > GetEventIndexToFoldMapping (UInt_t nEntries, UInt_t numFolds, UInt_t seed=100);  Generates a vector of fold assignments. ;  ; std::vector< std::vector< Event * > > SplitSets (std::vector< TMVA::Event * > &oldSet, UInt_t numFolds, UInt_t numClasses);  Split sets for into k-folds. ;  . Private Attributes; friend CrossValidation;  ; std::map< const TMVA::Event *, UInt_t > fEventToFoldMapping;  ; UInt_t fSeed;  ; std::unique_ptr< CvSplitKFoldsExpr > fSplitExpr;  ; TString fSplitExprString;  ! Expression used to split data into folds. Should output values between 0 and numFolds. ;  ; Bool_t fStratified;  If true, use stratified split. (Balance class presence in each fold). ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceT",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:14375,Modifiability,inherit,inherited,14375,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:14474,Modifiability,inherit,inherited,14474,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:14881,Modifiability,inherit,inherited,14881,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:15086,Modifiability,inherit,inherited,15086,"nnotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds(). TMVA::CvSplitKFolds::CvSplitKFolds ; (; UInt_t ; numFolds, . TString ; splitExpr = """", . Bool_t ; stratified = kTRUE, . UInt_t ; seed = 100 . ). Splits a dataset into k folds, ready for use in cross validation. ; Parameters. [in]numFoldsNumber of folds to split data into ; [in]stratifiedIf true, use stratified splitting, balancing the number of events across classes and folds. If false, no such balancing is done. For ; [in]splitExprExpression used to split data into folds. If """" a random assignment will be done. Otherwise the expression is fed into ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:15308,Modifiability,inherit,inherited,15308,"nnotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds(). TMVA::CvSplitKFolds::CvSplitKFolds ; (; UInt_t ; numFolds, . TString ; splitExpr = """", . Bool_t ; stratified = kTRUE, . UInt_t ; seed = 100 . ). Splits a dataset into k folds, ready for use in cross validation. ; Parameters. [in]numFoldsNumber of folds to split data into ; [in]stratifiedIf true, use stratified splitting, balancing the number of events across classes and folds. If false, no such balancing is done. For ; [in]splitExprExpression used to split data into folds. If """" a random assignment will be done. Otherwise the expression is fed into ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:15409,Modifiability,inherit,inherited,15409,"nnotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds(). TMVA::CvSplitKFolds::CvSplitKFolds ; (; UInt_t ; numFolds, . TString ; splitExpr = """", . Bool_t ; stratified = kTRUE, . UInt_t ; seed = 100 . ). Splits a dataset into k folds, ready for use in cross validation. ; Parameters. [in]numFoldsNumber of folds to split data into ; [in]stratifiedIf true, use stratified splitting, balancing the number of events across classes and folds. If false, no such balancing is done. For ; [in]splitExprExpression used to split data into folds. If """" a random assignment will be done. Otherwise the expression is fed into ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:540,Security,validat,validation,540,". ROOT: TMVA::CvSplitKFolds Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CvSplitKFolds Class Reference. . Definition at line 92 of file CvSplit.h. Public Member Functions;  CvSplitKFolds (UInt_t numFolds, TString splitExpr="""", Bool_t stratified=kTRUE, UInt_t seed=100);  Splits a dataset into k folds, ready for use in cross validation. ;  ;  ~CvSplitKFolds () override;  ; TClass * IsA () const override;  ; void MakeKFoldDataSet (DataSetInfo &dsi) override;  Prepares a DataSet for cross validation. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::CvSplit;  CvSplit (UInt_t numFolds);  ; virtual ~CvSplit ();  ; UInt_t GetNumFolds ();  ; Bool_t NeedsRebuild ();  ; virtual void PrepareFoldDataSet (DataSetInfo &dsi, UInt_t foldNumber, Types::ETreeType tt);  Set training and test set vectors of dataset described by dsi. ;  ; virtual void RecombineKFoldDataSet (DataSetInfo &dsi, Types::ETreeType tt=Types::kTraining);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TStri",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:705,Security,validat,validation,705,". ROOT: TMVA::CvSplitKFolds Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CvSplitKFolds Class Reference. . Definition at line 92 of file CvSplit.h. Public Member Functions;  CvSplitKFolds (UInt_t numFolds, TString splitExpr="""", Bool_t stratified=kTRUE, UInt_t seed=100);  Splits a dataset into k folds, ready for use in cross validation. ;  ;  ~CvSplitKFolds () override;  ; TClass * IsA () const override;  ; void MakeKFoldDataSet (DataSetInfo &dsi) override;  Prepares a DataSet for cross validation. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::CvSplit;  CvSplit (UInt_t numFolds);  ; virtual ~CvSplit ();  ; UInt_t GetNumFolds ();  ; Bool_t NeedsRebuild ();  ; virtual void PrepareFoldDataSet (DataSetInfo &dsi, UInt_t foldNumber, Types::ETreeType tt);  Set training and test set vectors of dataset described by dsi. ;  ; virtual void RecombineKFoldDataSet (DataSetInfo &dsi, Types::ETreeType tt=Types::kTraining);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TStri",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:3991,Security,hash,hash,3991,"ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:15896,Security,validat,validation,15896,"ctions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds(). TMVA::CvSplitKFolds::CvSplitKFolds ; (; UInt_t ; numFolds, . TString ; splitExpr = """", . Bool_t ; stratified = kTRUE, . UInt_t ; seed = 100 . ). Splits a dataset into k folds, ready for use in cross validation. ; Parameters. [in]numFoldsNumber of folds to split data into ; [in]stratifiedIf true, use stratified splitting, balancing the number of events across classes and folds. If false, no such balancing is done. For ; [in]splitExprExpression used to split data into folds. If """" a random assignment will be done. Otherwise the expression is fed into a TFormula and evaluated per event. The resulting value is the fold assignment. ; [in]seedUsed only when using random splitting (i.e. when splitExpr is """"). Seed is used to initialise the random number generator when assigning events to folds. . Definition at line 243 of file CvSplit.cxx. ◆ ~CvSplitKFolds(). TMVA::CvSplitKFolds::~CvSplitKFolds ; (; ). inlineoverride . Definition at line 98 of file CvSplit.h. Member Function Documentation. ◆ Class(). static TClass * TMVA::CvSplitKFolds::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::CvSplitKFolds::Class_Name ; ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:18136,Security,validat,validation,18136,"c const char * TMVA::CvSplitKFolds::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 117 of file CvSplit.h. ◆ GetEventIndexToFoldMapping(). std::vector< UInt_t > TMVA::CvSplitKFolds::GetEventIndexToFoldMapping ; (; UInt_t ; nEntries, . UInt_t ; numFolds, . UInt_t ; seed = 100 . ). private . Generates a vector of fold assignments. ; Parameters. [in]nEntriesNumber of events in range ; [in]numFoldsNumber of folds to split data into ; [in]seedRandom seed. Randomly assigns events to numFolds folds. Each fold will hold at most nEntries / numFolds + 1 events. ; Definition at line 293 of file CvSplit.cxx. ◆ IsA(). TClass * TMVA::CvSplitKFolds::IsA ; (; ); const. inlineoverridevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::CvSplit.; Definition at line 117 of file CvSplit.h. ◆ MakeKFoldDataSet(). void TMVA::CvSplitKFolds::MakeKFoldDataSet ; (; DataSetInfo & ; dsi). overridevirtual . Prepares a DataSet for cross validation. ; Implements TMVA::CvSplit.; Definition at line 255 of file CvSplit.cxx. ◆ SplitSets(). std::vector< std::vector< TMVA::Event * > > TMVA::CvSplitKFolds::SplitSets ; (; std::vector< TMVA::Event * > & ; oldSet, . UInt_t ; numFolds, . UInt_t ; numClasses . ). private . Split sets for into k-folds. ; Parameters. [in]oldSetOriginal, unsplit, events ; [in]numFoldsNumber of folds to split data into ; [in]numClassesnumber of classes to stratify into . Definition at line 320 of file CvSplit.cxx. ◆ Streamer(). void TMVA::CvSplitKFolds::Streamer ; (; TBuffer & ; R__b). overridevirtual . Stream an object of class TObject. ; Reimplemented from TMVA::CvSplit. ◆ StreamerNVirtual(). void TMVA::CvSplitKFolds::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 117 of file CvSplit.h. Member Data Documentation. ◆ CrossValidation. friend TMVA::CvSplitKFolds::CrossValidation. private . Definition at line 94 of file CvSplit.h. ◆ fEventToFoldMappin",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:1137,Testability,test,test,1137,". ROOT: TMVA::CvSplitKFolds Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; List of all members ; TMVA::CvSplitKFolds Class Reference. . Definition at line 92 of file CvSplit.h. Public Member Functions;  CvSplitKFolds (UInt_t numFolds, TString splitExpr="""", Bool_t stratified=kTRUE, UInt_t seed=100);  Splits a dataset into k folds, ready for use in cross validation. ;  ;  ~CvSplitKFolds () override;  ; TClass * IsA () const override;  ; void MakeKFoldDataSet (DataSetInfo &dsi) override;  Prepares a DataSet for cross validation. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::CvSplit;  CvSplit (UInt_t numFolds);  ; virtual ~CvSplit ();  ; UInt_t GetNumFolds ();  ; Bool_t NeedsRebuild ();  ; virtual void PrepareFoldDataSet (DataSetInfo &dsi, UInt_t foldNumber, Types::ETreeType tt);  Set training and test set vectors of dataset described by dsi. ;  ; virtual void RecombineKFoldDataSet (DataSetInfo &dsi, Types::ETreeType tt=Types::kTraining);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TStri",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html:15375,Testability,log,logger,15375,"nnotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::CvSplit; Bool_t fMakeFoldDataSet;  ; UInt_t fNumFolds;  ; std::vector< std::vector< TMVA::Event * > > fTestEvents;  ; std::vector< std::vector< TMVA::Event * > > fTrainEvents;  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/CvSplit.h>. Inheritance diagram for TMVA::CvSplitKFolds:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ CvSplitKFolds(). TMVA::CvSplitKFolds::CvSplitKFolds ; (; UInt_t ; numFolds, . TString ; splitExpr = """", . Bool_t ; stratified = kTRUE, . UInt_t ; seed = 100 . ). Splits a dataset into k folds, ready for use in cross validation. ; Parameters. [in]numFoldsNumber of folds to split data into ; [in]stratifiedIf true, use stratified splitting, balancing the number of events across classes and folds. If false, no such balancing is done. For ; [in]splitExprExpression used to split data into folds. If """" a random assignment will be done. Otherwise the expression is fed into ",MatchSource.WIKI,doc/master/classTMVA_1_1CvSplitKFolds.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1CvSplitKFolds.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:4814,Availability,error,error,4814,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Return",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:4903,Availability,error,error,4903,"ar *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns strin",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:5058,Availability,error,error,5058,";  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title o",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:5355,Availability,error,error,5355,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:9919,Availability,error,error,9919," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:4820,Integrability,message,message,4820,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Return",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:5361,Integrability,message,message,5361,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:6563,Integrability,message,message,6563,"ived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this metho",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:9925,Integrability,message,message,9925," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:10298,Integrability,message,message,10298," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Priva",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:11854,Integrability,message,message,11854,"n_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; UInt_t GetEntries (const std::vector< TreeInfo > &tiV) const;  return number of entries in tree ;  ; MsgLogger & Log () const;  ; TTree * ReadInputTree (const TString &dataFile);  create trees from these ascii files ;  . Private Attributes; std::map< std::string, Bool_t > fExplicitTrainTest;  if set to true the user has specified training and testing data explicitly ;  ; std::map< TString, std::vector< TreeInfo > > fInputTrees;  list of input trees per class (classname is given as first parameter in the map) ;  ; MsgLogger * fLogger;  ! message logger ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:20820,Integrability,message,message,20820,"ataInputHandler.h. ◆ ReadInputTree(). TTree * TMVA::DataInputHandler::ReadInputTree ; (; const TString & ; dataFile). private . create trees from these ascii files ; Definition at line 152 of file DataInputHandler.cxx. ◆ Sbegin(). std::vector< TreeInfo >::const_iterator TMVA::DataInputHandler::Sbegin ; (; ); const. inline . Definition at line 113 of file DataInputHandler.h. ◆ Send(). std::vector< TreeInfo >::const_iterator TMVA::DataInputHandler::Send ; (; ); const. inline . Definition at line 114 of file DataInputHandler.h. ◆ SignalTreeInfo(). const TreeInfo & TMVA::DataInputHandler::SignalTreeInfo ; (; Int_t ; i); const. inline . Definition at line 108 of file DataInputHandler.h. ◆ Streamer(). virtual void TMVA::DataInputHandler::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataInputHandler::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 134 of file DataInputHandler.h. Member Data Documentation. ◆ fExplicitTrainTest. std::map< std::string, Bool_t > TMVA::DataInputHandler::fExplicitTrainTest. private . if set to true the user has specified training and testing data explicitly ; Definition at line 130 of file DataInputHandler.h. ◆ fInputTrees. std::map< TString, std::vector<TreeInfo> > TMVA::DataInputHandler::fInputTrees. mutableprivate . list of input trees per class (classname is given as first parameter in the map) ; Definition at line 129 of file DataInputHandler.h. ◆ fLogger. MsgLogger* TMVA::DataInputHandler::fLogger. mutableprivate . ! message logger ; Definition at line 131 of file DataInputHandler.h. Libraries for TMVA::DataInputHandler:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DataInputHandler.h; tmva/tmva/src/DataInputHandler.cxx. TMVADataInputHandler. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:3053,Modifiability,inherit,inherited,3053,"className) const;  ; std::vector< TreeInfo >::const_iterator Bend () const;  ; void ClearBackgroundTreeList ();  ; void ClearSignalTreeList ();  ; void ClearTreeList (const TString &className);  ; std::vector< TreeInfo >::const_iterator end (const TString &className) const;  ; UInt_t GetBackgroundEntries () const;  ; std::vector< TString > * GetClassList () const;  ; UInt_t GetEntries () const;  return number of entries in tree ;  ; UInt_t GetEntries (const TString &name) const;  ; UInt_t GetNBackgroundTrees () const;  ; UInt_t GetNSignalTrees () const;  ; UInt_t GetNTrees (const TString &name) const;  ; UInt_t GetSignalEntries () const;  ; virtual TClass * IsA () const;  ; std::vector< TreeInfo >::const_iterator Sbegin () const;  ; std::vector< TreeInfo >::const_iterator Send () const;  ; const TreeInfo & SignalTreeInfo (Int_t i) const;  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) c",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:4450,Modifiability,inherit,inheritance,4450,"pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:6661,Modifiability,inherit,inherits,6661,"ed by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a ",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:6778,Modifiability,inherit,inherits,6778,"ame of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its p",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:10799,Modifiability,inherit,inherited,10799,"reamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; UInt_t GetEntries (const std::vector< TreeInfo > &tiV) const;  return number of entries in tree ;  ; MsgLogger & Log () const;  ; TTree * ReadInputTree (const TString &dataFile);  create trees from these ascii files ;  . Private Attributes; std::map< std::string, Bool_t > fExplicitTrainTest;  if set to true the user has specified training and testing data explicitly ;  ; std::map< TString, std::vector< TreeInfo > > fInputTrees;  list of input trees per class (classname is given as first paramet",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:11918,Modifiability,inherit,inherited,11918,"atic Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; UInt_t GetEntries (const std::vector< TreeInfo > &tiV) const;  return number of entries in tree ;  ; MsgLogger & Log () const;  ; TTree * ReadInputTree (const TString &dataFile);  create trees from these ascii files ;  . Private Attributes; std::map< std::string, Bool_t > fExplicitTrainTest;  if set to true the user has specified training and testing data explicitly ;  ; std::map< TString, std::vector< TreeInfo > > fInputTrees;  list of input trees per class (classname is given as first parameter in the map) ;  ; MsgLogger * fLogger;  ! message logger ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataInputHandler.h>. Inheritance diagram for TMVA::DataInputHandler:. This browser is not able to show SVG: try Firefox, Chro",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:12559,Modifiability,inherit,inherited,12559,"icitly ;  ; std::map< TString, std::vector< TreeInfo > > fInputTrees;  list of input trees per class (classname is given as first parameter in the map) ;  ; MsgLogger * fLogger;  ! message logger ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataInputHandler.h>. Inheritance diagram for TMVA::DataInputHandler:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DataInputHandler(). TMVA::DataInputHandler::DataInputHandler ; (; ). constructor ; Definition at line 51 of file DataInputHandler.cxx. ◆ ~DataInputHandler(). TMVA::DataInputHandler::~DataInputHandler ; (; ). destructor ; Definition at line 60 of file DataInputHandler.cxx. Member Function Documentation. ◆ AddBackgroundTree() [1/2]. void TMVA::DataInputHandler::AddBackgroundTree ; (; const TString & ; tr, . Double_t ; weight = 1.0, . Types::ETreeType ; tt = Types::kMaxTreeType . ). add a background tree to the dataset to be used as input ; Definition at line 142 of file DataInputHandler.cxx. ◆ AddBackgrou",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:12658,Modifiability,inherit,inherited,12658,"icitly ;  ; std::map< TString, std::vector< TreeInfo > > fInputTrees;  list of input trees per class (classname is given as first parameter in the map) ;  ; MsgLogger * fLogger;  ! message logger ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataInputHandler.h>. Inheritance diagram for TMVA::DataInputHandler:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DataInputHandler(). TMVA::DataInputHandler::DataInputHandler ; (; ). constructor ; Definition at line 51 of file DataInputHandler.cxx. ◆ ~DataInputHandler(). TMVA::DataInputHandler::~DataInputHandler ; (; ). destructor ; Definition at line 60 of file DataInputHandler.cxx. Member Function Documentation. ◆ AddBackgroundTree() [1/2]. void TMVA::DataInputHandler::AddBackgroundTree ; (; const TString & ; tr, . Double_t ; weight = 1.0, . Types::ETreeType ; tt = Types::kMaxTreeType . ). add a background tree to the dataset to be used as input ; Definition at line 142 of file DataInputHandler.cxx. ◆ AddBackgrou",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:6292,Security,hash,hash,6292," Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:11656,Testability,test,testing,11656,"n_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; UInt_t GetEntries (const std::vector< TreeInfo > &tiV) const;  return number of entries in tree ;  ; MsgLogger & Log () const;  ; TTree * ReadInputTree (const TString &dataFile);  create trees from these ascii files ;  . Private Attributes; std::map< std::string, Bool_t > fExplicitTrainTest;  if set to true the user has specified training and testing data explicitly ;  ; std::map< TString, std::vector< TreeInfo > > fInputTrees;  list of input trees per class (classname is given as first parameter in the map) ;  ; MsgLogger * fLogger;  ! message logger ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:11862,Testability,log,logger,11862,"n_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; UInt_t GetEntries (const std::vector< TreeInfo > &tiV) const;  return number of entries in tree ;  ; MsgLogger & Log () const;  ; TTree * ReadInputTree (const TString &dataFile);  create trees from these ascii files ;  . Private Attributes; std::map< std::string, Bool_t > fExplicitTrainTest;  if set to true the user has specified training and testing data explicitly ;  ; std::map< TString, std::vector< TreeInfo > > fInputTrees;  list of input trees per class (classname is given as first parameter in the map) ;  ; MsgLogger * fLogger;  ! message logger ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:20423,Testability,test,testing,20423,"ataInputHandler.h. ◆ ReadInputTree(). TTree * TMVA::DataInputHandler::ReadInputTree ; (; const TString & ; dataFile). private . create trees from these ascii files ; Definition at line 152 of file DataInputHandler.cxx. ◆ Sbegin(). std::vector< TreeInfo >::const_iterator TMVA::DataInputHandler::Sbegin ; (; ); const. inline . Definition at line 113 of file DataInputHandler.h. ◆ Send(). std::vector< TreeInfo >::const_iterator TMVA::DataInputHandler::Send ; (; ); const. inline . Definition at line 114 of file DataInputHandler.h. ◆ SignalTreeInfo(). const TreeInfo & TMVA::DataInputHandler::SignalTreeInfo ; (; Int_t ; i); const. inline . Definition at line 108 of file DataInputHandler.h. ◆ Streamer(). virtual void TMVA::DataInputHandler::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataInputHandler::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 134 of file DataInputHandler.h. Member Data Documentation. ◆ fExplicitTrainTest. std::map< std::string, Bool_t > TMVA::DataInputHandler::fExplicitTrainTest. private . if set to true the user has specified training and testing data explicitly ; Definition at line 130 of file DataInputHandler.h. ◆ fInputTrees. std::map< TString, std::vector<TreeInfo> > TMVA::DataInputHandler::fInputTrees. mutableprivate . list of input trees per class (classname is given as first parameter in the map) ; Definition at line 129 of file DataInputHandler.h. ◆ fLogger. MsgLogger* TMVA::DataInputHandler::fLogger. mutableprivate . ! message logger ; Definition at line 131 of file DataInputHandler.h. Libraries for TMVA::DataInputHandler:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DataInputHandler.h; tmva/tmva/src/DataInputHandler.cxx. TMVADataInputHandler. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html:20828,Testability,log,logger,20828,"ataInputHandler.h. ◆ ReadInputTree(). TTree * TMVA::DataInputHandler::ReadInputTree ; (; const TString & ; dataFile). private . create trees from these ascii files ; Definition at line 152 of file DataInputHandler.cxx. ◆ Sbegin(). std::vector< TreeInfo >::const_iterator TMVA::DataInputHandler::Sbegin ; (; ); const. inline . Definition at line 113 of file DataInputHandler.h. ◆ Send(). std::vector< TreeInfo >::const_iterator TMVA::DataInputHandler::Send ; (; ); const. inline . Definition at line 114 of file DataInputHandler.h. ◆ SignalTreeInfo(). const TreeInfo & TMVA::DataInputHandler::SignalTreeInfo ; (; Int_t ; i); const. inline . Definition at line 108 of file DataInputHandler.h. ◆ Streamer(). virtual void TMVA::DataInputHandler::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataInputHandler::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 134 of file DataInputHandler.h. Member Data Documentation. ◆ fExplicitTrainTest. std::map< std::string, Bool_t > TMVA::DataInputHandler::fExplicitTrainTest. private . if set to true the user has specified training and testing data explicitly ; Definition at line 130 of file DataInputHandler.h. ◆ fInputTrees. std::map< TString, std::vector<TreeInfo> > TMVA::DataInputHandler::fInputTrees. mutableprivate . list of input trees per class (classname is given as first parameter in the map) ; Definition at line 129 of file DataInputHandler.h. ◆ fLogger. MsgLogger* TMVA::DataInputHandler::fLogger. mutableprivate . ! message logger ; Definition at line 131 of file DataInputHandler.h. Libraries for TMVA::DataInputHandler:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DataInputHandler.h; tmva/tmva/src/DataInputHandler.cxx. TMVADataInputHandler. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:30 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DataInputHandler.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataInputHandler.html
https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html:5289,Availability,error,error,5289,"tring &desc="""")TMVA::Configurable; DeclareOptionRef(T &ref, const TString &name, const TString &desc)TMVA::Configurable; DeclareOptionRef(T *&ref, Int_t size, const TString &name, const TString &desc)TMVA::Configurable; DeclFileName()TMVA::DataLoaderinlinestatic; DefaultDataSetInfo()TMVA::DataLoaderprivate; Delete(Option_t *option="""")TObjectvirtual; DistancetoPrimitive(Int_t px, Int_t py)TObjectvirtual; DoError(int level, const char *location, const char *fmt, va_list va) constTObjectprotectedvirtual; Draw(Option_t *option="""")TObjectvirtual; DrawClass() constTObjectvirtual; DrawClone(Option_t *option="""") constTObjectvirtual; Dump() constTObjectvirtual; EDeprecatedStatusBits enum nameTObject; EnableLooseOptions(Bool_t b=kTRUE)TMVA::Configurableinlineprotected; Error(const char *method, const char *msgfmt,...) constTObjectvirtual; EStatusBits enum nameTObject; Execute(const char *method, const char *params, Int_t *error=nullptr)TObjectvirtual; Execute(TMethod *method, TObjArray *params, Int_t *error=nullptr)TObjectvirtual; ExecuteEvent(Int_t event, Int_t px, Int_t py)TObjectvirtual; fAnalysisTypeTMVA::DataLoaderprivate; Fatal(const char *method, const char *msgfmt,...) constTObjectvirtual; fATreeEventTMVA::DataLoaderprivate; fATreeTypeTMVA::DataLoaderprivate; fATreeWeightTMVA::DataLoaderprivate; fBitsTObjectprivate; fConfigDescriptionTMVA::Configurableprivate; fDataAssignTypeTMVA::DataLoaderprivate; fDataInputHandlerTMVA::DataLoaderprivate; fDataSetManagerTMVA::DataLoaderprivate; fDefaultTrfsTMVA::DataLoaderprivate; fgDtorOnlyTObjectprivatestatic; fgObjectStatTObjectprivatestatic; FillBuffer(char *&buffer)TNamedvirtual; FindObject(const char *name) constTObjectvirtual; FindObject(const TObject *obj) constTObjectvirtual; fLastDeclaredOptionTMVA::Configurableprivate; fListOfOptionsTMVA::Configurableprivate; fLoggerTMVA::Configurablemutableprotected; fLooseOptionCheckingEnabledTMVA::Configurableprivate; fNameTNamedprotected; fOptionsTMVA::DataLoaderprivate; fReferenceFil",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader-members.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html
https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html:5370,Availability,error,error,5370,"tring &desc="""")TMVA::Configurable; DeclareOptionRef(T &ref, const TString &name, const TString &desc)TMVA::Configurable; DeclareOptionRef(T *&ref, Int_t size, const TString &name, const TString &desc)TMVA::Configurable; DeclFileName()TMVA::DataLoaderinlinestatic; DefaultDataSetInfo()TMVA::DataLoaderprivate; Delete(Option_t *option="""")TObjectvirtual; DistancetoPrimitive(Int_t px, Int_t py)TObjectvirtual; DoError(int level, const char *location, const char *fmt, va_list va) constTObjectprotectedvirtual; Draw(Option_t *option="""")TObjectvirtual; DrawClass() constTObjectvirtual; DrawClone(Option_t *option="""") constTObjectvirtual; Dump() constTObjectvirtual; EDeprecatedStatusBits enum nameTObject; EnableLooseOptions(Bool_t b=kTRUE)TMVA::Configurableinlineprotected; Error(const char *method, const char *msgfmt,...) constTObjectvirtual; EStatusBits enum nameTObject; Execute(const char *method, const char *params, Int_t *error=nullptr)TObjectvirtual; Execute(TMethod *method, TObjArray *params, Int_t *error=nullptr)TObjectvirtual; ExecuteEvent(Int_t event, Int_t px, Int_t py)TObjectvirtual; fAnalysisTypeTMVA::DataLoaderprivate; Fatal(const char *method, const char *msgfmt,...) constTObjectvirtual; fATreeEventTMVA::DataLoaderprivate; fATreeTypeTMVA::DataLoaderprivate; fATreeWeightTMVA::DataLoaderprivate; fBitsTObjectprivate; fConfigDescriptionTMVA::Configurableprivate; fDataAssignTypeTMVA::DataLoaderprivate; fDataInputHandlerTMVA::DataLoaderprivate; fDataSetManagerTMVA::DataLoaderprivate; fDefaultTrfsTMVA::DataLoaderprivate; fgDtorOnlyTObjectprivatestatic; fgObjectStatTObjectprivatestatic; FillBuffer(char *&buffer)TNamedvirtual; FindObject(const char *name) constTObjectvirtual; FindObject(const TObject *obj) constTObjectvirtual; fLastDeclaredOptionTMVA::Configurableprivate; fListOfOptionsTMVA::Configurableprivate; fLoggerTMVA::Configurablemutableprotected; fLooseOptionCheckingEnabledTMVA::Configurableprivate; fNameTNamedprotected; fOptionsTMVA::DataLoaderprivate; fReferenceFil",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader-members.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html
https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html:205,Modifiability,inherit,inherited,205,". ROOT: Member List. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. TMVA::DataLoader Member List. This is the complete list of members for TMVA::DataLoader, including all inherited members. AbstractMethod(const char *method) constTObject; AddBackgroundTestEvent(const std::vector< Double_t > &event, Double_t weight=1.0)TMVA::DataLoader; AddBackgroundTrainingEvent(const std::vector< Double_t > &event, Double_t weight=1.0)TMVA::DataLoader; AddBackgroundTree(TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)TMVA::DataLoader; AddBackgroundTree(TString datFileB, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)TMVA::DataLoader; AddBackgroundTree(TTree *background, Double_t weight, const TString &treetype)TMVA::DataLoader; AddCut(const TString &cut, const TString &className="""")TMVA::DataLoader; AddCut(const TCut &cut, const TString &className="""")TMVA::DataLoader; AddDataSet(DataSetInfo &)TMVA::DataLoader; AddDataSet(const TString &)TMVA::DataLoader; AddEvent(const TString &className, Types::ETreeType tt, const std::vector< Double_t > &event, Double_t weight)TMVA::DataLoader; AddOptionsXMLTo(void *parent) constTMVA::Configurable; AddPreDefVal(const T &)TMVA::Configurable; AddPreDefVal(const TString &optname, const T &)TMVA::Configurable; AddRegressionTarget(const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0)TMVA::DataLoaderinline; AddRegressionTree(TTree *tree, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)TMVA::DataLoaderinline; AddSignalTestEvent(const std::vector< Double_t > &event, Double_t weight=1.0)TMVA::DataLoader; AddSignalTrainingEvent(const std::vector< Double_t > &event, Double_t weight=1.0)TMVA::DataLoader; AddSignalTree(TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType)TMVA::DataLoader; AddSignalTree(TString datFileS, Double_t weight=1.0, Types::ETreeType treetype=Typ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader-members.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html
https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html:10855,Modifiability,variab,variable,10855,"pt=""SplitMode=Random:!V"")TMVA::DataLoader; Print(Option_t *option="""") const overrideTNamedvirtual; PrintOptions() constTMVA::Configurable; Read(const char *name)TObjectvirtual; ReadOptionsFromStream(std::istream &istr)TMVA::Configurable; ReadOptionsFromXML(void *node)TMVA::Configurable; RecombineKFoldDataSet(CvSplit &s, Types::ETreeType tt=Types::kTraining)TMVA::DataLoader; RecursiveRemove(TObject *obj)TObjectvirtual; ResetBit(UInt_t f)TObjectinline; ResetSetFlag()TMVA::Configurableprotected; SaveAs(const char *filename="""", Option_t *option="""") constTObjectvirtual; SavePrimitive(std::ostream &out, Option_t *option="""")TObjectvirtual; SetBackgroundTree(TTree *background, Double_t weight=1.0)TMVA::DataLoader; SetBackgroundWeightExpression(const TString &variable)TMVA::DataLoader; SetBit(UInt_t f, Bool_t set)TObject; SetBit(UInt_t f)TObjectinline; SetConfigDescription(const char *d)TMVA::Configurableinline; SetConfigName(const char *n)TMVA::Configurableinline; SetCut(const TString &cut, const TString &className="""")TMVA::DataLoader; SetCut(const TCut &cut, const TString &className="""")TMVA::DataLoader; SetDrawOption(Option_t *option="""")TObjectvirtual; SetDtorOnly(void *obj)TObjectstatic; SetInputTrees(const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0)TMVA::DataLoader; SetInputTrees(TTree *inputTree, const TCut &SigCut, const TCut &BgCut)TMVA::DataLoader; SetInputTrees(TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0)TMVA::DataLoader; SetInputTreesFromEventAssignTrees()TMVA::DataLoaderprivate; SetInputVariables(std::vector< TString > *theVariables)TMVA::DataLoader; SetMsgType(EMsgType t)TMVA::Configurableinline; SetName(const char *name)TNamedvirtual; SetNameTitle(const char *name, const char *title)TNamedvirtual; SetObjectStat(Bool_t stat)TObjectstatic; SetOptions(const TString &s)TMVA::Configurableinline; SetSignalTree(TTree *signal, Double_t weight=1.0)TMVA:",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader-members.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html
https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html:12149,Modifiability,variab,variable,12149,", Double_t backgroundWeight=1.0)TMVA::DataLoader; SetInputTrees(TTree *inputTree, const TCut &SigCut, const TCut &BgCut)TMVA::DataLoader; SetInputTrees(TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0)TMVA::DataLoader; SetInputTreesFromEventAssignTrees()TMVA::DataLoaderprivate; SetInputVariables(std::vector< TString > *theVariables)TMVA::DataLoader; SetMsgType(EMsgType t)TMVA::Configurableinline; SetName(const char *name)TNamedvirtual; SetNameTitle(const char *name, const char *title)TNamedvirtual; SetObjectStat(Bool_t stat)TObjectstatic; SetOptions(const TString &s)TMVA::Configurableinline; SetSignalTree(TTree *signal, Double_t weight=1.0)TMVA::DataLoader; SetSignalWeightExpression(const TString &variable)TMVA::DataLoader; SetTitle(const char *title="""")TNamedvirtual; SetTree(TTree *tree, const TString &className, Double_t weight)TMVA::DataLoader; SetUniqueID(UInt_t uid)TObjectvirtual; SetWeightExpression(const TString &variable, const TString &className="""")TMVA::DataLoader; Sizeof() constTNamedvirtual; SplitOptions(const TString &theOpt, TList &loo) constTMVA::Configurableprivate; Streamer(TBuffer &)TMVA::DataLoadervirtual; StreamerNVirtual(TBuffer &ClassDef_StreamerNVirtual_b)TMVA::DataLoaderinline; SysError(const char *method, const char *msgfmt,...) constTObjectvirtual; TestBit(UInt_t f) constTObjectinline; TestBits(UInt_t f) constTObjectinline; TNamed()TNamedinline; TNamed(const char *name, const char *title)TNamedinline; TNamed(const TString &name, const TString &title)TNamedinline; TNamed(const TNamed &named)TNamed; TObject()TObjectinline; TObject(const TObject &object)TObjectinline; UseCurrentStyle()TObjectvirtual; UserAssignEvents(UInt_t clIndex)TMVA::DataLoader; VarTransform(TString trafoDefinition)TMVA::DataLoader; Warning(const char *method, const char *msgfmt,...) constTObjectvirtual; Write(const char *name=nullptr, Int_t option=0, Int_t bufsize=0)TObjectvirtual; Write(const char *name=nullptr, Int_t option=0, I",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader-members.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html
https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html:12376,Modifiability,variab,variable,12376,", Double_t backgroundWeight=1.0)TMVA::DataLoader; SetInputTrees(TTree *inputTree, const TCut &SigCut, const TCut &BgCut)TMVA::DataLoader; SetInputTrees(TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0)TMVA::DataLoader; SetInputTreesFromEventAssignTrees()TMVA::DataLoaderprivate; SetInputVariables(std::vector< TString > *theVariables)TMVA::DataLoader; SetMsgType(EMsgType t)TMVA::Configurableinline; SetName(const char *name)TNamedvirtual; SetNameTitle(const char *name, const char *title)TNamedvirtual; SetObjectStat(Bool_t stat)TObjectstatic; SetOptions(const TString &s)TMVA::Configurableinline; SetSignalTree(TTree *signal, Double_t weight=1.0)TMVA::DataLoader; SetSignalWeightExpression(const TString &variable)TMVA::DataLoader; SetTitle(const char *title="""")TNamedvirtual; SetTree(TTree *tree, const TString &className, Double_t weight)TMVA::DataLoader; SetUniqueID(UInt_t uid)TObjectvirtual; SetWeightExpression(const TString &variable, const TString &className="""")TMVA::DataLoader; Sizeof() constTNamedvirtual; SplitOptions(const TString &theOpt, TList &loo) constTMVA::Configurableprivate; Streamer(TBuffer &)TMVA::DataLoadervirtual; StreamerNVirtual(TBuffer &ClassDef_StreamerNVirtual_b)TMVA::DataLoaderinline; SysError(const char *method, const char *msgfmt,...) constTObjectvirtual; TestBit(UInt_t f) constTObjectinline; TestBits(UInt_t f) constTObjectinline; TNamed()TNamedinline; TNamed(const char *name, const char *title)TNamedinline; TNamed(const TString &name, const TString &title)TNamedinline; TNamed(const TNamed &named)TNamed; TObject()TObjectinline; TObject(const TObject &object)TObjectinline; UseCurrentStyle()TObjectvirtual; UserAssignEvents(UInt_t clIndex)TMVA::DataLoader; VarTransform(TString trafoDefinition)TMVA::DataLoader; Warning(const char *method, const char *msgfmt,...) constTObjectvirtual; Write(const char *name=nullptr, Int_t option=0, Int_t bufsize=0)TObjectvirtual; Write(const char *name=nullptr, Int_t option=0, I",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader-members.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader-members.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:11995,Availability,error,error,11995,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:12084,Availability,error,error,12084,"ecord whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:12239,Availability,error,error,12239,"Name () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:12536,Availability,error,error,12536,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:16620,Availability,error,error,16620,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constex",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:12001,Integrability,message,message,12001,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:12542,Integrability,message,message,12542,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:13532,Integrability,message,message,13532,"ssue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class shou",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:16626,Integrability,message,message,16626,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constex",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:16999,Integrability,message,message,16999," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:21139,Integrability,message,message,21139,"); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataLoader.h>. Inheritance diagram for TMVA::DataLoader:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ DataAssignType. enum TMVA::DataLoader::DataAssignType. private . EnumeratorkUndefined ; kAssignTrees ; kAssignEvents . Definition at line 199 of file DataLoader.h. Constructor & Destructor Documentation. ◆ DataLoader(). TMVA::DataLoader::DataLoader ; (; TString ; thedlName = ""default""). Definition at line 80 of file DataLoader.cxx. ◆ ~DataLoader(). TMVA::DataLoader::~DataLoader ; (; void ; ). virtual . Definition at line 96 of file DataLoader.cxx. Member Function Documentation. ◆ AddBackgroundTestEvent(). void TMVA::DataLoader::AddBackgroundTestEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal training event ; Definition at line 252 of f",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:32162,Integrability,depend,depend,32162,"VA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ; NsigTest, . Int_t ; NbkgTest, . const TString & ; otherOpt = ""SplitMode=Random:!V"" . ). prepare the training and test trees ; Definition at line 602 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [3/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; Ntrain, . Int_t ; Ntest = -1 . ). prepare the training and test trees kept for backward compatibility ; Definition at line 618 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [4/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; TCut ; sigcut, . TCut ; bkgcut, . const TString & ; splitOpt . ). prepare the training and test trees ; Definition at line 644 of file DataLoader.cxx. ◆ RecombineKFoldDataSet(). void TMVA::DataLoader::RecombineKFoldDataSet ; (; CvSplit & ; s, . Types::ETreeType ; tt = Types::kTraining . ). Recombines the dataset. ; The precise semantics depend on the actual split.; Similar to the inverse operation of MakeKFoldDataSet but will differ. See documentation for each particular split for more information. ; Definition at line 683 of file DataLoader.cxx. ◆ SetBackgroundTree(). void TMVA::DataLoader::SetBackgroundTree ; (; TTree * ; background, . Double_t ; weight = 1.0 . ). Definition at line 439 of file DataLoader.cxx. ◆ SetBackgroundWeightExpression(). void TMVA::DataLoader::SetBackgroundWeightExpression ; (; const TString & ; variable). Definition at line 556 of file DataLoader.cxx. ◆ SetCut() [1/2]. void TMVA::DataLoader::SetCut ; (; const TCut & ; cut, . const TString & ; className = """" . ). Definition at line 581 of file DataLoader.cxx. ◆ SetCut() [2/2]. void TMVA::DataLoader::SetCut ; (; const TString & ; cut, . const TString & ; className = """" . ). Definition at line 575 of file DataLoader.cxx. ◆ SetInputTrees() [1/3]. void TMVA::DataLoader::SetInputTrees ; (; const TString & ; signalFileName, . const TString & ; backg",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:1538,Modifiability,variab,variables,1538,"r Functions;  DataLoader (TString thedlName=""default"");  ; virtual ~DataLoader ();  ; void AddBackgroundTestEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal training event ;  ; void AddBackgroundTrainingEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal training event ;  ; void AddBackgroundTree (TString datFileB, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  add background tree from text file ;  ; void AddBackgroundTree (TTree *background, Double_t weight, const TString &treetype);  ; void AddBackgroundTree (TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  number of signal events (used to compute significance) ;  ; void AddCut (const TCut &cut, const TString &className="""");  ; void AddCut (const TString &cut, const TString &className="""");  ; DataSetInfo & AddDataSet (const TString &);  ; DataSetInfo & AddDataSet (DataSetInfo &);  ; void AddEvent (const TString &className, Types::ETreeType tt, const std::vector< Double_t > &event, Double_t weight);  add event vector event : the order of values is: variables + targets + spectators ;  ; void AddRegressionTarget (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0);  ; void AddRegressionTree (TTree *tree, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  ; void AddSignalTestEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal testing event ;  ; void AddSignalTrainingEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal training event ;  ; void AddSignalTree (TString datFileS, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  add signal tree from text file ;  ; void AddSignalTree (TTree *signal, Double_t weight, const TString &treetype);  ; void AddSignalTree (TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  number of sig",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:3490,Modifiability,variab,variable,3490,"< Double_t > &event, Double_t weight);  add signal test event ;  ; void AddTrainingEvent (const TString &className, const std::vector< Double_t > &event, Double_t weight);  add signal training event ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight, const TCut &cut, const TString &treeType);  number of signal events (used to compute significance) ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight=1.0, const TCut &cut="""", Types::ETreeType tt=Types::kMaxTreeType);  ; void AddVariable (const TString &expression, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariable (const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariablesArray (const TString &expression, int size, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating array of variables in data set info in case input tree provides an array of values ;  ; TTree * CreateEventAssignTrees (const TString &name);  create the data assignment tree (for event-wise data assignment by user) ;  ; DataInputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and b",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:3686,Modifiability,variab,variable,3686,"< Double_t > &event, Double_t weight);  add signal test event ;  ; void AddTrainingEvent (const TString &className, const std::vector< Double_t > &event, Double_t weight);  add signal training event ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight, const TCut &cut, const TString &treeType);  number of signal events (used to compute significance) ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight=1.0, const TCut &cut="""", Types::ETreeType tt=Types::kMaxTreeType);  ; void AddVariable (const TString &expression, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariable (const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariablesArray (const TString &expression, int size, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating array of variables in data set info in case input tree provides an array of values ;  ; TTree * CreateEventAssignTrees (const TString &name);  create the data assignment tree (for event-wise data assignment by user) ;  ; DataInputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and b",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:3864,Modifiability,variab,variables,3864,"< Double_t > &event, Double_t weight);  add signal test event ;  ; void AddTrainingEvent (const TString &className, const std::vector< Double_t > &event, Double_t weight);  add signal training event ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight, const TCut &cut, const TString &treeType);  number of signal events (used to compute significance) ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight=1.0, const TCut &cut="""", Types::ETreeType tt=Types::kMaxTreeType);  ; void AddVariable (const TString &expression, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariable (const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariablesArray (const TString &expression, int size, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating array of variables in data set info in case input tree provides an array of values ;  ; TTree * CreateEventAssignTrees (const TString &name);  create the data assignment tree (for event-wise data assignment by user) ;  ; DataInputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and b",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:5583,Modifiability,variab,variable,5583,"testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and background ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t NsigTrain, Int_t NbkgTrain, Int_t NsigTest, Int_t NbkgTest, const TString &otherOpt=""SplitMode=Random:!V"");  prepare the training and test trees ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t Ntrain, Int_t Ntest=-1);  prepare the training and test trees kept for backward compatibility ;  ; void PrepareTrainingAndTestTree (TCut sigcut, TCut bkgcut, const TString &splitOpt);  prepare the training and test trees ;  ; void RecombineKFoldDataSet (CvSplit &s, Types::ETreeType tt=Types::kTraining);  Recombines the dataset. ;  ; void SetBackgroundTree (TTree *background, Double_t weight=1.0);  ; void SetBackgroundWeightExpression (const TString &variable);  ; void SetCut (const TCut &cut, const TString &className="""");  ; void SetCut (const TString &cut, const TString &className="""");  ; void SetInputTrees (const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  ; void SetInputTrees (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguished by the type identifiers: SigCut and BgCut ;  ; void SetInputTrees (TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  define the input trees for signal and background; no cuts are applied ;  ; void SetInputVariables (std::vector< TString > *theVariables);  deprecated ;  ; void SetSignalTree (TTree *signal, Double_t weight=1.0);  ; void SetSignalWeightExpression (const TString &variable);  ; void SetTree (TTree *tree, const TString &className, Double_t weight);  deprecated ;  ; void SetWeightExpression (const TString &variable, const TString &class",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:6508,Modifiability,variab,variable,6508,"(const TCut &cut, const TString &className="""");  ; void SetCut (const TString &cut, const TString &className="""");  ; void SetInputTrees (const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  ; void SetInputTrees (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguished by the type identifiers: SigCut and BgCut ;  ; void SetInputTrees (TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  define the input trees for signal and background; no cuts are applied ;  ; void SetInputVariables (std::vector< TString > *theVariables);  deprecated ;  ; void SetSignalTree (TTree *signal, Double_t weight=1.0);  ; void SetSignalWeightExpression (const TString &variable);  ; void SetTree (TTree *tree, const TString &className, Double_t weight);  deprecated ;  ; void SetWeightExpression (const TString &variable, const TString &className="""");  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UserAssignEvents (UInt_t clIndex);  ; DataLoader * VarTransform (TString trafoDefinition);  Transforms the variables and return a new DataLoader with the transformed variables. ;  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; temp",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:6651,Modifiability,variab,variable,6651,"(const TCut &cut, const TString &className="""");  ; void SetCut (const TString &cut, const TString &className="""");  ; void SetInputTrees (const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  ; void SetInputTrees (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguished by the type identifiers: SigCut and BgCut ;  ; void SetInputTrees (TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  define the input trees for signal and background; no cuts are applied ;  ; void SetInputVariables (std::vector< TString > *theVariables);  deprecated ;  ; void SetSignalTree (TTree *signal, Double_t weight=1.0);  ; void SetSignalWeightExpression (const TString &variable);  ; void SetTree (TTree *tree, const TString &className, Double_t weight);  deprecated ;  ; void SetWeightExpression (const TString &variable, const TString &className="""");  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UserAssignEvents (UInt_t clIndex);  ; DataLoader * VarTransform (TString trafoDefinition);  Transforms the variables and return a new DataLoader with the transformed variables. ;  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; temp",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:6949,Modifiability,variab,variables,6949,"s (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguished by the type identifiers: SigCut and BgCut ;  ; void SetInputTrees (TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  define the input trees for signal and background; no cuts are applied ;  ; void SetInputVariables (std::vector< TString > *theVariables);  deprecated ;  ; void SetSignalTree (TTree *signal, Double_t weight=1.0);  ; void SetSignalWeightExpression (const TString &variable);  ; void SetTree (TTree *tree, const TString &className, Double_t weight);  deprecated ;  ; void SetWeightExpression (const TString &variable, const TString &className="""");  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UserAssignEvents (UInt_t clIndex);  ; DataLoader * VarTransform (TString trafoDefinition);  Transforms the variables and return a new DataLoader with the transformed variables. ;  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc="""");  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:7008,Modifiability,variab,variables,7008,"s (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguished by the type identifiers: SigCut and BgCut ;  ; void SetInputTrees (TTree *signal, TTree *background, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  define the input trees for signal and background; no cuts are applied ;  ; void SetInputVariables (std::vector< TString > *theVariables);  deprecated ;  ; void SetSignalTree (TTree *signal, Double_t weight=1.0);  ; void SetSignalWeightExpression (const TString &variable);  ; void SetTree (TTree *tree, const TString &className, Double_t weight);  deprecated ;  ; void SetWeightExpression (const TString &variable, const TString &className="""");  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Bool_t UserAssignEvents (UInt_t clIndex);  ; DataLoader * VarTransform (TString trafoDefinition);  Transforms the variables and return a new DataLoader with the transformed variables. ;  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc="""");  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:7049,Modifiability,inherit,inherited,7049,";  ; DataLoader * VarTransform (TString trafoDefinition);  Transforms the variables and return a new DataLoader with the transformed variables. ;  ;  Public Member Functions inherited from TMVA::Configurable;  Configurable (const TString &theOption="""");  constructor ;  ; virtual ~Configurable ();  default destructor ;  ; void AddOptionsXMLTo (void *parent) const;  write options to XML file ;  ; template<class T > ; void AddPreDefVal (const T &);  ; template<class T > ; void AddPreDefVal (const TString &optname, const T &);  ; void CheckForUnusedOptions () const;  checks for unused options in option string ;  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T &ref, const TString &name, const TString &desc="""");  ; template<class T > ; TMVA::OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc);  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; virtual void ParseOptions ();  options parser ;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *t",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:8798,Modifiability,inherit,inherited,8798,"  ; template<class T > ; OptionBase * DeclareOptionRef (T *&ref, Int_t size, const TString &name, const TString &desc="""");  ; const char * GetConfigDescription () const;  ; const char * GetConfigName () const;  ; const TString & GetOptions () const;  ; MsgLogger & Log () const;  ; virtual void ParseOptions ();  options parser ;  ; void PrintOptions () const;  prints out the options set in the options string and the defaults ;  ; void ReadOptionsFromStream (std::istream &istr);  read option back from the weight file ;  ; void ReadOptionsFromXML (void *node);  ; void SetConfigDescription (const char *d);  ; void SetConfigName (const char *n);  ; void SetMsgType (EMsgType t);  ; void SetOptions (const TString &s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; void WriteOptionsToStream (std::ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed na",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:10538,Modifiability,inherit,inherited,10538,"ide;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:11631,Modifiability,inherit,inheritance,11631,"  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:13630,Modifiability,inherit,inherits,13630,"e) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:13747,Modifiability,inherit,inherits,13747,"t be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is necessary). ;  ; virtual Bool_t Notify ();  This method must be overridden to handle object notification (the base ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:17500,Modifiability,inherit,inherited,17500," ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Types; enum  DataAssignType { kUndefined = 0; , kAssignTrees; , kAssignEvents; };  . Private Member Functions; DataSetInfo & DefaultDataSetInfo ();  default creation ;  ; void SetInputTreesFromEventAssignTrees ();  assign event-wise local trees to data set ;  . Private Attributes; Types::EAnalysisTyp",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:17722,Modifiability,inherit,inherited,17722," ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Types; enum  DataAssignType { kUndefined = 0; , kAssignTrees; , kAssignEvents; };  . Private Member Functions; DataSetInfo & DefaultDataSetInfo ();  default creation ;  ; void SetInputTreesFromEventAssignTrees ();  assign event-wise local trees to data set ;  . Private Attributes; Types::EAnalysisTyp",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:17932,Modifiability,inherit,inherited,17932," ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::Configurable; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Types; enum  DataAssignType { kUndefined = 0; , kAssignTrees; , kAssignEvents; };  . Private Member Functions; DataSetInfo & DefaultDataSetInfo ();  default creation ;  ; void SetInputTreesFromEventAssignTrees ();  assign event-wise local trees to data set ;  . Private Attributes; Types::EAnalysisTyp",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:18811,Modifiability,variab,variables,18811,"ss_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Types; enum  DataAssignType { kUndefined = 0; , kAssignTrees; , kAssignEvents; };  . Private Member Functions; DataSetInfo & DefaultDataSetInfo ();  default creation ;  ; void SetInputTreesFromEventAssignTrees ();  assign event-wise local trees to data set ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  the training type ;  ; std::vector< Float_t > fATreeEvent;  event variables ;  ; Int_t fATreeType = 0;  type of event (=classIndex) ;  ; Float_t fATreeWeight = 0.0;  weight of the event ;  ; DataAssignType fDataAssignType;  flags for data assigning ;  ; DataInputHandler * fDataInputHandler;  -> ;  ; DataSetManager * fDataSetManager;  ; std::vector< TMVA::VariableTransformBase * > fDefaultTrfs;  list of transformations on default DataSet ;  ; TString fOptions;  option string given by construction (presently only ""V"") ;  ; std::vector< TTree * > fTestAssignTree;  for each class: tmp tree if user wants to assign the events directly ;  ; std::vector< TTree * > fTrainAssignTree;  for each class: tmp tree if user wants to assign the events directly ;  ; TString fTransformations;  List of transformations to test. ;  ; Bool_t fVerbose;  verbose mode ;  . Friends; void DataLoaderCopy (TMVA::DataLoader *des, TMVA::DataLoader *src);  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDel",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:19728,Modifiability,inherit,inherited,19728," fATreeEvent;  event variables ;  ; Int_t fATreeType = 0;  type of event (=classIndex) ;  ; Float_t fATreeWeight = 0.0;  weight of the event ;  ; DataAssignType fDataAssignType;  flags for data assigning ;  ; DataInputHandler * fDataInputHandler;  -> ;  ; DataSetManager * fDataSetManager;  ; std::vector< TMVA::VariableTransformBase * > fDefaultTrfs;  list of transformations on default DataSet ;  ; TString fOptions;  option string given by construction (presently only ""V"") ;  ; std::vector< TTree * > fTestAssignTree;  for each class: tmp tree if user wants to assign the events directly ;  ; std::vector< TTree * > fTrainAssignTree;  for each class: tmp tree if user wants to assign the events directly ;  ; TString fTransformations;  List of transformations to test. ;  ; Bool_t fVerbose;  verbose mode ;  . Friends; void DataLoaderCopy (TMVA::DataLoader *des, TMVA::DataLoader *src);  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceT",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:20369,Modifiability,inherit,inherited,20369,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataLoader.h>. Inheritance diagram for TMVA::DataLoader:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ DataAssignType. enum TMVA::DataLoader::DataAssignType. private . EnumeratorkUndefined ; kAssignTrees ; kAssignEvents . Definition at line 199 of file DataLoader.h. Constructor & Destructor Documentation. ◆ DataLoader(). TMVA::DataLoader::Dat",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:20468,Modifiability,inherit,inherited,20468,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataLoader.h>. Inheritance diagram for TMVA::DataLoader:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ DataAssignType. enum TMVA::DataLoader::DataAssignType. private . EnumeratorkUndefined ; kAssignTrees ; kAssignEvents . Definition at line 199 of file DataLoader.h. Constructor & Destructor Documentation. ◆ DataLoader(). TMVA::DataLoader::Dat",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:20875,Modifiability,inherit,inherited,20875,"itional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataLoader.h>. Inheritance diagram for TMVA::DataLoader:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ DataAssignType. enum TMVA::DataLoader::DataAssignType. private . EnumeratorkUndefined ; kAssignTrees ; kAssignEvents . Definition at line 199 of file DataLoader.h. Constructor & Destructor Documentation. ◆ DataLoader(). TMVA::DataLoader::Dat",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:21080,Modifiability,inherit,inherited,21080,"); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataLoader.h>. Inheritance diagram for TMVA::DataLoader:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ DataAssignType. enum TMVA::DataLoader::DataAssignType. private . EnumeratorkUndefined ; kAssignTrees ; kAssignEvents . Definition at line 199 of file DataLoader.h. Constructor & Destructor Documentation. ◆ DataLoader(). TMVA::DataLoader::DataLoader ; (; TString ; thedlName = ""default""). Definition at line 80 of file DataLoader.cxx. ◆ ~DataLoader(). TMVA::DataLoader::~DataLoader ; (; void ; ). virtual . Definition at line 96 of file DataLoader.cxx. Member Function Documentation. ◆ AddBackgroundTestEvent(). void TMVA::DataLoader::AddBackgroundTestEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal training event ; Definition at line 252 of f",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:21181,Modifiability,inherit,inherited,21181,"); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataLoader.h>. Inheritance diagram for TMVA::DataLoader:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ DataAssignType. enum TMVA::DataLoader::DataAssignType. private . EnumeratorkUndefined ; kAssignTrees ; kAssignEvents . Definition at line 199 of file DataLoader.h. Constructor & Destructor Documentation. ◆ DataLoader(). TMVA::DataLoader::DataLoader ; (; TString ; thedlName = ""default""). Definition at line 80 of file DataLoader.cxx. ◆ ~DataLoader(). TMVA::DataLoader::~DataLoader ; (; void ; ). virtual . Definition at line 96 of file DataLoader.cxx. Member Function Documentation. ◆ AddBackgroundTestEvent(). void TMVA::DataLoader::AddBackgroundTestEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal training event ; Definition at line 252 of f",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:23977,Modifiability,variab,variables,23977,"ype ; treetype = Types::kMaxTreeType . ). number of signal events (used to compute significance) ; Definition at line 402 of file DataLoader.cxx. ◆ AddCut() [1/2]. void TMVA::DataLoader::AddCut ; (; const TCut & ; cut, . const TString & ; className = """" . ). Definition at line 594 of file DataLoader.cxx. ◆ AddCut() [2/2]. void TMVA::DataLoader::AddCut ; (; const TString & ; cut, . const TString & ; className = """" . ). Definition at line 588 of file DataLoader.cxx. ◆ AddDataSet() [1/2]. TMVA::DataSetInfo & TMVA::DataLoader::AddDataSet ; (; const TString & ; dsiName). Definition at line 126 of file DataLoader.cxx. ◆ AddDataSet() [2/2]. TMVA::DataSetInfo & TMVA::DataLoader::AddDataSet ; (; DataSetInfo & ; dsi). Definition at line 119 of file DataLoader.cxx. ◆ AddEvent(). void TMVA::DataLoader::AddEvent ; (; const TString & ; className, . Types::ETreeType ; tt, . const std::vector< Double_t > & ; event, . Double_t ; weight . ). add event vector event : the order of values is: variables + targets + spectators ; Definition at line 277 of file DataLoader.cxx. ◆ AddRegressionTarget(). void TMVA::DataLoader::AddRegressionTarget ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0 . ). inline . Definition at line 132 of file DataLoader.h. ◆ AddRegressionTree(). void TMVA::DataLoader::AddRegressionTree ; (; TTree * ; tree, . Double_t ; weight = 1.0, . Types::ETreeType ; treetype = Types::kMaxTreeType . ). inline . Definition at line 103 of file DataLoader.h. ◆ AddSignalTestEvent(). void TMVA::DataLoader::AddSignalTestEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal testing event ; Definition at line 236 of file DataLoader.cxx. ◆ AddSignalTrainingEvent(). void TMVA::DataLoader::AddSignalTrainingEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal training event ; Definition at line 228 of file DataLoader.cxx",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:27463,Modifiability,variab,variable,27463,"taLoader.cxx. ◆ AddTrainingEvent(). void TMVA::DataLoader::AddTrainingEvent ; (; const TString & ; className, . const std::vector< Double_t > & ; event, . Double_t ; weight . ). add signal training event ; Definition at line 260 of file DataLoader.cxx. ◆ AddTree() [1/2]. void TMVA::DataLoader::AddTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight, . const TCut & ; cut, . const TString & ; treeType . ). number of signal events (used to compute significance) ; Definition at line 334 of file DataLoader.cxx. ◆ AddTree() [2/2]. void TMVA::DataLoader::AddTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight = 1.0, . const TCut & ; cut = """", . Types::ETreeType ; tt = Types::kMaxTreeType . ). Definition at line 351 of file DataLoader.cxx. ◆ AddVariable() [1/2]. void TMVA::DataLoader::AddVariable ; (; const TString & ; expression, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating variable in data set info ; Definition at line 494 of file DataLoader.cxx. ◆ AddVariable() [2/2]. void TMVA::DataLoader::AddVariable ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating variable in data set info ; Definition at line 485 of file DataLoader.cxx. ◆ AddVariablesArray(). void TMVA::DataLoader::AddVariablesArray ; (; const TString & ; expression, . int ; size, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating array of variables in data set info in case input tree provides an array of values ; Definition at line 504 of file DataLoader.cxx. ◆ Class(). static TClass * TMVA::DataLoader::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataLoader::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DataLoader::Class_",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:27781,Modifiability,variab,variable,27781,"tree, . const TString & ; className, . Double_t ; weight, . const TCut & ; cut, . const TString & ; treeType . ). number of signal events (used to compute significance) ; Definition at line 334 of file DataLoader.cxx. ◆ AddTree() [2/2]. void TMVA::DataLoader::AddTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight = 1.0, . const TCut & ; cut = """", . Types::ETreeType ; tt = Types::kMaxTreeType . ). Definition at line 351 of file DataLoader.cxx. ◆ AddVariable() [1/2]. void TMVA::DataLoader::AddVariable ; (; const TString & ; expression, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating variable in data set info ; Definition at line 494 of file DataLoader.cxx. ◆ AddVariable() [2/2]. void TMVA::DataLoader::AddVariable ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating variable in data set info ; Definition at line 485 of file DataLoader.cxx. ◆ AddVariablesArray(). void TMVA::DataLoader::AddVariablesArray ; (; const TString & ; expression, . int ; size, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating array of variables in data set info in case input tree provides an array of values ; Definition at line 504 of file DataLoader.cxx. ◆ Class(). static TClass * TMVA::DataLoader::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataLoader::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DataLoader::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 214 of file DataLoader.h. ◆ CreateEventAssignTrees(). TTree * TMVA::DataLoader::CreateEventAssignTrees ; (; const TString & ; name). create the data assignment tree (for event-wise data assignment by user) ; Definition at line 1",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:28075,Modifiability,variab,variables,28075,"Name, . Double_t ; weight = 1.0, . const TCut & ; cut = """", . Types::ETreeType ; tt = Types::kMaxTreeType . ). Definition at line 351 of file DataLoader.cxx. ◆ AddVariable() [1/2]. void TMVA::DataLoader::AddVariable ; (; const TString & ; expression, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating variable in data set info ; Definition at line 494 of file DataLoader.cxx. ◆ AddVariable() [2/2]. void TMVA::DataLoader::AddVariable ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating variable in data set info ; Definition at line 485 of file DataLoader.cxx. ◆ AddVariablesArray(). void TMVA::DataLoader::AddVariablesArray ; (; const TString & ; expression, . int ; size, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminating array of variables in data set info in case input tree provides an array of values ; Definition at line 504 of file DataLoader.cxx. ◆ Class(). static TClass * TMVA::DataLoader::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataLoader::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DataLoader::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 214 of file DataLoader.h. ◆ CreateEventAssignTrees(). TTree * TMVA::DataLoader::CreateEventAssignTrees ; (; const TString & ; name). create the data assignment tree (for event-wise data assignment by user) ; Definition at line 195 of file DataLoader.cxx. ◆ DataInput(). DataInputHandler & TMVA::DataLoader::DataInput ; (; ). inline . Definition at line 172 of file DataLoader.h. ◆ DeclFileName(). static const char * TMVA::DataLoader::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:32656,Modifiability,variab,variable,32656,"he training and test trees kept for backward compatibility ; Definition at line 618 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [4/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; TCut ; sigcut, . TCut ; bkgcut, . const TString & ; splitOpt . ). prepare the training and test trees ; Definition at line 644 of file DataLoader.cxx. ◆ RecombineKFoldDataSet(). void TMVA::DataLoader::RecombineKFoldDataSet ; (; CvSplit & ; s, . Types::ETreeType ; tt = Types::kTraining . ). Recombines the dataset. ; The precise semantics depend on the actual split.; Similar to the inverse operation of MakeKFoldDataSet but will differ. See documentation for each particular split for more information. ; Definition at line 683 of file DataLoader.cxx. ◆ SetBackgroundTree(). void TMVA::DataLoader::SetBackgroundTree ; (; TTree * ; background, . Double_t ; weight = 1.0 . ). Definition at line 439 of file DataLoader.cxx. ◆ SetBackgroundWeightExpression(). void TMVA::DataLoader::SetBackgroundWeightExpression ; (; const TString & ; variable). Definition at line 556 of file DataLoader.cxx. ◆ SetCut() [1/2]. void TMVA::DataLoader::SetCut ; (; const TCut & ; cut, . const TString & ; className = """" . ). Definition at line 581 of file DataLoader.cxx. ◆ SetCut() [2/2]. void TMVA::DataLoader::SetCut ; (; const TString & ; cut, . const TString & ; className = """" . ). Definition at line 575 of file DataLoader.cxx. ◆ SetInputTrees() [1/3]. void TMVA::DataLoader::SetInputTrees ; (; const TString & ; signalFileName, . const TString & ; backgroundFileName, . Double_t ; signalWeight = 1.0, . Double_t ; backgroundWeight = 1.0 . ). Definition at line 464 of file DataLoader.cxx. ◆ SetInputTrees() [2/3]. void TMVA::DataLoader::SetInputTrees ; (; TTree * ; inputTree, . const TCut & ; SigCut, . const TCut & ; BgCut . ). define the input trees for signal and background from single input tree, containing both signal and background events distinguished by the type identifiers: SigCut and BgCut ; Definit",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:34305,Modifiability,variab,variables,34305,"oid TMVA::DataLoader::SetInputTrees ; (; TTree * ; inputTree, . const TCut & ; SigCut, . const TCut & ; BgCut . ). define the input trees for signal and background from single input tree, containing both signal and background events distinguished by the type identifiers: SigCut and BgCut ; Definition at line 476 of file DataLoader.cxx. ◆ SetInputTrees() [3/3]. void TMVA::DataLoader::SetInputTrees ; (; TTree * ; signal, . TTree * ; background, . Double_t ; signalWeight = 1.0, . Double_t ; backgroundWeight = 1.0 . ). define the input trees for signal and background; no cuts are applied ; Definition at line 455 of file DataLoader.cxx. ◆ SetInputTreesFromEventAssignTrees(). void TMVA::DataLoader::SetInputTreesFromEventAssignTrees ; (; ). private . assign event-wise local trees to data set ; Definition at line 319 of file DataLoader.cxx. ◆ SetInputVariables(). void TMVA::DataLoader::SetInputVariables ; (; std::vector< TString > * ; theVariables). deprecated ; fill input variables in data set ; Definition at line 541 of file DataLoader.cxx. ◆ SetSignalTree(). void TMVA::DataLoader::SetSignalTree ; (; TTree * ; signal, . Double_t ; weight = 1.0 . ). Definition at line 432 of file DataLoader.cxx. ◆ SetSignalWeightExpression(). void TMVA::DataLoader::SetSignalWeightExpression ; (; const TString & ; variable). Definition at line 549 of file DataLoader.cxx. ◆ SetTree(). void TMVA::DataLoader::SetTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight . ). deprecated ; set background tree ; Definition at line 447 of file DataLoader.cxx. ◆ SetWeightExpression(). void TMVA::DataLoader::SetWeightExpression ; (; const TString & ; variable, . const TString & ; className = """" . ). Definition at line 563 of file DataLoader.cxx. ◆ Streamer(). virtual void TMVA::DataLoader::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable. ◆ StreamerNVirtual(). void TMVA::DataLoader::StreamerNVirtual ; (; TBuffer & ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:34636,Modifiability,variab,variable,34636,"and BgCut ; Definition at line 476 of file DataLoader.cxx. ◆ SetInputTrees() [3/3]. void TMVA::DataLoader::SetInputTrees ; (; TTree * ; signal, . TTree * ; background, . Double_t ; signalWeight = 1.0, . Double_t ; backgroundWeight = 1.0 . ). define the input trees for signal and background; no cuts are applied ; Definition at line 455 of file DataLoader.cxx. ◆ SetInputTreesFromEventAssignTrees(). void TMVA::DataLoader::SetInputTreesFromEventAssignTrees ; (; ). private . assign event-wise local trees to data set ; Definition at line 319 of file DataLoader.cxx. ◆ SetInputVariables(). void TMVA::DataLoader::SetInputVariables ; (; std::vector< TString > * ; theVariables). deprecated ; fill input variables in data set ; Definition at line 541 of file DataLoader.cxx. ◆ SetSignalTree(). void TMVA::DataLoader::SetSignalTree ; (; TTree * ; signal, . Double_t ; weight = 1.0 . ). Definition at line 432 of file DataLoader.cxx. ◆ SetSignalWeightExpression(). void TMVA::DataLoader::SetSignalWeightExpression ; (; const TString & ; variable). Definition at line 549 of file DataLoader.cxx. ◆ SetTree(). void TMVA::DataLoader::SetTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight . ). deprecated ; set background tree ; Definition at line 447 of file DataLoader.cxx. ◆ SetWeightExpression(). void TMVA::DataLoader::SetWeightExpression ; (; const TString & ; variable, . const TString & ; className = """" . ). Definition at line 563 of file DataLoader.cxx. ◆ Streamer(). virtual void TMVA::DataLoader::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable. ◆ StreamerNVirtual(). void TMVA::DataLoader::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 214 of file DataLoader.h. ◆ UserAssignEvents(). Bool_t TMVA::DataLoader::UserAssignEvents ; (; UInt_t ; clIndex). Definition at line 311 of file DataLoader.cxx. ◆ VarTransform(). TMVA::DataLoader * TMVA::DataLoader::Var",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:34988,Modifiability,variab,variable,34988,"cxx. ◆ SetInputTreesFromEventAssignTrees(). void TMVA::DataLoader::SetInputTreesFromEventAssignTrees ; (; ). private . assign event-wise local trees to data set ; Definition at line 319 of file DataLoader.cxx. ◆ SetInputVariables(). void TMVA::DataLoader::SetInputVariables ; (; std::vector< TString > * ; theVariables). deprecated ; fill input variables in data set ; Definition at line 541 of file DataLoader.cxx. ◆ SetSignalTree(). void TMVA::DataLoader::SetSignalTree ; (; TTree * ; signal, . Double_t ; weight = 1.0 . ). Definition at line 432 of file DataLoader.cxx. ◆ SetSignalWeightExpression(). void TMVA::DataLoader::SetSignalWeightExpression ; (; const TString & ; variable). Definition at line 549 of file DataLoader.cxx. ◆ SetTree(). void TMVA::DataLoader::SetTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight . ). deprecated ; set background tree ; Definition at line 447 of file DataLoader.cxx. ◆ SetWeightExpression(). void TMVA::DataLoader::SetWeightExpression ; (; const TString & ; variable, . const TString & ; className = """" . ). Definition at line 563 of file DataLoader.cxx. ◆ Streamer(). virtual void TMVA::DataLoader::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable. ◆ StreamerNVirtual(). void TMVA::DataLoader::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 214 of file DataLoader.h. ◆ UserAssignEvents(). Bool_t TMVA::DataLoader::UserAssignEvents ; (; UInt_t ; clIndex). Definition at line 311 of file DataLoader.cxx. ◆ VarTransform(). TMVA::DataLoader * TMVA::DataLoader::VarTransform ; (; TString ; trafoDefinition). Transforms the variables and return a new DataLoader with the transformed variables. ; Definition at line 146 of file DataLoader.cxx. Friends And Related Symbol Documentation. ◆ DataLoaderCopy. void DataLoaderCopy ; (; TMVA::DataLoader * ; des, . TMVA::DataLoader * ; src . ). friend . Member Data Documentation.",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:35663,Modifiability,variab,variables,35663,"cxx. ◆ SetTree(). void TMVA::DataLoader::SetTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight . ). deprecated ; set background tree ; Definition at line 447 of file DataLoader.cxx. ◆ SetWeightExpression(). void TMVA::DataLoader::SetWeightExpression ; (; const TString & ; variable, . const TString & ; className = """" . ). Definition at line 563 of file DataLoader.cxx. ◆ Streamer(). virtual void TMVA::DataLoader::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable. ◆ StreamerNVirtual(). void TMVA::DataLoader::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 214 of file DataLoader.h. ◆ UserAssignEvents(). Bool_t TMVA::DataLoader::UserAssignEvents ; (; UInt_t ; clIndex). Definition at line 311 of file DataLoader.cxx. ◆ VarTransform(). TMVA::DataLoader * TMVA::DataLoader::VarTransform ; (; TString ; trafoDefinition). Transforms the variables and return a new DataLoader with the transformed variables. ; Definition at line 146 of file DataLoader.cxx. Friends And Related Symbol Documentation. ◆ DataLoaderCopy. void DataLoaderCopy ; (; TMVA::DataLoader * ; des, . TMVA::DataLoader * ; src . ). friend . Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DataLoader::fAnalysisType. private . the training type ; Definition at line 210 of file DataLoader.h. ◆ fATreeEvent. std::vector<Float_t> TMVA::DataLoader::fATreeEvent. private . event variables ; Definition at line 208 of file DataLoader.h. ◆ fATreeType. Int_t TMVA::DataLoader::fATreeType = 0. private . type of event (=classIndex) ; Definition at line 206 of file DataLoader.h. ◆ fATreeWeight. Float_t TMVA::DataLoader::fATreeWeight = 0.0. private . weight of the event ; Definition at line 207 of file DataLoader.h. ◆ fDataAssignType. DataAssignType TMVA::DataLoader::fDataAssignType. private . flags for data assigning ; Definition at line 202 of file DataLoader.h. ◆ fDataInputHa",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:35722,Modifiability,variab,variables,35722,"cxx. ◆ SetTree(). void TMVA::DataLoader::SetTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight . ). deprecated ; set background tree ; Definition at line 447 of file DataLoader.cxx. ◆ SetWeightExpression(). void TMVA::DataLoader::SetWeightExpression ; (; const TString & ; variable, . const TString & ; className = """" . ). Definition at line 563 of file DataLoader.cxx. ◆ Streamer(). virtual void TMVA::DataLoader::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TMVA::Configurable. ◆ StreamerNVirtual(). void TMVA::DataLoader::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 214 of file DataLoader.h. ◆ UserAssignEvents(). Bool_t TMVA::DataLoader::UserAssignEvents ; (; UInt_t ; clIndex). Definition at line 311 of file DataLoader.cxx. ◆ VarTransform(). TMVA::DataLoader * TMVA::DataLoader::VarTransform ; (; TString ; trafoDefinition). Transforms the variables and return a new DataLoader with the transformed variables. ; Definition at line 146 of file DataLoader.cxx. Friends And Related Symbol Documentation. ◆ DataLoaderCopy. void DataLoaderCopy ; (; TMVA::DataLoader * ; des, . TMVA::DataLoader * ; src . ). friend . Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DataLoader::fAnalysisType. private . the training type ; Definition at line 210 of file DataLoader.h. ◆ fATreeEvent. std::vector<Float_t> TMVA::DataLoader::fATreeEvent. private . event variables ; Definition at line 208 of file DataLoader.h. ◆ fATreeType. Int_t TMVA::DataLoader::fATreeType = 0. private . type of event (=classIndex) ; Definition at line 206 of file DataLoader.h. ◆ fATreeWeight. Float_t TMVA::DataLoader::fATreeWeight = 0.0. private . weight of the event ; Definition at line 207 of file DataLoader.h. ◆ fDataAssignType. DataAssignType TMVA::DataLoader::fDataAssignType. private . flags for data assigning ; Definition at line 202 of file DataLoader.h. ◆ fDataInputHa",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:36190,Modifiability,variab,variables,36190,"mplemented from TMVA::Configurable. ◆ StreamerNVirtual(). void TMVA::DataLoader::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 214 of file DataLoader.h. ◆ UserAssignEvents(). Bool_t TMVA::DataLoader::UserAssignEvents ; (; UInt_t ; clIndex). Definition at line 311 of file DataLoader.cxx. ◆ VarTransform(). TMVA::DataLoader * TMVA::DataLoader::VarTransform ; (; TString ; trafoDefinition). Transforms the variables and return a new DataLoader with the transformed variables. ; Definition at line 146 of file DataLoader.cxx. Friends And Related Symbol Documentation. ◆ DataLoaderCopy. void DataLoaderCopy ; (; TMVA::DataLoader * ; des, . TMVA::DataLoader * ; src . ). friend . Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DataLoader::fAnalysisType. private . the training type ; Definition at line 210 of file DataLoader.h. ◆ fATreeEvent. std::vector<Float_t> TMVA::DataLoader::fATreeEvent. private . event variables ; Definition at line 208 of file DataLoader.h. ◆ fATreeType. Int_t TMVA::DataLoader::fATreeType = 0. private . type of event (=classIndex) ; Definition at line 206 of file DataLoader.h. ◆ fATreeWeight. Float_t TMVA::DataLoader::fATreeWeight = 0.0. private . weight of the event ; Definition at line 207 of file DataLoader.h. ◆ fDataAssignType. DataAssignType TMVA::DataLoader::fDataAssignType. private . flags for data assigning ; Definition at line 202 of file DataLoader.h. ◆ fDataInputHandler. DataInputHandler* TMVA::DataLoader::fDataInputHandler. private . -> ; Definition at line 189 of file DataLoader.h. ◆ fDataSetManager. DataSetManager* TMVA::DataLoader::fDataSetManager. private . Definition at line 186 of file DataLoader.h. ◆ fDefaultTrfs. std::vector<TMVA::VariableTransformBase*> TMVA::DataLoader::fDefaultTrfs. private . list of transformations on default DataSet ; Definition at line 191 of file DataLoader.h. ◆ fOptions. TString TMVA::DataLoader::fOptions. private . option string given by constr",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:9666,Security,hash,hash,9666,"ostream &o, const TString &prefix) const;  write options to output stream (e.g. in writing the MVA weight files ;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:30465,Security,validat,validation,30465,"asets ; Definition at line 717 of file DataLoader.cxx. ◆ GetDataSetInfo(). TMVA::DataSetInfo & TMVA::DataLoader::GetDataSetInfo ; (; ). Definition at line 137 of file DataLoader.cxx. ◆ GetDefaultDataSetInfo(). const DataSetInfo & TMVA::DataLoader::GetDefaultDataSetInfo ; (; ). inline . Definition at line 165 of file DataLoader.h. ◆ IsA(). virtual TClass * TMVA::DataLoader::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Configurable.; Definition at line 214 of file DataLoader.h. ◆ MakeCopy(). TMVA::DataLoader * TMVA::DataLoader::MakeCopy ; (; TString ; name). Copy method use in VI and CV. ; Definition at line 691 of file DataLoader.cxx. ◆ MakeKFoldDataSet(). void TMVA::DataLoader::MakeKFoldDataSet ; (; CvSplit & ; s). Function required to split the training and testing datasets into a number of folds. ; Required by the CrossValidation and HyperParameterOptimisation classes. The option to split the training dataset into a training set and a validation set is implemented but not currently used. ; Definition at line 662 of file DataLoader.cxx. ◆ PrepareFoldDataSet(). void TMVA::DataLoader::PrepareFoldDataSet ; (; CvSplit & ; s, . UInt_t ; foldNumber, . Types::ETreeType ; tt = Types::kTraining . ). Function for assigning the correct folds to the testing or training set. ; Definition at line 670 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [1/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . const TString & ; splitOpt . ). prepare the training and test trees -> same cuts for signal and background ; Definition at line 632 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [2/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ; NsigTest, . Int_t ; NbkgTest, . const TString & ; otherOpt = ""SplitMode=Random:!V"" . ). prepare the training and test trees ; Definition at line 602 of file DataLoader.cxx. ◆ ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:1920,Testability,test,testing,1920,"eetype);  ; void AddBackgroundTree (TTree *background, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  number of signal events (used to compute significance) ;  ; void AddCut (const TCut &cut, const TString &className="""");  ; void AddCut (const TString &cut, const TString &className="""");  ; DataSetInfo & AddDataSet (const TString &);  ; DataSetInfo & AddDataSet (DataSetInfo &);  ; void AddEvent (const TString &className, Types::ETreeType tt, const std::vector< Double_t > &event, Double_t weight);  add event vector event : the order of values is: variables + targets + spectators ;  ; void AddRegressionTarget (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0);  ; void AddRegressionTree (TTree *tree, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  ; void AddSignalTestEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal testing event ;  ; void AddSignalTrainingEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal training event ;  ; void AddSignalTree (TString datFileS, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  add signal tree from text file ;  ; void AddSignalTree (TTree *signal, Double_t weight, const TString &treetype);  ; void AddSignalTree (TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  number of signal events (used to compute significance) ;  ; void AddSpectator (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0);  user inserts target in data set info ;  ; void AddTarget (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0);  user inserts target in data set info ;  ; void AddTestEvent (const TString &className, const std::vector< Double_t > &event, Double_t weight);  add signal test event ;  ; void AddTrainingEvent (const TString &classNam",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:2902,Testability,test,test,2902,"  ; void AddSignalTestEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal testing event ;  ; void AddSignalTrainingEvent (const std::vector< Double_t > &event, Double_t weight=1.0);  add signal training event ;  ; void AddSignalTree (TString datFileS, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  add signal tree from text file ;  ; void AddSignalTree (TTree *signal, Double_t weight, const TString &treetype);  ; void AddSignalTree (TTree *signal, Double_t weight=1.0, Types::ETreeType treetype=Types::kMaxTreeType);  number of signal events (used to compute significance) ;  ; void AddSpectator (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0);  user inserts target in data set info ;  ; void AddTarget (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0);  user inserts target in data set info ;  ; void AddTestEvent (const TString &className, const std::vector< Double_t > &event, Double_t weight);  add signal test event ;  ; void AddTrainingEvent (const TString &className, const std::vector< Double_t > &event, Double_t weight);  add signal training event ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight, const TCut &cut, const TString &treeType);  number of signal events (used to compute significance) ;  ; void AddTree (TTree *tree, const TString &className, Double_t weight=1.0, const TCut &cut="""", Types::ETreeType tt=Types::kMaxTreeType);  ; void AddVariable (const TString &expression, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariable (const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariablesArray (const TString &expression, int size, char type='F', Double_t min=0, Double_t m",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:4490,Testability,test,testing,4490," inserts discriminating variable in data set info ;  ; void AddVariable (const TString &expression, const TString &title, const TString &unit, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariablesArray (const TString &expression, int size, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating array of variables in data set info in case input tree provides an array of values ;  ; TTree * CreateEventAssignTrees (const TString &name);  create the data assignment tree (for event-wise data assignment by user) ;  ; DataInputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and background ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t NsigTrain, Int_t NbkgTrain, Int_t NsigTest, Int_t NbkgTest, const TString &otherOpt=""SplitMode=Random:!V"");  prepare the training and test trees ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t Ntrain, Int_t Ntest=-1);  prepare the training and test trees kept for backward compatibility ;  ; void PrepareTrainingAndTestTree (TCut sigcut, TCut bkgcut, const TString &splitOpt);  prepare the training and test trees ;  ; void RecombineKFoldDataSet (CvSplit &s, Types::ETreeType tt=Types::kTraining);  Recombines the dataset. ;  ; voi",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:4680,Testability,test,testing,4680,"e='F', Double_t min=0, Double_t max=0);  user inserts discriminating variable in data set info ;  ; void AddVariablesArray (const TString &expression, int size, char type='F', Double_t min=0, Double_t max=0);  user inserts discriminating array of variables in data set info in case input tree provides an array of values ;  ; TTree * CreateEventAssignTrees (const TString &name);  create the data assignment tree (for event-wise data assignment by user) ;  ; DataInputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and background ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t NsigTrain, Int_t NbkgTrain, Int_t NsigTest, Int_t NbkgTest, const TString &otherOpt=""SplitMode=Random:!V"");  prepare the training and test trees ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t Ntrain, Int_t Ntest=-1);  prepare the training and test trees kept for backward compatibility ;  ; void PrepareTrainingAndTestTree (TCut sigcut, TCut bkgcut, const TString &splitOpt);  prepare the training and test trees ;  ; void RecombineKFoldDataSet (CvSplit &s, Types::ETreeType tt=Types::kTraining);  Recombines the dataset. ;  ; void SetBackgroundTree (TTree *background, Double_t weight=1.0);  ; void SetBackgroundWeightExpression (const TString &variable);  ; void SetCut (const T",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:4812,Testability,test,test,4812,"nputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and background ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t NsigTrain, Int_t NbkgTrain, Int_t NsigTest, Int_t NbkgTest, const TString &otherOpt=""SplitMode=Random:!V"");  prepare the training and test trees ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t Ntrain, Int_t Ntest=-1);  prepare the training and test trees kept for backward compatibility ;  ; void PrepareTrainingAndTestTree (TCut sigcut, TCut bkgcut, const TString &splitOpt);  prepare the training and test trees ;  ; void RecombineKFoldDataSet (CvSplit &s, Types::ETreeType tt=Types::kTraining);  Recombines the dataset. ;  ; void SetBackgroundTree (TTree *background, Double_t weight=1.0);  ; void SetBackgroundWeightExpression (const TString &variable);  ; void SetCut (const TCut &cut, const TString &className="""");  ; void SetCut (const TString &cut, const TString &className="""");  ; void SetInputTrees (const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  ; void SetInputTrees (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguis",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:5057,Testability,test,test,5057,"nputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and background ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t NsigTrain, Int_t NbkgTrain, Int_t NsigTest, Int_t NbkgTest, const TString &otherOpt=""SplitMode=Random:!V"");  prepare the training and test trees ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t Ntrain, Int_t Ntest=-1);  prepare the training and test trees kept for backward compatibility ;  ; void PrepareTrainingAndTestTree (TCut sigcut, TCut bkgcut, const TString &splitOpt);  prepare the training and test trees ;  ; void RecombineKFoldDataSet (CvSplit &s, Types::ETreeType tt=Types::kTraining);  Recombines the dataset. ;  ; void SetBackgroundTree (TTree *background, Double_t weight=1.0);  ; void SetBackgroundWeightExpression (const TString &variable);  ; void SetCut (const TCut &cut, const TString &className="""");  ; void SetCut (const TString &cut, const TString &className="""");  ; void SetInputTrees (const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  ; void SetInputTrees (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguis",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:5180,Testability,test,test,5180,"nputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and background ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t NsigTrain, Int_t NbkgTrain, Int_t NsigTest, Int_t NbkgTest, const TString &otherOpt=""SplitMode=Random:!V"");  prepare the training and test trees ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t Ntrain, Int_t Ntest=-1);  prepare the training and test trees kept for backward compatibility ;  ; void PrepareTrainingAndTestTree (TCut sigcut, TCut bkgcut, const TString &splitOpt);  prepare the training and test trees ;  ; void RecombineKFoldDataSet (CvSplit &s, Types::ETreeType tt=Types::kTraining);  Recombines the dataset. ;  ; void SetBackgroundTree (TTree *background, Double_t weight=1.0);  ; void SetBackgroundWeightExpression (const TString &variable);  ; void SetCut (const TCut &cut, const TString &className="""");  ; void SetCut (const TString &cut, const TString &className="""");  ; void SetInputTrees (const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  ; void SetInputTrees (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguis",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:5339,Testability,test,test,5339,"nputHandler & DataInput ();  ; TH2 * GetCorrelationMatrix (const TString &className);  returns the correlation matrix of datasets ;  ; DataSetInfo & GetDataSetInfo ();  ; const DataSetInfo & GetDefaultDataSetInfo ();  ; virtual TClass * IsA () const;  ; DataLoader * MakeCopy (TString name);  Copy method use in VI and CV. ;  ; void MakeKFoldDataSet (CvSplit &s);  Function required to split the training and testing datasets into a number of folds. ;  ; void PrepareFoldDataSet (CvSplit &s, UInt_t foldNumber, Types::ETreeType tt=Types::kTraining);  Function for assigning the correct folds to the testing or training set. ;  ; void PrepareTrainingAndTestTree (const TCut &cut, const TString &splitOpt);  prepare the training and test trees -> same cuts for signal and background ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t NsigTrain, Int_t NbkgTrain, Int_t NsigTest, Int_t NbkgTest, const TString &otherOpt=""SplitMode=Random:!V"");  prepare the training and test trees ;  ; void PrepareTrainingAndTestTree (const TCut &cut, Int_t Ntrain, Int_t Ntest=-1);  prepare the training and test trees kept for backward compatibility ;  ; void PrepareTrainingAndTestTree (TCut sigcut, TCut bkgcut, const TString &splitOpt);  prepare the training and test trees ;  ; void RecombineKFoldDataSet (CvSplit &s, Types::ETreeType tt=Types::kTraining);  Recombines the dataset. ;  ; void SetBackgroundTree (TTree *background, Double_t weight=1.0);  ; void SetBackgroundWeightExpression (const TString &variable);  ; void SetCut (const TCut &cut, const TString &className="""");  ; void SetCut (const TString &cut, const TString &className="""");  ; void SetInputTrees (const TString &signalFileName, const TString &backgroundFileName, Double_t signalWeight=1.0, Double_t backgroundWeight=1.0);  ; void SetInputTrees (TTree *inputTree, const TCut &SigCut, const TCut &BgCut);  define the input trees for signal and background from single input tree, containing both signal and background events distinguis",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:19557,Testability,test,test,19557,"tat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Types; enum  DataAssignType { kUndefined = 0; , kAssignTrees; , kAssignEvents; };  . Private Member Functions; DataSetInfo & DefaultDataSetInfo ();  default creation ;  ; void SetInputTreesFromEventAssignTrees ();  assign event-wise local trees to data set ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  the training type ;  ; std::vector< Float_t > fATreeEvent;  event variables ;  ; Int_t fATreeType = 0;  type of event (=classIndex) ;  ; Float_t fATreeWeight = 0.0;  weight of the event ;  ; DataAssignType fDataAssignType;  flags for data assigning ;  ; DataInputHandler * fDataInputHandler;  -> ;  ; DataSetManager * fDataSetManager;  ; std::vector< TMVA::VariableTransformBase * > fDefaultTrfs;  list of transformations on default DataSet ;  ; TString fOptions;  option string given by construction (presently only ""V"") ;  ; std::vector< TTree * > fTestAssignTree;  for each class: tmp tree if user wants to assign the events directly ;  ; std::vector< TTree * > fTrainAssignTree;  for each class: tmp tree if user wants to assign the events directly ;  ; TString fTransformations;  List of transformations to test. ;  ; Bool_t fVerbose;  verbose mode ;  . Friends; void DataLoaderCopy (TMVA::DataLoader *des, TMVA::DataLoader *src);  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 ));",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:21147,Testability,log,logger,21147,"); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TMVA::Configurable; void EnableLooseOptions (Bool_t b=kTRUE);  ; const TString & GetReferenceFile () const;  ; Bool_t LooseOptionCheckingEnabled () const;  ; void ResetSetFlag ();  resets the IsSet flag for all declare options to be called before options are read from stream ;  ; void WriteOptionsReferenceToFile ();  write complete options to output stream ;  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TMVA::Configurable; MsgLogger * fLogger;  ! message logger ;  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataLoader.h>. Inheritance diagram for TMVA::DataLoader:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ DataAssignType. enum TMVA::DataLoader::DataAssignType. private . EnumeratorkUndefined ; kAssignTrees ; kAssignEvents . Definition at line 199 of file DataLoader.h. Constructor & Destructor Documentation. ◆ DataLoader(). TMVA::DataLoader::DataLoader ; (; TString ; thedlName = ""default""). Definition at line 80 of file DataLoader.cxx. ◆ ~DataLoader(). TMVA::DataLoader::~DataLoader ; (; void ; ). virtual . Definition at line 96 of file DataLoader.cxx. Member Function Documentation. ◆ AddBackgroundTestEvent(). void TMVA::DataLoader::AddBackgroundTestEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal training event ; Definition at line 252 of f",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:24704,Testability,test,testing,24704,"119 of file DataLoader.cxx. ◆ AddEvent(). void TMVA::DataLoader::AddEvent ; (; const TString & ; className, . Types::ETreeType ; tt, . const std::vector< Double_t > & ; event, . Double_t ; weight . ). add event vector event : the order of values is: variables + targets + spectators ; Definition at line 277 of file DataLoader.cxx. ◆ AddRegressionTarget(). void TMVA::DataLoader::AddRegressionTarget ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0 . ). inline . Definition at line 132 of file DataLoader.h. ◆ AddRegressionTree(). void TMVA::DataLoader::AddRegressionTree ; (; TTree * ; tree, . Double_t ; weight = 1.0, . Types::ETreeType ; treetype = Types::kMaxTreeType . ). inline . Definition at line 103 of file DataLoader.h. ◆ AddSignalTestEvent(). void TMVA::DataLoader::AddSignalTestEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal testing event ; Definition at line 236 of file DataLoader.cxx. ◆ AddSignalTrainingEvent(). void TMVA::DataLoader::AddSignalTrainingEvent ; (; const std::vector< Double_t > & ; event, . Double_t ; weight = 1.0 . ). add signal training event ; Definition at line 228 of file DataLoader.cxx. ◆ AddSignalTree() [1/3]. void TMVA::DataLoader::AddSignalTree ; (; TString ; datFileS, . Double_t ; weight = 1.0, . Types::ETreeType ; treetype = Types::kMaxTreeType . ). add signal tree from text file ; Definition at line 379 of file DataLoader.cxx. ◆ AddSignalTree() [2/3]. void TMVA::DataLoader::AddSignalTree ; (; TTree * ; signal, . Double_t ; weight, . const TString & ; treetype . ). Definition at line 394 of file DataLoader.cxx. ◆ AddSignalTree() [3/3]. void TMVA::DataLoader::AddSignalTree ; (; TTree * ; signal, . Double_t ; weight = 1.0, . Types::ETreeType ; treetype = Types::kMaxTreeType . ). number of signal events (used to compute significance) ; Definition at line 371 of file DataLoader.cxx. ◆ AddSpectator(). void ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:26437,Testability,test,test,26437,"id TMVA::DataLoader::AddSignalTree ; (; TTree * ; signal, . Double_t ; weight = 1.0, . Types::ETreeType ; treetype = Types::kMaxTreeType . ). number of signal events (used to compute significance) ; Definition at line 371 of file DataLoader.cxx. ◆ AddSpectator(). void TMVA::DataLoader::AddSpectator ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts target in data set info ; Definition at line 524 of file DataLoader.cxx. ◆ AddTarget(). void TMVA::DataLoader::AddTarget ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts target in data set info ; Definition at line 512 of file DataLoader.cxx. ◆ AddTestEvent(). void TMVA::DataLoader::AddTestEvent ; (; const TString & ; className, . const std::vector< Double_t > & ; event, . Double_t ; weight . ). add signal test event ; Definition at line 268 of file DataLoader.cxx. ◆ AddTrainingEvent(). void TMVA::DataLoader::AddTrainingEvent ; (; const TString & ; className, . const std::vector< Double_t > & ; event, . Double_t ; weight . ). add signal training event ; Definition at line 260 of file DataLoader.cxx. ◆ AddTree() [1/2]. void TMVA::DataLoader::AddTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight, . const TCut & ; cut, . const TString & ; treeType . ). number of signal events (used to compute significance) ; Definition at line 334 of file DataLoader.cxx. ◆ AddTree() [2/2]. void TMVA::DataLoader::AddTree ; (; TTree * ; tree, . const TString & ; className, . Double_t ; weight = 1.0, . const TCut & ; cut = """", . Types::ETreeType ; tt = Types::kMaxTreeType . ). Definition at line 351 of file DataLoader.cxx. ◆ AddVariable() [1/2]. void TMVA::DataLoader::AddVariable ; (; const TString & ; expression, . char ; type = 'F', . Double_t ; min = 0, . Double_t ; max = 0 . ). user inserts discriminat",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:30283,Testability,test,testing,30283," line 533 of file DataLoader.cxx. ◆ GetCorrelationMatrix(). TH2 * TMVA::DataLoader::GetCorrelationMatrix ; (; const TString & ; className). returns the correlation matrix of datasets ; Definition at line 717 of file DataLoader.cxx. ◆ GetDataSetInfo(). TMVA::DataSetInfo & TMVA::DataLoader::GetDataSetInfo ; (; ). Definition at line 137 of file DataLoader.cxx. ◆ GetDefaultDataSetInfo(). const DataSetInfo & TMVA::DataLoader::GetDefaultDataSetInfo ; (; ). inline . Definition at line 165 of file DataLoader.h. ◆ IsA(). virtual TClass * TMVA::DataLoader::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Configurable.; Definition at line 214 of file DataLoader.h. ◆ MakeCopy(). TMVA::DataLoader * TMVA::DataLoader::MakeCopy ; (; TString ; name). Copy method use in VI and CV. ; Definition at line 691 of file DataLoader.cxx. ◆ MakeKFoldDataSet(). void TMVA::DataLoader::MakeKFoldDataSet ; (; CvSplit & ; s). Function required to split the training and testing datasets into a number of folds. ; Required by the CrossValidation and HyperParameterOptimisation classes. The option to split the training dataset into a training set and a validation set is implemented but not currently used. ; Definition at line 662 of file DataLoader.cxx. ◆ PrepareFoldDataSet(). void TMVA::DataLoader::PrepareFoldDataSet ; (; CvSplit & ; s, . UInt_t ; foldNumber, . Types::ETreeType ; tt = Types::kTraining . ). Function for assigning the correct folds to the testing or training set. ; Definition at line 670 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [1/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . const TString & ; splitOpt . ). prepare the training and test trees -> same cuts for signal and background ; Definition at line 632 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [2/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:30773,Testability,test,testing,30773,"ne 165 of file DataLoader.h. ◆ IsA(). virtual TClass * TMVA::DataLoader::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::Configurable.; Definition at line 214 of file DataLoader.h. ◆ MakeCopy(). TMVA::DataLoader * TMVA::DataLoader::MakeCopy ; (; TString ; name). Copy method use in VI and CV. ; Definition at line 691 of file DataLoader.cxx. ◆ MakeKFoldDataSet(). void TMVA::DataLoader::MakeKFoldDataSet ; (; CvSplit & ; s). Function required to split the training and testing datasets into a number of folds. ; Required by the CrossValidation and HyperParameterOptimisation classes. The option to split the training dataset into a training set and a validation set is implemented but not currently used. ; Definition at line 662 of file DataLoader.cxx. ◆ PrepareFoldDataSet(). void TMVA::DataLoader::PrepareFoldDataSet ; (; CvSplit & ; s, . UInt_t ; foldNumber, . Types::ETreeType ; tt = Types::kTraining . ). Function for assigning the correct folds to the testing or training set. ; Definition at line 670 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [1/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . const TString & ; splitOpt . ). prepare the training and test trees -> same cuts for signal and background ; Definition at line 632 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [2/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ; NsigTest, . Int_t ; NbkgTest, . const TString & ; otherOpt = ""SplitMode=Random:!V"" . ). prepare the training and test trees ; Definition at line 602 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [3/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; Ntrain, . Int_t ; Ntest = -1 . ). prepare the training and test trees kept for backward compatibility ; Definition at line 618 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:31019,Testability,test,test,31019,"y ; (; TString ; name). Copy method use in VI and CV. ; Definition at line 691 of file DataLoader.cxx. ◆ MakeKFoldDataSet(). void TMVA::DataLoader::MakeKFoldDataSet ; (; CvSplit & ; s). Function required to split the training and testing datasets into a number of folds. ; Required by the CrossValidation and HyperParameterOptimisation classes. The option to split the training dataset into a training set and a validation set is implemented but not currently used. ; Definition at line 662 of file DataLoader.cxx. ◆ PrepareFoldDataSet(). void TMVA::DataLoader::PrepareFoldDataSet ; (; CvSplit & ; s, . UInt_t ; foldNumber, . Types::ETreeType ; tt = Types::kTraining . ). Function for assigning the correct folds to the testing or training set. ; Definition at line 670 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [1/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . const TString & ; splitOpt . ). prepare the training and test trees -> same cuts for signal and background ; Definition at line 632 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [2/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ; NsigTest, . Int_t ; NbkgTest, . const TString & ; otherOpt = ""SplitMode=Random:!V"" . ). prepare the training and test trees ; Definition at line 602 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [3/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; Ntrain, . Int_t ; Ntest = -1 . ). prepare the training and test trees kept for backward compatibility ; Definition at line 618 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [4/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; TCut ; sigcut, . TCut ; bkgcut, . const TString & ; splitOpt . ). prepare the training and test trees ; Definition at line 644 of file DataLoader.cxx. ◆ RecombineKFoldDataSet(). void TMVA::DataLoader::RecombineKFoldDataSet ; (; Cv",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:31396,Testability,test,test,31396,"o split the training dataset into a training set and a validation set is implemented but not currently used. ; Definition at line 662 of file DataLoader.cxx. ◆ PrepareFoldDataSet(). void TMVA::DataLoader::PrepareFoldDataSet ; (; CvSplit & ; s, . UInt_t ; foldNumber, . Types::ETreeType ; tt = Types::kTraining . ). Function for assigning the correct folds to the testing or training set. ; Definition at line 670 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [1/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . const TString & ; splitOpt . ). prepare the training and test trees -> same cuts for signal and background ; Definition at line 632 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [2/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ; NsigTest, . Int_t ; NbkgTest, . const TString & ; otherOpt = ""SplitMode=Random:!V"" . ). prepare the training and test trees ; Definition at line 602 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [3/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; Ntrain, . Int_t ; Ntest = -1 . ). prepare the training and test trees kept for backward compatibility ; Definition at line 618 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [4/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; TCut ; sigcut, . TCut ; bkgcut, . const TString & ; splitOpt . ). prepare the training and test trees ; Definition at line 644 of file DataLoader.cxx. ◆ RecombineKFoldDataSet(). void TMVA::DataLoader::RecombineKFoldDataSet ; (; CvSplit & ; s, . Types::ETreeType ; tt = Types::kTraining . ). Recombines the dataset. ; The precise semantics depend on the actual split.; Similar to the inverse operation of MakeKFoldDataSet but will differ. See documentation for each particular split for more information. ; Definition at line 683 of file DataLoader.cxx. ◆ SetBackgroundTree(). void TMVA::D",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:31638,Testability,test,test,31638,"dNumber, . Types::ETreeType ; tt = Types::kTraining . ). Function for assigning the correct folds to the testing or training set. ; Definition at line 670 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [1/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . const TString & ; splitOpt . ). prepare the training and test trees -> same cuts for signal and background ; Definition at line 632 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [2/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ; NsigTest, . Int_t ; NbkgTest, . const TString & ; otherOpt = ""SplitMode=Random:!V"" . ). prepare the training and test trees ; Definition at line 602 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [3/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; Ntrain, . Int_t ; Ntest = -1 . ). prepare the training and test trees kept for backward compatibility ; Definition at line 618 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [4/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; TCut ; sigcut, . TCut ; bkgcut, . const TString & ; splitOpt . ). prepare the training and test trees ; Definition at line 644 of file DataLoader.cxx. ◆ RecombineKFoldDataSet(). void TMVA::DataLoader::RecombineKFoldDataSet ; (; CvSplit & ; s, . Types::ETreeType ; tt = Types::kTraining . ). Recombines the dataset. ; The precise semantics depend on the actual split.; Similar to the inverse operation of MakeKFoldDataSet but will differ. See documentation for each particular split for more information. ; Definition at line 683 of file DataLoader.cxx. ◆ SetBackgroundTree(). void TMVA::DataLoader::SetBackgroundTree ; (; TTree * ; background, . Double_t ; weight = 1.0 . ). Definition at line 439 of file DataLoader.cxx. ◆ SetBackgroundWeightExpression(). void TMVA::DataLoader::SetBackgroundWeightExpression ; (; const TString & ; variable). De",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:31914,Testability,test,test,31914,"stTree ; (; const TCut & ; cut, . const TString & ; splitOpt . ). prepare the training and test trees -> same cuts for signal and background ; Definition at line 632 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [2/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; NsigTrain, . Int_t ; NbkgTrain, . Int_t ; NsigTest, . Int_t ; NbkgTest, . const TString & ; otherOpt = ""SplitMode=Random:!V"" . ). prepare the training and test trees ; Definition at line 602 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [3/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; const TCut & ; cut, . Int_t ; Ntrain, . Int_t ; Ntest = -1 . ). prepare the training and test trees kept for backward compatibility ; Definition at line 618 of file DataLoader.cxx. ◆ PrepareTrainingAndTestTree() [4/4]. void TMVA::DataLoader::PrepareTrainingAndTestTree ; (; TCut ; sigcut, . TCut ; bkgcut, . const TString & ; splitOpt . ). prepare the training and test trees ; Definition at line 644 of file DataLoader.cxx. ◆ RecombineKFoldDataSet(). void TMVA::DataLoader::RecombineKFoldDataSet ; (; CvSplit & ; s, . Types::ETreeType ; tt = Types::kTraining . ). Recombines the dataset. ; The precise semantics depend on the actual split.; Similar to the inverse operation of MakeKFoldDataSet but will differ. See documentation for each particular split for more information. ; Definition at line 683 of file DataLoader.cxx. ◆ SetBackgroundTree(). void TMVA::DataLoader::SetBackgroundTree ; (; TTree * ; background, . Double_t ; weight = 1.0 . ). Definition at line 439 of file DataLoader.cxx. ◆ SetBackgroundWeightExpression(). void TMVA::DataLoader::SetBackgroundWeightExpression ; (; const TString & ; variable). Definition at line 556 of file DataLoader.cxx. ◆ SetCut() [1/2]. void TMVA::DataLoader::SetCut ; (; const TCut & ; cut, . const TString & ; className = """" . ). Definition at line 581 of file DataLoader.cxx. ◆ SetCut() [2/2]. void TMVA::DataLoader::SetCut ; (; co",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataLoader.html:37792,Testability,test,test,37792,"nt_t TMVA::DataLoader::fATreeType = 0. private . type of event (=classIndex) ; Definition at line 206 of file DataLoader.h. ◆ fATreeWeight. Float_t TMVA::DataLoader::fATreeWeight = 0.0. private . weight of the event ; Definition at line 207 of file DataLoader.h. ◆ fDataAssignType. DataAssignType TMVA::DataLoader::fDataAssignType. private . flags for data assigning ; Definition at line 202 of file DataLoader.h. ◆ fDataInputHandler. DataInputHandler* TMVA::DataLoader::fDataInputHandler. private . -> ; Definition at line 189 of file DataLoader.h. ◆ fDataSetManager. DataSetManager* TMVA::DataLoader::fDataSetManager. private . Definition at line 186 of file DataLoader.h. ◆ fDefaultTrfs. std::vector<TMVA::VariableTransformBase*> TMVA::DataLoader::fDefaultTrfs. private . list of transformations on default DataSet ; Definition at line 191 of file DataLoader.h. ◆ fOptions. TString TMVA::DataLoader::fOptions. private . option string given by construction (presently only ""V"") ; Definition at line 194 of file DataLoader.h. ◆ fTestAssignTree. std::vector<TTree*> TMVA::DataLoader::fTestAssignTree. private . for each class: tmp tree if user wants to assign the events directly ; Definition at line 204 of file DataLoader.h. ◆ fTrainAssignTree. std::vector<TTree*> TMVA::DataLoader::fTrainAssignTree. private . for each class: tmp tree if user wants to assign the events directly ; Definition at line 203 of file DataLoader.h. ◆ fTransformations. TString TMVA::DataLoader::fTransformations. private . List of transformations to test. ; Definition at line 195 of file DataLoader.h. ◆ fVerbose. Bool_t TMVA::DataLoader::fVerbose. private . verbose mode ; Definition at line 196 of file DataLoader.h. Libraries for TMVA::DataLoader:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DataLoader.h; tmva/tmva/src/DataLoader.cxx. TMVADataLoader. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataLoader.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:7010,Availability,error,error,7010,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:7099,Availability,error,error,7099,"ecord whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:7254,Availability,error,error,7254,"Name () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:7551,Availability,error,error,7551,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:11635,Availability,error,error,11635,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:7016,Integrability,message,message,7016,"ject. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:7557,Integrability,message,message,7557,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:8547,Integrability,message,message,8547,"ssue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class shou",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:11641,Integrability,message,message,11641,"n object drawn in a pad to the top of the display list. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:12014,Integrability,message,message,12014," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  G",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:14304,Integrability,message,message,14304,"yTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ;  ; Long64_t fTrainingBlockSize;  block size into which the training dataset is divided ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:26430,Integrability,message,message,26430,"s. private . number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ; Definition at line 158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEventWeights. private . true if at least one signal or bkg event has negative weight ; Definition at line 161 of file DataSet.h. ◆ fLogger. MsgLogger* TMVA::DataSet::fLogger. mutableprivate . ! message logger ; Definition at line 163 of file DataSet.h. ◆ fResults. std::vector< std::map< TString, Results* > > TMVA::DataSet::fResults. private . ! [train/test/...][method-identifier] ; Definition at line 143 of file DataSet.h. ◆ fSampling. std::vector<Char_t> TMVA::DataSet::fSampling. private . random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ; Definition at line 149 of file DataSet.h. ◆ fSamplingEventList. std::vector< std::vector< std::pair< Float_t, Long64_t > > > TMVA::DataSet::fSamplingEventList. mutableprivate . weights and indices for sampling ; Definition at line 152 of file DataSet.h. ◆ fSamplingNEvents. std::vector<Int_t> TMVA::DataSet::fSamplingNEvents. private . number of events which should be sampled ; Definition at line 150 of file DataSet.h. ◆ fSamplingRandom. TRandom3* TMVA::DataSet::fSamplingRandom. private . -> random generator for sampling ; Definition at line 154 of file DataSet.h. ◆ fSamplingSel",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:2520,Modifiability,variab,variables,2520,"Event (Long64_t ievt, Types::ETreeType type) const;  ; const std::vector< Event * > & GetEventCollection (Types::ETreeType type=Types::kMaxTreeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of background training events in dataset ;  ; Long64_t GetNEvtSigTest ();  return number of signal test events in dataset ;  ; Long64_t GetNEvtSigTrain ();  return number of signal training events in dataset ;  ; UInt_t GetNSpectators () const;  access the number of targets through the datasetinfo ;  ; UInt_t GetNTargets () const;  access the number of targets through the datasetinfo ;  ; Long64_t GetNTestEvents () const;  ; Long64_t GetNTrainingEvents () const;  ; UInt_t GetNVariables () const;  access the number of variables through the datasetinfo ;  ; Results * GetResults (const TString &, Types::ETreeType type, Types::EAnalysisType analysistype);  ; const Event * GetTestEvent (Long64_t ievt) const;  ; const Event * GetTrainingEvent (Long64_t ievt) const;  ; TTree * GetTree (Types::ETreeType type);  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t weight, UInt_t seed=0);  initialize random or importance sampling ;  ; virtual TClass * IsA () const;  ; void MoveTrainingBlock (Int_t blockInd, Types::ETreeType dest, Bool_t applyChanges=kTRUE);  move training block ;  ; void SetCurrentEvent (Long64_t ievt) const;  ; void SetCurrentType (Types::ETreeType type) const;  ; void SetEventCollection (std::vector< Event * > *, Types::ETreeType, Bool_t deleteEvents=true);  Sets the ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:2856,Modifiability,variab,variables,2856,"reeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of background training events in dataset ;  ; Long64_t GetNEvtSigTest ();  return number of signal test events in dataset ;  ; Long64_t GetNEvtSigTrain ();  return number of signal training events in dataset ;  ; UInt_t GetNSpectators () const;  access the number of targets through the datasetinfo ;  ; UInt_t GetNTargets () const;  access the number of targets through the datasetinfo ;  ; Long64_t GetNTestEvents () const;  ; Long64_t GetNTrainingEvents () const;  ; UInt_t GetNVariables () const;  access the number of variables through the datasetinfo ;  ; Results * GetResults (const TString &, Types::ETreeType type, Types::EAnalysisType analysistype);  ; const Event * GetTestEvent (Long64_t ievt) const;  ; const Event * GetTrainingEvent (Long64_t ievt) const;  ; TTree * GetTree (Types::ETreeType type);  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t weight, UInt_t seed=0);  initialize random or importance sampling ;  ; virtual TClass * IsA () const;  ; void MoveTrainingBlock (Int_t blockInd, Types::ETreeType dest, Bool_t applyChanges=kTRUE);  move training block ;  ; void SetCurrentEvent (Long64_t ievt) const;  ; void SetCurrentType (Types::ETreeType type) const;  ; void SetEventCollection (std::vector< Event * > *, Types::ETreeType, Bool_t deleteEvents=true);  Sets the event collection (by DataSetFactory) ;  ; void SetVerbose (Bool_t);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject.",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:3813,Modifiability,inherit,inherited,3813,");  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t weight, UInt_t seed=0);  initialize random or importance sampling ;  ; virtual TClass * IsA () const;  ; void MoveTrainingBlock (Int_t blockInd, Types::ETreeType dest, Bool_t applyChanges=kTRUE);  move training block ;  ; void SetCurrentEvent (Long64_t ievt) const;  ; void SetCurrentType (Types::ETreeType type) const;  ; void SetEventCollection (std::vector< Event * > *, Types::ETreeType, Bool_t deleteEvents=true);  Sets the event collection (by DataSetFactory) ;  ; void SetVerbose (Bool_t);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; UInt_t TreeIndex (Types::ETreeType type) const;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:5553,Modifiability,inherit,inherited,5553,"ide;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:6646,Modifiability,inherit,inheritance,6646,"  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:8645,Modifiability,inherit,inherits,8645,"e) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:8762,Modifiability,inherit,inherits,8762,"t be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its parent, however, sometimes it is necessary). ;  ; virtual Bool_t Notify ();  This method must be overridden to handle object notification (the base ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:12515,Modifiability,inherit,inherited,12515,"stem error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; void ApplyTrainingBlockDivision ();  ; void ApplyTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block i",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:12725,Modifiability,inherit,inherited,12725,"stem error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TNamed; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; void ApplyTrainingBlockDivision ();  ; void ApplyTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block i",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:15250,Modifiability,inherit,inherited,15250,"logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ;  ; Long64_t fTrainingBlockSize;  block size into which the training dataset is divided ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataSet.h>. Inheritance diagram for TMVA::D",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:15891,Modifiability,inherit,inherited,15891,"plingWeight;  weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ;  ; Long64_t fTrainingBlockSize;  block size into which the training dataset is divided ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataSet.h>. Inheritance diagram for TMVA::DataSet:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DataSet() [1/2]. TMVA::DataSet::DataSet ; (; ). constructor ; Definition at line 91 of file DataSet.cxx. ◆ DataSet() [2/2]. TMVA::DataSet::DataSet ; (; const DataSetInfo & ; dsi). constructor ; Definition at line 58 of file DataSet.cxx. ◆ ~DataSet(). TMVA::DataSet::~DataSet ; (; ). virtual . destructor ; Definition at line 123 of file DataSet.cxx. Member Function Documentation. ◆ AddEvent(). void TMVA::DataSet::AddEvent ; (; Event * ; ev, . Types::ETreeType ; type . ). add event to event list after which the event is owned by the dataset ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:15990,Modifiability,inherit,inherited,15990,"plingWeight;  weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ;  ; Long64_t fTrainingBlockSize;  block size into which the training dataset is divided ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataSet.h>. Inheritance diagram for TMVA::DataSet:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DataSet() [1/2]. TMVA::DataSet::DataSet ; (; ). constructor ; Definition at line 91 of file DataSet.cxx. ◆ DataSet() [2/2]. TMVA::DataSet::DataSet ; (; const DataSetInfo & ; dsi). constructor ; Definition at line 58 of file DataSet.cxx. ◆ ~DataSet(). TMVA::DataSet::~DataSet ; (; ). virtual . destructor ; Definition at line 123 of file DataSet.cxx. Member Function Documentation. ◆ AddEvent(). void TMVA::DataSet::AddEvent ; (; Event * ; ev, . Types::ETreeType ; type . ). add event to event list after which the event is owned by the dataset ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:16195,Modifiability,inherit,inherited,16195,"d ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  ;  Protected Attributes inherited from TNamed; TString fName;  ; TString fTitle;  . #include <TMVA/DataSet.h>. Inheritance diagram for TMVA::DataSet:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DataSet() [1/2]. TMVA::DataSet::DataSet ; (; ). constructor ; Definition at line 91 of file DataSet.cxx. ◆ DataSet() [2/2]. TMVA::DataSet::DataSet ; (; const DataSetInfo & ; dsi). constructor ; Definition at line 58 of file DataSet.cxx. ◆ ~DataSet(). TMVA::DataSet::~DataSet ; (; ). virtual . destructor ; Definition at line 123 of file DataSet.cxx. Member Function Documentation. ◆ AddEvent(). void TMVA::DataSet::AddEvent ; (; Event * ; ev, . Types::ETreeType ; type . ). add event to event list after which the event is owned by the dataset ; Definition at line 241 of file DataSet.cxx. ◆ ApplyTrainingBlockDivision(). void TMVA::DataSet::ApplyTrainingBlockDivision ; (; ). private . ◆ ApplyTrainingSetDivision(). void TMVA::DataSet::Ap",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:22043,Modifiability,variab,variables,22043,"DataSet::GetNEvtSigTest ; (; ). return number of signal test events in dataset ; Definition at line 427 of file DataSet.cxx. ◆ GetNEvtSigTrain(). Long64_t TMVA::DataSet::GetNEvtSigTrain ; (; ). return number of signal training events in dataset ; Definition at line 443 of file DataSet.cxx. ◆ GetNSpectators(). UInt_t TMVA::DataSet::GetNSpectators ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 232 of file DataSet.cxx. ◆ GetNTargets(). UInt_t TMVA::DataSet::GetNTargets ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 224 of file DataSet.cxx. ◆ GetNTestEvents(). Long64_t TMVA::DataSet::GetNTestEvents ; (; ); const. inline . Definition at line 69 of file DataSet.h. ◆ GetNTrainingEvents(). Long64_t TMVA::DataSet::GetNTrainingEvents ; (; ); const. inline . Definition at line 68 of file DataSet.h. ◆ GetNVariables(). UInt_t TMVA::DataSet::GetNVariables ; (; ); const. access the number of variables through the datasetinfo ; Definition at line 216 of file DataSet.cxx. ◆ GetResults(). TMVA::Results * TMVA::DataSet::GetResults ; (; const TString & ; resultsName, . Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). Definition at line 265 of file DataSet.cxx. ◆ GetTestEvent(). const Event * TMVA::DataSet::GetTestEvent ; (; Long64_t ; ievt); const. inline . Definition at line 75 of file DataSet.h. ◆ GetTrainingEvent(). const Event * TMVA::DataSet::GetTrainingEvent ; (; Long64_t ; ievt); const. inline . Definition at line 74 of file DataSet.h. ◆ GetTree(). TTree * TMVA::DataSet::GetTree ; (; Types::ETreeType ; type). create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ; Definition at line 609 of file DataSet.cxx. ◆ HasNegativeEventWeights(). Bool_t TMVA::DataSet::HasNegativeEventWeights ; (; ); const. inline . Definition at line 101 of file DataSet.h. ◆ IncrementNClassEvents(). void TMVA::DataSet::IncrementNClassE",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:22738,Modifiability,variab,variables,22738,". Definition at line 69 of file DataSet.h. ◆ GetNTrainingEvents(). Long64_t TMVA::DataSet::GetNTrainingEvents ; (; ); const. inline . Definition at line 68 of file DataSet.h. ◆ GetNVariables(). UInt_t TMVA::DataSet::GetNVariables ; (; ); const. access the number of variables through the datasetinfo ; Definition at line 216 of file DataSet.cxx. ◆ GetResults(). TMVA::Results * TMVA::DataSet::GetResults ; (; const TString & ; resultsName, . Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). Definition at line 265 of file DataSet.cxx. ◆ GetTestEvent(). const Event * TMVA::DataSet::GetTestEvent ; (; Long64_t ; ievt); const. inline . Definition at line 75 of file DataSet.h. ◆ GetTrainingEvent(). const Event * TMVA::DataSet::GetTrainingEvent ; (; Long64_t ; ievt); const. inline . Definition at line 74 of file DataSet.h. ◆ GetTree(). TTree * TMVA::DataSet::GetTree ; (; Types::ETreeType ; type). create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ; Definition at line 609 of file DataSet.cxx. ◆ HasNegativeEventWeights(). Bool_t TMVA::DataSet::HasNegativeEventWeights ; (; ); const. inline . Definition at line 101 of file DataSet.h. ◆ IncrementNClassEvents(). void TMVA::DataSet::IncrementNClassEvents ; (; Int_t ; type, . UInt_t ; classNumber . ). Definition at line 151 of file DataSet.cxx. ◆ InitSampling(). void TMVA::DataSet::InitSampling ; (; Float_t ; fraction, . Float_t ; weight, . UInt_t ; seed = 0 . ). initialize random or importance sampling ; Definition at line 459 of file DataSet.cxx. ◆ IsA(). virtual TClass * TMVA::DataSet::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TObject.; Definition at line 175 of file DataSet.h. ◆ Log(). MsgLogger & TMVA::DataSet::Log ; (; ); const. inlineprivate . Definition at line 164 of file DataSet.h. ◆ MoveTrainingBlock(). void TMVA::DataSet::MoveTrainingBlock ; (; Int_t ; blockInd, . Types::ETreeTy",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:13733,Performance,perform,performance,13733,"();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; void ApplyTrainingBlockDivision ();  ; void ApplyTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  w",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:14576,Performance,perform,performance,14576,"ce reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ;  ; Long64_t fTrainingBlockSize;  block size into which the training dataset is divided ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:25280,Performance,perform,performance,25280,"d::vector< Event * > * ; events, . Types::ETreeType ; type, . Bool_t ; deleteEvents = true . ). Sets the event collection (by DataSetFactory) ; Definition at line 250 of file DataSet.cxx. ◆ SetVerbose(). void TMVA::DataSet::SetVerbose ; (; Bool_t ; ). inline . Definition at line 112 of file DataSet.h. ◆ Streamer(). virtual void TMVA::DataSet::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSet::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 175 of file DataSet.h. ◆ TreeIndex(). UInt_t TMVA::DataSet::TreeIndex ; (; Types::ETreeType ; type); const. inline . Definition at line 181 of file DataSet.h. Member Data Documentation. ◆ fBlockBelongToTraining. std::vector<Char_t> TMVA::DataSet::fBlockBelongToTraining. private . when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ; Definition at line 165 of file DataSet.h. ◆ fClassEvents. std::vector< std::vector<Long64_t> > TMVA::DataSet::fClassEvents. private . number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ; Definition at line 158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEvent",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:26850,Performance,perform,performance,26850,"ine 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEventWeights. private . true if at least one signal or bkg event has negative weight ; Definition at line 161 of file DataSet.h. ◆ fLogger. MsgLogger* TMVA::DataSet::fLogger. mutableprivate . ! message logger ; Definition at line 163 of file DataSet.h. ◆ fResults. std::vector< std::map< TString, Results* > > TMVA::DataSet::fResults. private . ! [train/test/...][method-identifier] ; Definition at line 143 of file DataSet.h. ◆ fSampling. std::vector<Char_t> TMVA::DataSet::fSampling. private . random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ; Definition at line 149 of file DataSet.h. ◆ fSamplingEventList. std::vector< std::vector< std::pair< Float_t, Long64_t > > > TMVA::DataSet::fSamplingEventList. mutableprivate . weights and indices for sampling ; Definition at line 152 of file DataSet.h. ◆ fSamplingNEvents. std::vector<Int_t> TMVA::DataSet::fSamplingNEvents. private . number of events which should be sampled ; Definition at line 150 of file DataSet.h. ◆ fSamplingRandom. TRandom3* TMVA::DataSet::fSamplingRandom. private . -> random generator for sampling ; Definition at line 154 of file DataSet.h. ◆ fSamplingSelected. std::vector< std::vector< std::pair< Float_t, Long64_t > > > TMVA::DataSet::fSamplingSelected. mutableprivate . selected events ; Definition at line 153 of file DataSet.h. ◆ fSamplingWeight. std::vector<Float_t> TMVA::DataSet::fSamplingWeight. private . weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ; Definitio",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:2243,Security,access,access,2243,"ance sampling weight of the event when not successful and decrease it when successful ;  ; Types::ETreeType GetCurrentType () const;  ; const Event * GetEvent () const;  returns event without transformations ;  ; const Event * GetEvent (Long64_t ievt) const;  ; const Event * GetEvent (Long64_t ievt, Types::ETreeType type) const;  ; const std::vector< Event * > & GetEventCollection (Types::ETreeType type=Types::kMaxTreeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of background training events in dataset ;  ; Long64_t GetNEvtSigTest ();  return number of signal test events in dataset ;  ; Long64_t GetNEvtSigTrain ();  return number of signal training events in dataset ;  ; UInt_t GetNSpectators () const;  access the number of targets through the datasetinfo ;  ; UInt_t GetNTargets () const;  access the number of targets through the datasetinfo ;  ; Long64_t GetNTestEvents () const;  ; Long64_t GetNTrainingEvents () const;  ; UInt_t GetNVariables () const;  access the number of variables through the datasetinfo ;  ; Results * GetResults (const TString &, Types::ETreeType type, Types::EAnalysisType analysistype);  ; const Event * GetTestEvent (Long64_t ievt) const;  ; const Event * GetTrainingEvent (Long64_t ievt) const;  ; TTree * GetTree (Types::ETreeType type);  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t weight, UInt_t seed=0);  initialize random or importance sampling ;  ; virtual TClass * IsA () const;  ; void MoveTrainingBlock (Int_t blockInd, Types:",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:2331,Security,access,access,2331," ; Types::ETreeType GetCurrentType () const;  ; const Event * GetEvent () const;  returns event without transformations ;  ; const Event * GetEvent (Long64_t ievt) const;  ; const Event * GetEvent (Long64_t ievt, Types::ETreeType type) const;  ; const std::vector< Event * > & GetEventCollection (Types::ETreeType type=Types::kMaxTreeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of background training events in dataset ;  ; Long64_t GetNEvtSigTest ();  return number of signal test events in dataset ;  ; Long64_t GetNEvtSigTrain ();  return number of signal training events in dataset ;  ; UInt_t GetNSpectators () const;  access the number of targets through the datasetinfo ;  ; UInt_t GetNTargets () const;  access the number of targets through the datasetinfo ;  ; Long64_t GetNTestEvents () const;  ; Long64_t GetNTrainingEvents () const;  ; UInt_t GetNVariables () const;  access the number of variables through the datasetinfo ;  ; Results * GetResults (const TString &, Types::ETreeType type, Types::EAnalysisType analysistype);  ; const Event * GetTestEvent (Long64_t ievt) const;  ; const Event * GetTrainingEvent (Long64_t ievt) const;  ; TTree * GetTree (Types::ETreeType type);  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t weight, UInt_t seed=0);  initialize random or importance sampling ;  ; virtual TClass * IsA () const;  ; void MoveTrainingBlock (Int_t blockInd, Types::ETreeType dest, Bool_t applyChanges=kTRUE);  move training block ;  ; void SetCurrentEv",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:2499,Security,access,access,2499,"t;  ; const Event * GetEvent (Long64_t ievt, Types::ETreeType type) const;  ; const std::vector< Event * > & GetEventCollection (Types::ETreeType type=Types::kMaxTreeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of background training events in dataset ;  ; Long64_t GetNEvtSigTest ();  return number of signal test events in dataset ;  ; Long64_t GetNEvtSigTrain ();  return number of signal training events in dataset ;  ; UInt_t GetNSpectators () const;  access the number of targets through the datasetinfo ;  ; UInt_t GetNTargets () const;  access the number of targets through the datasetinfo ;  ; Long64_t GetNTestEvents () const;  ; Long64_t GetNTrainingEvents () const;  ; UInt_t GetNVariables () const;  access the number of variables through the datasetinfo ;  ; Results * GetResults (const TString &, Types::ETreeType type, Types::EAnalysisType analysistype);  ; const Event * GetTestEvent (Long64_t ievt) const;  ; const Event * GetTrainingEvent (Long64_t ievt) const;  ; TTree * GetTree (Types::ETreeType type);  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t weight, UInt_t seed=0);  initialize random or importance sampling ;  ; virtual TClass * IsA () const;  ; void MoveTrainingBlock (Int_t blockInd, Types::ETreeType dest, Bool_t applyChanges=kTRUE);  move training block ;  ; void SetCurrentEvent (Long64_t ievt) const;  ; void SetCurrentType (Types::ETreeType type) const;  ; void SetEventCollection (std::vector< Event * > *, Types::ETreeType, Bool_t deleteEv",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:4681,Security,hash,hash,4681," void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; UInt_t TreeIndex (Types::ETreeType type) const;  ;  Public Member Functions inherited from TNamed;  TNamed ();  ;  TNamed (const char *name, const char *title);  ;  TNamed (const TNamed &named);  TNamed copy ctor. ;  ;  TNamed (const TString &name, const TString &title);  ; virtual ~TNamed ();  TNamed destructor. ;  ; void Clear (Option_t *option="""") override;  Set name and title to empty strings (""""). ;  ; TObject * Clone (const char *newname="""") const override;  Make a clone of an object using the Streamer facility. ;  ; Int_t Compare (const TObject *obj) const override;  Compare two TNamed objects. ;  ; void Copy (TObject &named) const override;  Copy this to obj. ;  ; virtual void FillBuffer (char *&buffer);  Encode TNamed into output buffer. ;  ; const char * GetName () const override;  Returns name of object. ;  ; const char * GetTitle () const override;  Returns title of object. ;  ; ULong_t Hash () const override;  Return hash value for this object. ;  ; TClass * IsA () const override;  ; Bool_t IsSortable () const override;  ; void ls (Option_t *option="""") const override;  List TNamed name and title. ;  ; TNamed & operator= (const TNamed &rhs);  TNamed assignment operator. ;  ; void Print (Option_t *option="""") const override;  Print TNamed name and title. ;  ; virtual void SetName (const char *name);  Set the name of the TNamed. ;  ; virtual void SetNameTitle (const char *name, const char *title);  Set all the TNamed parameters (name and title). ;  ; virtual void SetTitle (const char *title="""");  Set the title of the TNamed. ;  ; virtual Int_t Sizeof () const;  Return size of the TNamed part of the TObject. ;  ; void Streamer (TBuffer &) override;  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:13668,Security,validat,validation,13668,"();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; void ApplyTrainingBlockDivision ();  ; void ApplyTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  w",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:13913,Security,validat,validation,13913,"stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; void ApplyTrainingBlockDivision ();  ; void ApplyTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:21432,Security,access,access,21432,"Number . ). Definition at line 168 of file DataSet.cxx. ◆ GetNEvents(). Long64_t TMVA::DataSet::GetNEvents ; (; Types::ETreeType ; type = Types::kMaxTreeType); const. inline . Definition at line 206 of file DataSet.h. ◆ GetNEvtBkgdTest(). Long64_t TMVA::DataSet::GetNEvtBkgdTest ; (; ). return number of background test events in dataset ; Definition at line 435 of file DataSet.cxx. ◆ GetNEvtBkgdTrain(). Long64_t TMVA::DataSet::GetNEvtBkgdTrain ; (; ). return number of background training events in dataset ; Definition at line 451 of file DataSet.cxx. ◆ GetNEvtSigTest(). Long64_t TMVA::DataSet::GetNEvtSigTest ; (; ). return number of signal test events in dataset ; Definition at line 427 of file DataSet.cxx. ◆ GetNEvtSigTrain(). Long64_t TMVA::DataSet::GetNEvtSigTrain ; (; ). return number of signal training events in dataset ; Definition at line 443 of file DataSet.cxx. ◆ GetNSpectators(). UInt_t TMVA::DataSet::GetNSpectators ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 232 of file DataSet.cxx. ◆ GetNTargets(). UInt_t TMVA::DataSet::GetNTargets ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 224 of file DataSet.cxx. ◆ GetNTestEvents(). Long64_t TMVA::DataSet::GetNTestEvents ; (; ); const. inline . Definition at line 69 of file DataSet.h. ◆ GetNTrainingEvents(). Long64_t TMVA::DataSet::GetNTrainingEvents ; (; ); const. inline . Definition at line 68 of file DataSet.h. ◆ GetNVariables(). UInt_t TMVA::DataSet::GetNVariables ; (; ); const. access the number of variables through the datasetinfo ; Definition at line 216 of file DataSet.cxx. ◆ GetResults(). TMVA::Results * TMVA::DataSet::GetResults ; (; const TString & ; resultsName, . Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). Definition at line 265 of file DataSet.cxx. ◆ GetTestEvent(). const Event * TMVA::DataSet::GetTestEvent ; (; Long64_t ; ievt); const. inline . Definition at line 75 of file DataSet.h. ◆ GetTra",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:21597,Security,access,access,21597,". inline . Definition at line 206 of file DataSet.h. ◆ GetNEvtBkgdTest(). Long64_t TMVA::DataSet::GetNEvtBkgdTest ; (; ). return number of background test events in dataset ; Definition at line 435 of file DataSet.cxx. ◆ GetNEvtBkgdTrain(). Long64_t TMVA::DataSet::GetNEvtBkgdTrain ; (; ). return number of background training events in dataset ; Definition at line 451 of file DataSet.cxx. ◆ GetNEvtSigTest(). Long64_t TMVA::DataSet::GetNEvtSigTest ; (; ). return number of signal test events in dataset ; Definition at line 427 of file DataSet.cxx. ◆ GetNEvtSigTrain(). Long64_t TMVA::DataSet::GetNEvtSigTrain ; (; ). return number of signal training events in dataset ; Definition at line 443 of file DataSet.cxx. ◆ GetNSpectators(). UInt_t TMVA::DataSet::GetNSpectators ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 232 of file DataSet.cxx. ◆ GetNTargets(). UInt_t TMVA::DataSet::GetNTargets ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 224 of file DataSet.cxx. ◆ GetNTestEvents(). Long64_t TMVA::DataSet::GetNTestEvents ; (; ); const. inline . Definition at line 69 of file DataSet.h. ◆ GetNTrainingEvents(). Long64_t TMVA::DataSet::GetNTrainingEvents ; (; ); const. inline . Definition at line 68 of file DataSet.h. ◆ GetNVariables(). UInt_t TMVA::DataSet::GetNVariables ; (; ); const. access the number of variables through the datasetinfo ; Definition at line 216 of file DataSet.cxx. ◆ GetResults(). TMVA::Results * TMVA::DataSet::GetResults ; (; const TString & ; resultsName, . Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). Definition at line 265 of file DataSet.cxx. ◆ GetTestEvent(). const Event * TMVA::DataSet::GetTestEvent ; (; Long64_t ; ievt); const. inline . Definition at line 75 of file DataSet.h. ◆ GetTrainingEvent(). const Event * TMVA::DataSet::GetTrainingEvent ; (; Long64_t ; ievt); const. inline . Definition at line 74 of file DataSet.h. ◆ GetTree(). TTree * TMVA",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:22022,Security,access,access,22022,"DataSet::GetNEvtSigTest ; (; ). return number of signal test events in dataset ; Definition at line 427 of file DataSet.cxx. ◆ GetNEvtSigTrain(). Long64_t TMVA::DataSet::GetNEvtSigTrain ; (; ). return number of signal training events in dataset ; Definition at line 443 of file DataSet.cxx. ◆ GetNSpectators(). UInt_t TMVA::DataSet::GetNSpectators ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 232 of file DataSet.cxx. ◆ GetNTargets(). UInt_t TMVA::DataSet::GetNTargets ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 224 of file DataSet.cxx. ◆ GetNTestEvents(). Long64_t TMVA::DataSet::GetNTestEvents ; (; ); const. inline . Definition at line 69 of file DataSet.h. ◆ GetNTrainingEvents(). Long64_t TMVA::DataSet::GetNTrainingEvents ; (; ); const. inline . Definition at line 68 of file DataSet.h. ◆ GetNVariables(). UInt_t TMVA::DataSet::GetNVariables ; (; ); const. access the number of variables through the datasetinfo ; Definition at line 216 of file DataSet.cxx. ◆ GetResults(). TMVA::Results * TMVA::DataSet::GetResults ; (; const TString & ; resultsName, . Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). Definition at line 265 of file DataSet.cxx. ◆ GetTestEvent(). const Event * TMVA::DataSet::GetTestEvent ; (; Long64_t ; ievt); const. inline . Definition at line 75 of file DataSet.h. ◆ GetTrainingEvent(). const Event * TMVA::DataSet::GetTrainingEvent ; (; Long64_t ; ievt); const. inline . Definition at line 74 of file DataSet.h. ◆ GetTree(). TTree * TMVA::DataSet::GetTree ; (; Types::ETreeType ; type). create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ; Definition at line 609 of file DataSet.cxx. ◆ HasNegativeEventWeights(). Bool_t TMVA::DataSet::HasNegativeEventWeights ; (; ); const. inline . Definition at line 101 of file DataSet.h. ◆ IncrementNClassEvents(). void TMVA::DataSet::IncrementNClassE",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:25215,Security,validat,validation,25215,"d::vector< Event * > * ; events, . Types::ETreeType ; type, . Bool_t ; deleteEvents = true . ). Sets the event collection (by DataSetFactory) ; Definition at line 250 of file DataSet.cxx. ◆ SetVerbose(). void TMVA::DataSet::SetVerbose ; (; Bool_t ; ). inline . Definition at line 112 of file DataSet.h. ◆ Streamer(). virtual void TMVA::DataSet::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSet::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 175 of file DataSet.h. ◆ TreeIndex(). UInt_t TMVA::DataSet::TreeIndex ; (; Types::ETreeType ; type); const. inline . Definition at line 181 of file DataSet.h. Member Data Documentation. ◆ fBlockBelongToTraining. std::vector<Char_t> TMVA::DataSet::fBlockBelongToTraining. private . when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ; Definition at line 165 of file DataSet.h. ◆ fClassEvents. std::vector< std::vector<Long64_t> > TMVA::DataSet::fClassEvents. private . number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ; Definition at line 158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEvent",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:25537,Security,validat,validation,25537,". virtual void TMVA::DataSet::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSet::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 175 of file DataSet.h. ◆ TreeIndex(). UInt_t TMVA::DataSet::TreeIndex ; (; Types::ETreeType ; type); const. inline . Definition at line 181 of file DataSet.h. Member Data Documentation. ◆ fBlockBelongToTraining. std::vector<Char_t> TMVA::DataSet::fBlockBelongToTraining. private . when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ; Definition at line 165 of file DataSet.h. ◆ fClassEvents. std::vector< std::vector<Long64_t> > TMVA::DataSet::fClassEvents. private . number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ; Definition at line 158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEventWeights. private . true if at least one signal or bkg event has negative weight ; Definition at line 161 of file DataSet.h. ◆ fLogger. MsgLogger* TMVA::DataSet::fLogger. mutableprivate . ! message logger ; Definition at line 163 of file DataSet.h. ◆ fResults. std::vector< std::map< TString, Results* > > TMVA::Data",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:1924,Testability,test,test,1924," ;  ; void DivideTrainingSet (UInt_t blockNum);  divide training set ;  ; void EventResult (Bool_t successful, Long64_t evtNumber=-1);  increase the importance sampling weight of the event when not successful and decrease it when successful ;  ; Types::ETreeType GetCurrentType () const;  ; const Event * GetEvent () const;  returns event without transformations ;  ; const Event * GetEvent (Long64_t ievt) const;  ; const Event * GetEvent (Long64_t ievt, Types::ETreeType type) const;  ; const std::vector< Event * > & GetEventCollection (Types::ETreeType type=Types::kMaxTreeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of b",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:2096,Testability,test,test,2096,"oid DivideTrainingSet (UInt_t blockNum);  divide training set ;  ; void EventResult (Bool_t successful, Long64_t evtNumber=-1);  increase the importance sampling weight of the event when not successful and decrease it when successful ;  ; Types::ETreeType GetCurrentType () const;  ; const Event * GetEvent () const;  returns event without transformations ;  ; const Event * GetEvent (Long64_t ievt) const;  ; const Event * GetEvent (Long64_t ievt, Types::ETreeType type) const;  ; const std::vector< Event * > & GetEventCollection (Types::ETreeType type=Types::kMaxTreeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of background training events in dataset ;  ; Long64_t GetNEvtSigTest ();  return number of signal test events in dataset ;  ; Long64_t GetNEvtSigTrain ();  return number of signal training events in dataset ;  ; UInt_t GetNSpectators () const;  access the number of targets through the datasetinfo ;  ; UInt_t GetNTargets () const;  access the number of targets through the datasetinfo ;  ; Long64_t GetNTestEvents () const;  ; Long64_t GetNTrainingEvents () const;  ; UInt_t GetNVariables () const;  access the number of variables through the datasetinfo ;  ; Results * GetResults (const TString &, Types::ETreeType type, Types::EAnalysisType analysistype);  ; const Event * GetTestEvent (Long64_t ievt) const;  ; const Event * GetTrainingEvent (Long64_t ievt) const;  ; TTree * GetTree (Types::ETreeType type);  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t wei",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:2823,Testability,test,test,2823,"reeType) const;  ; const TTree * GetEventCollectionAsTree ();  ; Long64_t GetNClassEvents (Int_t type, UInt_t classNumber);  ; Long64_t GetNEvents (Types::ETreeType type=Types::kMaxTreeType) const;  ; Long64_t GetNEvtBkgdTest ();  return number of background test events in dataset ;  ; Long64_t GetNEvtBkgdTrain ();  return number of background training events in dataset ;  ; Long64_t GetNEvtSigTest ();  return number of signal test events in dataset ;  ; Long64_t GetNEvtSigTrain ();  return number of signal training events in dataset ;  ; UInt_t GetNSpectators () const;  access the number of targets through the datasetinfo ;  ; UInt_t GetNTargets () const;  access the number of targets through the datasetinfo ;  ; Long64_t GetNTestEvents () const;  ; Long64_t GetNTrainingEvents () const;  ; UInt_t GetNVariables () const;  access the number of variables through the datasetinfo ;  ; Results * GetResults (const TString &, Types::ETreeType type, Types::EAnalysisType analysistype);  ; const Event * GetTestEvent (Long64_t ievt) const;  ; const Event * GetTrainingEvent (Long64_t ievt) const;  ; TTree * GetTree (Types::ETreeType type);  create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ;  ; Bool_t HasNegativeEventWeights () const;  ; void IncrementNClassEvents (Int_t type, UInt_t classNumber);  ; void InitSampling (Float_t fraction, Float_t weight, UInt_t seed=0);  initialize random or importance sampling ;  ; virtual TClass * IsA () const;  ; void MoveTrainingBlock (Int_t blockInd, Types::ETreeType dest, Bool_t applyChanges=kTRUE);  move training block ;  ; void SetCurrentEvent (Long64_t ievt) const;  ; void SetCurrentType (Types::ETreeType type) const;  ; void SetEventCollection (std::vector< Event * > *, Types::ETreeType, Bool_t deleteEvents=true);  Sets the event collection (by DataSetFactory) ;  ; void SetVerbose (Bool_t);  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject.",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:13900,Testability,test,testing,13900,"stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; void ApplyTrainingBlockDivision ();  ; void ApplyTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:14163,Testability,test,testing,14163,"stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; void ApplyTrainingBlockDivision ();  ; void ApplyTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:14312,Testability,log,logger,14312,"yTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ;  ; Long64_t fTrainingBlockSize;  block size into which the training dataset is divided ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:14390,Testability,test,test,14390,"yTrainingSetDivision ();  apply division of data set ;  ; void DestroyCollection (Types::ETreeType type, Bool_t deleteEvents);  destroys the event collection (events + vector) ;  ; MsgLogger & Log () const;  . Private Attributes; std::vector< Char_t > fBlockBelongToTraining;  when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ;  ; std::vector< std::vector< Long64_t > > fClassEvents;  number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ;  ; Long64_t fCurrentEventIdx;  ; UInt_t fCurrentTreeIdx;  ; const DataSetInfo * fdsi;  -> datasetinfo that created this dataset ;  ; std::vector< std::vector< Event * > > fEventCollection;  list of events for training/testing/... ;  ; Bool_t fHasNegativeEventWeights;  true if at least one signal or bkg event has negative weight ;  ; MsgLogger * fLogger;  ! message logger ;  ; std::vector< std::map< TString, Results * > > fResults;  ! [train/test/...][method-identifier] ;  ; std::vector< Char_t > fSampling;  random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingEventList;  weights and indices for sampling ;  ; std::vector< Int_t > fSamplingNEvents;  number of events which should be sampled ;  ; TRandom3 * fSamplingRandom;  -> random generator for sampling ;  ; std::vector< std::vector< std::pair< Float_t, Long64_t > > > fSamplingSelected;  selected events ;  ; std::vector< Float_t > fSamplingWeight;  weight change factor [weight is indicating if sampling is random (1.0) or importance (<1.0)] ;  ; Long64_t fTrainingBlockSize;  block size into which the training dataset is divided ;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:18719,Testability,test,testing,18719," of file DataSet.h. ◆ ClearNClassEvents(). void TMVA::DataSet::ClearNClassEvents ; (; Int_t ; type). Definition at line 160 of file DataSet.cxx. ◆ CreateSampling(). void TMVA::DataSet::CreateSampling ; (; ); const. create an event sampling (random or importance sampling) ; Definition at line 508 of file DataSet.cxx. ◆ DeclFileName(). static const char * TMVA::DataSet::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 175 of file DataSet.h. ◆ DeleteAllResults(). void TMVA::DataSet::DeleteAllResults ; (; Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). Deletes all results currently in the dataset. ; Definition at line 343 of file DataSet.cxx. ◆ DeleteResults(). void TMVA::DataSet::DeleteResults ; (; const TString & ; resultsName, . Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). delete the results stored for this particular Method instance. ; (here apparently called resultsName instead of MethodTitle Tree type (Training, testing etc..) Analysis Type (Classification, Multiclass, Regression etc..) ; Definition at line 316 of file DataSet.cxx. ◆ DestroyCollection(). void TMVA::DataSet::DestroyCollection ; (; Types::ETreeType ; type, . Bool_t ; deleteEvents . ). private . destroys the event collection (events + vector) ; Definition at line 189 of file DataSet.cxx. ◆ DivideTrainingSet(). void TMVA::DataSet::DivideTrainingSet ; (; UInt_t ; blockNum). divide training set ; Definition at line 371 of file DataSet.cxx. ◆ EventResult(). void TMVA::DataSet::EventResult ; (; Bool_t ; successful, . Long64_t ; evtNumber = -1 . ). increase the importance sampling weight of the event when not successful and decrease it when successful ; Definition at line 572 of file DataSet.cxx. ◆ GetCurrentType(). TMVA::Types::ETreeType TMVA::DataSet::GetCurrentType ; (; ); const. inline . Definition at line 194 of file DataSet.h. ◆ GetEvent() [1/3]. const TMVA::Event * TMVA::DataSet::GetEvent ; (; ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:20793,Testability,test,test,20793,"* TMVA::DataSet::GetEvent ; (; Long64_t ; ievt); const. inline . Definition at line 73 of file DataSet.h. ◆ GetEvent() [3/3]. const Event * TMVA::DataSet::GetEvent ; (; Long64_t ; ievt, . Types::ETreeType ; type . ); const. inline . Definition at line 76 of file DataSet.h. ◆ GetEventCollection(). const std::vector< TMVA::Event * > & TMVA::DataSet::GetEventCollection ; (; Types::ETreeType ; type = Types::kMaxTreeType); const. inline . Definition at line 216 of file DataSet.h. ◆ GetEventCollectionAsTree(). const TTree * TMVA::DataSet::GetEventCollectionAsTree ; (; ). ◆ GetNClassEvents(). Long64_t TMVA::DataSet::GetNClassEvents ; (; Int_t ; type, . UInt_t ; classNumber . ). Definition at line 168 of file DataSet.cxx. ◆ GetNEvents(). Long64_t TMVA::DataSet::GetNEvents ; (; Types::ETreeType ; type = Types::kMaxTreeType); const. inline . Definition at line 206 of file DataSet.h. ◆ GetNEvtBkgdTest(). Long64_t TMVA::DataSet::GetNEvtBkgdTest ; (; ). return number of background test events in dataset ; Definition at line 435 of file DataSet.cxx. ◆ GetNEvtBkgdTrain(). Long64_t TMVA::DataSet::GetNEvtBkgdTrain ; (; ). return number of background training events in dataset ; Definition at line 451 of file DataSet.cxx. ◆ GetNEvtSigTest(). Long64_t TMVA::DataSet::GetNEvtSigTest ; (; ). return number of signal test events in dataset ; Definition at line 427 of file DataSet.cxx. ◆ GetNEvtSigTrain(). Long64_t TMVA::DataSet::GetNEvtSigTrain ; (; ). return number of signal training events in dataset ; Definition at line 443 of file DataSet.cxx. ◆ GetNSpectators(). UInt_t TMVA::DataSet::GetNSpectators ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 232 of file DataSet.cxx. ◆ GetNTargets(). UInt_t TMVA::DataSet::GetNTargets ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 224 of file DataSet.cxx. ◆ GetNTestEvents(). Long64_t TMVA::DataSet::GetNTestEvents ; (; ); const. inline . Definition at line 69 of file Da",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:21125,Testability,test,test,21125," TMVA::DataSet::GetEventCollection ; (; Types::ETreeType ; type = Types::kMaxTreeType); const. inline . Definition at line 216 of file DataSet.h. ◆ GetEventCollectionAsTree(). const TTree * TMVA::DataSet::GetEventCollectionAsTree ; (; ). ◆ GetNClassEvents(). Long64_t TMVA::DataSet::GetNClassEvents ; (; Int_t ; type, . UInt_t ; classNumber . ). Definition at line 168 of file DataSet.cxx. ◆ GetNEvents(). Long64_t TMVA::DataSet::GetNEvents ; (; Types::ETreeType ; type = Types::kMaxTreeType); const. inline . Definition at line 206 of file DataSet.h. ◆ GetNEvtBkgdTest(). Long64_t TMVA::DataSet::GetNEvtBkgdTest ; (; ). return number of background test events in dataset ; Definition at line 435 of file DataSet.cxx. ◆ GetNEvtBkgdTrain(). Long64_t TMVA::DataSet::GetNEvtBkgdTrain ; (; ). return number of background training events in dataset ; Definition at line 451 of file DataSet.cxx. ◆ GetNEvtSigTest(). Long64_t TMVA::DataSet::GetNEvtSigTest ; (; ). return number of signal test events in dataset ; Definition at line 427 of file DataSet.cxx. ◆ GetNEvtSigTrain(). Long64_t TMVA::DataSet::GetNEvtSigTrain ; (; ). return number of signal training events in dataset ; Definition at line 443 of file DataSet.cxx. ◆ GetNSpectators(). UInt_t TMVA::DataSet::GetNSpectators ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 232 of file DataSet.cxx. ◆ GetNTargets(). UInt_t TMVA::DataSet::GetNTargets ; (; ); const. access the number of targets through the datasetinfo ; Definition at line 224 of file DataSet.cxx. ◆ GetNTestEvents(). Long64_t TMVA::DataSet::GetNTestEvents ; (; ); const. inline . Definition at line 69 of file DataSet.h. ◆ GetNTrainingEvents(). Long64_t TMVA::DataSet::GetNTrainingEvents ; (; ); const. inline . Definition at line 68 of file DataSet.h. ◆ GetNVariables(). UInt_t TMVA::DataSet::GetNVariables ; (; ); const. access the number of variables through the datasetinfo ; Definition at line 216 of file DataSet.cxx. ◆ GetResults(). TMVA::",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:22705,Testability,test,test,22705,". Definition at line 69 of file DataSet.h. ◆ GetNTrainingEvents(). Long64_t TMVA::DataSet::GetNTrainingEvents ; (; ); const. inline . Definition at line 68 of file DataSet.h. ◆ GetNVariables(). UInt_t TMVA::DataSet::GetNVariables ; (; ); const. access the number of variables through the datasetinfo ; Definition at line 216 of file DataSet.cxx. ◆ GetResults(). TMVA::Results * TMVA::DataSet::GetResults ; (; const TString & ; resultsName, . Types::ETreeType ; type, . Types::EAnalysisType ; analysistype . ). Definition at line 265 of file DataSet.cxx. ◆ GetTestEvent(). const Event * TMVA::DataSet::GetTestEvent ; (; Long64_t ; ievt); const. inline . Definition at line 75 of file DataSet.h. ◆ GetTrainingEvent(). const Event * TMVA::DataSet::GetTrainingEvent ; (; Long64_t ; ievt); const. inline . Definition at line 74 of file DataSet.h. ◆ GetTree(). TTree * TMVA::DataSet::GetTree ; (; Types::ETreeType ; type). create the test/trainings tree with all the variables, the weights, the classes, the targets, the spectators, the MVA outputs ; Definition at line 609 of file DataSet.cxx. ◆ HasNegativeEventWeights(). Bool_t TMVA::DataSet::HasNegativeEventWeights ; (; ); const. inline . Definition at line 101 of file DataSet.h. ◆ IncrementNClassEvents(). void TMVA::DataSet::IncrementNClassEvents ; (; Int_t ; type, . UInt_t ; classNumber . ). Definition at line 151 of file DataSet.cxx. ◆ InitSampling(). void TMVA::DataSet::InitSampling ; (; Float_t ; fraction, . Float_t ; weight, . UInt_t ; seed = 0 . ). initialize random or importance sampling ; Definition at line 459 of file DataSet.cxx. ◆ IsA(). virtual TClass * TMVA::DataSet::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TObject.; Definition at line 175 of file DataSet.h. ◆ Log(). MsgLogger & TMVA::DataSet::Log ; (; ); const. inlineprivate . Definition at line 164 of file DataSet.h. ◆ MoveTrainingBlock(). void TMVA::DataSet::MoveTrainingBlock ; (; Int_t ; blockInd, . Types::ETreeTy",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:25524,Testability,test,testing,25524,". virtual void TMVA::DataSet::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSet::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 175 of file DataSet.h. ◆ TreeIndex(). UInt_t TMVA::DataSet::TreeIndex ; (; Types::ETreeType ; type); const. inline . Definition at line 181 of file DataSet.h. Member Data Documentation. ◆ fBlockBelongToTraining. std::vector<Char_t> TMVA::DataSet::fBlockBelongToTraining. private . when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ; Definition at line 165 of file DataSet.h. ◆ fClassEvents. std::vector< std::vector<Long64_t> > TMVA::DataSet::fClassEvents. private . number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ; Definition at line 158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEventWeights. private . true if at least one signal or bkg event has negative weight ; Definition at line 161 of file DataSet.h. ◆ fLogger. MsgLogger* TMVA::DataSet::fLogger. mutableprivate . ! message logger ; Definition at line 163 of file DataSet.h. ◆ fResults. std::vector< std::map< TString, Results* > > TMVA::Data",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:26118,Testability,test,testing,26118," when dividing the dataset to blocks, sets whether the certain block is in the Training set or else in the validation set boolean are stored, taken std::vector<Char_t> for performance reasons (instead of std::vector<Bool_t>) ; Definition at line 165 of file DataSet.h. ◆ fClassEvents. std::vector< std::vector<Long64_t> > TMVA::DataSet::fClassEvents. private . number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ; Definition at line 158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEventWeights. private . true if at least one signal or bkg event has negative weight ; Definition at line 161 of file DataSet.h. ◆ fLogger. MsgLogger* TMVA::DataSet::fLogger. mutableprivate . ! message logger ; Definition at line 163 of file DataSet.h. ◆ fResults. std::vector< std::map< TString, Results* > > TMVA::DataSet::fResults. private . ! [train/test/...][method-identifier] ; Definition at line 143 of file DataSet.h. ◆ fSampling. std::vector<Char_t> TMVA::DataSet::fSampling. private . random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ; Definition at line 149 of file DataSet.h. ◆ fSamplingEventList. std::vector< std::vector< std::pair< Float_t, Long64_t > > > TMVA::DataSet::fSamplingEventList. mutableprivate . weights and indices for sampling ; Definition at line 152",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:26438,Testability,log,logger,26438,"s. private . number of events of class 0,1,2,... in training[0] and testing[1] (+validation, trainingoriginal) ; Definition at line 158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEventWeights. private . true if at least one signal or bkg event has negative weight ; Definition at line 161 of file DataSet.h. ◆ fLogger. MsgLogger* TMVA::DataSet::fLogger. mutableprivate . ! message logger ; Definition at line 163 of file DataSet.h. ◆ fResults. std::vector< std::map< TString, Results* > > TMVA::DataSet::fResults. private . ! [train/test/...][method-identifier] ; Definition at line 143 of file DataSet.h. ◆ fSampling. std::vector<Char_t> TMVA::DataSet::fSampling. private . random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ; Definition at line 149 of file DataSet.h. ◆ fSamplingEventList. std::vector< std::vector< std::pair< Float_t, Long64_t > > > TMVA::DataSet::fSamplingEventList. mutableprivate . weights and indices for sampling ; Definition at line 152 of file DataSet.h. ◆ fSamplingNEvents. std::vector<Int_t> TMVA::DataSet::fSamplingNEvents. private . number of events which should be sampled ; Definition at line 150 of file DataSet.h. ◆ fSamplingRandom. TRandom3* TMVA::DataSet::fSamplingRandom. private . -> random generator for sampling ; Definition at line 154 of file DataSet.h. ◆ fSamplingSel",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSet.html:26590,Testability,test,test,26590,"158 of file DataSet.h. ◆ fCurrentEventIdx. Long64_t TMVA::DataSet::fCurrentEventIdx. mutableprivate . Definition at line 146 of file DataSet.h. ◆ fCurrentTreeIdx. UInt_t TMVA::DataSet::fCurrentTreeIdx. mutableprivate . Definition at line 145 of file DataSet.h. ◆ fdsi. const DataSetInfo* TMVA::DataSet::fdsi. private . -> datasetinfo that created this dataset ; Definition at line 139 of file DataSet.h. ◆ fEventCollection. std::vector< std::vector<Event*> > TMVA::DataSet::fEventCollection. private . list of events for training/testing/... ; Definition at line 141 of file DataSet.h. ◆ fHasNegativeEventWeights. Bool_t TMVA::DataSet::fHasNegativeEventWeights. private . true if at least one signal or bkg event has negative weight ; Definition at line 161 of file DataSet.h. ◆ fLogger. MsgLogger* TMVA::DataSet::fLogger. mutableprivate . ! message logger ; Definition at line 163 of file DataSet.h. ◆ fResults. std::vector< std::map< TString, Results* > > TMVA::DataSet::fResults. private . ! [train/test/...][method-identifier] ; Definition at line 143 of file DataSet.h. ◆ fSampling. std::vector<Char_t> TMVA::DataSet::fSampling. private . random or importance sampling (not all events are taken) !! Bool_t are stored ( no std::vector<bool> taken for speed (performance) issues ) ; Definition at line 149 of file DataSet.h. ◆ fSamplingEventList. std::vector< std::vector< std::pair< Float_t, Long64_t > > > TMVA::DataSet::fSamplingEventList. mutableprivate . weights and indices for sampling ; Definition at line 152 of file DataSet.h. ◆ fSamplingNEvents. std::vector<Int_t> TMVA::DataSet::fSamplingNEvents. private . number of events which should be sampled ; Definition at line 150 of file DataSet.h. ◆ fSamplingRandom. TRandom3* TMVA::DataSet::fSamplingRandom. private . -> random generator for sampling ; Definition at line 154 of file DataSet.h. ◆ fSamplingSelected. std::vector< std::vector< std::pair< Float_t, Long64_t > > > TMVA::DataSet::fSamplingSelected. mutableprivate . selected eve",MatchSource.WIKI,doc/master/classTMVA_1_1DataSet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSet.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:2655,Availability,error,error,2655,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Return",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:2744,Availability,error,error,2744,"ar *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns strin",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:2899,Availability,error,error,2899,";  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title o",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:3196,Availability,error,error,3196,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:7760,Availability,error,error,7760," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:2661,Integrability,message,message,2661,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Return",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:3202,Integrability,message,message,3202,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:4404,Integrability,message,message,4404,"ived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this metho",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:7766,Integrability,message,message,7766," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:8139,Integrability,message,message,8139," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Prote",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:12119,Integrability,message,message,12119,"torOfClassesOfTreeType &eventsmap, const EvtStatsPerClass &eventCounts, const TString &normMode);  renormalisation of the TRAINING event weights ;  ; void ResetBranchAndEventAddresses (TTree *);  ; void ResetCurrentTree ();  ; Bool_t Verbose ();  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; Bool_t fComputeCorrelations = kFALSE;  Whether to force computation of correlations or not. ;  ; Bool_t fCorrelations = kFALSE;  Whether to print correlations or not. ;  ; UInt_t fCurrentEvtIdx;  the current event (to avoid reading of the same event) ;  ; TTree * fCurrentTree;  the tree, events are currently read from ;  ; std::vector< TTreeFormula * > fCutFormulas;  cuts ;  ; std::vector< TTreeFormula * > fInputFormulas;  input variables ;  ; std::vector< std::pair< TTreeFormula *, Int_t > > fInputTableFormulas;  ! input variables expression for arrays ;  ; MsgLogger * fLogger;  ! message logger ;  ; Bool_t fScaleWithPreselEff;  how to deal with requested #events in connection with preselection cuts ;  ; std::vector< TTreeFormula * > fSpectatorFormulas;  spectators ;  ; std::vector< TTreeFormula * > fTargetFormulas;  targets ;  ; Bool_t fVerbose;  Verbosity. ;  ; TString fVerboseLevel;  VerboseLevel. ;  ; std::vector< TTreeFormula * > fWeightFormula;  weights ;  . Private Types; typedef std::vector< Event * > EventVector;  ; typedef std::vector< EventVector > EventVectorOfClasses;  ; typedef std::map< Types::ETreeType, EventVectorOfClasses > EventVectorOfClassesOfTreeType;  ; typedef std::map< Types::ETreeType, EventVector > EventVectorOfTreeType;  ; typedef std::vector< EventStats > EvtStatsPerClass;  ; typedef std::vector< int > NumberPerClass;  ; typedef std::vector< Double_t > ValuePerClass;  ; typedef std::map< Types::ETreeType, ValuePerClass > ValuePerClassOfTreeType;  . Additional Inher",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:22597,Integrability,message,message,22597,"line 218 of file DataSetFactory.h. ◆ fCurrentEvtIdx. UInt_t TMVA::DataSetFactory::fCurrentEvtIdx. protected . the current event (to avoid reading of the same event) ; Definition at line 225 of file DataSetFactory.h. ◆ fCurrentTree. TTree* TMVA::DataSetFactory::fCurrentTree. protected . the tree, events are currently read from ; Definition at line 224 of file DataSetFactory.h. ◆ fCutFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fCutFormulas. protected . cuts ; Definition at line 231 of file DataSetFactory.h. ◆ fInputFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fInputFormulas. protected . input variables ; Definition at line 228 of file DataSetFactory.h. ◆ fInputTableFormulas. std::vector<std::pair<TTreeFormula*, Int_t> > TMVA::DataSetFactory::fInputTableFormulas. protected . ! input variables expression for arrays ; Definition at line 229 of file DataSetFactory.h. ◆ fLogger. MsgLogger* TMVA::DataSetFactory::fLogger. protected . ! message logger ; Definition at line 235 of file DataSetFactory.h. ◆ fScaleWithPreselEff. Bool_t TMVA::DataSetFactory::fScaleWithPreselEff. protected . how to deal with requested #events in connection with preselection cuts ; Definition at line 221 of file DataSetFactory.h. ◆ fSpectatorFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fSpectatorFormulas. protected . spectators ; Definition at line 233 of file DataSetFactory.h. ◆ fTargetFormulas. std::vector<TTreeFormula *> TMVA::DataSetFactory::fTargetFormulas. protected . targets ; Definition at line 230 of file DataSetFactory.h. ◆ fVerbose. Bool_t TMVA::DataSetFactory::fVerbose. protected . Verbosity. ; Definition at line 214 of file DataSetFactory.h. ◆ fVerboseLevel. TString TMVA::DataSetFactory::fVerboseLevel. protected . VerboseLevel. ; Definition at line 215 of file DataSetFactory.h. ◆ fWeightFormula. std::vector<TTreeFormula*> TMVA::DataSetFactory::fWeightFormula. protected . weights ; Definition at line 232 of file DataSetFactory.h. Libraries for TM",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:894,Modifiability,inherit,inherited,894,". ROOT: TMVA::DataSetFactory Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Static Public Member Functions |; Protected Member Functions |; Protected Attributes |; Private Types |; List of all members ; TMVA::DataSetFactory Class ReferenceTMVA. ; Class that contains all the data information. ; Definition at line 116 of file DataSetFactory.h. Classes; class  EventStats;  . Public Member Functions;  DataSetFactory ();  constructor ;  ;  ~DataSetFactory ();  destructor ;  ; DataSet * CreateDataSet (DataSetInfo &, DataInputHandler &);  steering the creation of a new dataset ;  ; virtual TClass * IsA () const;  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (O",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:2291,Modifiability,inherit,inheritance,2291,"pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:4502,Modifiability,inherit,inherits,4502,"ed by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:4619,Modifiability,inherit,inherits,4619,"ame of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its p",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:8640,Modifiability,inherit,inherited,8640,"reamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; DataSet * BuildDynamicDataSet (DataSetInfo &);  ; void BuildEventVector (DataSetInfo &dsi, DataInputHandler &dataInput, EventVectorOfClassesOfTreeType &eventsmap, EvtStatsPerClass &eventCounts);  build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ;  ; DataSet * BuildInitialDataSet (DataSetInfo &, TMVA::DataInputHandler &);  if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ;  ; TMatrixD * CalcCorrelationM",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:9585,Modifiability,variab,variables,9585,"_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; DataSet * BuildDynamicDataSet (DataSetInfo &);  ; void BuildEventVector (DataSetInfo &dsi, DataInputHandler &dataInput, EventVectorOfClassesOfTreeType &eventsmap, EvtStatsPerClass &eventCounts);  build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ;  ; DataSet * BuildInitialDataSet (DataSetInfo &, TMVA::DataInputHandler &);  if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ;  ; TMatrixD * CalcCorrelationMatrix (DataSet *, const UInt_t classNumber);  computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ;  ; TMatrixD * CalcCovarianceMatrix (DataSet *, const UInt_t classNumber);  compute covariance matrix ;  ; void CalcMinMax (DataSet *, DataSetInfo &dsi);  compute covariance matrix ;  ; void ChangeToNewTree (TreeInfo &, const DataSetInfo &);  While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ;  ; Bool_t CheckTTreeFormula (TTreeFormula *ttf, const TString &expression, Bool_t &hasDollar);  checks a TTreeFormula for problems ;  ; void InitOptions (DataSetInfo &dsi, EvtStatsPerClass &eventsmap, TString &normMode, UInt_t &splitSeed, TString &splitMode, TString &mixMode);  the dataset splitting ;  ; MsgLogger & Log () const;  ; DataSet * MixEvents (DataSetInfo",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:9608,Modifiability,variab,variables,9608,"_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; DataSet * BuildDynamicDataSet (DataSetInfo &);  ; void BuildEventVector (DataSetInfo &dsi, DataInputHandler &dataInput, EventVectorOfClassesOfTreeType &eventsmap, EvtStatsPerClass &eventCounts);  build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ;  ; DataSet * BuildInitialDataSet (DataSetInfo &, TMVA::DataInputHandler &);  if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ;  ; TMatrixD * CalcCorrelationMatrix (DataSet *, const UInt_t classNumber);  computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ;  ; TMatrixD * CalcCovarianceMatrix (DataSet *, const UInt_t classNumber);  compute covariance matrix ;  ; void CalcMinMax (DataSet *, DataSetInfo &dsi);  compute covariance matrix ;  ; void ChangeToNewTree (TreeInfo &, const DataSetInfo &);  While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ;  ; Bool_t CheckTTreeFormula (TTreeFormula *ttf, const TString &expression, Bool_t &hasDollar);  checks a TTreeFormula for problems ;  ; void InitOptions (DataSetInfo &dsi, EvtStatsPerClass &eventsmap, TString &normMode, UInt_t &splitSeed, TString &splitMode, TString &mixMode);  the dataset splitting ;  ; MsgLogger & Log () const;  ; DataSet * MixEvents (DataSetInfo",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:9729,Modifiability,variab,variables,9729,"_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; DataSet * BuildDynamicDataSet (DataSetInfo &);  ; void BuildEventVector (DataSetInfo &dsi, DataInputHandler &dataInput, EventVectorOfClassesOfTreeType &eventsmap, EvtStatsPerClass &eventCounts);  build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ;  ; DataSet * BuildInitialDataSet (DataSetInfo &, TMVA::DataInputHandler &);  if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ;  ; TMatrixD * CalcCorrelationMatrix (DataSet *, const UInt_t classNumber);  computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ;  ; TMatrixD * CalcCovarianceMatrix (DataSet *, const UInt_t classNumber);  compute covariance matrix ;  ; void CalcMinMax (DataSet *, DataSetInfo &dsi);  compute covariance matrix ;  ; void ChangeToNewTree (TreeInfo &, const DataSetInfo &);  While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ;  ; Bool_t CheckTTreeFormula (TTreeFormula *ttf, const TString &expression, Bool_t &hasDollar);  checks a TTreeFormula for problems ;  ; void InitOptions (DataSetInfo &dsi, EvtStatsPerClass &eventsmap, TString &normMode, UInt_t &splitSeed, TString &splitMode, TString &mixMode);  the dataset splitting ;  ; MsgLogger & Log () const;  ; DataSet * MixEvents (DataSetInfo",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:9810,Modifiability,variab,variable,9810,"_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; DataSet * BuildDynamicDataSet (DataSetInfo &);  ; void BuildEventVector (DataSetInfo &dsi, DataInputHandler &dataInput, EventVectorOfClassesOfTreeType &eventsmap, EvtStatsPerClass &eventCounts);  build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ;  ; DataSet * BuildInitialDataSet (DataSetInfo &, TMVA::DataInputHandler &);  if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ;  ; TMatrixD * CalcCorrelationMatrix (DataSet *, const UInt_t classNumber);  computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ;  ; TMatrixD * CalcCovarianceMatrix (DataSet *, const UInt_t classNumber);  compute covariance matrix ;  ; void CalcMinMax (DataSet *, DataSetInfo &dsi);  compute covariance matrix ;  ; void ChangeToNewTree (TreeInfo &, const DataSetInfo &);  While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ;  ; Bool_t CheckTTreeFormula (TTreeFormula *ttf, const TString &expression, Bool_t &hasDollar);  checks a TTreeFormula for problems ;  ; void InitOptions (DataSetInfo &dsi, EvtStatsPerClass &eventsmap, TString &normMode, UInt_t &splitSeed, TString &splitMode, TString &mixMode);  the dataset splitting ;  ; MsgLogger & Log () const;  ; DataSet * MixEvents (DataSetInfo",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:11325,Modifiability,inherit,inherited,11325,"tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ;  ; Bool_t CheckTTreeFormula (TTreeFormula *ttf, const TString &expression, Bool_t &hasDollar);  checks a TTreeFormula for problems ;  ; void InitOptions (DataSetInfo &dsi, EvtStatsPerClass &eventsmap, TString &normMode, UInt_t &splitSeed, TString &splitMode, TString &mixMode);  the dataset splitting ;  ; MsgLogger & Log () const;  ; DataSet * MixEvents (DataSetInfo &dsi, EventVectorOfClassesOfTreeType &eventsmap, EvtStatsPerClass &eventCounts, const TString &splitMode, const TString &mixMode, const TString &normMode, UInt_t splitSeed);  Select and distribute unassigned events to kTraining and kTesting. ;  ; void RenormEvents (DataSetInfo &dsi, EventVectorOfClassesOfTreeType &eventsmap, const EvtStatsPerClass &eventCounts, const TString &normMode);  renormalisation of the TRAINING event weights ;  ; void ResetBranchAndEventAddresses (TTree *);  ; void ResetCurrentTree ();  ; Bool_t Verbose ();  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; Bool_t fComputeCorrelations = kFALSE;  Whether to force computation of correlations or not. ;  ; Bool_t fCorrelations = kFALSE;  Whether to print correlations or not. ;  ; UInt_t fCurrentEvtIdx;  the current event (to avoid reading of the same event) ;  ; TTree * fCurrentTree;  the tree, events are currently read from ;  ; std::vector< TTreeFormula * > fCutFormulas;  cuts ;  ; std::vector< TTreeFormula * > fInputFormulas;  input variables ;  ; std::vector< std::pair< TTreeFormula *, Int_t > > fInputTableFormulas;  ! input variables expression for arrays ;  ; MsgLogger * fLogger;  ! message logger ;  ; Bool_t fScaleWithPreselEff;  how to deal with requested #events in connection with preselection cut",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:11963,Modifiability,variab,variables,11963,"torOfClassesOfTreeType &eventsmap, const EvtStatsPerClass &eventCounts, const TString &normMode);  renormalisation of the TRAINING event weights ;  ; void ResetBranchAndEventAddresses (TTree *);  ; void ResetCurrentTree ();  ; Bool_t Verbose ();  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; Bool_t fComputeCorrelations = kFALSE;  Whether to force computation of correlations or not. ;  ; Bool_t fCorrelations = kFALSE;  Whether to print correlations or not. ;  ; UInt_t fCurrentEvtIdx;  the current event (to avoid reading of the same event) ;  ; TTree * fCurrentTree;  the tree, events are currently read from ;  ; std::vector< TTreeFormula * > fCutFormulas;  cuts ;  ; std::vector< TTreeFormula * > fInputFormulas;  input variables ;  ; std::vector< std::pair< TTreeFormula *, Int_t > > fInputTableFormulas;  ! input variables expression for arrays ;  ; MsgLogger * fLogger;  ! message logger ;  ; Bool_t fScaleWithPreselEff;  how to deal with requested #events in connection with preselection cuts ;  ; std::vector< TTreeFormula * > fSpectatorFormulas;  spectators ;  ; std::vector< TTreeFormula * > fTargetFormulas;  targets ;  ; Bool_t fVerbose;  Verbosity. ;  ; TString fVerboseLevel;  VerboseLevel. ;  ; std::vector< TTreeFormula * > fWeightFormula;  weights ;  . Private Types; typedef std::vector< Event * > EventVector;  ; typedef std::vector< EventVector > EventVectorOfClasses;  ; typedef std::map< Types::ETreeType, EventVectorOfClasses > EventVectorOfClassesOfTreeType;  ; typedef std::map< Types::ETreeType, EventVector > EventVectorOfTreeType;  ; typedef std::vector< EventStats > EvtStatsPerClass;  ; typedef std::vector< int > NumberPerClass;  ; typedef std::vector< Double_t > ValuePerClass;  ; typedef std::map< Types::ETreeType, ValuePerClass > ValuePerClassOfTreeType;  . Additional Inher",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:12058,Modifiability,variab,variables,12058,"torOfClassesOfTreeType &eventsmap, const EvtStatsPerClass &eventCounts, const TString &normMode);  renormalisation of the TRAINING event weights ;  ; void ResetBranchAndEventAddresses (TTree *);  ; void ResetCurrentTree ();  ; Bool_t Verbose ();  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; Bool_t fComputeCorrelations = kFALSE;  Whether to force computation of correlations or not. ;  ; Bool_t fCorrelations = kFALSE;  Whether to print correlations or not. ;  ; UInt_t fCurrentEvtIdx;  the current event (to avoid reading of the same event) ;  ; TTree * fCurrentTree;  the tree, events are currently read from ;  ; std::vector< TTreeFormula * > fCutFormulas;  cuts ;  ; std::vector< TTreeFormula * > fInputFormulas;  input variables ;  ; std::vector< std::pair< TTreeFormula *, Int_t > > fInputTableFormulas;  ! input variables expression for arrays ;  ; MsgLogger * fLogger;  ! message logger ;  ; Bool_t fScaleWithPreselEff;  how to deal with requested #events in connection with preselection cuts ;  ; std::vector< TTreeFormula * > fSpectatorFormulas;  spectators ;  ; std::vector< TTreeFormula * > fTargetFormulas;  targets ;  ; Bool_t fVerbose;  Verbosity. ;  ; TString fVerboseLevel;  VerboseLevel. ;  ; std::vector< TTreeFormula * > fWeightFormula;  weights ;  . Private Types; typedef std::vector< Event * > EventVector;  ; typedef std::vector< EventVector > EventVectorOfClasses;  ; typedef std::map< Types::ETreeType, EventVectorOfClasses > EventVectorOfClassesOfTreeType;  ; typedef std::map< Types::ETreeType, EventVector > EventVectorOfTreeType;  ; typedef std::vector< EventStats > EvtStatsPerClass;  ; typedef std::vector< int > NumberPerClass;  ; typedef std::vector< Double_t > ValuePerClass;  ; typedef std::map< Types::ETreeType, ValuePerClass > ValuePerClassOfTreeType;  . Additional Inher",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:13077,Modifiability,inherit,inherited,13077,"Bool_t fScaleWithPreselEff;  how to deal with requested #events in connection with preselection cuts ;  ; std::vector< TTreeFormula * > fSpectatorFormulas;  spectators ;  ; std::vector< TTreeFormula * > fTargetFormulas;  targets ;  ; Bool_t fVerbose;  Verbosity. ;  ; TString fVerboseLevel;  VerboseLevel. ;  ; std::vector< TTreeFormula * > fWeightFormula;  weights ;  . Private Types; typedef std::vector< Event * > EventVector;  ; typedef std::vector< EventVector > EventVectorOfClasses;  ; typedef std::map< Types::ETreeType, EventVectorOfClasses > EventVectorOfClassesOfTreeType;  ; typedef std::map< Types::ETreeType, EventVector > EventVectorOfTreeType;  ; typedef std::vector< EventStats > EvtStatsPerClass;  ; typedef std::vector< int > NumberPerClass;  ; typedef std::vector< Double_t > ValuePerClass;  ; typedef std::map< Types::ETreeType, ValuePerClass > ValuePerClassOfTreeType;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  . #include <TMVA/DataSetFactory.h>. Inheritance diagram for TMVA::DataSetFactory:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ EventVector. typedef std::vector<Event* > TMVA::DataSetFactory::EventVector. private . Definition at line 118 of file DataSetFactory.h. ◆ Even",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:13718,Modifiability,inherit,inherited,13718,"p< Types::ETreeType, EventVector > EventVectorOfTreeType;  ; typedef std::vector< EventStats > EvtStatsPerClass;  ; typedef std::vector< int > NumberPerClass;  ; typedef std::vector< Double_t > ValuePerClass;  ; typedef std::map< Types::ETreeType, ValuePerClass > ValuePerClassOfTreeType;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  . #include <TMVA/DataSetFactory.h>. Inheritance diagram for TMVA::DataSetFactory:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ EventVector. typedef std::vector<Event* > TMVA::DataSetFactory::EventVector. private . Definition at line 118 of file DataSetFactory.h. ◆ EventVectorOfClasses. typedef std::vector< EventVector > TMVA::DataSetFactory::EventVectorOfClasses. private . Definition at line 119 of file DataSetFactory.h. ◆ EventVectorOfClassesOfTreeType. typedef std::map<Types::ETreeType, EventVectorOfClasses > TMVA::DataSetFactory::EventVectorOfClassesOfTreeType. private . Definition at line 120 of file DataSetFactory.h. ◆ EventVectorOfTreeType. typedef std::map<Types::ETreeType, EventVector > TMVA::DataSetFactory::EventVectorOfTreeType. private . Definition at line 121 of file DataSetFactory.h. ◆ EvtStatsPerClass. typedef std::vector< EventStats > TMVA::Da",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:16425,Modifiability,variab,variables,16425,"ion at line 107 of file DataSetFactory.cxx. ◆ DataSetFactory(). TMVA::DataSetFactory::DataSetFactory ; (; ). constructor ; Definition at line 93 of file DataSetFactory.cxx. Member Function Documentation. ◆ BuildDynamicDataSet(). TMVA::DataSet * TMVA::DataSetFactory::BuildDynamicDataSet ; (; TMVA::DataSetInfo & ; dsi). protected . Definition at line 149 of file DataSetFactory.cxx. ◆ BuildEventVector(). void TMVA::DataSetFactory::BuildEventVector ; (; TMVA::DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput, . EventVectorOfClassesOfTreeType & ; eventsmap, . EvtStatsPerClass & ; eventCounts . ). protected . build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ; Definition at line 728 of file DataSetFactory.cxx. ◆ BuildInitialDataSet(). TMVA::DataSet * TMVA::DataSetFactory::BuildInitialDataSet ; (; DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput . ). protected . if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ; Definition at line 202 of file DataSetFactory.cxx. ◆ CalcCorrelationMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCorrelationMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ; Definition at line 548 of file DataSetFactory.cxx. ◆ CalcCovarianceMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCovarianceMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . compute covariance matrix ; Definition at line 579 of file DataSetFactory.cxx. ◆ CalcMinMax(). void TMVA::DataSetFactory::CalcMinMax ; (; DataSet * ; ds, . TMVA::DataSetInfo & ; dsi . ). protected . compute covariance matrix ; Definition at line 479 of file DataSetFactory.cxx. ◆ ChangeToNewTree(). void TMVA::DataSetFactory::ChangeToNewTree ; (; TreeInfo & ; tinfo, . const DataSetInfo & ; dsi . ). protected . While the ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:16448,Modifiability,variab,variables,16448,"ion at line 107 of file DataSetFactory.cxx. ◆ DataSetFactory(). TMVA::DataSetFactory::DataSetFactory ; (; ). constructor ; Definition at line 93 of file DataSetFactory.cxx. Member Function Documentation. ◆ BuildDynamicDataSet(). TMVA::DataSet * TMVA::DataSetFactory::BuildDynamicDataSet ; (; TMVA::DataSetInfo & ; dsi). protected . Definition at line 149 of file DataSetFactory.cxx. ◆ BuildEventVector(). void TMVA::DataSetFactory::BuildEventVector ; (; TMVA::DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput, . EventVectorOfClassesOfTreeType & ; eventsmap, . EvtStatsPerClass & ; eventCounts . ). protected . build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ; Definition at line 728 of file DataSetFactory.cxx. ◆ BuildInitialDataSet(). TMVA::DataSet * TMVA::DataSetFactory::BuildInitialDataSet ; (; DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput . ). protected . if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ; Definition at line 202 of file DataSetFactory.cxx. ◆ CalcCorrelationMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCorrelationMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ; Definition at line 548 of file DataSetFactory.cxx. ◆ CalcCovarianceMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCovarianceMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . compute covariance matrix ; Definition at line 579 of file DataSetFactory.cxx. ◆ CalcMinMax(). void TMVA::DataSetFactory::CalcMinMax ; (; DataSet * ; ds, . TMVA::DataSetInfo & ; dsi . ). protected . compute covariance matrix ; Definition at line 479 of file DataSetFactory.cxx. ◆ ChangeToNewTree(). void TMVA::DataSetFactory::ChangeToNewTree ; (; TreeInfo & ; tinfo, . const DataSetInfo & ; dsi . ). protected . While the ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:16693,Modifiability,variab,variables,16693,"d . Definition at line 149 of file DataSetFactory.cxx. ◆ BuildEventVector(). void TMVA::DataSetFactory::BuildEventVector ; (; TMVA::DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput, . EventVectorOfClassesOfTreeType & ; eventsmap, . EvtStatsPerClass & ; eventCounts . ). protected . build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ; Definition at line 728 of file DataSetFactory.cxx. ◆ BuildInitialDataSet(). TMVA::DataSet * TMVA::DataSetFactory::BuildInitialDataSet ; (; DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput . ). protected . if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ; Definition at line 202 of file DataSetFactory.cxx. ◆ CalcCorrelationMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCorrelationMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ; Definition at line 548 of file DataSetFactory.cxx. ◆ CalcCovarianceMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCovarianceMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . compute covariance matrix ; Definition at line 579 of file DataSetFactory.cxx. ◆ CalcMinMax(). void TMVA::DataSetFactory::CalcMinMax ; (; DataSet * ; ds, . TMVA::DataSetInfo & ; dsi . ). protected . compute covariance matrix ; Definition at line 479 of file DataSetFactory.cxx. ◆ ChangeToNewTree(). void TMVA::DataSetFactory::ChangeToNewTree ; (; TreeInfo & ; tinfo, . const DataSetInfo & ; dsi . ). protected . While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ; Definition at line 293 of file DataSet",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:16774,Modifiability,variab,variable,16774,"d . Definition at line 149 of file DataSetFactory.cxx. ◆ BuildEventVector(). void TMVA::DataSetFactory::BuildEventVector ; (; TMVA::DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput, . EventVectorOfClassesOfTreeType & ; eventsmap, . EvtStatsPerClass & ; eventCounts . ). protected . build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ; Definition at line 728 of file DataSetFactory.cxx. ◆ BuildInitialDataSet(). TMVA::DataSet * TMVA::DataSetFactory::BuildInitialDataSet ; (; DataSetInfo & ; dsi, . TMVA::DataInputHandler & ; dataInput . ). protected . if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ; Definition at line 202 of file DataSetFactory.cxx. ◆ CalcCorrelationMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCorrelationMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ; Definition at line 548 of file DataSetFactory.cxx. ◆ CalcCovarianceMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCovarianceMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . compute covariance matrix ; Definition at line 579 of file DataSetFactory.cxx. ◆ CalcMinMax(). void TMVA::DataSetFactory::CalcMinMax ; (; DataSet * ; ds, . TMVA::DataSetInfo & ; dsi . ). protected . compute covariance matrix ; Definition at line 479 of file DataSetFactory.cxx. ◆ ChangeToNewTree(). void TMVA::DataSetFactory::ChangeToNewTree ; (; TreeInfo & ; tinfo, . const DataSetInfo & ; dsi . ). protected . While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ; Definition at line 293 of file DataSet",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:22254,Modifiability,variab,variables,22254,"umentation. ◆ fComputeCorrelations. Bool_t TMVA::DataSetFactory::fComputeCorrelations = kFALSE. protected . Whether to force computation of correlations or not. ; Definition at line 219 of file DataSetFactory.h. ◆ fCorrelations. Bool_t TMVA::DataSetFactory::fCorrelations = kFALSE. protected . Whether to print correlations or not. ; Definition at line 218 of file DataSetFactory.h. ◆ fCurrentEvtIdx. UInt_t TMVA::DataSetFactory::fCurrentEvtIdx. protected . the current event (to avoid reading of the same event) ; Definition at line 225 of file DataSetFactory.h. ◆ fCurrentTree. TTree* TMVA::DataSetFactory::fCurrentTree. protected . the tree, events are currently read from ; Definition at line 224 of file DataSetFactory.h. ◆ fCutFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fCutFormulas. protected . cuts ; Definition at line 231 of file DataSetFactory.h. ◆ fInputFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fInputFormulas. protected . input variables ; Definition at line 228 of file DataSetFactory.h. ◆ fInputTableFormulas. std::vector<std::pair<TTreeFormula*, Int_t> > TMVA::DataSetFactory::fInputTableFormulas. protected . ! input variables expression for arrays ; Definition at line 229 of file DataSetFactory.h. ◆ fLogger. MsgLogger* TMVA::DataSetFactory::fLogger. protected . ! message logger ; Definition at line 235 of file DataSetFactory.h. ◆ fScaleWithPreselEff. Bool_t TMVA::DataSetFactory::fScaleWithPreselEff. protected . how to deal with requested #events in connection with preselection cuts ; Definition at line 221 of file DataSetFactory.h. ◆ fSpectatorFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fSpectatorFormulas. protected . spectators ; Definition at line 233 of file DataSetFactory.h. ◆ fTargetFormulas. std::vector<TTreeFormula *> TMVA::DataSetFactory::fTargetFormulas. protected . targets ; Definition at line 230 of file DataSetFactory.h. ◆ fVerbose. Bool_t TMVA::DataSetFactory::fVerbose. protected . Verbosity. ; Definition ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:22447,Modifiability,variab,variables,22447,"ctory.h. ◆ fCorrelations. Bool_t TMVA::DataSetFactory::fCorrelations = kFALSE. protected . Whether to print correlations or not. ; Definition at line 218 of file DataSetFactory.h. ◆ fCurrentEvtIdx. UInt_t TMVA::DataSetFactory::fCurrentEvtIdx. protected . the current event (to avoid reading of the same event) ; Definition at line 225 of file DataSetFactory.h. ◆ fCurrentTree. TTree* TMVA::DataSetFactory::fCurrentTree. protected . the tree, events are currently read from ; Definition at line 224 of file DataSetFactory.h. ◆ fCutFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fCutFormulas. protected . cuts ; Definition at line 231 of file DataSetFactory.h. ◆ fInputFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fInputFormulas. protected . input variables ; Definition at line 228 of file DataSetFactory.h. ◆ fInputTableFormulas. std::vector<std::pair<TTreeFormula*, Int_t> > TMVA::DataSetFactory::fInputTableFormulas. protected . ! input variables expression for arrays ; Definition at line 229 of file DataSetFactory.h. ◆ fLogger. MsgLogger* TMVA::DataSetFactory::fLogger. protected . ! message logger ; Definition at line 235 of file DataSetFactory.h. ◆ fScaleWithPreselEff. Bool_t TMVA::DataSetFactory::fScaleWithPreselEff. protected . how to deal with requested #events in connection with preselection cuts ; Definition at line 221 of file DataSetFactory.h. ◆ fSpectatorFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fSpectatorFormulas. protected . spectators ; Definition at line 233 of file DataSetFactory.h. ◆ fTargetFormulas. std::vector<TTreeFormula *> TMVA::DataSetFactory::fTargetFormulas. protected . targets ; Definition at line 230 of file DataSetFactory.h. ◆ fVerbose. Bool_t TMVA::DataSetFactory::fVerbose. protected . Verbosity. ; Definition at line 214 of file DataSetFactory.h. ◆ fVerboseLevel. TString TMVA::DataSetFactory::fVerboseLevel. protected . VerboseLevel. ; Definition at line 215 of file DataSetFactory.h. ◆ fWeightFormula. std::vec",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:11748,Safety,avoid,avoid,11748,"torOfClassesOfTreeType &eventsmap, const EvtStatsPerClass &eventCounts, const TString &normMode);  renormalisation of the TRAINING event weights ;  ; void ResetBranchAndEventAddresses (TTree *);  ; void ResetCurrentTree ();  ; Bool_t Verbose ();  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; Bool_t fComputeCorrelations = kFALSE;  Whether to force computation of correlations or not. ;  ; Bool_t fCorrelations = kFALSE;  Whether to print correlations or not. ;  ; UInt_t fCurrentEvtIdx;  the current event (to avoid reading of the same event) ;  ; TTree * fCurrentTree;  the tree, events are currently read from ;  ; std::vector< TTreeFormula * > fCutFormulas;  cuts ;  ; std::vector< TTreeFormula * > fInputFormulas;  input variables ;  ; std::vector< std::pair< TTreeFormula *, Int_t > > fInputTableFormulas;  ! input variables expression for arrays ;  ; MsgLogger * fLogger;  ! message logger ;  ; Bool_t fScaleWithPreselEff;  how to deal with requested #events in connection with preselection cuts ;  ; std::vector< TTreeFormula * > fSpectatorFormulas;  spectators ;  ; std::vector< TTreeFormula * > fTargetFormulas;  targets ;  ; Bool_t fVerbose;  Verbosity. ;  ; TString fVerboseLevel;  VerboseLevel. ;  ; std::vector< TTreeFormula * > fWeightFormula;  weights ;  . Private Types; typedef std::vector< Event * > EventVector;  ; typedef std::vector< EventVector > EventVectorOfClasses;  ; typedef std::map< Types::ETreeType, EventVectorOfClasses > EventVectorOfClassesOfTreeType;  ; typedef std::map< Types::ETreeType, EventVector > EventVectorOfTreeType;  ; typedef std::vector< EventStats > EvtStatsPerClass;  ; typedef std::vector< int > NumberPerClass;  ; typedef std::vector< Double_t > ValuePerClass;  ; typedef std::map< Types::ETreeType, ValuePerClass > ValuePerClassOfTreeType;  . Additional Inher",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:21759,Safety,avoid,avoid,21759,"file DataSetFactory.h. ◆ Streamer(). virtual void TMVA::DataSetFactory::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSetFactory::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 238 of file DataSetFactory.h. ◆ Verbose(). Bool_t TMVA::DataSetFactory::Verbose ; (; ). inlineprotected . Definition at line 209 of file DataSetFactory.h. Member Data Documentation. ◆ fComputeCorrelations. Bool_t TMVA::DataSetFactory::fComputeCorrelations = kFALSE. protected . Whether to force computation of correlations or not. ; Definition at line 219 of file DataSetFactory.h. ◆ fCorrelations. Bool_t TMVA::DataSetFactory::fCorrelations = kFALSE. protected . Whether to print correlations or not. ; Definition at line 218 of file DataSetFactory.h. ◆ fCurrentEvtIdx. UInt_t TMVA::DataSetFactory::fCurrentEvtIdx. protected . the current event (to avoid reading of the same event) ; Definition at line 225 of file DataSetFactory.h. ◆ fCurrentTree. TTree* TMVA::DataSetFactory::fCurrentTree. protected . the tree, events are currently read from ; Definition at line 224 of file DataSetFactory.h. ◆ fCutFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fCutFormulas. protected . cuts ; Definition at line 231 of file DataSetFactory.h. ◆ fInputFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fInputFormulas. protected . input variables ; Definition at line 228 of file DataSetFactory.h. ◆ fInputTableFormulas. std::vector<std::pair<TTreeFormula*, Int_t> > TMVA::DataSetFactory::fInputTableFormulas. protected . ! input variables expression for arrays ; Definition at line 229 of file DataSetFactory.h. ◆ fLogger. MsgLogger* TMVA::DataSetFactory::fLogger. protected . ! message logger ; Definition at line 235 of file DataSetFactory.h. ◆ fScaleWithPreselEff. Bool_t TMVA::DataSetFactory::fScaleWithPreselEff. protected . how to deal with requested #events in co",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:4133,Security,hash,hash,4133," Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:10143,Testability,test,testing,10143,"_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Protected Member Functions; DataSet * BuildDynamicDataSet (DataSetInfo &);  ; void BuildEventVector (DataSetInfo &dsi, DataInputHandler &dataInput, EventVectorOfClassesOfTreeType &eventsmap, EvtStatsPerClass &eventCounts);  build empty event vectors distributes events between kTraining/kTesting/kMaxTreeType ;  ; DataSet * BuildInitialDataSet (DataSetInfo &, TMVA::DataInputHandler &);  if no entries, than create a DataSet with one Event which uses dynamic variables (pointers to variables) ;  ; TMatrixD * CalcCorrelationMatrix (DataSet *, const UInt_t classNumber);  computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ;  ; TMatrixD * CalcCovarianceMatrix (DataSet *, const UInt_t classNumber);  compute covariance matrix ;  ; void CalcMinMax (DataSet *, DataSetInfo &dsi);  compute covariance matrix ;  ; void ChangeToNewTree (TreeInfo &, const DataSetInfo &);  While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ;  ; Bool_t CheckTTreeFormula (TTreeFormula *ttf, const TString &expression, Bool_t &hasDollar);  checks a TTreeFormula for problems ;  ; void InitOptions (DataSetInfo &dsi, EvtStatsPerClass &eventsmap, TString &normMode, UInt_t &splitSeed, TString &splitMode, TString &mixMode);  the dataset splitting ;  ; MsgLogger & Log () const;  ; DataSet * MixEvents (DataSetInfo",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:12127,Testability,log,logger,12127,"torOfClassesOfTreeType &eventsmap, const EvtStatsPerClass &eventCounts, const TString &normMode);  renormalisation of the TRAINING event weights ;  ; void ResetBranchAndEventAddresses (TTree *);  ; void ResetCurrentTree ();  ; Bool_t Verbose ();  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . Protected Attributes; Bool_t fComputeCorrelations = kFALSE;  Whether to force computation of correlations or not. ;  ; Bool_t fCorrelations = kFALSE;  Whether to print correlations or not. ;  ; UInt_t fCurrentEvtIdx;  the current event (to avoid reading of the same event) ;  ; TTree * fCurrentTree;  the tree, events are currently read from ;  ; std::vector< TTreeFormula * > fCutFormulas;  cuts ;  ; std::vector< TTreeFormula * > fInputFormulas;  input variables ;  ; std::vector< std::pair< TTreeFormula *, Int_t > > fInputTableFormulas;  ! input variables expression for arrays ;  ; MsgLogger * fLogger;  ! message logger ;  ; Bool_t fScaleWithPreselEff;  how to deal with requested #events in connection with preselection cuts ;  ; std::vector< TTreeFormula * > fSpectatorFormulas;  spectators ;  ; std::vector< TTreeFormula * > fTargetFormulas;  targets ;  ; Bool_t fVerbose;  Verbosity. ;  ; TString fVerboseLevel;  VerboseLevel. ;  ; std::vector< TTreeFormula * > fWeightFormula;  weights ;  . Private Types; typedef std::vector< Event * > EventVector;  ; typedef std::vector< EventVector > EventVectorOfClasses;  ; typedef std::map< Types::ETreeType, EventVectorOfClasses > EventVectorOfClassesOfTreeType;  ; typedef std::map< Types::ETreeType, EventVector > EventVectorOfTreeType;  ; typedef std::vector< EventStats > EvtStatsPerClass;  ; typedef std::vector< int > NumberPerClass;  ; typedef std::vector< Double_t > ValuePerClass;  ; typedef std::map< Types::ETreeType, ValuePerClass > ValuePerClassOfTreeType;  . Additional Inher",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:17475,Testability,test,testing,17475,"ry::CalcCorrelationMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . computes correlation matrix for variables ""theVars"" in tree; ""theType"" defines the required event ""type"" (""type"" variable must be present in tree) ; Definition at line 548 of file DataSetFactory.cxx. ◆ CalcCovarianceMatrix(). TMatrixD * TMVA::DataSetFactory::CalcCovarianceMatrix ; (; DataSet * ; ds, . const UInt_t ; classNumber . ). protected . compute covariance matrix ; Definition at line 579 of file DataSetFactory.cxx. ◆ CalcMinMax(). void TMVA::DataSetFactory::CalcMinMax ; (; DataSet * ; ds, . TMVA::DataSetInfo & ; dsi . ). protected . compute covariance matrix ; Definition at line 479 of file DataSetFactory.cxx. ◆ ChangeToNewTree(). void TMVA::DataSetFactory::ChangeToNewTree ; (; TreeInfo & ; tinfo, . const DataSetInfo & ; dsi . ). protected . While the data gets copied into the local training and testing trees, the input tree can change (for instance when changing from signal to background tree, or using TChains as input) The TTreeFormulas, that hold the input expressions need to be re-associated with the new tree, which is done here. ; Definition at line 293 of file DataSetFactory.cxx. ◆ CheckTTreeFormula(). Bool_t TMVA::DataSetFactory::CheckTTreeFormula ; (; TTreeFormula * ; ttf, . const TString & ; expression, . Bool_t & ; hasDollar . ). protected . checks a TTreeFormula for problems ; Definition at line 251 of file DataSetFactory.cxx. ◆ Class(). static TClass * TMVA::DataSetFactory::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataSetFactory::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DataSetFactory::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 238 of file DataSetFactory.h. ◆ CreateDataSet(). TMVA::DataSet * TMVA::DataSetFactory::CreateDataSet ; (; TMVA::DataSetInfo & ; dsi, . TMVA::DataInputHa",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html:22605,Testability,log,logger,22605,"line 218 of file DataSetFactory.h. ◆ fCurrentEvtIdx. UInt_t TMVA::DataSetFactory::fCurrentEvtIdx. protected . the current event (to avoid reading of the same event) ; Definition at line 225 of file DataSetFactory.h. ◆ fCurrentTree. TTree* TMVA::DataSetFactory::fCurrentTree. protected . the tree, events are currently read from ; Definition at line 224 of file DataSetFactory.h. ◆ fCutFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fCutFormulas. protected . cuts ; Definition at line 231 of file DataSetFactory.h. ◆ fInputFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fInputFormulas. protected . input variables ; Definition at line 228 of file DataSetFactory.h. ◆ fInputTableFormulas. std::vector<std::pair<TTreeFormula*, Int_t> > TMVA::DataSetFactory::fInputTableFormulas. protected . ! input variables expression for arrays ; Definition at line 229 of file DataSetFactory.h. ◆ fLogger. MsgLogger* TMVA::DataSetFactory::fLogger. protected . ! message logger ; Definition at line 235 of file DataSetFactory.h. ◆ fScaleWithPreselEff. Bool_t TMVA::DataSetFactory::fScaleWithPreselEff. protected . how to deal with requested #events in connection with preselection cuts ; Definition at line 221 of file DataSetFactory.h. ◆ fSpectatorFormulas. std::vector<TTreeFormula*> TMVA::DataSetFactory::fSpectatorFormulas. protected . spectators ; Definition at line 233 of file DataSetFactory.h. ◆ fTargetFormulas. std::vector<TTreeFormula *> TMVA::DataSetFactory::fTargetFormulas. protected . targets ; Definition at line 230 of file DataSetFactory.h. ◆ fVerbose. Bool_t TMVA::DataSetFactory::fVerbose. protected . Verbosity. ; Definition at line 214 of file DataSetFactory.h. ◆ fVerboseLevel. TString TMVA::DataSetFactory::fVerboseLevel. protected . VerboseLevel. ; Definition at line 215 of file DataSetFactory.h. ◆ fWeightFormula. std::vector<TTreeFormula*> TMVA::DataSetFactory::fWeightFormula. protected . weights ; Definition at line 232 of file DataSetFactory.h. Libraries for TM",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetFactory.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetFactory.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:8254,Availability,error,error,8254,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:8343,Availability,error,error,8343,"ar *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:8498,Availability,error,error,8498,";  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the uniqu",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:8795,Availability,error,error,8795,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have a",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:13290,Availability,error,error,13290," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:8260,Integrability,message,message,8260,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:8801,Integrability,message,message,8801,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have a",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:9934,Integrability,message,message,9934,"ct * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this metho",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:13296,Integrability,message,message,13296," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:13669,Integrability,message,message,13669," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Priva",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:15146,Integrability,message,message,15146,"r * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions;  DataSetInfo (const DataSetInfo &)=delete;  ; MsgLogger & Log () const;  ; DataSetInfo & operator= (const DataSetInfo &)=delete;  ; void PrintCorrelationMatrix (TTree *theTree);  ; void SetDataSetManager (DataSetManager *dsm);  . Private Attributes; std::vector< ClassInfo * > fClasses;  name and other infos of the classes ;  ; DataSet * fDataSet;  dataset, owned by this datasetinfo object ;  ; TMVA::DataSetManager * fDataSetManager;  ; MsgLogger * fLogger;  ! message logger ;  ; TString fName;  name of the dataset info object ;  ; Bool_t fNeedsRebuilding;  flag if rebuilding of dataset is needed (after change of cuts, vars, etc.) ;  ; TString fNormalization;  ; TDirectory * fOwnRootDir;  ROOT output dir. ;  ; UInt_t fSignalClass;  index of the class with the name signal ;  ; std::vector< VariableInfo > fSpectators;  list of spectators expressions/internal names ;  ; TString fSplitOptions;  ; std::vector< VariableInfo > fTargets;  list of targets expressions/internal names ;  ; std::vector< Float_t > * fTargetsForMulticlass;  -> all targets 0 except the one with index==classNumber ;  ; Double_t fTestingSumBackgrWeights;  ; Double_t fTestingSumSignalWeights;  ; Double_t fTrainingSumBackgrWeights;  ; Double_t fTrainingSumSignalWeights;  ; std::map< TString, int > fVarArrays;  ; std::vector< VariableInfo > fVariables;  list of variable expressions/internal names ;  ; Bool_t fVerbose;  Verbosity. ;  . Friends; clas",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:31486,Integrability,message,message,31486,"o::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSetInfo::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 244 of file DataSetInfo.h. Friends And Related Symbol Documentation. ◆ DataSetManager. friend class DataSetManager. friend . Definition at line 199 of file DataSetInfo.h. Member Data Documentation. ◆ fClasses. std::vector<ClassInfo*> TMVA::DataSetInfo::fClasses. mutableprivate . name and other infos of the classes ; Definition at line 220 of file DataSetInfo.h. ◆ fDataSet. DataSet* TMVA::DataSetInfo::fDataSet. mutableprivate . dataset, owned by this datasetinfo object ; Definition at line 208 of file DataSetInfo.h. ◆ fDataSetManager. TMVA::DataSetManager* TMVA::DataSetInfo::fDataSetManager. private . Definition at line 197 of file DataSetInfo.h. ◆ fLogger. MsgLogger* TMVA::DataSetInfo::fLogger. mutableprivate . ! message logger ; Definition at line 239 of file DataSetInfo.h. ◆ fName. TString TMVA::DataSetInfo::fName. private . name of the dataset info object ; Definition at line 206 of file DataSetInfo.h. ◆ fNeedsRebuilding. Bool_t TMVA::DataSetInfo::fNeedsRebuilding. mutableprivate . flag if rebuilding of dataset is needed (after change of cuts, vars, etc.) ; Definition at line 209 of file DataSetInfo.h. ◆ fNormalization. TString TMVA::DataSetInfo::fNormalization. private . Definition at line 222 of file DataSetInfo.h. ◆ fOwnRootDir. TDirectory* TMVA::DataSetInfo::fOwnRootDir. private . ROOT output dir. ; Definition at line 232 of file DataSetInfo.h. ◆ fSignalClass. UInt_t TMVA::DataSetInfo::fSignalClass. private . index of the class with the name signal ; Definition at line 235 of file DataSetInfo.h. ◆ fSpectators. std::vector<VariableInfo> TMVA::DataSetInfo::fSpectators. private . list of spectators expressions/internal names ; Definition at line 214 of file DataSetInfo.h. ◆ fSplitOptions. TString TMVA::DataSetInfo::fSp",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:520,Modifiability,inherit,inherited,520,". ROOT: TMVA::DataSetInfo Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; Friends |; List of all members ; TMVA::DataSetInfo Class ReferenceTMVA. ; Class that contains all the data information. ; Definition at line 62 of file DataSetInfo.h. Public Types; enum  { kIsArrayVariable = (1ULL << ( 15 )); };  ;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  . Public Member Functions;  DataSetInfo (const TString &name=""Default"");  constructor ;  ; virtual ~DataSetInfo ();  destructor ;  ; ClassInfo * AddClass (const TString &className);  ; void AddCut (const TCut &cut, const TString &className);  set the cut for the classes ;  ; VariableInfo & AddSpectator (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:1681,Modifiability,variab,variables,1681," Public Member Functions;  DataSetInfo (const TString &name=""Default"");  constructor ;  ; virtual ~DataSetInfo ();  destructor ;  ; ClassInfo * AddClass (const TString &className);  ; void AddCut (const TCut &cut, const TString &className);  set the cut for the classes ;  ; VariableInfo & AddSpectator (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:2004,Modifiability,variab,variable,2004," Public Member Functions;  DataSetInfo (const TString &name=""Default"");  constructor ;  ; virtual ~DataSetInfo ();  destructor ;  ; ClassInfo * AddClass (const TString &className);  ; void AddCut (const TCut &cut, const TString &className);  set the cut for the classes ;  ; VariableInfo & AddSpectator (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:2057,Modifiability,variab,variables,2057," Public Member Functions;  DataSetInfo (const TString &name=""Default"");  constructor ;  ; virtual ~DataSetInfo ();  destructor ;  ; ClassInfo * AddClass (const TString &className);  ; void AddCut (const TCut &cut, const TString &className);  set the cut for the classes ;  ; VariableInfo & AddSpectator (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:2404,Modifiability,variab,variable,2404,"sses ;  ; VariableInfo & AddSpectator (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddTarget (const VariableInfo &varInfo);  add target with given VariableInfo ;  ; VariableInfo & AddVariable (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char varType='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddVariable (const VariableInfo &varInfo);  add variable with given VariableInfo ;  ; void AddVariablesArray (const TString &expression, Int_t size, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add an array of variables identified by an expression corresponding to an array entry in the tree ;  ; void ClearDataSet () const;  ; const TMatrixD * CorrelationMatrix (const TString &className) const;  ; TH2 * CreateCorrelationMatrixHist (const TMatrixD *m, const TString &hName, const TString &hTitle) const;  ; Int_t FindVarIndex (const TString &) const;  find variable by name ;  ; ClassInfo * GetClassInfo (const TString &name) const;  ; ClassInfo * GetClassInfo (Int_t clNum) const;  ; Int_t GetClassNameMaxLength () const;  ; const TCut & GetCut (const TString &className) const;  ; const TCut ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:2457,Modifiability,variab,variables,2457,"expression, const TString &title, const TString &unit, Double_t min, Double_t max, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddTarget (const VariableInfo &varInfo);  add target with given VariableInfo ;  ; VariableInfo & AddVariable (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char varType='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddVariable (const VariableInfo &varInfo);  add variable with given VariableInfo ;  ; void AddVariablesArray (const TString &expression, Int_t size, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add an array of variables identified by an expression corresponding to an array entry in the tree ;  ; void ClearDataSet () const;  ; const TMatrixD * CorrelationMatrix (const TString &className) const;  ; TH2 * CreateCorrelationMatrixHist (const TMatrixD *m, const TString &hName, const TString &hTitle) const;  ; Int_t FindVarIndex (const TString &) const;  find variable by name ;  ; ClassInfo * GetClassInfo (const TString &name) const;  ; ClassInfo * GetClassInfo (Int_t clNum) const;  ; Int_t GetClassNameMaxLength () const;  ; const TCut & GetCut (const TString &className) const;  ; const TCut & GetCut (Int_t i) const;  ; DataSet * GetDataSet () ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:2559,Modifiability,variab,variable,2559,"l_t normalized=kTRUE, void *external=nullptr);  add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddTarget (const VariableInfo &varInfo);  add target with given VariableInfo ;  ; VariableInfo & AddVariable (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char varType='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddVariable (const VariableInfo &varInfo);  add variable with given VariableInfo ;  ; void AddVariablesArray (const TString &expression, Int_t size, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add an array of variables identified by an expression corresponding to an array entry in the tree ;  ; void ClearDataSet () const;  ; const TMatrixD * CorrelationMatrix (const TString &className) const;  ; TH2 * CreateCorrelationMatrixHist (const TMatrixD *m, const TString &hName, const TString &hTitle) const;  ; Int_t FindVarIndex (const TString &) const;  find variable by name ;  ; ClassInfo * GetClassInfo (const TString &name) const;  ; ClassInfo * GetClassInfo (Int_t clNum) const;  ; Int_t GetClassNameMaxLength () const;  ; const TCut & GetCut (const TString &className) const;  ; const TCut & GetCut (Int_t i) const;  ; DataSet * GetDataSet () const;  returns data set ;  ; DataSetManager * GetDataSetManager ();  ; std::vector< TString > GetList",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:2823,Modifiability,variab,variables,2823," variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddTarget (const VariableInfo &varInfo);  add target with given VariableInfo ;  ; VariableInfo & AddVariable (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char varType='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddVariable (const VariableInfo &varInfo);  add variable with given VariableInfo ;  ; void AddVariablesArray (const TString &expression, Int_t size, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add an array of variables identified by an expression corresponding to an array entry in the tree ;  ; void ClearDataSet () const;  ; const TMatrixD * CorrelationMatrix (const TString &className) const;  ; TH2 * CreateCorrelationMatrixHist (const TMatrixD *m, const TString &hName, const TString &hTitle) const;  ; Int_t FindVarIndex (const TString &) const;  find variable by name ;  ; ClassInfo * GetClassInfo (const TString &name) const;  ; ClassInfo * GetClassInfo (Int_t clNum) const;  ; Int_t GetClassNameMaxLength () const;  ; const TCut & GetCut (const TString &className) const;  ; const TCut & GetCut (Int_t i) const;  ; DataSet * GetDataSet () const;  returns data set ;  ; DataSetManager * GetDataSetManager ();  ; std::vector< TString > GetListOfVariables () const;  returns list of variables ;  ; virtual const char * GetName () const;  Returns name of object.",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:3172,Modifiability,variab,variable,3172," variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddTarget (const VariableInfo &varInfo);  add target with given VariableInfo ;  ; VariableInfo & AddVariable (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char varType='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddVariable (const VariableInfo &varInfo);  add variable with given VariableInfo ;  ; void AddVariablesArray (const TString &expression, Int_t size, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add an array of variables identified by an expression corresponding to an array entry in the tree ;  ; void ClearDataSet () const;  ; const TMatrixD * CorrelationMatrix (const TString &className) const;  ; TH2 * CreateCorrelationMatrixHist (const TMatrixD *m, const TString &hName, const TString &hTitle) const;  ; Int_t FindVarIndex (const TString &) const;  find variable by name ;  ; ClassInfo * GetClassInfo (const TString &name) const;  ; ClassInfo * GetClassInfo (Int_t clNum) const;  ; Int_t GetClassNameMaxLength () const;  ; const TCut & GetCut (const TString &className) const;  ; const TCut & GetCut (Int_t i) const;  ; DataSet * GetDataSet () const;  returns data set ;  ; DataSetManager * GetDataSetManager ();  ; std::vector< TString > GetListOfVariables () const;  returns list of variables ;  ; virtual const char * GetName () const;  Returns name of object.",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:3603,Modifiability,variab,variables,3603," variables used in the MV analysis ;  ; VariableInfo & AddSpectator (const VariableInfo &varInfo);  add spectator with given VariableInfo ;  ; VariableInfo & AddTarget (const TString &expression, const TString &title, const TString &unit, Double_t min, Double_t max, Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddTarget (const VariableInfo &varInfo);  add target with given VariableInfo ;  ; VariableInfo & AddVariable (const TString &expression, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char varType='F', Bool_t normalized=kTRUE, void *external=nullptr);  add a variable (can be a complex expression) to the set of variables used in the MV analysis ;  ; VariableInfo & AddVariable (const VariableInfo &varInfo);  add variable with given VariableInfo ;  ; void AddVariablesArray (const TString &expression, Int_t size, const TString &title="""", const TString &unit="""", Double_t min=0, Double_t max=0, char type='F', Bool_t normalized=kTRUE, void *external=nullptr);  add an array of variables identified by an expression corresponding to an array entry in the tree ;  ; void ClearDataSet () const;  ; const TMatrixD * CorrelationMatrix (const TString &className) const;  ; TH2 * CreateCorrelationMatrixHist (const TMatrixD *m, const TString &hName, const TString &hTitle) const;  ; Int_t FindVarIndex (const TString &) const;  find variable by name ;  ; ClassInfo * GetClassInfo (const TString &name) const;  ; ClassInfo * GetClassInfo (Int_t clNum) const;  ; Int_t GetClassNameMaxLength () const;  ; const TCut & GetCut (const TString &className) const;  ; const TCut & GetCut (Int_t i) const;  ; DataSet * GetDataSet () const;  returns data set ;  ; DataSetManager * GetDataSetManager ();  ; std::vector< TString > GetListOfVariables () const;  returns list of variables ;  ; virtual const char * GetName () const;  Returns name of object.",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:6493,Modifiability,inherit,inherited,6493,"etCorrelationMatrix (const TString &className, TMatrixD *matrix);  ; void SetCut (const TCut &cut, const TString &className);  set the cut for the classes ;  ; void SetMsgType (EMsgType t) const;  ; void SetNormalization (const TString &norm);  ; void SetRootDir (TDirectory *d);  ; void SetSplitOptions (const TString &so);  ; void SetTestingSumBackgrWeights (Double_t testingSumBackgrWeights);  ; void SetTestingSumSignalWeights (Double_t testingSumSignalWeights);  ; void SetTrainingSumBackgrWeights (Double_t trainingSumBackgrWeights);  ; void SetTrainingSumSignalWeights (Double_t trainingSumSignalWeights);  ; void SetWeightExpression (const TString &exp, const TString &className="""");  set the weight expressions for the classes if class name is specified, set only for this class if class name is unknown, register new class with this name ;  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) c",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:7890,Modifiability,inherit,inheritance,7890,"pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:10032,Modifiability,inherit,inherits,10032,"asses. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:10149,Modifiability,inherit,inherits,10149," ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its p",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:14170,Modifiability,inherit,inherited,14170,"reamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions;  DataSetInfo (const DataSetInfo &)=delete;  ; MsgLogger & Log () const;  ; DataSetInfo & operator= (const DataSetInfo &)=delete;  ; void PrintCorrelationMatrix (TTree *theTree);  ; void SetDataSetManager (DataSetManager *dsm);  . Private Attributes; std::vector< ClassInfo * > fClasses;  name and other infos of the classes ;  ; DataSet * fDataSet;  dataset, owned by this datasetinfo object ;  ; TMVA::DataSetManager * fDataSetManager;  ; MsgLogger * fLogger;  ! message logger ;  ; TString fName; ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:16027,Modifiability,variab,variable,16027,") const;  ; DataSetInfo & operator= (const DataSetInfo &)=delete;  ; void PrintCorrelationMatrix (TTree *theTree);  ; void SetDataSetManager (DataSetManager *dsm);  . Private Attributes; std::vector< ClassInfo * > fClasses;  name and other infos of the classes ;  ; DataSet * fDataSet;  dataset, owned by this datasetinfo object ;  ; TMVA::DataSetManager * fDataSetManager;  ; MsgLogger * fLogger;  ! message logger ;  ; TString fName;  name of the dataset info object ;  ; Bool_t fNeedsRebuilding;  flag if rebuilding of dataset is needed (after change of cuts, vars, etc.) ;  ; TString fNormalization;  ; TDirectory * fOwnRootDir;  ROOT output dir. ;  ; UInt_t fSignalClass;  index of the class with the name signal ;  ; std::vector< VariableInfo > fSpectators;  list of spectators expressions/internal names ;  ; TString fSplitOptions;  ; std::vector< VariableInfo > fTargets;  list of targets expressions/internal names ;  ; std::vector< Float_t > * fTargetsForMulticlass;  -> all targets 0 except the one with index==classNumber ;  ; Double_t fTestingSumBackgrWeights;  ; Double_t fTestingSumSignalWeights;  ; Double_t fTrainingSumBackgrWeights;  ; Double_t fTrainingSumSignalWeights;  ; std::map< TString, int > fVarArrays;  ; std::vector< VariableInfo > fVariables;  list of variable expressions/internal names ;  ; Bool_t fVerbose;  Verbosity. ;  . Friends; class DataSetManager;  . Additional Inherited Members;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataSetInfo.h>. Inheritance diagram for TMVA::DataSetInfo:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ anonymous enum. anonymous enum. EnumeratorkIsArrayVariable . Definition at",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:16183,Modifiability,inherit,inherited,16183,"(after change of cuts, vars, etc.) ;  ; TString fNormalization;  ; TDirectory * fOwnRootDir;  ROOT output dir. ;  ; UInt_t fSignalClass;  index of the class with the name signal ;  ; std::vector< VariableInfo > fSpectators;  list of spectators expressions/internal names ;  ; TString fSplitOptions;  ; std::vector< VariableInfo > fTargets;  list of targets expressions/internal names ;  ; std::vector< Float_t > * fTargetsForMulticlass;  -> all targets 0 except the one with index==classNumber ;  ; Double_t fTestingSumBackgrWeights;  ; Double_t fTestingSumSignalWeights;  ; Double_t fTrainingSumBackgrWeights;  ; Double_t fTrainingSumSignalWeights;  ; std::map< TString, int > fVarArrays;  ; std::vector< VariableInfo > fVariables;  list of variable expressions/internal names ;  ; Bool_t fVerbose;  Verbosity. ;  . Friends; class DataSetManager;  . Additional Inherited Members;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataSetInfo.h>. Inheritance diagram for TMVA::DataSetInfo:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ anonymous enum. anonymous enum. EnumeratorkIsArrayVariable . Definition at line 66 of file DataSetInfo.h. Constructor & Destructor Documentation. ◆ DataSetInfo() [1/2]. TMVA::DataSetInfo::DataSetInfo ; (; const TString & ; name = ""Default""). constructor ; Definition at line 56 of file DataSetInfo.cxx. ◆ ~DataSetInfo(). TMVA::DataSetInfo::~DataSetInfo ; (; ). virtual . destructor ; Definition at line 83 of file DataSetInfo.cxx. ◆ DataSetInfo() [2/2]. TMVA::DataSetInfo::DataSetInfo ; (; const DataSetInfo & ; ). privatedelete . Member Function Documentation. ◆ AddClass(). TMVA::ClassInfo * TMVA::DataSetInfo::A",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:16282,Modifiability,inherit,inherited,16282,"(after change of cuts, vars, etc.) ;  ; TString fNormalization;  ; TDirectory * fOwnRootDir;  ROOT output dir. ;  ; UInt_t fSignalClass;  index of the class with the name signal ;  ; std::vector< VariableInfo > fSpectators;  list of spectators expressions/internal names ;  ; TString fSplitOptions;  ; std::vector< VariableInfo > fTargets;  list of targets expressions/internal names ;  ; std::vector< Float_t > * fTargetsForMulticlass;  -> all targets 0 except the one with index==classNumber ;  ; Double_t fTestingSumBackgrWeights;  ; Double_t fTestingSumSignalWeights;  ; Double_t fTrainingSumBackgrWeights;  ; Double_t fTrainingSumSignalWeights;  ; std::map< TString, int > fVarArrays;  ; std::vector< VariableInfo > fVariables;  list of variable expressions/internal names ;  ; Bool_t fVerbose;  Verbosity. ;  . Friends; class DataSetManager;  . Additional Inherited Members;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataSetInfo.h>. Inheritance diagram for TMVA::DataSetInfo:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Enumeration Documentation. ◆ anonymous enum. anonymous enum. EnumeratorkIsArrayVariable . Definition at line 66 of file DataSetInfo.h. Constructor & Destructor Documentation. ◆ DataSetInfo() [1/2]. TMVA::DataSetInfo::DataSetInfo ; (; const TString & ; name = ""Default""). constructor ; Definition at line 56 of file DataSetInfo.cxx. ◆ ~DataSetInfo(). TMVA::DataSetInfo::~DataSetInfo ; (; ). virtual . destructor ; Definition at line 83 of file DataSetInfo.cxx. ◆ DataSetInfo() [2/2]. TMVA::DataSetInfo::DataSetInfo ; (; const DataSetInfo & ; ). privatedelete . Member Function Documentation. ◆ AddClass(). TMVA::ClassInfo * TMVA::DataSetInfo::A",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:17916,Modifiability,variab,variables,17916,"uctor ; Definition at line 56 of file DataSetInfo.cxx. ◆ ~DataSetInfo(). TMVA::DataSetInfo::~DataSetInfo ; (; ). virtual . destructor ; Definition at line 83 of file DataSetInfo.cxx. ◆ DataSetInfo() [2/2]. TMVA::DataSetInfo::DataSetInfo ; (; const DataSetInfo & ; ). privatedelete . Member Function Documentation. ◆ AddClass(). TMVA::ClassInfo * TMVA::DataSetInfo::AddClass ; (; const TString & ; className). Definition at line 113 of file DataSetInfo.cxx. ◆ AddCut(). void TMVA::DataSetInfo::AddCut ; (; const TCut & ; cut, . const TString & ; className . ). set the cut for the classes ; Definition at line 389 of file DataSetInfo.cxx. ◆ AddSpectator() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddSpectator ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . char ; type = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ; Definition at line 302 of file DataSetInfo.cxx. ◆ AddSpectator() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddSpectator ; (; const VariableInfo & ; varInfo). add spectator with given VariableInfo ; Definition at line 319 of file DataSetInfo.cxx. ◆ AddTarget() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 272 of file DataSetInfo.cxx. ◆ AddTarget() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const VariableInfo & ; varInfo). add target with given VariableInfo ; Definition at line 292 of file DataSetInfo.cxx. ◆ AddVariable() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const TString & ; expression, . const T",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:18470,Modifiability,variab,variable,18470," of file DataSetInfo.cxx. ◆ AddSpectator() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddSpectator ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . char ; type = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ; Definition at line 302 of file DataSetInfo.cxx. ◆ AddSpectator() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddSpectator ; (; const VariableInfo & ; varInfo). add spectator with given VariableInfo ; Definition at line 319 of file DataSetInfo.cxx. ◆ AddTarget() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 272 of file DataSetInfo.cxx. ◆ AddTarget() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const VariableInfo & ; varInfo). add target with given VariableInfo ; Definition at line 292 of file DataSetInfo.cxx. ◆ AddVariable() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; varType = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 207 of file DataSetInfo.cxx. ◆ AddVariable() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const VariableInfo & ; varInfo). add variable with given VariableInfo ; Definition at line 226 of file DataSetInfo.cxx. ◆ AddVariablesArray(). void TMVA::DataSetInfo::AddVariablesArray ; (; const TStrin",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:18523,Modifiability,variab,variables,18523," of file DataSetInfo.cxx. ◆ AddSpectator() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddSpectator ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . char ; type = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a spectator (can be a complex expression) to the set of spectator variables used in the MV analysis ; Definition at line 302 of file DataSetInfo.cxx. ◆ AddSpectator() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddSpectator ; (; const VariableInfo & ; varInfo). add spectator with given VariableInfo ; Definition at line 319 of file DataSetInfo.cxx. ◆ AddTarget() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 272 of file DataSetInfo.cxx. ◆ AddTarget() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const VariableInfo & ; varInfo). add target with given VariableInfo ; Definition at line 292 of file DataSetInfo.cxx. ◆ AddVariable() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; varType = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 207 of file DataSetInfo.cxx. ◆ AddVariable() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const VariableInfo & ; varInfo). add variable with given VariableInfo ; Definition at line 226 of file DataSetInfo.cxx. ◆ AddVariablesArray(). void TMVA::DataSetInfo::AddVariablesArray ; (; const TStrin",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:19114,Modifiability,variab,variable,19114,"9 of file DataSetInfo.cxx. ◆ AddTarget() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 272 of file DataSetInfo.cxx. ◆ AddTarget() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const VariableInfo & ; varInfo). add target with given VariableInfo ; Definition at line 292 of file DataSetInfo.cxx. ◆ AddVariable() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; varType = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 207 of file DataSetInfo.cxx. ◆ AddVariable() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const VariableInfo & ; varInfo). add variable with given VariableInfo ; Definition at line 226 of file DataSetInfo.cxx. ◆ AddVariablesArray(). void TMVA::DataSetInfo::AddVariablesArray ; (; const TString & ; expression, . Int_t ; size, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; type = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add an array of variables identified by an expression corresponding to an array entry in the tree ; Definition at line 235 of file DataSetInfo.cxx. ◆ Class(). static TClass * TMVA::DataSetInfo::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataSetInfo::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:19167,Modifiability,variab,variables,19167,"9 of file DataSetInfo.cxx. ◆ AddTarget() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const TString & ; expression, . const TString & ; title, . const TString & ; unit, . Double_t ; min, . Double_t ; max, . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 272 of file DataSetInfo.cxx. ◆ AddTarget() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const VariableInfo & ; varInfo). add target with given VariableInfo ; Definition at line 292 of file DataSetInfo.cxx. ◆ AddVariable() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; varType = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 207 of file DataSetInfo.cxx. ◆ AddVariable() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const VariableInfo & ; varInfo). add variable with given VariableInfo ; Definition at line 226 of file DataSetInfo.cxx. ◆ AddVariablesArray(). void TMVA::DataSetInfo::AddVariablesArray ; (; const TString & ; expression, . Int_t ; size, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; type = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add an array of variables identified by an expression corresponding to an array entry in the tree ; Definition at line 235 of file DataSetInfo.cxx. ◆ Class(). static TClass * TMVA::DataSetInfo::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataSetInfo::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:19368,Modifiability,variab,variable,19368,"_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 272 of file DataSetInfo.cxx. ◆ AddTarget() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddTarget ; (; const VariableInfo & ; varInfo). add target with given VariableInfo ; Definition at line 292 of file DataSetInfo.cxx. ◆ AddVariable() [1/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; varType = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 207 of file DataSetInfo.cxx. ◆ AddVariable() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const VariableInfo & ; varInfo). add variable with given VariableInfo ; Definition at line 226 of file DataSetInfo.cxx. ◆ AddVariablesArray(). void TMVA::DataSetInfo::AddVariablesArray ; (; const TString & ; expression, . Int_t ; size, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; type = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add an array of variables identified by an expression corresponding to an array entry in the tree ; Definition at line 235 of file DataSetInfo.cxx. ◆ Class(). static TClass * TMVA::DataSetInfo::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataSetInfo::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DataSetInfo::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 244 of file DataSetInfo.h. ◆ ClearDataSet(). void TMVA::DataSetInfo::ClearDataSet ; (; ); const. Definition at line 9",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:19777,Modifiability,variab,variables,19777,"riableInfo & TMVA::DataSetInfo::AddVariable ; (; const TString & ; expression, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; varType = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add a variable (can be a complex expression) to the set of variables used in the MV analysis ; Definition at line 207 of file DataSetInfo.cxx. ◆ AddVariable() [2/2]. TMVA::VariableInfo & TMVA::DataSetInfo::AddVariable ; (; const VariableInfo & ; varInfo). add variable with given VariableInfo ; Definition at line 226 of file DataSetInfo.cxx. ◆ AddVariablesArray(). void TMVA::DataSetInfo::AddVariablesArray ; (; const TString & ; expression, . Int_t ; size, . const TString & ; title = """", . const TString & ; unit = """", . Double_t ; min = 0, . Double_t ; max = 0, . char ; type = 'F', . Bool_t ; normalized = kTRUE, . void * ; external = nullptr . ). add an array of variables identified by an expression corresponding to an array entry in the tree ; Definition at line 235 of file DataSetInfo.cxx. ◆ Class(). static TClass * TMVA::DataSetInfo::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DataSetInfo::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DataSetInfo::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 244 of file DataSetInfo.h. ◆ ClearDataSet(). void TMVA::DataSetInfo::ClearDataSet ; (; ); const. Definition at line 98 of file DataSetInfo.cxx. ◆ CorrelationMatrix(). const TMatrixD * TMVA::DataSetInfo::CorrelationMatrix ; (; const TString & ; className); const. Definition at line 197 of file DataSetInfo.cxx. ◆ CreateCorrelationMatrixHist(). TH2 * TMVA::DataSetInfo::CreateCorrelationMatrixHist ; (; const TMatrixD * ; m, . const TString & ; hName, . const TString & ; hTitle . ); const. Definition at line 429 of file DataSetInfo.cxx. ◆ Decl",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:21121,Modifiability,variab,variable,21121,"tic constexpr Version_t TMVA::DataSetInfo::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 244 of file DataSetInfo.h. ◆ ClearDataSet(). void TMVA::DataSetInfo::ClearDataSet ; (; ); const. Definition at line 98 of file DataSetInfo.cxx. ◆ CorrelationMatrix(). const TMatrixD * TMVA::DataSetInfo::CorrelationMatrix ; (; const TString & ; className); const. Definition at line 197 of file DataSetInfo.cxx. ◆ CreateCorrelationMatrixHist(). TH2 * TMVA::DataSetInfo::CreateCorrelationMatrixHist ; (; const TMatrixD * ; m, . const TString & ; hName, . const TString & ; hTitle . ); const. Definition at line 429 of file DataSetInfo.cxx. ◆ DeclFileName(). static const char * TMVA::DataSetInfo::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 244 of file DataSetInfo.h. ◆ FindVarIndex(). Int_t TMVA::DataSetInfo::FindVarIndex ; (; const TString & ; var); const. find variable by name ; Definition at line 328 of file DataSetInfo.cxx. ◆ GetClassInfo() [1/2]. TMVA::ClassInfo * TMVA::DataSetInfo::GetClassInfo ; (; const TString & ; name); const. Definition at line 136 of file DataSetInfo.cxx. ◆ GetClassInfo() [2/2]. TMVA::ClassInfo * TMVA::DataSetInfo::GetClassInfo ; (; Int_t ; clNum); const. Definition at line 146 of file DataSetInfo.cxx. ◆ GetClassNameMaxLength(). Int_t TMVA::DataSetInfo::GetClassNameMaxLength ; (; ); const. Definition at line 524 of file DataSetInfo.cxx. ◆ GetCut() [1/2]. const TCut & TMVA::DataSetInfo::GetCut ; (; const TString & ; className); const. inline . Definition at line 169 of file DataSetInfo.h. ◆ GetCut() [2/2]. const TCut & TMVA::DataSetInfo::GetCut ; (; Int_t ; i); const. inline . Definition at line 168 of file DataSetInfo.h. ◆ GetDataSet(). TMVA::DataSet * TMVA::DataSetInfo::GetDataSet ; (; ); const. returns data set ; Definition at line 493 of file DataSetInfo.cxx. ◆ GetDataSetManager(). DataSetManager * TMVA::DataSetInfo::GetDataSetManager ; (;",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:22324,Modifiability,variab,variables,22324,xx. ◆ GetClassInfo() [2/2]. TMVA::ClassInfo * TMVA::DataSetInfo::GetClassInfo ; (; Int_t ; clNum); const. Definition at line 146 of file DataSetInfo.cxx. ◆ GetClassNameMaxLength(). Int_t TMVA::DataSetInfo::GetClassNameMaxLength ; (; ); const. Definition at line 524 of file DataSetInfo.cxx. ◆ GetCut() [1/2]. const TCut & TMVA::DataSetInfo::GetCut ; (; const TString & ; className); const. inline . Definition at line 169 of file DataSetInfo.h. ◆ GetCut() [2/2]. const TCut & TMVA::DataSetInfo::GetCut ; (; Int_t ; i); const. inline . Definition at line 168 of file DataSetInfo.h. ◆ GetDataSet(). TMVA::DataSet * TMVA::DataSetInfo::GetDataSet ; (; ); const. returns data set ; Definition at line 493 of file DataSetInfo.cxx. ◆ GetDataSetManager(). DataSetManager * TMVA::DataSetInfo::GetDataSetManager ; (; ). inline . Definition at line 194 of file DataSetInfo.h. ◆ GetListOfVariables(). std::vector< TString > TMVA::DataSetInfo::GetListOfVariables ; (; ); const. returns list of variables ; Definition at line 406 of file DataSetInfo.cxx. ◆ GetName(). virtual const char * TMVA::DataSetInfo::GetName ; (; ); const. inlinevirtual . Returns name of object. ; This default method returns the class name. Classes that give objects a name should override this method. ; Reimplemented from TObject.; Definition at line 71 of file DataSetInfo.h. ◆ GetNClasses(). UInt_t TMVA::DataSetInfo::GetNClasses ; (; ); const. inline . Definition at line 155 of file DataSetInfo.h. ◆ GetNormalization(). const TString & TMVA::DataSetInfo::GetNormalization ; (; ); const. inline . Definition at line 131 of file DataSetInfo.h. ◆ GetNSpectators(). UInt_t TMVA::DataSetInfo::GetNSpectators ; (; bool ; all = kTRUE); const. Definition at line 511 of file DataSetInfo.cxx. ◆ GetNTargets(). UInt_t TMVA::DataSetInfo::GetNTargets ; (; ); const. inline . Definition at line 128 of file DataSetInfo.h. ◆ GetNVariables(). UInt_t TMVA::DataSetInfo::GetNVariables ; (; ); const. inline . Definition at line 127 of file DataSetIn,MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:33723,Modifiability,variab,variable,33723,"he name signal ; Definition at line 235 of file DataSetInfo.h. ◆ fSpectators. std::vector<VariableInfo> TMVA::DataSetInfo::fSpectators. private . list of spectators expressions/internal names ; Definition at line 214 of file DataSetInfo.h. ◆ fSplitOptions. TString TMVA::DataSetInfo::fSplitOptions. private . Definition at line 223 of file DataSetInfo.h. ◆ fTargets. std::vector<VariableInfo> TMVA::DataSetInfo::fTargets. private . list of targets expressions/internal names ; Definition at line 213 of file DataSetInfo.h. ◆ fTargetsForMulticlass. std::vector<Float_t>* TMVA::DataSetInfo::fTargetsForMulticlass. private . -> all targets 0 except the one with index==classNumber ; Definition at line 237 of file DataSetInfo.h. ◆ fTestingSumBackgrWeights. Double_t TMVA::DataSetInfo::fTestingSumBackgrWeights. private . Definition at line 228 of file DataSetInfo.h. ◆ fTestingSumSignalWeights. Double_t TMVA::DataSetInfo::fTestingSumSignalWeights. private . Definition at line 227 of file DataSetInfo.h. ◆ fTrainingSumBackgrWeights. Double_t TMVA::DataSetInfo::fTrainingSumBackgrWeights. private . Definition at line 226 of file DataSetInfo.h. ◆ fTrainingSumSignalWeights. Double_t TMVA::DataSetInfo::fTrainingSumSignalWeights. private . Definition at line 225 of file DataSetInfo.h. ◆ fVarArrays. std::map<TString, int> TMVA::DataSetInfo::fVarArrays. private . Definition at line 217 of file DataSetInfo.h. ◆ fVariables. std::vector<VariableInfo> TMVA::DataSetInfo::fVariables. private . list of variable expressions/internal names ; Definition at line 212 of file DataSetInfo.h. ◆ fVerbose. Bool_t TMVA::DataSetInfo::fVerbose. private . Verbosity. ; Definition at line 233 of file DataSetInfo.h. Libraries for TMVA::DataSetInfo:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DataSetInfo.h; tmva/tmva/src/DataSetInfo.cxx. TMVADataSetInfo. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:9663,Security,hash,hash,9663,"xecute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:5844,Testability,test,testingSumBackgrWeights,5844,"bleInfo > & GetTargetInfos () const;  ; Int_t GetTargetNameMaxLength () const;  ; std::vector< Float_t > * GetTargetsForMulticlass (const Event *ev);  ; Double_t GetTestingSumBackgrWeights ();  ; Double_t GetTestingSumSignalWeights ();  ; Double_t GetTrainingSumBackgrWeights ();  ; Double_t GetTrainingSumSignalWeights ();  ; Int_t GetVarArraySize (const TString &expression) const;  ; VariableInfo & GetVariableInfo (Int_t i);  ; const VariableInfo & GetVariableInfo (Int_t i) const;  ; std::vector< VariableInfo > & GetVariableInfos ();  ; const std::vector< VariableInfo > & GetVariableInfos () const;  ; Int_t GetVariableNameMaxLength () const;  ; const TString GetWeightExpression (Int_t i) const;  ; Bool_t HasCuts () const;  ; virtual TClass * IsA () const;  ; Bool_t IsSignal (const Event *ev) const;  ; Bool_t IsVariableFromArray (Int_t i) const;  ; void PrintClasses () const;  ; void PrintCorrelationMatrix (const TString &className);  calculates the correlation matrices for signal and background, prints them to standard output, and fills 2D histograms ;  ; void SetCorrelationMatrix (const TString &className, TMatrixD *matrix);  ; void SetCut (const TCut &cut, const TString &className);  set the cut for the classes ;  ; void SetMsgType (EMsgType t) const;  ; void SetNormalization (const TString &norm);  ; void SetRootDir (TDirectory *d);  ; void SetSplitOptions (const TString &so);  ; void SetTestingSumBackgrWeights (Double_t testingSumBackgrWeights);  ; void SetTestingSumSignalWeights (Double_t testingSumSignalWeights);  ; void SetTrainingSumBackgrWeights (Double_t trainingSumBackgrWeights);  ; void SetTrainingSumSignalWeights (Double_t trainingSumSignalWeights);  ; void SetWeightExpression (const TString &exp, const TString &className="""");  set the weight expressions for the classes if class name is specified, set only for this class if class name is unknown, register new class with this name ;  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject.",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:5915,Testability,test,testingSumSignalWeights,5915,"bleInfo > & GetTargetInfos () const;  ; Int_t GetTargetNameMaxLength () const;  ; std::vector< Float_t > * GetTargetsForMulticlass (const Event *ev);  ; Double_t GetTestingSumBackgrWeights ();  ; Double_t GetTestingSumSignalWeights ();  ; Double_t GetTrainingSumBackgrWeights ();  ; Double_t GetTrainingSumSignalWeights ();  ; Int_t GetVarArraySize (const TString &expression) const;  ; VariableInfo & GetVariableInfo (Int_t i);  ; const VariableInfo & GetVariableInfo (Int_t i) const;  ; std::vector< VariableInfo > & GetVariableInfos ();  ; const std::vector< VariableInfo > & GetVariableInfos () const;  ; Int_t GetVariableNameMaxLength () const;  ; const TString GetWeightExpression (Int_t i) const;  ; Bool_t HasCuts () const;  ; virtual TClass * IsA () const;  ; Bool_t IsSignal (const Event *ev) const;  ; Bool_t IsVariableFromArray (Int_t i) const;  ; void PrintClasses () const;  ; void PrintCorrelationMatrix (const TString &className);  calculates the correlation matrices for signal and background, prints them to standard output, and fills 2D histograms ;  ; void SetCorrelationMatrix (const TString &className, TMatrixD *matrix);  ; void SetCut (const TCut &cut, const TString &className);  set the cut for the classes ;  ; void SetMsgType (EMsgType t) const;  ; void SetNormalization (const TString &norm);  ; void SetRootDir (TDirectory *d);  ; void SetSplitOptions (const TString &so);  ; void SetTestingSumBackgrWeights (Double_t testingSumBackgrWeights);  ; void SetTestingSumSignalWeights (Double_t testingSumSignalWeights);  ; void SetTrainingSumBackgrWeights (Double_t trainingSumBackgrWeights);  ; void SetTrainingSumSignalWeights (Double_t trainingSumSignalWeights);  ; void SetWeightExpression (const TString &exp, const TString &className="""");  set the weight expressions for the classes if class name is specified, set only for this class if class name is unknown, register new class with this name ;  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject.",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:15154,Testability,log,logger,15154,"r * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions;  DataSetInfo (const DataSetInfo &)=delete;  ; MsgLogger & Log () const;  ; DataSetInfo & operator= (const DataSetInfo &)=delete;  ; void PrintCorrelationMatrix (TTree *theTree);  ; void SetDataSetManager (DataSetManager *dsm);  . Private Attributes; std::vector< ClassInfo * > fClasses;  name and other infos of the classes ;  ; DataSet * fDataSet;  dataset, owned by this datasetinfo object ;  ; TMVA::DataSetManager * fDataSetManager;  ; MsgLogger * fLogger;  ! message logger ;  ; TString fName;  name of the dataset info object ;  ; Bool_t fNeedsRebuilding;  flag if rebuilding of dataset is needed (after change of cuts, vars, etc.) ;  ; TString fNormalization;  ; TDirectory * fOwnRootDir;  ROOT output dir. ;  ; UInt_t fSignalClass;  index of the class with the name signal ;  ; std::vector< VariableInfo > fSpectators;  list of spectators expressions/internal names ;  ; TString fSplitOptions;  ; std::vector< VariableInfo > fTargets;  list of targets expressions/internal names ;  ; std::vector< Float_t > * fTargetsForMulticlass;  -> all targets 0 except the one with index==classNumber ;  ; Double_t fTestingSumBackgrWeights;  ; Double_t fTestingSumSignalWeights;  ; Double_t fTrainingSumBackgrWeights;  ; Double_t fTrainingSumSignalWeights;  ; std::map< TString, int > fVarArrays;  ; std::vector< VariableInfo > fVariables;  list of variable expressions/internal names ;  ; Bool_t fVerbose;  Verbosity. ;  . Friends; clas",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:29502,Testability,test,testingSumBackgrWeights,29502,"at line 365 of file DataSetInfo.cxx. ◆ SetCut(). void TMVA::DataSetInfo::SetCut ; (; const TCut & ; cut, . const TString & ; className . ). set the cut for the classes ; Definition at line 373 of file DataSetInfo.cxx. ◆ SetDataSetManager(). void TMVA::DataSetInfo::SetDataSetManager ; (; DataSetManager * ; dsm). inlineprivate . Definition at line 198 of file DataSetInfo.h. ◆ SetMsgType(). void TMVA::DataSetInfo::SetMsgType ; (; EMsgType ; t); const. Definition at line 106 of file DataSetInfo.cxx. ◆ SetNormalization(). void TMVA::DataSetInfo::SetNormalization ; (; const TString & ; norm). inline . Definition at line 132 of file DataSetInfo.h. ◆ SetRootDir(). void TMVA::DataSetInfo::SetRootDir ; (; TDirectory * ; d). inline . Definition at line 189 of file DataSetInfo.h. ◆ SetSplitOptions(). void TMVA::DataSetInfo::SetSplitOptions ; (; const TString & ; so). inline . Definition at line 185 of file DataSetInfo.h. ◆ SetTestingSumBackgrWeights(). void TMVA::DataSetInfo::SetTestingSumBackgrWeights ; (; Double_t ; testingSumBackgrWeights). inline . Definition at line 139 of file DataSetInfo.h. ◆ SetTestingSumSignalWeights(). void TMVA::DataSetInfo::SetTestingSumSignalWeights ; (; Double_t ; testingSumSignalWeights). inline . Definition at line 138 of file DataSetInfo.h. ◆ SetTrainingSumBackgrWeights(). void TMVA::DataSetInfo::SetTrainingSumBackgrWeights ; (; Double_t ; trainingSumBackgrWeights). inline . Definition at line 137 of file DataSetInfo.h. ◆ SetTrainingSumSignalWeights(). void TMVA::DataSetInfo::SetTrainingSumSignalWeights ; (; Double_t ; trainingSumSignalWeights). inline . Definition at line 134 of file DataSetInfo.h. ◆ SetWeightExpression(). void TMVA::DataSetInfo::SetWeightExpression ; (; const TString & ; exp, . const TString & ; className = """" . ). set the weight expressions for the classes if class name is specified, set only for this class if class name is unknown, register new class with this name ; Definition at line 346 of file DataSetInfo.cxx. ◆ Streamer",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:29682,Testability,test,testingSumSignalWeights,29682," at line 373 of file DataSetInfo.cxx. ◆ SetDataSetManager(). void TMVA::DataSetInfo::SetDataSetManager ; (; DataSetManager * ; dsm). inlineprivate . Definition at line 198 of file DataSetInfo.h. ◆ SetMsgType(). void TMVA::DataSetInfo::SetMsgType ; (; EMsgType ; t); const. Definition at line 106 of file DataSetInfo.cxx. ◆ SetNormalization(). void TMVA::DataSetInfo::SetNormalization ; (; const TString & ; norm). inline . Definition at line 132 of file DataSetInfo.h. ◆ SetRootDir(). void TMVA::DataSetInfo::SetRootDir ; (; TDirectory * ; d). inline . Definition at line 189 of file DataSetInfo.h. ◆ SetSplitOptions(). void TMVA::DataSetInfo::SetSplitOptions ; (; const TString & ; so). inline . Definition at line 185 of file DataSetInfo.h. ◆ SetTestingSumBackgrWeights(). void TMVA::DataSetInfo::SetTestingSumBackgrWeights ; (; Double_t ; testingSumBackgrWeights). inline . Definition at line 139 of file DataSetInfo.h. ◆ SetTestingSumSignalWeights(). void TMVA::DataSetInfo::SetTestingSumSignalWeights ; (; Double_t ; testingSumSignalWeights). inline . Definition at line 138 of file DataSetInfo.h. ◆ SetTrainingSumBackgrWeights(). void TMVA::DataSetInfo::SetTrainingSumBackgrWeights ; (; Double_t ; trainingSumBackgrWeights). inline . Definition at line 137 of file DataSetInfo.h. ◆ SetTrainingSumSignalWeights(). void TMVA::DataSetInfo::SetTrainingSumSignalWeights ; (; Double_t ; trainingSumSignalWeights). inline . Definition at line 134 of file DataSetInfo.h. ◆ SetWeightExpression(). void TMVA::DataSetInfo::SetWeightExpression ; (; const TString & ; exp, . const TString & ; className = """" . ). set the weight expressions for the classes if class name is specified, set only for this class if class name is unknown, register new class with this name ; Definition at line 346 of file DataSetInfo.cxx. ◆ Streamer(). virtual void TMVA::DataSetInfo::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::Dat",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html:31494,Testability,log,logger,31494,"o::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSetInfo::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 244 of file DataSetInfo.h. Friends And Related Symbol Documentation. ◆ DataSetManager. friend class DataSetManager. friend . Definition at line 199 of file DataSetInfo.h. Member Data Documentation. ◆ fClasses. std::vector<ClassInfo*> TMVA::DataSetInfo::fClasses. mutableprivate . name and other infos of the classes ; Definition at line 220 of file DataSetInfo.h. ◆ fDataSet. DataSet* TMVA::DataSetInfo::fDataSet. mutableprivate . dataset, owned by this datasetinfo object ; Definition at line 208 of file DataSetInfo.h. ◆ fDataSetManager. TMVA::DataSetManager* TMVA::DataSetInfo::fDataSetManager. private . Definition at line 197 of file DataSetInfo.h. ◆ fLogger. MsgLogger* TMVA::DataSetInfo::fLogger. mutableprivate . ! message logger ; Definition at line 239 of file DataSetInfo.h. ◆ fName. TString TMVA::DataSetInfo::fName. private . name of the dataset info object ; Definition at line 206 of file DataSetInfo.h. ◆ fNeedsRebuilding. Bool_t TMVA::DataSetInfo::fNeedsRebuilding. mutableprivate . flag if rebuilding of dataset is needed (after change of cuts, vars, etc.) ; Definition at line 209 of file DataSetInfo.h. ◆ fNormalization. TString TMVA::DataSetInfo::fNormalization. private . Definition at line 222 of file DataSetInfo.h. ◆ fOwnRootDir. TDirectory* TMVA::DataSetInfo::fOwnRootDir. private . ROOT output dir. ; Definition at line 232 of file DataSetInfo.h. ◆ fSignalClass. UInt_t TMVA::DataSetInfo::fSignalClass. private . index of the class with the name signal ; Definition at line 235 of file DataSetInfo.h. ◆ fSpectators. std::vector<VariableInfo> TMVA::DataSetInfo::fSpectators. private . list of spectators expressions/internal names ; Definition at line 214 of file DataSetInfo.h. ◆ fSplitOptions. TString TMVA::DataSetInfo::fSp",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetInfo.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetInfo.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:2966,Availability,error,error,2966,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Return",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:3055,Availability,error,error,3055,"ar *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns strin",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:3210,Availability,error,error,3210,";  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title o",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:3507,Availability,error,error,3507,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:8071,Availability,error,error,8071," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:2972,Integrability,message,message,2972,"object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Return",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:3513,Integrability,message,message,3513,"ption="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:4715,Integrability,message,message,4715,"ived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this metho",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:8077,Integrability,message,message,8077," must be overridden when a class wants to print itself. ;  ; virtual Int_t Read (const char *name);  Read contents of object with specified name from the current directory. ;  ; virtual void RecursiveRemove (TObject *obj);  Recursively remove this object from a list. ;  ; void ResetBit (UInt_t f);  ; virtual void SaveAs (const char *filename="""", Option_t *option="""") const;  Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:8450,Integrability,message,message,8450," Save this object in the file specified by filename. ;  ; virtual void SavePrimitive (std::ostream &out, Option_t *option="""");  Save a primitive as a C++ statement(s) on output stream ""out"". ;  ; void SetBit (UInt_t f);  ; void SetBit (UInt_t f, Bool_t set);  Set or unset the user status bits as specified in f. ;  ; virtual void SetDrawOption (Option_t *option="""");  Set drawing option for object. ;  ; virtual void SetUniqueID (UInt_t uid);  Set the unique object id. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Priva",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:9744,Integrability,message,message,9744,"_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; DataInputHandler & DataInput ();  ; MsgLogger & Log () const;  . Private Attributes; DataInputHandler * fDataInput;  source of input data ;  ; TMVA::DataSetFactory * fDatasetFactory;  ; TList fDataSetInfoCollection;  all registered dataset definitions ;  ; MsgLogger * fLogger;  ! message logger ;  . Friends; class Envelop;  ; class Factory;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int le",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:14347,Integrability,message,message,14347,Info(). TMVA::DataSetInfo * TMVA::DataSetManager::GetDataSetInfo ; (; const TString & ; dsiName). returns datasetinfo object for given name ; Definition at line 97 of file DataSetManager.cxx. ◆ IsA(). virtual TClass * TMVA::DataSetManager::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TObject.; Definition at line 84 of file DataSetManager.h. ◆ Log(). MsgLogger & TMVA::DataSetManager::Log ; (; ); const. inlineprivate . Definition at line 81 of file DataSetManager.h. ◆ Streamer(). virtual void TMVA::DataSetManager::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSetManager::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 84 of file DataSetManager.h. Friends And Related Symbol Documentation. ◆ Envelop. friend class Envelop. friend . Definition at line 53 of file DataSetManager.h. ◆ Factory. friend class Factory. friend . Definition at line 52 of file DataSetManager.h. Member Data Documentation. ◆ fDataInput. DataInputHandler* TMVA::DataSetManager::fDataInput. private . source of input data ; Definition at line 78 of file DataSetManager.h. ◆ fDatasetFactory. TMVA::DataSetFactory* TMVA::DataSetManager::fDatasetFactory. private . Definition at line 73 of file DataSetManager.h. ◆ fDataSetInfoCollection. TList TMVA::DataSetManager::fDataSetInfoCollection. private . all registered dataset definitions ; Definition at line 79 of file DataSetManager.h. ◆ fLogger. MsgLogger* TMVA::DataSetManager::fLogger. private . ! message logger ; Definition at line 80 of file DataSetManager.h. Libraries for TMVA::DataSetManager:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DataSetManager.h; tmva/tmva/src/DataSetManager.cxx. TMVADataSetManager. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:1205,Modifiability,inherit,inherited,1205,"blic Member Functions |; Private Member Functions |; Private Attributes |; Friends |; List of all members ; TMVA::DataSetManager Class ReferenceTMVA. ; Class that contains all the data information. ; Definition at line 51 of file DataSetManager.h. Public Member Functions;  DataSetManager ();  constructor ;  ;  DataSetManager (DataInputHandler &dataInput);  constructor ;  ;  ~DataSetManager ();  destructor fDataSetInfoCollection.SetOwner(); // DSMTEST --> created a segfault because the DataSetInfo-objects got deleted twice ;  ; DataSetInfo & AddDataSetInfo (DataSetInfo &dsi);  stores a copy of the dataset info object ;  ; DataSet * CreateDataSet (const TString &dsiName);  Creates the singleton dataset. ;  ; DataSetInfo * GetDataSetInfo (const TString &dsiName);  returns datasetinfo object for given name ;  ; virtual TClass * IsA () const;  ; virtual void Streamer (TBuffer &);  Stream an object of class TObject. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TObject;  TObject ();  TObject constructor. ;  ;  TObject (const TObject &object);  TObject copy ctor. ;  ; virtual ~TObject ();  TObject destructor. ;  ; void AbstractMethod (const char *method) const;  Use this method to implement an ""abstract"" method that you don't want to leave purely abstract. ;  ; virtual void AppendPad (Option_t *option="""");  Append graphics object to current pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) c",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:2602,Modifiability,inherit,inheritance,2602,"pad. ;  ; virtual void Browse (TBrowser *b);  Browse object. May be overridden for another default action. ;  ; ULong_t CheckedHash ();  Check and record whether this class has a consistent Hash/RecursiveRemove setup (*) and then return the regular Hash value for this object. ;  ; virtual const char * ClassName () const;  Returns name of class to which the object belongs. ;  ; virtual void Clear (Option_t *="""");  ; virtual TObject * Clone (const char *newname="""") const;  Make a clone of an object using the Streamer facility. ;  ; virtual Int_t Compare (const TObject *obj) const;  Compare abstract method. ;  ; virtual void Copy (TObject &object) const;  Copy this to obj. ;  ; virtual void Delete (Option_t *option="""");  Delete this object. ;  ; virtual Int_t DistancetoPrimitive (Int_t px, Int_t py);  Computes distance from point (px,py) to the object. ;  ; virtual void Draw (Option_t *option="""");  Default Draw method for all objects. ;  ; virtual void DrawClass () const;  Draw class inheritance tree of the class to which this object belongs. ;  ; virtual TObject * DrawClone (Option_t *option="""") const;  Draw a clone of this object in the current selected pad with: gROOT->SetSelectedPad(c1). ;  ; virtual void Dump () const;  Dump contents of object on stdout. ;  ; virtual void Error (const char *method, const char *msgfmt,...) const;  Issue error message. ;  ; virtual void Execute (const char *method, const char *params, Int_t *error=nullptr);  Execute method on this object with the given parameter string, e.g. ;  ; virtual void Execute (TMethod *method, TObjArray *params, Int_t *error=nullptr);  Execute method on this object with parameters stored in the TObjArray. ;  ; virtual void ExecuteEvent (Int_t event, Int_t px, Int_t py);  Execute action corresponding to an event at (px,py). ;  ; virtual void Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in de",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:4813,Modifiability,inherit,inherits,4813,"ed by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:4930,Modifiability,inherit,inherits,4930,"ame of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t IsOnHeap () const;  ; virtual Bool_t IsSortable () const;  ; R__ALWAYS_INLINE Bool_t IsZombie () const;  ; virtual void ls (Option_t *option="""") const;  The ls function lists the contents of a class on stdout. ;  ; void MayNotUse (const char *method) const;  Use this method to signal that a method (defined in a base class) may not be called in a derived class (in principle against good design since a child class should not provide less functionality than its p",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:8951,Modifiability,inherit,inherited,8951,"reamerNVirtual_b);  ; virtual void SysError (const char *method, const char *msgfmt,...) const;  Issue system error message. ;  ; R__ALWAYS_INLINE Bool_t TestBit (UInt_t f) const;  ; Int_t TestBits (UInt_t f) const;  ; virtual void UseCurrentStyle ();  Set current style settings in this object This function is called when either TCanvas::UseCurrentStyle or TROOT::ForceStyle have been invoked. ;  ; virtual void Warning (const char *method, const char *msgfmt,...) const;  Issue warning message. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0);  Write this object to the current directory. ;  ; virtual Int_t Write (const char *name=nullptr, Int_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; DataInputHandler & DataInput ();  ; MsgLogger & Log () const;  . Private Attributes; DataInputHandler * fDataInput;  source of input data ;  ; TMVA::DataSetFactory * fDatasetFactory;  ; TList fDataSetInfoCollection;  all registered dataset definitions ;  ; MsgLogger * fLogger;  ! message logger ;  . Friends; class Envelop;  ; class Factory;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x0400000",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:9853,Modifiability,inherit,inherited,9853,"  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; DataInputHandler & DataInput ();  ; MsgLogger & Log () const;  . Private Attributes; DataInputHandler * fDataInput;  source of input data ;  ; TMVA::DataSetFactory * fDatasetFactory;  ; TList fDataSetInfoCollection;  all registered dataset definitions ;  ; MsgLogger * fLogger;  ! message logger ;  . Friends; class Envelop;  ; class Factory;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataSetManager.h>. Inheritance diagram for TMVA::DataSetManager:. This browser is not able to show SVG: try Firefox, Chrome, ",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:10494,Modifiability,inherit,inherited,10494,"VA::DataSetFactory * fDatasetFactory;  ; TList fDataSetInfoCollection;  all registered dataset definitions ;  ; MsgLogger * fLogger;  ! message logger ;  . Friends; class Envelop;  ; class Factory;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataSetManager.h>. Inheritance diagram for TMVA::DataSetManager:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DataSetManager() [1/2]. TMVA::DataSetManager::DataSetManager ; (; ). constructor ; Definition at line 60 of file DataSetManager.cxx. ◆ DataSetManager() [2/2]. TMVA::DataSetManager::DataSetManager ; (; DataInputHandler & ; dataInput). constructor ; Definition at line 49 of file DataSetManager.cxx. ◆ ~DataSetManager(). TMVA::DataSetManager::~DataSetManager ; (; ). destructor fDataSetInfoCollection.SetOwner(); // DSMTEST --> created a segfault because the DataSetInfo-objects got deleted twice ; Definition at line 73 of file DataSetManager.cxx. Member Function Documentation. ◆ AddDataSetInfo(). TMVA::DataSetInf",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:10593,Modifiability,inherit,inherited,10593,"VA::DataSetFactory * fDatasetFactory;  ; TList fDataSetInfoCollection;  all registered dataset definitions ;  ; MsgLogger * fLogger;  ! message logger ;  . Friends; class Envelop;  ; class Factory;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int level, const char *location, const char *fmt, va_list va) const;  Interface to ErrorHandler (protected). ;  ; void MakeZombie ();  . #include <TMVA/DataSetManager.h>. Inheritance diagram for TMVA::DataSetManager:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DataSetManager() [1/2]. TMVA::DataSetManager::DataSetManager ; (; ). constructor ; Definition at line 60 of file DataSetManager.cxx. ◆ DataSetManager() [2/2]. TMVA::DataSetManager::DataSetManager ; (; DataInputHandler & ; dataInput). constructor ; Definition at line 49 of file DataSetManager.cxx. ◆ ~DataSetManager(). TMVA::DataSetManager::~DataSetManager ; (; ). destructor fDataSetInfoCollection.SetOwner(); // DSMTEST --> created a segfault because the DataSetInfo-objects got deleted twice ; Definition at line 73 of file DataSetManager.cxx. Member Function Documentation. ◆ AddDataSetInfo(). TMVA::DataSetInf",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:4444,Security,hash,hash,4444," Fatal (const char *method, const char *msgfmt,...) const;  Issue fatal error message. ;  ; virtual TObject * FindObject (const char *name) const;  Must be redefined in derived classes. ;  ; virtual TObject * FindObject (const TObject *obj) const;  Must be redefined in derived classes. ;  ; virtual Option_t * GetDrawOption () const;  Get option used by the graphics system to draw this object. ;  ; virtual const char * GetIconName () const;  Returns mime type name of object. ;  ; virtual const char * GetName () const;  Returns name of object. ;  ; virtual char * GetObjectInfo (Int_t px, Int_t py) const;  Returns string containing info about the object at position (px,py). ;  ; virtual Option_t * GetOption () const;  ; virtual const char * GetTitle () const;  Returns title of object. ;  ; virtual UInt_t GetUniqueID () const;  Return the unique object id. ;  ; virtual Bool_t HandleTimer (TTimer *timer);  Execute action in response of a timer timing out. ;  ; virtual ULong_t Hash () const;  Return hash value for this object. ;  ; Bool_t HasInconsistentHash () const;  Return true is the type of this object is known to have an inconsistent setup for Hash and RecursiveRemove (i.e. ;  ; virtual void Info (const char *method, const char *msgfmt,...) const;  Issue info message. ;  ; virtual Bool_t InheritsFrom (const char *classname) const;  Returns kTRUE if object inherits from class ""classname"". ;  ; virtual Bool_t InheritsFrom (const TClass *cl) const;  Returns kTRUE if object inherits from TClass cl. ;  ; virtual void Inspect () const;  Dump contents of this object in a graphics canvas. ;  ; void InvertBit (UInt_t f);  ; Bool_t IsDestructed () const;  IsDestructed. ;  ; virtual Bool_t IsEqual (const TObject *obj) const;  Default equal comparison (objects are equal if they have the same address in memory). ;  ; virtual Bool_t IsFolder () const;  Returns kTRUE in case object contains browsable objects (like containers or lists of other objects). ;  ; R__ALWAYS_INLINE Bool_t",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:9752,Testability,log,logger,9752,"_t option=0, Int_t bufsize=0) const;  Write this object to the current directory. ;  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TObject; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static Longptr_t GetDtorOnly ();  Return destructor only flag. ;  ; static Bool_t GetObjectStat ();  Get status of object stat flag. ;  ; static void SetDtorOnly (void *obj);  Set destructor only flag. ;  ; static void SetObjectStat (Bool_t stat);  Turn on/off tracking of objects in the TObjectTable. ;  . Private Member Functions; DataInputHandler & DataInput ();  ; MsgLogger & Log () const;  . Private Attributes; DataInputHandler * fDataInput;  source of input data ;  ; TMVA::DataSetFactory * fDatasetFactory;  ; TList fDataSetInfoCollection;  all registered dataset definitions ;  ; MsgLogger * fLogger;  ! message logger ;  . Friends; class Envelop;  ; class Factory;  . Additional Inherited Members;  Public Types inherited from TObject; enum  { ;   kIsOnHeap = 0x01000000; , kNotDeleted = 0x02000000; , kZombie = 0x04000000; , kInconsistent = 0x08000000; , ;   kBitMask = 0x00ffffff. };  ; enum  { kSingleKey = (1ULL << ( 0 )); , kOverwrite = (1ULL << ( 1 )); , kWriteDelete = (1ULL << ( 2 )); };  ; enum  EDeprecatedStatusBits { kObjInCanvas = (1ULL << ( 3 )); };  ; enum  EStatusBits { ;   kCanDelete = (1ULL << ( 0 )); , kMustCleanup = (1ULL << ( 3 )); , kIsReferenced = (1ULL << ( 4 )); , kHasUUID = (1ULL << ( 5 )); , ;   kCannotPick = (1ULL << ( 6 )); , kNoContextMenu = (1ULL << ( 8 )); , kInvalidObject = (1ULL << ( 13 )). };  ;  Protected Types inherited from TObject; enum  { kOnlyPrepStep = (1ULL << ( 3 )); };  ;  Protected Member Functions inherited from TObject; virtual void DoError (int le",MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DataSetManager.html:14355,Testability,log,logger,14355,Info(). TMVA::DataSetInfo * TMVA::DataSetManager::GetDataSetInfo ; (; const TString & ; dsiName). returns datasetinfo object for given name ; Definition at line 97 of file DataSetManager.cxx. ◆ IsA(). virtual TClass * TMVA::DataSetManager::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TObject.; Definition at line 84 of file DataSetManager.h. ◆ Log(). MsgLogger & TMVA::DataSetManager::Log ; (; ); const. inlineprivate . Definition at line 81 of file DataSetManager.h. ◆ Streamer(). virtual void TMVA::DataSetManager::Streamer ; (; TBuffer & ; R__b). virtual . Stream an object of class TObject. ; Reimplemented from TObject. ◆ StreamerNVirtual(). void TMVA::DataSetManager::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 84 of file DataSetManager.h. Friends And Related Symbol Documentation. ◆ Envelop. friend class Envelop. friend . Definition at line 53 of file DataSetManager.h. ◆ Factory. friend class Factory. friend . Definition at line 52 of file DataSetManager.h. Member Data Documentation. ◆ fDataInput. DataInputHandler* TMVA::DataSetManager::fDataInput. private . source of input data ; Definition at line 78 of file DataSetManager.h. ◆ fDatasetFactory. TMVA::DataSetFactory* TMVA::DataSetManager::fDatasetFactory. private . Definition at line 73 of file DataSetManager.h. ◆ fDataSetInfoCollection. TList TMVA::DataSetManager::fDataSetInfoCollection. private . all registered dataset definitions ; Definition at line 79 of file DataSetManager.h. ◆ fLogger. MsgLogger* TMVA::DataSetManager::fLogger. private . ! message logger ; Definition at line 80 of file DataSetManager.h. Libraries for TMVA::DataSetManager:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DataSetManager.h; tmva/tmva/src/DataSetManager.cxx. TMVADataSetManager. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DataSetManager.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DataSetManager.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:3724,Availability,down,down,3724,"x as separation criterion, no restrictions on minium number of events in a leave note or the separation gain in the node splitting ;  ; virtual ~DecisionTree (void);  destructor ;  ; void ApplyValidationSample (const EventConstList *validationSample) const;  run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ;  ; UInt_t BuildTree (const EventConstList &eventSample, DecisionTreeNode *node=nullptr);  building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ;  ; Double_t CheckEvent (const TMVA::Event *, Bool_t UseYesNoLeaf=kFALSE) const;  the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ;  ; void CheckEventWithPrunedTree (const TMVA::Event *) const;  pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ;  ; virtual const char * ClassName () const;  ; UInt_t CleanTree (DecisionTreeNode *node=nullptr);  remove those last splits that result in two leaf nodes that are both of the type (i.e. ;  ; void ClearTree ();  clear the tree nodes (their S/N, Nevents etc), just keep the structure of the tree ;  ; UInt_t CountLeafNodes (TMVA::Node *n=nullptr);  return the number of terminal nodes in the sub-tree below Node n ;  ; virtual DecisionTreeNode * CreateNode (UInt_t) const;  ; virtual BinaryTree * CreateTree () const;  ; void DescendTree (Node *n=nullptr);  descend a tree to find all its leaf nodes ;  ; Bool_t DoRegression () const;  ; void FillEvent (const TMVA::Event &event, TMVA::DecisionTreeNode *node);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; void FillTree (const EventList &eventSampl",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:16408,Availability,down,down,16408,"f file DecisionTree.cxx. ◆ BuildTree(). UInt_t TMVA::DecisionTree::BuildTree ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node = nullptr . ). building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ; Definition at line 377 of file DecisionTree.cxx. ◆ CheckEvent(). Double_t TMVA::DecisionTree::CheckEvent ; (; const TMVA::Event * ; e, . Bool_t ; UseYesNoLeaf = kFALSE . ); const. the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ; I.e. the result of the classification if the event for this decision tree. ; Definition at line 2690 of file DecisionTree.cxx. ◆ CheckEventWithPrunedTree(). void TMVA::DecisionTree::CheckEventWithPrunedTree ; (; const TMVA::Event * ; e); const. pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ; Definition at line 1085 of file DecisionTree.cxx. ◆ Class(). static TClass * TMVA::DecisionTree::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DecisionTree::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DecisionTree::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 243 of file DecisionTree.h. ◆ ClassName(). virtual const char * TMVA::DecisionTree::ClassName ; (; ); const. inlinevirtual . Implements TMVA::BinaryTree.; Definition at line 98 of file DecisionTree.h. ◆ CleanTree(). UInt_t TMVA::DecisionTree::CleanTree ; (; DecisionTreeNode * ; node = nullptr). remove those last splits that result in two leaf nodes that are both of the type (i.e. ; both signal or both background) this of course is only a reasonable thing to do when you use ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:10835,Energy Efficiency,monitor,monitor,10835,"n_Code=262657);  re-create a new tree (decision tree or search tree) from XML ;  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::BinaryTree; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Private Member Functions; Double_t SamplePurity (EventList eventSample);  calculates the purity S/(S+B) of a given event sample ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  kClassification(=0=false) or kRegression(=1=true) ;  ; DataSetInfo * fDataSetInfo;  ; UInt_t fMaxDepth;  max depth ;  ; Double_t fMinLinCorrForFisher;  the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ;  ; Double_t fMinNodeSize;  min fraction of training events in node ;  ; Double_t fMinSepGain;  min number of separation gain to perform node splitting ;  ; Double_t fMinSize;  min number of events in node ;  ; TRandom3 * fMyTrandom;  random number generator for randomised trees ;  ; Int_t fNCuts;  number of grid point in variable cut scans ;  ; Int_t fNNodesBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:29240,Energy Efficiency,monitor,monitor,29240,"sher criterium in node splitting ; Definition at line 208 of file DecisionTree.h. ◆ fMinNodeSize. Double_t TMVA::DecisionTree::fMinNodeSize. private . min fraction of training events in node ; Definition at line 215 of file DecisionTree.h. ◆ fMinSepGain. Double_t TMVA::DecisionTree::fMinSepGain. private . min number of separation gain to perform node splitting ; Definition at line 216 of file DecisionTree.h. ◆ fMinSize. Double_t TMVA::DecisionTree::fMinSize. private . min number of events in node ; Definition at line 214 of file DecisionTree.h. ◆ fMyTrandom. TRandom3* TMVA::DecisionTree::fMyTrandom. private . random number generator for randomised trees ; Definition at line 230 of file DecisionTree.h. ◆ fNCuts. Int_t TMVA::DecisionTree::fNCuts. private . number of grid point in variable cut scans ; Definition at line 206 of file DecisionTree.h. ◆ fNNodesBeforePruning. Int_t TMVA::DecisionTree::fNNodesBeforePruning. private . remember this one (in case of pruning, it allows to monitor the before/after ; Definition at line 222 of file DecisionTree.h. ◆ fNodePurityLimit. Double_t TMVA::DecisionTree::fNodePurityLimit. private . purity limit to decide whether a node is signal ; Definition at line 224 of file DecisionTree.h. ◆ fNvars. UInt_t TMVA::DecisionTree::fNvars. private . number of variables used to separate S and B ; Definition at line 205 of file DecisionTree.h. ◆ fPruneMethod. EPruneMethod TMVA::DecisionTree::fPruneMethod. private . method used for pruning ; Definition at line 221 of file DecisionTree.h. ◆ fPruneStrength. Double_t TMVA::DecisionTree::fPruneStrength. private . a parameter to set the ""amount"" of pruning..needs to be adjusted ; Definition at line 219 of file DecisionTree.h. ◆ fRandomisedTree. Bool_t TMVA::DecisionTree::fRandomisedTree. private . choose at each node splitting a random set of variables ; Definition at line 226 of file DecisionTree.h. ◆ fRegType. RegressionVariance* TMVA::DecisionTree::fRegType. private . the separation criteria used ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:578,Modifiability,variab,variable,578,". ROOT: TMVA::DecisionTree Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DecisionTree Class ReferenceTMVA. ; Implementation of a Decision Tree. ; In a decision tree successive decision nodes are used to categorize the events out of the sample as either signal or background. Each node uses only a single discriminating variable to decide if the event is signal-like (""goes right"") or background-like (""goes left""). This forms a tree like structure with ""baskets"" at the end (leave nodes), and an event is classified as either signal or background according to whether the basket where it ends up has been classified signal or background during the training. Training of a decision tree is the process to define the ""cut criteria"" for each node. The training starts with the root node. Here one takes the full training event sample and selects the variable and corresponding cut value that gives the best separation between signal and background at this stage. Using this cut criterion, the sample is then divided into two subsamples, a signal-like (right) and a background-like (left) sample. Two new nodes are then created for each of the two sub-samples and they are constructed using the same mechanism as described for the root node. The devision is stopped once a certain node has reached either a minimum number of events, or a minimum or maximum signal purity. These leave nodes are then called ""signal"" or ""background"" if they contain more signal respective background events from the training sample. ; Definition at line 65 of file DecisionTree.h. Public Types; enum  EPruneMethod { kExpectedErrorPruning =0; , kCostComplexityPruning; , kNoPruning; };  ; typedef std::vector< const TMVA::Event * > EventConstList;  ; typedef std::ve",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:1106,Modifiability,variab,variable,1106,"of all members |; Public Types |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DecisionTree Class ReferenceTMVA. ; Implementation of a Decision Tree. ; In a decision tree successive decision nodes are used to categorize the events out of the sample as either signal or background. Each node uses only a single discriminating variable to decide if the event is signal-like (""goes right"") or background-like (""goes left""). This forms a tree like structure with ""baskets"" at the end (leave nodes), and an event is classified as either signal or background according to whether the basket where it ends up has been classified signal or background during the training. Training of a decision tree is the process to define the ""cut criteria"" for each node. The training starts with the root node. Here one takes the full training event sample and selects the variable and corresponding cut value that gives the best separation between signal and background at this stage. Using this cut criterion, the sample is then divided into two subsamples, a signal-like (right) and a background-like (left) sample. Two new nodes are then created for each of the two sub-samples and they are constructed using the same mechanism as described for the root node. The devision is stopped once a certain node has reached either a minimum number of events, or a minimum or maximum signal purity. These leave nodes are then called ""signal"" or ""background"" if they contain more signal respective background events from the training sample. ; Definition at line 65 of file DecisionTree.h. Public Types; enum  EPruneMethod { kExpectedErrorPruning =0; , kCostComplexityPruning; , kNoPruning; };  ; typedef std::vector< const TMVA::Event * > EventConstList;  ; typedef std::vector< TMVA::Event * > EventList;  . Public Member Functions;  DecisionTree (const DecisionTree &d);  copy constructor that creates",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:3042,Modifiability,variab,variables,3042,"rrorPruning =0; , kCostComplexityPruning; , kNoPruning; };  ; typedef std::vector< const TMVA::Event * > EventConstList;  ; typedef std::vector< TMVA::Event * > EventList;  . Public Member Functions;  DecisionTree (const DecisionTree &d);  copy constructor that creates a true copy, i.e. ;  ;  DecisionTree (SeparationBase *sepType, Float_t minSize, Int_t nCuts, DataSetInfo *=nullptr, UInt_t cls=0, Bool_t randomisedTree=kFALSE, Int_t useNvars=0, Bool_t usePoissonNvars=kFALSE, UInt_t nMaxDepth=9999999, Int_t iSeed=fgRandomSeed, Float_t purityLimit=0.5, Int_t treeID=0);  constructor specifying the separation type, the min number of events in a no that is still subjected to further splitting, the number of bins in the grid used in applying the cut for the node splitting. ;  ;  DecisionTree (void);  default constructor using the GiniIndex as separation criterion, no restrictions on minium number of events in a leave note or the separation gain in the node splitting ;  ; virtual ~DecisionTree (void);  destructor ;  ; void ApplyValidationSample (const EventConstList *validationSample) const;  run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ;  ; UInt_t BuildTree (const EventConstList &eventSample, DecisionTreeNode *node=nullptr);  building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ;  ; Double_t CheckEvent (const TMVA::Event *, Bool_t UseYesNoLeaf=kFALSE) const;  the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ;  ; void CheckEventWithPrunedTree (const TMVA::Event *) const;  pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ;  ; virtual const char * ClassName ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:5248,Modifiability,variab,variables,5248,"al nodes in the sub-tree below Node n ;  ; virtual DecisionTreeNode * CreateNode (UInt_t) const;  ; virtual BinaryTree * CreateTree () const;  ; void DescendTree (Node *n=nullptr);  descend a tree to find all its leaf nodes ;  ; Bool_t DoRegression () const;  ; void FillEvent (const TMVA::Event &event, TMVA::DecisionTreeNode *node);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; void FillTree (const EventList &eventSample);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; Types::EAnalysisType GetAnalysisType (void);  ; TMVA::DecisionTreeNode * GetEventNode (const TMVA::Event &e) const;  get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ;  ; std::vector< Double_t > GetFisherCoefficients (const EventConstList &eventSample, UInt_t nFisherVars, UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:5526,Modifiability,variab,variableMap,5526," up ;  ; void FillTree (const EventList &eventSample);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; Types::EAnalysisType GetAnalysisType (void);  ; TMVA::DecisionTreeNode * GetEventNode (const TMVA::Event &e) const;  get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ;  ; std::vector< Double_t > GetFisherCoefficients (const EventConstList &eventSample, UInt_t nFisherVars, UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisTy",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:5844,Modifiability,variab,variable,5844," up ;  ; void FillTree (const EventList &eventSample);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; Types::EAnalysisType GetAnalysisType (void);  ; TMVA::DecisionTreeNode * GetEventNode (const TMVA::Event &e) const;  get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ;  ; std::vector< Double_t > GetFisherCoefficients (const EventConstList &eventSample, UInt_t nFisherVars, UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisTy",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:5883,Modifiability,variab,variables,5883," up ;  ; void FillTree (const EventList &eventSample);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; Types::EAnalysisType GetAnalysisType (void);  ; TMVA::DecisionTreeNode * GetEventNode (const TMVA::Event &e) const;  get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ;  ; std::vector< Double_t > GetFisherCoefficients (const EventConstList &eventSample, UInt_t nFisherVars, UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisTy",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:6014,Modifiability,variab,variable,6014,", UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bo",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:6606,Modifiability,variab,variable,6606,"ariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence t",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:7452,Modifiability,variab,variable,7452,"List *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ;  ; Double_t TrainNode (const EventConstList &eventSample, DecisionTreeNode *node);  ; Double_t TrainNodeFast (const EventConstList &eventSample, DecisionTreeNode *node);  Decide how to split a node using one of the variables that gives the best separation of signal/background. ;  ; Double_t TrainNodeFull (const EventConstList &eventSample, DecisionTreeNode *node);  train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ;  ;  Public Member Functions inherited from TMVA::BinaryTree;  BinaryTree (void);  constructor for a yet ""empty"" tree. Needs to be filled afterwards ;  ; virtual ~BinaryTree ();  destructor (deletes the nodes and ""events"" if owned by the tree ;  ; virtual void * AddXMLTo (void ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:7856,Modifiability,variab,variables,7856,"List *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ;  ; Double_t TrainNode (const EventConstList &eventSample, DecisionTreeNode *node);  ; Double_t TrainNodeFast (const EventConstList &eventSample, DecisionTreeNode *node);  Decide how to split a node using one of the variables that gives the best separation of signal/background. ;  ; Double_t TrainNodeFull (const EventConstList &eventSample, DecisionTreeNode *node);  train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ;  ;  Public Member Functions inherited from TMVA::BinaryTree;  BinaryTree (void);  constructor for a yet ""empty"" tree. Needs to be filled afterwards ;  ; virtual ~BinaryTree ();  destructor (deletes the nodes and ""events"" if owned by the tree ;  ; virtual void * AddXMLTo (void ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:8069,Modifiability,variab,variable,8069,"UseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ;  ; Double_t TrainNode (const EventConstList &eventSample, DecisionTreeNode *node);  ; Double_t TrainNodeFast (const EventConstList &eventSample, DecisionTreeNode *node);  Decide how to split a node using one of the variables that gives the best separation of signal/background. ;  ; Double_t TrainNodeFull (const EventConstList &eventSample, DecisionTreeNode *node);  train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ;  ;  Public Member Functions inherited from TMVA::BinaryTree;  BinaryTree (void);  constructor for a yet ""empty"" tree. Needs to be filled afterwards ;  ; virtual ~BinaryTree ();  destructor (deletes the nodes and ""events"" if owned by the tree ;  ; virtual void * AddXMLTo (void *parent) const;  add attributes to XML ;  ; UInt_t CountNodes (Node *n=nullptr);  return the number of nodes in the tree. (make a new count --> takes time) ;  ; Node * GetLeftDaughter (Node *n);  get left daughter node current node ""n"" ;  ; UInt_t GetNNodes () const;  ; Node * GetRightDaughter (Node *n);  get right daughter node current node ""n"" ;  ; UInt_t GetTotalTreeDepth () const;  ; virtual void Print (std::ostream &os) const;  recursively print the tree ;  ; virtual void Read (std::istream &istr, UInt_t tmva_Version_Code=262657);  Read the binary tree from an input stream. ;  ; virtual void ReadXML (void *node, UInt_t tmva_Version_Code=262657);  read",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:8182,Modifiability,inherit,inherited,8182,"UseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ;  ; Double_t TrainNode (const EventConstList &eventSample, DecisionTreeNode *node);  ; Double_t TrainNodeFast (const EventConstList &eventSample, DecisionTreeNode *node);  Decide how to split a node using one of the variables that gives the best separation of signal/background. ;  ; Double_t TrainNodeFull (const EventConstList &eventSample, DecisionTreeNode *node);  train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ;  ;  Public Member Functions inherited from TMVA::BinaryTree;  BinaryTree (void);  constructor for a yet ""empty"" tree. Needs to be filled afterwards ;  ; virtual ~BinaryTree ();  destructor (deletes the nodes and ""events"" if owned by the tree ;  ; virtual void * AddXMLTo (void *parent) const;  add attributes to XML ;  ; UInt_t CountNodes (Node *n=nullptr);  return the number of nodes in the tree. (make a new count --> takes time) ;  ; Node * GetLeftDaughter (Node *n);  get left daughter node current node ""n"" ;  ; UInt_t GetNNodes () const;  ; Node * GetRightDaughter (Node *n);  get right daughter node current node ""n"" ;  ; UInt_t GetTotalTreeDepth () const;  ; virtual void Print (std::ostream &os) const;  recursively print the tree ;  ; virtual void Read (std::istream &istr, UInt_t tmva_Version_Code=262657);  Read the binary tree from an input stream. ;  ; virtual void ReadXML (void *node, UInt_t tmva_Version_Code=262657);  read",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:9767,Modifiability,inherit,inherited,9767,"t_t GetNNodes () const;  ; Node * GetRightDaughter (Node *n);  get right daughter node current node ""n"" ;  ; UInt_t GetTotalTreeDepth () const;  ; virtual void Print (std::ostream &os) const;  recursively print the tree ;  ; virtual void Read (std::istream &istr, UInt_t tmva_Version_Code=262657);  Read the binary tree from an input stream. ;  ; virtual void ReadXML (void *node, UInt_t tmva_Version_Code=262657);  read attributes from XML ;  ; void SetRoot (Node *r);  ; void SetTotalTreeDepth (Int_t depth);  ; void SetTotalTreeDepth (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static DecisionTree * CreateFromXML (void *node, UInt_t tmva_Version_Code=262657);  re-create a new tree (decision tree or search tree) from XML ;  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::BinaryTree; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Private Member Functions; Double_t SamplePurity (EventList eventSample);  calculates the purity S/(S+B) of a given event sample ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  kClassification(=0=false) or kRegression(=1=true) ;  ; DataSetInfo * fDataSetInfo;  ; UInt_t fMaxDepth;  max depth ;  ; Double_t fMinLinCorrForFisher;  the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ;  ; Double_t fMinNodeSize;  min fraction of training events in node ;  ; Double_t fMinSepGain;  min number of separation gain to perform node splitting ;  ; Double_t fMinSize;  min number of events in node ;  ; TRandom3 * fMyTrandom;  random number generator for randomi",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:10340,Modifiability,variab,variables,10340,"n_Code=262657);  re-create a new tree (decision tree or search tree) from XML ;  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::BinaryTree; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Private Member Functions; Double_t SamplePurity (EventList eventSample);  calculates the purity S/(S+B) of a given event sample ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  kClassification(=0=false) or kRegression(=1=true) ;  ; DataSetInfo * fDataSetInfo;  ; UInt_t fMaxDepth;  max depth ;  ; Double_t fMinLinCorrForFisher;  the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ;  ; Double_t fMinNodeSize;  min fraction of training events in node ;  ; Double_t fMinSepGain;  min number of separation gain to perform node splitting ;  ; Double_t fMinSize;  min number of events in node ;  ; TRandom3 * fMyTrandom;  random number generator for randomised trees ;  ; Int_t fNCuts;  number of grid point in variable cut scans ;  ; Int_t fNNodesBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:10730,Modifiability,variab,variable,10730,"n_Code=262657);  re-create a new tree (decision tree or search tree) from XML ;  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::BinaryTree; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Private Member Functions; Double_t SamplePurity (EventList eventSample);  calculates the purity S/(S+B) of a given event sample ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  kClassification(=0=false) or kRegression(=1=true) ;  ; DataSetInfo * fDataSetInfo;  ; UInt_t fMaxDepth;  max depth ;  ; Double_t fMinLinCorrForFisher;  the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ;  ; Double_t fMinNodeSize;  min fraction of training events in node ;  ; Double_t fMinSepGain;  min number of separation gain to perform node splitting ;  ; Double_t fMinSize;  min number of events in node ;  ; TRandom3 * fMyTrandom;  random number generator for randomised trees ;  ; Int_t fNCuts;  number of grid point in variable cut scans ;  ; Int_t fNNodesBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:10972,Modifiability,variab,variables,10972,"n_Code=262657);  re-create a new tree (decision tree or search tree) from XML ;  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::BinaryTree; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Private Member Functions; Double_t SamplePurity (EventList eventSample);  calculates the purity S/(S+B) of a given event sample ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  kClassification(=0=false) or kRegression(=1=true) ;  ; DataSetInfo * fDataSetInfo;  ; UInt_t fMaxDepth;  max depth ;  ; Double_t fMinLinCorrForFisher;  the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ;  ; Double_t fMinNodeSize;  min fraction of training events in node ;  ; Double_t fMinSepGain;  min number of separation gain to perform node splitting ;  ; Double_t fMinSize;  min number of events in node ;  ; TRandom3 * fMyTrandom;  random number generator for randomised trees ;  ; Int_t fNCuts;  number of grid point in variable cut scans ;  ; Int_t fNNodesBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:11236,Modifiability,variab,variables,11236,"etween two variables demanded for use in fisher criterium in node splitting ;  ; Double_t fMinNodeSize;  min fraction of training events in node ;  ; Double_t fMinSepGain;  min number of separation gain to perform node splitting ;  ; Double_t fMinSize;  min number of events in node ;  ; TRandom3 * fMyTrandom;  random number generator for randomised trees ;  ; Int_t fNCuts;  number of grid point in variable cut scans ;  ; Int_t fNNodesBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual variables already used in fisher criterium are not anymore analysed individually for node splitting ;  ; Bool_t fUseFisherCuts;  use multivariate splits using the Fisher criterium ;  ; Int_t fUseNvars;  the number of variables used in randomised trees; ;  ; Bool_t fUsePoissonNvars;  use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ;  ; Bool_t fUseSearchTree;  cut scan done with binary trees or simple event loop. ;  ; std::vector< Double_t > fVariableImportance;  the relative importance of the different variables ;  . Static Private Attributes; static const Int_t fgDebugLevel = 0;  debug level determining some printout/control plots etc. ;  ; static const Int_t fgRandomSeed = ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:11613,Modifiability,variab,variables,11613,"sBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual variables already used in fisher criterium are not anymore analysed individually for node splitting ;  ; Bool_t fUseFisherCuts;  use multivariate splits using the Fisher criterium ;  ; Int_t fUseNvars;  the number of variables used in randomised trees; ;  ; Bool_t fUsePoissonNvars;  use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ;  ; Bool_t fUseSearchTree;  cut scan done with binary trees or simple event loop. ;  ; std::vector< Double_t > fVariableImportance;  the relative importance of the different variables ;  . Static Private Attributes; static const Int_t fgDebugLevel = 0;  debug level determining some printout/control plots etc. ;  ; static const Int_t fgRandomSeed = 0;  . Additional Inherited Members;  Protected Member Functions inherited from TMVA::BinaryTree; void DeleteNode (Node *);  protected, recursive, function used by the class destructor and when Pruning ;  ; MsgLogger & Log () const;  ;  Protected Attributes inherited from TMVA::BinaryTree; UInt_t fDepth;  maximal depth in tree reached ;  ; UInt_t fNNodes;  total number of nodes in the tree (counted) ;  ; Node * fRoot;  the root node o",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:11830,Modifiability,variab,variables,11830,"sBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual variables already used in fisher criterium are not anymore analysed individually for node splitting ;  ; Bool_t fUseFisherCuts;  use multivariate splits using the Fisher criterium ;  ; Int_t fUseNvars;  the number of variables used in randomised trees; ;  ; Bool_t fUsePoissonNvars;  use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ;  ; Bool_t fUseSearchTree;  cut scan done with binary trees or simple event loop. ;  ; std::vector< Double_t > fVariableImportance;  the relative importance of the different variables ;  . Static Private Attributes; static const Int_t fgDebugLevel = 0;  debug level determining some printout/control plots etc. ;  ; static const Int_t fgRandomSeed = 0;  . Additional Inherited Members;  Protected Member Functions inherited from TMVA::BinaryTree; void DeleteNode (Node *);  protected, recursive, function used by the class destructor and when Pruning ;  ; MsgLogger & Log () const;  ;  Protected Attributes inherited from TMVA::BinaryTree; UInt_t fDepth;  maximal depth in tree reached ;  ; UInt_t fNNodes;  total number of nodes in the tree (counted) ;  ; Node * fRoot;  the root node o",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:12154,Modifiability,variab,variables,12154,"the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual variables already used in fisher criterium are not anymore analysed individually for node splitting ;  ; Bool_t fUseFisherCuts;  use multivariate splits using the Fisher criterium ;  ; Int_t fUseNvars;  the number of variables used in randomised trees; ;  ; Bool_t fUsePoissonNvars;  use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ;  ; Bool_t fUseSearchTree;  cut scan done with binary trees or simple event loop. ;  ; std::vector< Double_t > fVariableImportance;  the relative importance of the different variables ;  . Static Private Attributes; static const Int_t fgDebugLevel = 0;  debug level determining some printout/control plots etc. ;  ; static const Int_t fgRandomSeed = 0;  . Additional Inherited Members;  Protected Member Functions inherited from TMVA::BinaryTree; void DeleteNode (Node *);  protected, recursive, function used by the class destructor and when Pruning ;  ; MsgLogger & Log () const;  ;  Protected Attributes inherited from TMVA::BinaryTree; UInt_t fDepth;  maximal depth in tree reached ;  ; UInt_t fNNodes;  total number of nodes in the tree (counted) ;  ; Node * fRoot;  the root node of the tree the tree only has it's root node, the ""daughters"" are taken care of by the ""node"" properties of the ""root"" ;  . #include <TMVA/DecisionTree.h>. Inheritance diagram for TMVA::DecisionTree:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ EventConstList. type",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:12394,Modifiability,inherit,inherited,12394," variables already used in fisher criterium are not anymore analysed individually for node splitting ;  ; Bool_t fUseFisherCuts;  use multivariate splits using the Fisher criterium ;  ; Int_t fUseNvars;  the number of variables used in randomised trees; ;  ; Bool_t fUsePoissonNvars;  use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ;  ; Bool_t fUseSearchTree;  cut scan done with binary trees or simple event loop. ;  ; std::vector< Double_t > fVariableImportance;  the relative importance of the different variables ;  . Static Private Attributes; static const Int_t fgDebugLevel = 0;  debug level determining some printout/control plots etc. ;  ; static const Int_t fgRandomSeed = 0;  . Additional Inherited Members;  Protected Member Functions inherited from TMVA::BinaryTree; void DeleteNode (Node *);  protected, recursive, function used by the class destructor and when Pruning ;  ; MsgLogger & Log () const;  ;  Protected Attributes inherited from TMVA::BinaryTree; UInt_t fDepth;  maximal depth in tree reached ;  ; UInt_t fNNodes;  total number of nodes in the tree (counted) ;  ; Node * fRoot;  the root node of the tree the tree only has it's root node, the ""daughters"" are taken care of by the ""node"" properties of the ""root"" ;  . #include <TMVA/DecisionTree.h>. Inheritance diagram for TMVA::DecisionTree:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ EventConstList. typedef std::vector<const TMVA::Event*> TMVA::DecisionTree::EventConstList. Definition at line 74 of file DecisionTree.h. ◆ EventList. typedef std::vector<TMVA::Event*> TMVA::DecisionTree::EventList. Definition at line 73 of file DecisionTree.h. Member Enumeration Documentation. ◆ EPruneMethod. enum TMVA::DecisionTree::EPruneMethod. EnumeratorkExpectedErrorPruning ; kCostComplexityPruning ; kNoPruning . Definition at line 139 of file DecisionTree.h. Constructor & Destructor Documentation. ◆ Decis",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:12587,Modifiability,inherit,inherited,12587," variables already used in fisher criterium are not anymore analysed individually for node splitting ;  ; Bool_t fUseFisherCuts;  use multivariate splits using the Fisher criterium ;  ; Int_t fUseNvars;  the number of variables used in randomised trees; ;  ; Bool_t fUsePoissonNvars;  use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ;  ; Bool_t fUseSearchTree;  cut scan done with binary trees or simple event loop. ;  ; std::vector< Double_t > fVariableImportance;  the relative importance of the different variables ;  . Static Private Attributes; static const Int_t fgDebugLevel = 0;  debug level determining some printout/control plots etc. ;  ; static const Int_t fgRandomSeed = 0;  . Additional Inherited Members;  Protected Member Functions inherited from TMVA::BinaryTree; void DeleteNode (Node *);  protected, recursive, function used by the class destructor and when Pruning ;  ; MsgLogger & Log () const;  ;  Protected Attributes inherited from TMVA::BinaryTree; UInt_t fDepth;  maximal depth in tree reached ;  ; UInt_t fNNodes;  total number of nodes in the tree (counted) ;  ; Node * fRoot;  the root node of the tree the tree only has it's root node, the ""daughters"" are taken care of by the ""node"" properties of the ""root"" ;  . #include <TMVA/DecisionTree.h>. Inheritance diagram for TMVA::DecisionTree:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ EventConstList. typedef std::vector<const TMVA::Event*> TMVA::DecisionTree::EventConstList. Definition at line 74 of file DecisionTree.h. ◆ EventList. typedef std::vector<TMVA::Event*> TMVA::DecisionTree::EventList. Definition at line 73 of file DecisionTree.h. Member Enumeration Documentation. ◆ EPruneMethod. enum TMVA::DecisionTree::EPruneMethod. EnumeratorkExpectedErrorPruning ; kCostComplexityPruning ; kNoPruning . Definition at line 139 of file DecisionTree.h. Constructor & Destructor Documentation. ◆ Decis",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:15210,Modifiability,variab,variables,15210,"_t ; nMaxDepth = 9999999, . Int_t ; iSeed = fgRandomSeed, . Float_t ; purityLimit = 0.5, . Int_t ; treeID = 0 . ). constructor specifying the separation type, the min number of events in a no that is still subjected to further splitting, the number of bins in the grid used in applying the cut for the node splitting. ; Definition at line 150 of file DecisionTree.cxx. ◆ DecisionTree() [3/3]. TMVA::DecisionTree::DecisionTree ; (; const DecisionTree & ; d). copy constructor that creates a true copy, i.e. ; a completely independent tree the node copy will recursively copy all the nodes ; Definition at line 200 of file DecisionTree.cxx. ◆ ~DecisionTree(). TMVA::DecisionTree::~DecisionTree ; (; void ; ). virtual . destructor ; Definition at line 236 of file DecisionTree.cxx. Member Function Documentation. ◆ ApplyValidationSample(). void TMVA::DecisionTree::ApplyValidationSample ; (; const EventConstList * ; validationSample); const. run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ; how many of the Signal and Background events from the validation sample. This is then later used when asking for the ""tree quality"" .. ; Definition at line 1029 of file DecisionTree.cxx. ◆ BuildTree(). UInt_t TMVA::DecisionTree::BuildTree ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node = nullptr . ). building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ; Definition at line 377 of file DecisionTree.cxx. ◆ CheckEvent(). Double_t TMVA::DecisionTree::CheckEvent ; (; const TMVA::Event * ; e, . Bool_t ; UseYesNoLeaf = kFALSE . ); const. the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ; I.e. the result of the classification if the event for this decision tree. ; Definition at line 2690 of",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:20382,Modifiability,variab,variables,20382,"hey happen to end up ; Definition at line 891 of file DecisionTree.cxx. ◆ FillTree(). void TMVA::DecisionTree::FillTree ; (; const EventList & ; eventSample). fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ; Definition at line 880 of file DecisionTree.cxx. ◆ GetAnalysisType(). Types::EAnalysisType TMVA::DecisionTree::GetAnalysisType ; (; void ; ). inline . Definition at line 190 of file DecisionTree.h. ◆ GetEventNode(). TMVA::DecisionTreeNode * TMVA::DecisionTree::GetEventNode ; (; const TMVA::Event & ; e); const. get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ; Definition at line 2673 of file DecisionTree.cxx. ◆ GetFisherCoefficients(). std::vector< Double_t > TMVA::DecisionTree::GetFisherCoefficients ; (; const EventConstList & ; eventSample, . UInt_t ; nFisherVars, . UInt_t * ; mapVarInFisher . ). calculate the fisher coefficients for the event sample and the variables used ; Definition at line 2342 of file DecisionTree.cxx. ◆ GetNNodesBeforePruning(). Int_t TMVA::DecisionTree::GetNNodesBeforePruning ; (; ). inline . Definition at line 180 of file DecisionTree.h. ◆ GetNode(). TMVA::Node * TMVA::DecisionTree::GetNode ; (; ULong_t ; sequence, . UInt_t ; depth . ). retrieve node from the tree. ; Its position (up to a maximal tree depth of 64) is coded as a sequence of left-right moves starting from the root, coded as 0-1 bit patterns stored in the ""long-integer"" (i.e. 0:left ; 1:right ; Definition at line 1231 of file DecisionTree.cxx. ◆ GetNodePurityLimit(). Double_t TMVA::DecisionTree::GetNodePurityLimit ; (; ); const. inline . Definition at line 162 of file DecisionTree.h. ◆ GetPruneStrength(). Double_t TMVA::DecisionTree::GetPruneStrength ; (; ); const. inline . Definition at line 147 of file DecisionTree.h. ◆ GetRandomisedVariables(). void TMVA::DecisionTree::GetRandomisedVariables ; (; Bool_t * ; useVariable, . UInt_t * ; variableMap, .",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:21367,Modifiability,variab,variableMap,21367,"ample and the variables used ; Definition at line 2342 of file DecisionTree.cxx. ◆ GetNNodesBeforePruning(). Int_t TMVA::DecisionTree::GetNNodesBeforePruning ; (; ). inline . Definition at line 180 of file DecisionTree.h. ◆ GetNode(). TMVA::Node * TMVA::DecisionTree::GetNode ; (; ULong_t ; sequence, . UInt_t ; depth . ). retrieve node from the tree. ; Its position (up to a maximal tree depth of 64) is coded as a sequence of left-right moves starting from the root, coded as 0-1 bit patterns stored in the ""long-integer"" (i.e. 0:left ; 1:right ; Definition at line 1231 of file DecisionTree.cxx. ◆ GetNodePurityLimit(). Double_t TMVA::DecisionTree::GetNodePurityLimit ; (; ); const. inline . Definition at line 162 of file DecisionTree.h. ◆ GetPruneStrength(). Double_t TMVA::DecisionTree::GetPruneStrength ; (; ); const. inline . Definition at line 147 of file DecisionTree.h. ◆ GetRandomisedVariables(). void TMVA::DecisionTree::GetRandomisedVariables ; (; Bool_t * ; useVariable, . UInt_t * ; variableMap, . UInt_t & ; nVars . ). Definition at line 1247 of file DecisionTree.cxx. ◆ GetRoot(). virtual DecisionTreeNode * TMVA::DecisionTree::GetRoot ; (; ); const. inlinevirtual . Reimplemented from TMVA::BinaryTree.; Definition at line 94 of file DecisionTree.h. ◆ GetSumWeights(). Double_t TMVA::DecisionTree::GetSumWeights ; (; const EventConstList * ; validationSample); const. calculate the normalization factor for a pruning validation sample ; Definition at line 1118 of file DecisionTree.cxx. ◆ GetTreeID(). Int_t TMVA::DecisionTree::GetTreeID ; (; ). inline . Definition at line 186 of file DecisionTree.h. ◆ GetVariableImportance() [1/2]. vector< Double_t > TMVA::DecisionTree::GetVariableImportance ; (; ). Return the relative variable importance, normalized to all variables together having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file D",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:22111,Modifiability,variab,variable,22111,"TMVA::DecisionTree::GetPruneStrength ; (; ); const. inline . Definition at line 147 of file DecisionTree.h. ◆ GetRandomisedVariables(). void TMVA::DecisionTree::GetRandomisedVariables ; (; Bool_t * ; useVariable, . UInt_t * ; variableMap, . UInt_t & ; nVars . ). Definition at line 1247 of file DecisionTree.cxx. ◆ GetRoot(). virtual DecisionTreeNode * TMVA::DecisionTree::GetRoot ; (; ); const. inlinevirtual . Reimplemented from TMVA::BinaryTree.; Definition at line 94 of file DecisionTree.h. ◆ GetSumWeights(). Double_t TMVA::DecisionTree::GetSumWeights ; (; const EventConstList * ; validationSample); const. calculate the normalization factor for a pruning validation sample ; Definition at line 1118 of file DecisionTree.cxx. ◆ GetTreeID(). Int_t TMVA::DecisionTree::GetTreeID ; (; ). inline . Definition at line 186 of file DecisionTree.h. ◆ GetVariableImportance() [1/2]. vector< Double_t > TMVA::DecisionTree::GetVariableImportance ; (; ). Return the relative variable importance, normalized to all variables together having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file DecisionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its desce",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:22150,Modifiability,variab,variables,22150,"TMVA::DecisionTree::GetPruneStrength ; (; ); const. inline . Definition at line 147 of file DecisionTree.h. ◆ GetRandomisedVariables(). void TMVA::DecisionTree::GetRandomisedVariables ; (; Bool_t * ; useVariable, . UInt_t * ; variableMap, . UInt_t & ; nVars . ). Definition at line 1247 of file DecisionTree.cxx. ◆ GetRoot(). virtual DecisionTreeNode * TMVA::DecisionTree::GetRoot ; (; ); const. inlinevirtual . Reimplemented from TMVA::BinaryTree.; Definition at line 94 of file DecisionTree.h. ◆ GetSumWeights(). Double_t TMVA::DecisionTree::GetSumWeights ; (; const EventConstList * ; validationSample); const. calculate the normalization factor for a pruning validation sample ; Definition at line 1118 of file DecisionTree.cxx. ◆ GetTreeID(). Int_t TMVA::DecisionTree::GetTreeID ; (; ). inline . Definition at line 186 of file DecisionTree.h. ◆ GetVariableImportance() [1/2]. vector< Double_t > TMVA::DecisionTree::GetVariableImportance ; (; ). Return the relative variable importance, normalized to all variables together having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file DecisionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its desce",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:22263,Modifiability,variab,variable,22263,"::DecisionTree::GetRandomisedVariables ; (; Bool_t * ; useVariable, . UInt_t * ; variableMap, . UInt_t & ; nVars . ). Definition at line 1247 of file DecisionTree.cxx. ◆ GetRoot(). virtual DecisionTreeNode * TMVA::DecisionTree::GetRoot ; (; ); const. inlinevirtual . Reimplemented from TMVA::BinaryTree.; Definition at line 94 of file DecisionTree.h. ◆ GetSumWeights(). Double_t TMVA::DecisionTree::GetSumWeights ; (; const EventConstList * ; validationSample); const. calculate the normalization factor for a pruning validation sample ; Definition at line 1118 of file DecisionTree.cxx. ◆ GetTreeID(). Int_t TMVA::DecisionTree::GetTreeID ; (; ). inline . Definition at line 186 of file DecisionTree.h. ◆ GetVariableImportance() [1/2]. vector< Double_t > TMVA::DecisionTree::GetVariableImportance ; (; ). Return the relative variable importance, normalized to all variables together having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file DecisionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ; Definition at line 1217 of f",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:22525,Modifiability,variab,variable,22525,"inlinevirtual . Reimplemented from TMVA::BinaryTree.; Definition at line 94 of file DecisionTree.h. ◆ GetSumWeights(). Double_t TMVA::DecisionTree::GetSumWeights ; (; const EventConstList * ; validationSample); const. calculate the normalization factor for a pruning validation sample ; Definition at line 1118 of file DecisionTree.cxx. ◆ GetTreeID(). Int_t TMVA::DecisionTree::GetTreeID ; (; ). inline . Definition at line 186 of file DecisionTree.h. ◆ GetVariableImportance() [1/2]. vector< Double_t > TMVA::DecisionTree::GetVariableImportance ; (; ). Return the relative variable importance, normalized to all variables together having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file DecisionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ; Definition at line 1217 of file DecisionTree.cxx. ◆ PruneTree(). Double_t TMVA::DecisionTree::PruneTree ; (; const EventConstList * ; validationSample = nullptr). prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be a",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:23564,Modifiability,variab,variable,23564,"lative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ; Definition at line 1217 of file DecisionTree.cxx. ◆ PruneTree(). Double_t TMVA::DecisionTree::PruneTree ; (; const EventConstList * ; validationSample = nullptr). prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ; Definition at line 964 of file DecisionTree.cxx. ◆ SamplePurity(). Double_t TMVA::DecisionTree::SamplePurity ; (; EventList ; eventSample). private . calculates the purity S/(S+B) of a given event sample ; Definition at line 2722 of file DecisionTree.cxx. ◆ SetAnalysisType(). void TMVA::DecisionTree::SetAnalysisType ; (; Types::EAnalysisType ; t). inline . Definition at line 189 of file DecisionTree.h. ◆ SetMinLinCorrForFisher(). void TMVA::DecisionTree::SetMinLinCorrForFisher ; (; Double_t ; min). inline . Definition at line 192 of file DecisionTree.h. ◆ SetNodePurityLimit(). void TMVA::DecisionTree::SetNodePurityLimit ; (; Double_t ; p). inline . Definition at line 161 of file DecisionTree.h. ◆ SetNVars(). void TMVA::DecisionTree::SetNVars ; (; Int_t ; n). inline . Definition at line 194 of file DecisionTree.h. ◆ SetParentTreeInNodes(). void TMVA::DecisionTree::SetParentTreeInNodes ; (; Node * ; n =",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:25923,Modifiability,variab,variable,25923,"isionTree::SetTreeID ; (; Int_t ; treeID). inline . Definition at line 185 of file DecisionTree.h. ◆ SetUseExclusiveVars(). void TMVA::DecisionTree::SetUseExclusiveVars ; (; Bool_t ; t = kTRUE). inline . Definition at line 193 of file DecisionTree.h. ◆ SetUseFisherCuts(). void TMVA::DecisionTree::SetUseFisherCuts ; (; Bool_t ; t = kTRUE). inline . Definition at line 191 of file DecisionTree.h. ◆ Streamer(). virtual void TMVA::DecisionTree::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::BinaryTree. ◆ StreamerNVirtual(). void TMVA::DecisionTree::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 243 of file DecisionTree.h. ◆ TestPrunedTreeQuality(). Double_t TMVA::DecisionTree::TestPrunedTreeQuality ; (; const DecisionTreeNode * ; dt = nullptr, . Int_t ; mode = 0 . ); const. return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFu",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:26536,Modifiability,variab,variables,26536,"TMVA::DecisionTree::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 243 of file DecisionTree.h. ◆ TestPrunedTreeQuality(). Double_t TMVA::DecisionTree::TestPrunedTreeQuality ; (; const DecisionTreeNode * ; dt = nullptr, . Int_t ; mode = 0 . ); const. return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFull(). Double_t TMVA::DecisionTree::TrainNodeFull ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ; Definition at line 2536 of file DecisionTree.cxx. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DecisionTree::fAnalysisType. private . kClassification(=0=false) or kRegression(=1=true) ; Definition at line 239 of file DecisionTree.h. ◆ fDataSetIn",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:26631,Modifiability,variab,variable,26631,".h. ◆ TestPrunedTreeQuality(). Double_t TMVA::DecisionTree::TestPrunedTreeQuality ; (; const DecisionTreeNode * ; dt = nullptr, . Int_t ; mode = 0 . ); const. return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFull(). Double_t TMVA::DecisionTree::TrainNodeFull ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ; Definition at line 2536 of file DecisionTree.cxx. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DecisionTree::fAnalysisType. private . kClassification(=0=false) or kRegression(=1=true) ; Definition at line 239 of file DecisionTree.h. ◆ fDataSetInfo. DataSetInfo* TMVA::DecisionTree::fDataSetInfo. private . Definition at line 241 of file DecisionTree.h. ◆ fgDebugLevel. const Int_t",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:26793,Modifiability,variab,variables,26793,"he misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFull(). Double_t TMVA::DecisionTree::TrainNodeFull ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ; Definition at line 2536 of file DecisionTree.cxx. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DecisionTree::fAnalysisType. private . kClassification(=0=false) or kRegression(=1=true) ; Definition at line 239 of file DecisionTree.h. ◆ fDataSetInfo. DataSetInfo* TMVA::DecisionTree::fDataSetInfo. private . Definition at line 241 of file DecisionTree.h. ◆ fgDebugLevel. const Int_t TMVA::DecisionTree::fgDebugLevel = 0. staticprivate . debug level determining some printout/control plots etc. ; Definition at line 236 of file DecisionTree.h. ◆ fgRa",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:26880,Modifiability,variab,variables,26880,"he misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFull(). Double_t TMVA::DecisionTree::TrainNodeFull ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ; Definition at line 2536 of file DecisionTree.cxx. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DecisionTree::fAnalysisType. private . kClassification(=0=false) or kRegression(=1=true) ; Definition at line 239 of file DecisionTree.h. ◆ fDataSetInfo. DataSetInfo* TMVA::DecisionTree::fDataSetInfo. private . Definition at line 241 of file DecisionTree.h. ◆ fgDebugLevel. const Int_t TMVA::DecisionTree::fgDebugLevel = 0. staticprivate . debug level determining some printout/control plots etc. ; Definition at line 236 of file DecisionTree.h. ◆ fgRa",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:27189,Modifiability,variab,variable,27189,"nst EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFull(). Double_t TMVA::DecisionTree::TrainNodeFull ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ; Definition at line 2536 of file DecisionTree.cxx. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DecisionTree::fAnalysisType. private . kClassification(=0=false) or kRegression(=1=true) ; Definition at line 239 of file DecisionTree.h. ◆ fDataSetInfo. DataSetInfo* TMVA::DecisionTree::fDataSetInfo. private . Definition at line 241 of file DecisionTree.h. ◆ fgDebugLevel. const Int_t TMVA::DecisionTree::fgDebugLevel = 0. staticprivate . debug level determining some printout/control plots etc. ; Definition at line 236 of file DecisionTree.h. ◆ fgRandomSeed. const Int_t TMVA::DecisionTree::fgRandomSeed = 0. staticprivate . Definition at line 69 of file DecisionTree.h. ◆ fMaxDepth. UInt_t TMVA::DecisionTree::fMaxDepth. private . max depth ; Definition at line 234 of file DecisionTree.h. ◆ fMinLinCorrForFisher. Double_t TMVA::DecisionTree::fMinLinCorrForFisher. private . the minimum linear correlation between two variabl",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:28217,Modifiability,variab,variables,28217," the separation gain) ; Definition at line 2536 of file DecisionTree.cxx. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DecisionTree::fAnalysisType. private . kClassification(=0=false) or kRegression(=1=true) ; Definition at line 239 of file DecisionTree.h. ◆ fDataSetInfo. DataSetInfo* TMVA::DecisionTree::fDataSetInfo. private . Definition at line 241 of file DecisionTree.h. ◆ fgDebugLevel. const Int_t TMVA::DecisionTree::fgDebugLevel = 0. staticprivate . debug level determining some printout/control plots etc. ; Definition at line 236 of file DecisionTree.h. ◆ fgRandomSeed. const Int_t TMVA::DecisionTree::fgRandomSeed = 0. staticprivate . Definition at line 69 of file DecisionTree.h. ◆ fMaxDepth. UInt_t TMVA::DecisionTree::fMaxDepth. private . max depth ; Definition at line 234 of file DecisionTree.h. ◆ fMinLinCorrForFisher. Double_t TMVA::DecisionTree::fMinLinCorrForFisher. private . the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ; Definition at line 208 of file DecisionTree.h. ◆ fMinNodeSize. Double_t TMVA::DecisionTree::fMinNodeSize. private . min fraction of training events in node ; Definition at line 215 of file DecisionTree.h. ◆ fMinSepGain. Double_t TMVA::DecisionTree::fMinSepGain. private . min number of separation gain to perform node splitting ; Definition at line 216 of file DecisionTree.h. ◆ fMinSize. Double_t TMVA::DecisionTree::fMinSize. private . min number of events in node ; Definition at line 214 of file DecisionTree.h. ◆ fMyTrandom. TRandom3* TMVA::DecisionTree::fMyTrandom. private . random number generator for randomised trees ; Definition at line 230 of file DecisionTree.h. ◆ fNCuts. Int_t TMVA::DecisionTree::fNCuts. private . number of grid point in variable cut scans ; Definition at line 206 of file DecisionTree.h. ◆ fNNodesBeforePruning. Int_t TMVA::DecisionTree::fNNodesBeforePruning. private . remember this one (in case of pruning, it allows to monitor the",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:29038,Modifiability,variab,variable,29038,"ne 234 of file DecisionTree.h. ◆ fMinLinCorrForFisher. Double_t TMVA::DecisionTree::fMinLinCorrForFisher. private . the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ; Definition at line 208 of file DecisionTree.h. ◆ fMinNodeSize. Double_t TMVA::DecisionTree::fMinNodeSize. private . min fraction of training events in node ; Definition at line 215 of file DecisionTree.h. ◆ fMinSepGain. Double_t TMVA::DecisionTree::fMinSepGain. private . min number of separation gain to perform node splitting ; Definition at line 216 of file DecisionTree.h. ◆ fMinSize. Double_t TMVA::DecisionTree::fMinSize. private . min number of events in node ; Definition at line 214 of file DecisionTree.h. ◆ fMyTrandom. TRandom3* TMVA::DecisionTree::fMyTrandom. private . random number generator for randomised trees ; Definition at line 230 of file DecisionTree.h. ◆ fNCuts. Int_t TMVA::DecisionTree::fNCuts. private . number of grid point in variable cut scans ; Definition at line 206 of file DecisionTree.h. ◆ fNNodesBeforePruning. Int_t TMVA::DecisionTree::fNNodesBeforePruning. private . remember this one (in case of pruning, it allows to monitor the before/after ; Definition at line 222 of file DecisionTree.h. ◆ fNodePurityLimit. Double_t TMVA::DecisionTree::fNodePurityLimit. private . purity limit to decide whether a node is signal ; Definition at line 224 of file DecisionTree.h. ◆ fNvars. UInt_t TMVA::DecisionTree::fNvars. private . number of variables used to separate S and B ; Definition at line 205 of file DecisionTree.h. ◆ fPruneMethod. EPruneMethod TMVA::DecisionTree::fPruneMethod. private . method used for pruning ; Definition at line 221 of file DecisionTree.h. ◆ fPruneStrength. Double_t TMVA::DecisionTree::fPruneStrength. private . a parameter to set the ""amount"" of pruning..needs to be adjusted ; Definition at line 219 of file DecisionTree.h. ◆ fRandomisedTree. Bool_t TMVA::DecisionTree::fRandomisedTree. private . choose at each ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:29553,Modifiability,variab,variables,29553," perform node splitting ; Definition at line 216 of file DecisionTree.h. ◆ fMinSize. Double_t TMVA::DecisionTree::fMinSize. private . min number of events in node ; Definition at line 214 of file DecisionTree.h. ◆ fMyTrandom. TRandom3* TMVA::DecisionTree::fMyTrandom. private . random number generator for randomised trees ; Definition at line 230 of file DecisionTree.h. ◆ fNCuts. Int_t TMVA::DecisionTree::fNCuts. private . number of grid point in variable cut scans ; Definition at line 206 of file DecisionTree.h. ◆ fNNodesBeforePruning. Int_t TMVA::DecisionTree::fNNodesBeforePruning. private . remember this one (in case of pruning, it allows to monitor the before/after ; Definition at line 222 of file DecisionTree.h. ◆ fNodePurityLimit. Double_t TMVA::DecisionTree::fNodePurityLimit. private . purity limit to decide whether a node is signal ; Definition at line 224 of file DecisionTree.h. ◆ fNvars. UInt_t TMVA::DecisionTree::fNvars. private . number of variables used to separate S and B ; Definition at line 205 of file DecisionTree.h. ◆ fPruneMethod. EPruneMethod TMVA::DecisionTree::fPruneMethod. private . method used for pruning ; Definition at line 221 of file DecisionTree.h. ◆ fPruneStrength. Double_t TMVA::DecisionTree::fPruneStrength. private . a parameter to set the ""amount"" of pruning..needs to be adjusted ; Definition at line 219 of file DecisionTree.h. ◆ fRandomisedTree. Bool_t TMVA::DecisionTree::fRandomisedTree. private . choose at each node splitting a random set of variables ; Definition at line 226 of file DecisionTree.h. ◆ fRegType. RegressionVariance* TMVA::DecisionTree::fRegType. private . the separation criteria used in Regression ; Definition at line 212 of file DecisionTree.h. ◆ fSepType. SeparationBase* TMVA::DecisionTree::fSepType. private . the separation criteria ; Definition at line 211 of file DecisionTree.h. ◆ fSigClass. UInt_t TMVA::DecisionTree::fSigClass. private . class which is treated as signal when building the tree ; Definition at li",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:30089,Modifiability,variab,variables,30089,"isionTree.h. ◆ fNNodesBeforePruning. Int_t TMVA::DecisionTree::fNNodesBeforePruning. private . remember this one (in case of pruning, it allows to monitor the before/after ; Definition at line 222 of file DecisionTree.h. ◆ fNodePurityLimit. Double_t TMVA::DecisionTree::fNodePurityLimit. private . purity limit to decide whether a node is signal ; Definition at line 224 of file DecisionTree.h. ◆ fNvars. UInt_t TMVA::DecisionTree::fNvars. private . number of variables used to separate S and B ; Definition at line 205 of file DecisionTree.h. ◆ fPruneMethod. EPruneMethod TMVA::DecisionTree::fPruneMethod. private . method used for pruning ; Definition at line 221 of file DecisionTree.h. ◆ fPruneStrength. Double_t TMVA::DecisionTree::fPruneStrength. private . a parameter to set the ""amount"" of pruning..needs to be adjusted ; Definition at line 219 of file DecisionTree.h. ◆ fRandomisedTree. Bool_t TMVA::DecisionTree::fRandomisedTree. private . choose at each node splitting a random set of variables ; Definition at line 226 of file DecisionTree.h. ◆ fRegType. RegressionVariance* TMVA::DecisionTree::fRegType. private . the separation criteria used in Regression ; Definition at line 212 of file DecisionTree.h. ◆ fSepType. SeparationBase* TMVA::DecisionTree::fSepType. private . the separation criteria ; Definition at line 211 of file DecisionTree.h. ◆ fSigClass. UInt_t TMVA::DecisionTree::fSigClass. private . class which is treated as signal when building the tree ; Definition at line 235 of file DecisionTree.h. ◆ fTreeID. Int_t TMVA::DecisionTree::fTreeID. private . just an ID number given to the tree.. makes debugging easier as tree knows who he is. ; Definition at line 237 of file DecisionTree.h. ◆ fUseExclusiveVars. Bool_t TMVA::DecisionTree::fUseExclusiveVars. private . individual variables already used in fisher criterium are not anymore analysed individually for node splitting ; Definition at line 209 of file DecisionTree.h. ◆ fUseFisherCuts. Bool_t TMVA::DecisionTree::fU",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:30898,Modifiability,variab,variables,30898,"e.h. ◆ fRandomisedTree. Bool_t TMVA::DecisionTree::fRandomisedTree. private . choose at each node splitting a random set of variables ; Definition at line 226 of file DecisionTree.h. ◆ fRegType. RegressionVariance* TMVA::DecisionTree::fRegType. private . the separation criteria used in Regression ; Definition at line 212 of file DecisionTree.h. ◆ fSepType. SeparationBase* TMVA::DecisionTree::fSepType. private . the separation criteria ; Definition at line 211 of file DecisionTree.h. ◆ fSigClass. UInt_t TMVA::DecisionTree::fSigClass. private . class which is treated as signal when building the tree ; Definition at line 235 of file DecisionTree.h. ◆ fTreeID. Int_t TMVA::DecisionTree::fTreeID. private . just an ID number given to the tree.. makes debugging easier as tree knows who he is. ; Definition at line 237 of file DecisionTree.h. ◆ fUseExclusiveVars. Bool_t TMVA::DecisionTree::fUseExclusiveVars. private . individual variables already used in fisher criterium are not anymore analysed individually for node splitting ; Definition at line 209 of file DecisionTree.h. ◆ fUseFisherCuts. Bool_t TMVA::DecisionTree::fUseFisherCuts. private . use multivariate splits using the Fisher criterium ; Definition at line 207 of file DecisionTree.h. ◆ fUseNvars. Int_t TMVA::DecisionTree::fUseNvars. private . the number of variables used in randomised trees; ; Definition at line 227 of file DecisionTree.h. ◆ fUsePoissonNvars. Bool_t TMVA::DecisionTree::fUsePoissonNvars. private . use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ; Definition at line 228 of file DecisionTree.h. ◆ fUseSearchTree. Bool_t TMVA::DecisionTree::fUseSearchTree. private . cut scan done with binary trees or simple event loop. ; Definition at line 218 of file DecisionTree.h. ◆ fVariableImportance. std::vector< Double_t > TMVA::DecisionTree::fVariableImportance. private . the relative importance of the different variables ; Definition at line 232 of file DecisionTree.h. Libraries ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:31292,Modifiability,variab,variables,31292,"; Definition at line 212 of file DecisionTree.h. ◆ fSepType. SeparationBase* TMVA::DecisionTree::fSepType. private . the separation criteria ; Definition at line 211 of file DecisionTree.h. ◆ fSigClass. UInt_t TMVA::DecisionTree::fSigClass. private . class which is treated as signal when building the tree ; Definition at line 235 of file DecisionTree.h. ◆ fTreeID. Int_t TMVA::DecisionTree::fTreeID. private . just an ID number given to the tree.. makes debugging easier as tree knows who he is. ; Definition at line 237 of file DecisionTree.h. ◆ fUseExclusiveVars. Bool_t TMVA::DecisionTree::fUseExclusiveVars. private . individual variables already used in fisher criterium are not anymore analysed individually for node splitting ; Definition at line 209 of file DecisionTree.h. ◆ fUseFisherCuts. Bool_t TMVA::DecisionTree::fUseFisherCuts. private . use multivariate splits using the Fisher criterium ; Definition at line 207 of file DecisionTree.h. ◆ fUseNvars. Int_t TMVA::DecisionTree::fUseNvars. private . the number of variables used in randomised trees; ; Definition at line 227 of file DecisionTree.h. ◆ fUsePoissonNvars. Bool_t TMVA::DecisionTree::fUsePoissonNvars. private . use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ; Definition at line 228 of file DecisionTree.h. ◆ fUseSearchTree. Bool_t TMVA::DecisionTree::fUseSearchTree. private . cut scan done with binary trees or simple event loop. ; Definition at line 218 of file DecisionTree.h. ◆ fVariableImportance. std::vector< Double_t > TMVA::DecisionTree::fVariableImportance. private . the relative importance of the different variables ; Definition at line 232 of file DecisionTree.h. Libraries for TMVA::DecisionTree:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DecisionTree.h; tmva/tmva/src/DecisionTree.cxx. TMVADecisionTree. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:31896,Modifiability,variab,variables,31896,"; Definition at line 212 of file DecisionTree.h. ◆ fSepType. SeparationBase* TMVA::DecisionTree::fSepType. private . the separation criteria ; Definition at line 211 of file DecisionTree.h. ◆ fSigClass. UInt_t TMVA::DecisionTree::fSigClass. private . class which is treated as signal when building the tree ; Definition at line 235 of file DecisionTree.h. ◆ fTreeID. Int_t TMVA::DecisionTree::fTreeID. private . just an ID number given to the tree.. makes debugging easier as tree knows who he is. ; Definition at line 237 of file DecisionTree.h. ◆ fUseExclusiveVars. Bool_t TMVA::DecisionTree::fUseExclusiveVars. private . individual variables already used in fisher criterium are not anymore analysed individually for node splitting ; Definition at line 209 of file DecisionTree.h. ◆ fUseFisherCuts. Bool_t TMVA::DecisionTree::fUseFisherCuts. private . use multivariate splits using the Fisher criterium ; Definition at line 207 of file DecisionTree.h. ◆ fUseNvars. Int_t TMVA::DecisionTree::fUseNvars. private . the number of variables used in randomised trees; ; Definition at line 227 of file DecisionTree.h. ◆ fUsePoissonNvars. Bool_t TMVA::DecisionTree::fUsePoissonNvars. private . use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ; Definition at line 228 of file DecisionTree.h. ◆ fUseSearchTree. Bool_t TMVA::DecisionTree::fUseSearchTree. private . cut scan done with binary trees or simple event loop. ; Definition at line 218 of file DecisionTree.h. ◆ fVariableImportance. std::vector< Double_t > TMVA::DecisionTree::fVariableImportance. private . the relative importance of the different variables ; Definition at line 232 of file DecisionTree.h. Libraries for TMVA::DecisionTree:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DecisionTree.h; tmva/tmva/src/DecisionTree.cxx. TMVADecisionTree. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:10535,Performance,perform,perform,10535,"n_Code=262657);  re-create a new tree (decision tree or search tree) from XML ;  ; static const char * DeclFileName ();  ;  Static Public Member Functions inherited from TMVA::BinaryTree; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Private Member Functions; Double_t SamplePurity (EventList eventSample);  calculates the purity S/(S+B) of a given event sample ;  . Private Attributes; Types::EAnalysisType fAnalysisType;  kClassification(=0=false) or kRegression(=1=true) ;  ; DataSetInfo * fDataSetInfo;  ; UInt_t fMaxDepth;  max depth ;  ; Double_t fMinLinCorrForFisher;  the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ;  ; Double_t fMinNodeSize;  min fraction of training events in node ;  ; Double_t fMinSepGain;  min number of separation gain to perform node splitting ;  ; Double_t fMinSize;  min number of events in node ;  ; TRandom3 * fMyTrandom;  random number generator for randomised trees ;  ; Int_t fNCuts;  number of grid point in variable cut scans ;  ; Int_t fNNodesBeforePruning;  remember this one (in case of pruning, it allows to monitor the before/after ;  ; Double_t fNodePurityLimit;  purity limit to decide whether a node is signal ;  ; UInt_t fNvars;  number of variables used to separate S and B ;  ; EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:26704,Performance,perform,performed,26704,".h. ◆ TestPrunedTreeQuality(). Double_t TMVA::DecisionTree::TestPrunedTreeQuality ; (; const DecisionTreeNode * ; dt = nullptr, . Int_t ; mode = 0 . ); const. return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFull(). Double_t TMVA::DecisionTree::TrainNodeFull ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ; Definition at line 2536 of file DecisionTree.cxx. Member Data Documentation. ◆ fAnalysisType. Types::EAnalysisType TMVA::DecisionTree::fAnalysisType. private . kClassification(=0=false) or kRegression(=1=true) ; Definition at line 239 of file DecisionTree.h. ◆ fDataSetInfo. DataSetInfo* TMVA::DecisionTree::fDataSetInfo. private . Definition at line 241 of file DecisionTree.h. ◆ fgDebugLevel. const Int_t",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:28589,Performance,perform,perform,28589,"Definition at line 241 of file DecisionTree.h. ◆ fgDebugLevel. const Int_t TMVA::DecisionTree::fgDebugLevel = 0. staticprivate . debug level determining some printout/control plots etc. ; Definition at line 236 of file DecisionTree.h. ◆ fgRandomSeed. const Int_t TMVA::DecisionTree::fgRandomSeed = 0. staticprivate . Definition at line 69 of file DecisionTree.h. ◆ fMaxDepth. UInt_t TMVA::DecisionTree::fMaxDepth. private . max depth ; Definition at line 234 of file DecisionTree.h. ◆ fMinLinCorrForFisher. Double_t TMVA::DecisionTree::fMinLinCorrForFisher. private . the minimum linear correlation between two variables demanded for use in fisher criterium in node splitting ; Definition at line 208 of file DecisionTree.h. ◆ fMinNodeSize. Double_t TMVA::DecisionTree::fMinNodeSize. private . min fraction of training events in node ; Definition at line 215 of file DecisionTree.h. ◆ fMinSepGain. Double_t TMVA::DecisionTree::fMinSepGain. private . min number of separation gain to perform node splitting ; Definition at line 216 of file DecisionTree.h. ◆ fMinSize. Double_t TMVA::DecisionTree::fMinSize. private . min number of events in node ; Definition at line 214 of file DecisionTree.h. ◆ fMyTrandom. TRandom3* TMVA::DecisionTree::fMyTrandom. private . random number generator for randomised trees ; Definition at line 230 of file DecisionTree.h. ◆ fNCuts. Int_t TMVA::DecisionTree::fNCuts. private . number of grid point in variable cut scans ; Definition at line 206 of file DecisionTree.h. ◆ fNNodesBeforePruning. Int_t TMVA::DecisionTree::fNNodesBeforePruning. private . remember this one (in case of pruning, it allows to monitor the before/after ; Definition at line 222 of file DecisionTree.h. ◆ fNodePurityLimit. Double_t TMVA::DecisionTree::fNodePurityLimit. private . purity limit to decide whether a node is signal ; Definition at line 224 of file DecisionTree.h. ◆ fNvars. UInt_t TMVA::DecisionTree::fNvars. private . number of variables used to separate S and B ; Definition at lin",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:6519,Safety,avoid,avoid,6519,"ariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence t",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:23477,Safety,avoid,avoid,23477,"lative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ; Definition at line 1217 of file DecisionTree.cxx. ◆ PruneTree(). Double_t TMVA::DecisionTree::PruneTree ; (; const EventConstList * ; validationSample = nullptr). prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ; Definition at line 964 of file DecisionTree.cxx. ◆ SamplePurity(). Double_t TMVA::DecisionTree::SamplePurity ; (; EventList ; eventSample). private . calculates the purity S/(S+B) of a given event sample ; Definition at line 2722 of file DecisionTree.cxx. ◆ SetAnalysisType(). void TMVA::DecisionTree::SetAnalysisType ; (; Types::EAnalysisType ; t). inline . Definition at line 189 of file DecisionTree.h. ◆ SetMinLinCorrForFisher(). void TMVA::DecisionTree::SetMinLinCorrForFisher ; (; Double_t ; min). inline . Definition at line 192 of file DecisionTree.h. ◆ SetNodePurityLimit(). void TMVA::DecisionTree::SetNodePurityLimit ; (; Double_t ; p). inline . Definition at line 161 of file DecisionTree.h. ◆ SetNVars(). void TMVA::DecisionTree::SetNVars ; (; Int_t ; n). inline . Definition at line 194 of file DecisionTree.h. ◆ SetParentTreeInNodes(). void TMVA::DecisionTree::SetParentTreeInNodes ; (; Node * ; n =",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:2938,Security,validat,validationSample,2938,"rrorPruning =0; , kCostComplexityPruning; , kNoPruning; };  ; typedef std::vector< const TMVA::Event * > EventConstList;  ; typedef std::vector< TMVA::Event * > EventList;  . Public Member Functions;  DecisionTree (const DecisionTree &d);  copy constructor that creates a true copy, i.e. ;  ;  DecisionTree (SeparationBase *sepType, Float_t minSize, Int_t nCuts, DataSetInfo *=nullptr, UInt_t cls=0, Bool_t randomisedTree=kFALSE, Int_t useNvars=0, Bool_t usePoissonNvars=kFALSE, UInt_t nMaxDepth=9999999, Int_t iSeed=fgRandomSeed, Float_t purityLimit=0.5, Int_t treeID=0);  constructor specifying the separation type, the min number of events in a no that is still subjected to further splitting, the number of bins in the grid used in applying the cut for the node splitting. ;  ;  DecisionTree (void);  default constructor using the GiniIndex as separation criterion, no restrictions on minium number of events in a leave note or the separation gain in the node splitting ;  ; virtual ~DecisionTree (void);  destructor ;  ; void ApplyValidationSample (const EventConstList *validationSample) const;  run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ;  ; UInt_t BuildTree (const EventConstList &eventSample, DecisionTreeNode *node=nullptr);  building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ;  ; Double_t CheckEvent (const TMVA::Event *, Bool_t UseYesNoLeaf=kFALSE) const;  the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ;  ; void CheckEventWithPrunedTree (const TMVA::Event *) const;  pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ;  ; virtual const char * ClassName ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:2972,Security,validat,validation,2972,"rrorPruning =0; , kCostComplexityPruning; , kNoPruning; };  ; typedef std::vector< const TMVA::Event * > EventConstList;  ; typedef std::vector< TMVA::Event * > EventList;  . Public Member Functions;  DecisionTree (const DecisionTree &d);  copy constructor that creates a true copy, i.e. ;  ;  DecisionTree (SeparationBase *sepType, Float_t minSize, Int_t nCuts, DataSetInfo *=nullptr, UInt_t cls=0, Bool_t randomisedTree=kFALSE, Int_t useNvars=0, Bool_t usePoissonNvars=kFALSE, UInt_t nMaxDepth=9999999, Int_t iSeed=fgRandomSeed, Float_t purityLimit=0.5, Int_t treeID=0);  constructor specifying the separation type, the min number of events in a no that is still subjected to further splitting, the number of bins in the grid used in applying the cut for the node splitting. ;  ;  DecisionTree (void);  default constructor using the GiniIndex as separation criterion, no restrictions on minium number of events in a leave note or the separation gain in the node splitting ;  ; virtual ~DecisionTree (void);  destructor ;  ; void ApplyValidationSample (const EventConstList *validationSample) const;  run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ;  ; UInt_t BuildTree (const EventConstList &eventSample, DecisionTreeNode *node=nullptr);  building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ;  ; Double_t CheckEvent (const TMVA::Event *, Bool_t UseYesNoLeaf=kFALSE) const;  the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ;  ; void CheckEventWithPrunedTree (const TMVA::Event *) const;  pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ;  ; virtual const char * ClassName ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:3665,Security,validat,validation,3665,"x as separation criterion, no restrictions on minium number of events in a leave note or the separation gain in the node splitting ;  ; virtual ~DecisionTree (void);  destructor ;  ; void ApplyValidationSample (const EventConstList *validationSample) const;  run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ;  ; UInt_t BuildTree (const EventConstList &eventSample, DecisionTreeNode *node=nullptr);  building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ;  ; Double_t CheckEvent (const TMVA::Event *, Bool_t UseYesNoLeaf=kFALSE) const;  the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ;  ; void CheckEventWithPrunedTree (const TMVA::Event *) const;  pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ;  ; virtual const char * ClassName () const;  ; UInt_t CleanTree (DecisionTreeNode *node=nullptr);  remove those last splits that result in two leaf nodes that are both of the type (i.e. ;  ; void ClearTree ();  clear the tree nodes (their S/N, Nevents etc), just keep the structure of the tree ;  ; UInt_t CountLeafNodes (TMVA::Node *n=nullptr);  return the number of terminal nodes in the sub-tree below Node n ;  ; virtual DecisionTreeNode * CreateNode (UInt_t) const;  ; virtual BinaryTree * CreateTree () const;  ; void DescendTree (Node *n=nullptr);  descend a tree to find all its leaf nodes ;  ; Bool_t DoRegression () const;  ; void FillEvent (const TMVA::Event &event, TMVA::DecisionTreeNode *node);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; void FillTree (const EventList &eventSampl",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:5652,Security,validat,validationSample,5652," up ;  ; void FillTree (const EventList &eventSample);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; Types::EAnalysisType GetAnalysisType (void);  ; TMVA::DecisionTreeNode * GetEventNode (const TMVA::Event &e) const;  get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ;  ; std::vector< Double_t > GetFisherCoefficients (const EventConstList &eventSample, UInt_t nFisherVars, UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisTy",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:5727,Security,validat,validation,5727," up ;  ; void FillTree (const EventList &eventSample);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; Types::EAnalysisType GetAnalysisType (void);  ; TMVA::DecisionTreeNode * GetEventNode (const TMVA::Event &e) const;  get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ;  ; std::vector< Double_t > GetFisherCoefficients (const EventConstList &eventSample, UInt_t nFisherVars, UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisTy",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:6436,Security,validat,validationSample,6436,"ariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence t",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:15106,Security,validat,validationSample,15106," cls = 0, . Bool_t ; randomisedTree = kFALSE, . Int_t ; useNvars = 0, . Bool_t ; usePoissonNvars = kFALSE, . UInt_t ; nMaxDepth = 9999999, . Int_t ; iSeed = fgRandomSeed, . Float_t ; purityLimit = 0.5, . Int_t ; treeID = 0 . ). constructor specifying the separation type, the min number of events in a no that is still subjected to further splitting, the number of bins in the grid used in applying the cut for the node splitting. ; Definition at line 150 of file DecisionTree.cxx. ◆ DecisionTree() [3/3]. TMVA::DecisionTree::DecisionTree ; (; const DecisionTree & ; d). copy constructor that creates a true copy, i.e. ; a completely independent tree the node copy will recursively copy all the nodes ; Definition at line 200 of file DecisionTree.cxx. ◆ ~DecisionTree(). TMVA::DecisionTree::~DecisionTree ; (; void ; ). virtual . destructor ; Definition at line 236 of file DecisionTree.cxx. Member Function Documentation. ◆ ApplyValidationSample(). void TMVA::DecisionTree::ApplyValidationSample ; (; const EventConstList * ; validationSample); const. run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ; how many of the Signal and Background events from the validation sample. This is then later used when asking for the ""tree quality"" .. ; Definition at line 1029 of file DecisionTree.cxx. ◆ BuildTree(). UInt_t TMVA::DecisionTree::BuildTree ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node = nullptr . ). building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ; Definition at line 377 of file DecisionTree.cxx. ◆ CheckEvent(). Double_t TMVA::DecisionTree::CheckEvent ; (; const TMVA::Event * ; e, . Bool_t ; UseYesNoLeaf = kFALSE . ); const. the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events e",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:15140,Security,validat,validation,15140,"_t ; nMaxDepth = 9999999, . Int_t ; iSeed = fgRandomSeed, . Float_t ; purityLimit = 0.5, . Int_t ; treeID = 0 . ). constructor specifying the separation type, the min number of events in a no that is still subjected to further splitting, the number of bins in the grid used in applying the cut for the node splitting. ; Definition at line 150 of file DecisionTree.cxx. ◆ DecisionTree() [3/3]. TMVA::DecisionTree::DecisionTree ; (; const DecisionTree & ; d). copy constructor that creates a true copy, i.e. ; a completely independent tree the node copy will recursively copy all the nodes ; Definition at line 200 of file DecisionTree.cxx. ◆ ~DecisionTree(). TMVA::DecisionTree::~DecisionTree ; (; void ; ). virtual . destructor ; Definition at line 236 of file DecisionTree.cxx. Member Function Documentation. ◆ ApplyValidationSample(). void TMVA::DecisionTree::ApplyValidationSample ; (; const EventConstList * ; validationSample); const. run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ; how many of the Signal and Background events from the validation sample. This is then later used when asking for the ""tree quality"" .. ; Definition at line 1029 of file DecisionTree.cxx. ◆ BuildTree(). UInt_t TMVA::DecisionTree::BuildTree ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node = nullptr . ). building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ; Definition at line 377 of file DecisionTree.cxx. ◆ CheckEvent(). Double_t TMVA::DecisionTree::CheckEvent ; (; const TMVA::Event * ; e, . Bool_t ; UseYesNoLeaf = kFALSE . ); const. the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ; I.e. the result of the classification if the event for this decision tree. ; Definition at line 2690 of",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:15313,Security,validat,validation,15313,"eeID = 0 . ). constructor specifying the separation type, the min number of events in a no that is still subjected to further splitting, the number of bins in the grid used in applying the cut for the node splitting. ; Definition at line 150 of file DecisionTree.cxx. ◆ DecisionTree() [3/3]. TMVA::DecisionTree::DecisionTree ; (; const DecisionTree & ; d). copy constructor that creates a true copy, i.e. ; a completely independent tree the node copy will recursively copy all the nodes ; Definition at line 200 of file DecisionTree.cxx. ◆ ~DecisionTree(). TMVA::DecisionTree::~DecisionTree ; (; void ; ). virtual . destructor ; Definition at line 236 of file DecisionTree.cxx. Member Function Documentation. ◆ ApplyValidationSample(). void TMVA::DecisionTree::ApplyValidationSample ; (; const EventConstList * ; validationSample); const. run the validation sample through the (pruned) tree and fill in the nodes the variables NSValidation and NBValidadtion (i.e. ; how many of the Signal and Background events from the validation sample. This is then later used when asking for the ""tree quality"" .. ; Definition at line 1029 of file DecisionTree.cxx. ◆ BuildTree(). UInt_t TMVA::DecisionTree::BuildTree ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node = nullptr . ). building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ; Definition at line 377 of file DecisionTree.cxx. ◆ CheckEvent(). Double_t TMVA::DecisionTree::CheckEvent ; (; const TMVA::Event * ; e, . Bool_t ; UseYesNoLeaf = kFALSE . ); const. the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ; I.e. the result of the classification if the event for this decision tree. ; Definition at line 2690 of file DecisionTree.cxx. ◆ CheckEventWithPrunedTree(). void TMVA::DecisionTree::CheckEventWithPrunedTr",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:16349,Security,validat,validation,16349,"f file DecisionTree.cxx. ◆ BuildTree(). UInt_t TMVA::DecisionTree::BuildTree ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node = nullptr . ). building the decision tree by recursively calling the splitting of one (root-) node into two daughter nodes (returns the number of nodes) ; Definition at line 377 of file DecisionTree.cxx. ◆ CheckEvent(). Double_t TMVA::DecisionTree::CheckEvent ; (; const TMVA::Event * ; e, . Bool_t ; UseYesNoLeaf = kFALSE . ); const. the event e is put into the decision tree (starting at the root node) and the output is NodeType (signal) or (background) of the final node (basket) in which the given events ends up. ; I.e. the result of the classification if the event for this decision tree. ; Definition at line 2690 of file DecisionTree.cxx. ◆ CheckEventWithPrunedTree(). void TMVA::DecisionTree::CheckEventWithPrunedTree ; (; const TMVA::Event * ; e); const. pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ; Definition at line 1085 of file DecisionTree.cxx. ◆ Class(). static TClass * TMVA::DecisionTree::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DecisionTree::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DecisionTree::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 243 of file DecisionTree.h. ◆ ClassName(). virtual const char * TMVA::DecisionTree::ClassName ; (; ); const. inlinevirtual . Implements TMVA::BinaryTree.; Definition at line 98 of file DecisionTree.h. ◆ CleanTree(). UInt_t TMVA::DecisionTree::CleanTree ; (; DecisionTreeNode * ; node = nullptr). remove those last splits that result in two leaf nodes that are both of the type (i.e. ; both signal or both background) this of course is only a reasonable thing to do when you use ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:21729,Security,validat,validationSample,21729," from the tree. ; Its position (up to a maximal tree depth of 64) is coded as a sequence of left-right moves starting from the root, coded as 0-1 bit patterns stored in the ""long-integer"" (i.e. 0:left ; 1:right ; Definition at line 1231 of file DecisionTree.cxx. ◆ GetNodePurityLimit(). Double_t TMVA::DecisionTree::GetNodePurityLimit ; (; ); const. inline . Definition at line 162 of file DecisionTree.h. ◆ GetPruneStrength(). Double_t TMVA::DecisionTree::GetPruneStrength ; (; ); const. inline . Definition at line 147 of file DecisionTree.h. ◆ GetRandomisedVariables(). void TMVA::DecisionTree::GetRandomisedVariables ; (; Bool_t * ; useVariable, . UInt_t * ; variableMap, . UInt_t & ; nVars . ). Definition at line 1247 of file DecisionTree.cxx. ◆ GetRoot(). virtual DecisionTreeNode * TMVA::DecisionTree::GetRoot ; (; ); const. inlinevirtual . Reimplemented from TMVA::BinaryTree.; Definition at line 94 of file DecisionTree.h. ◆ GetSumWeights(). Double_t TMVA::DecisionTree::GetSumWeights ; (; const EventConstList * ; validationSample); const. calculate the normalization factor for a pruning validation sample ; Definition at line 1118 of file DecisionTree.cxx. ◆ GetTreeID(). Int_t TMVA::DecisionTree::GetTreeID ; (; ). inline . Definition at line 186 of file DecisionTree.h. ◆ GetVariableImportance() [1/2]. vector< Double_t > TMVA::DecisionTree::GetVariableImportance ; (; ). Return the relative variable importance, normalized to all variables together having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file DecisionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:21804,Security,validat,validation,21804,"s starting from the root, coded as 0-1 bit patterns stored in the ""long-integer"" (i.e. 0:left ; 1:right ; Definition at line 1231 of file DecisionTree.cxx. ◆ GetNodePurityLimit(). Double_t TMVA::DecisionTree::GetNodePurityLimit ; (; ); const. inline . Definition at line 162 of file DecisionTree.h. ◆ GetPruneStrength(). Double_t TMVA::DecisionTree::GetPruneStrength ; (; ); const. inline . Definition at line 147 of file DecisionTree.h. ◆ GetRandomisedVariables(). void TMVA::DecisionTree::GetRandomisedVariables ; (; Bool_t * ; useVariable, . UInt_t * ; variableMap, . UInt_t & ; nVars . ). Definition at line 1247 of file DecisionTree.cxx. ◆ GetRoot(). virtual DecisionTreeNode * TMVA::DecisionTree::GetRoot ; (; ); const. inlinevirtual . Reimplemented from TMVA::BinaryTree.; Definition at line 94 of file DecisionTree.h. ◆ GetSumWeights(). Double_t TMVA::DecisionTree::GetSumWeights ; (; const EventConstList * ; validationSample); const. calculate the normalization factor for a pruning validation sample ; Definition at line 1118 of file DecisionTree.cxx. ◆ GetTreeID(). Int_t TMVA::DecisionTree::GetTreeID ; (; ). inline . Definition at line 186 of file DecisionTree.h. ◆ GetVariableImportance() [1/2]. vector< Double_t > TMVA::DecisionTree::GetVariableImportance ; (; ). Return the relative variable importance, normalized to all variables together having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file DecisionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode()",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:23393,Security,validat,validationSample,23393,"ionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ; Definition at line 1217 of file DecisionTree.cxx. ◆ PruneTree(). Double_t TMVA::DecisionTree::PruneTree ; (; const EventConstList * ; validationSample = nullptr). prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ; Definition at line 964 of file DecisionTree.cxx. ◆ SamplePurity(). Double_t TMVA::DecisionTree::SamplePurity ; (; EventList ; eventSample). private . calculates the purity S/(S+B) of a given event sample ; Definition at line 2722 of file DecisionTree.cxx. ◆ SetAnalysisType(). void TMVA::DecisionTree::SetAnalysisType ; (; Types::EAnalysisType ; t). inline . Definition at line 189 of file DecisionTree.h. ◆ SetMinLinCorrForFisher(). void TMVA::DecisionTree::SetMinLinCorrForFisher ; (; Double_t ; min). inline . Definition at line 192 of file DecisionTree.h. ◆ SetNodePurityLimit(). void TMVA::DecisionTree::SetNodePurityLimit ; (; Double_t ; p). inline . Definition at line 161 of file DecisionTree.h. ◆ SetNVars(). void TMVA::DecisionTree::SetNVars ; (; Int_t ; n). inline . Def",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:6294,Testability,test,testing,6294,", UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable, UInt_t *variableMap, UInt_t &nVars);  ; virtual DecisionTreeNode * GetRoot () const;  ; Double_t GetSumWeights (const EventConstList *validationSample) const;  calculate the normalization factor for a pruning validation sample ;  ; Int_t GetTreeID ();  ; std::vector< Double_t > GetVariableImportance ();  Return the relative variable importance, normalized to all variables together having the importance 1. ;  ; Double_t GetVariableImportance (UInt_t ivar);  returns the relative importance of variable ivar ;  ; virtual TClass * IsA () const;  ; void PruneNode (TMVA::DecisionTreeNode *node);  prune away the subtree below the node ;  ; void PruneNodeInPlace (TMVA::DecisionTreeNode *node);  prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ;  ; Double_t PruneTree (const EventConstList *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bo",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:7526,Testability,test,testing,7526,"List *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ;  ; Double_t TrainNode (const EventConstList &eventSample, DecisionTreeNode *node);  ; Double_t TrainNodeFast (const EventConstList &eventSample, DecisionTreeNode *node);  Decide how to split a node using one of the variables that gives the best separation of signal/background. ;  ; Double_t TrainNodeFull (const EventConstList &eventSample, DecisionTreeNode *node);  train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ;  ;  Public Member Functions inherited from TMVA::BinaryTree;  BinaryTree (void);  constructor for a yet ""empty"" tree. Needs to be filled afterwards ;  ; virtual ~BinaryTree ();  destructor (deletes the nodes and ""events"" if owned by the tree ;  ; virtual void * AddXMLTo (void ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:7557,Testability,test,test,7557,"List *validationSample=nullptr);  prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ;  ; void SetAnalysisType (Types::EAnalysisType t);  ; void SetMinLinCorrForFisher (Double_t min);  ; void SetNodePurityLimit (Double_t p);  ; void SetNVars (Int_t n);  ; void SetParentTreeInNodes (Node *n=nullptr);  descend a tree to find all its leaf nodes, fill max depth reached in the tree at the same time. ;  ; void SetPruneMethod (EPruneMethod m=kCostComplexityPruning);  ; void SetPruneStrength (Double_t p);  ; void SetTreeID (Int_t treeID);  ; void SetUseExclusiveVars (Bool_t t=kTRUE);  ; void SetUseFisherCuts (Bool_t t=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ; Double_t TestPrunedTreeQuality (const DecisionTreeNode *dt=nullptr, Int_t mode=0) const;  return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ;  ; Double_t TrainNode (const EventConstList &eventSample, DecisionTreeNode *node);  ; Double_t TrainNodeFast (const EventConstList &eventSample, DecisionTreeNode *node);  Decide how to split a node using one of the variables that gives the best separation of signal/background. ;  ; Double_t TrainNodeFull (const EventConstList &eventSample, DecisionTreeNode *node);  train a node by finding the single optimal cut for a single variable that best separates signal and background (maximizes the separation gain) ;  ;  Public Member Functions inherited from TMVA::BinaryTree;  BinaryTree (void);  constructor for a yet ""empty"" tree. Needs to be filled afterwards ;  ; virtual ~BinaryTree ();  destructor (deletes the nodes and ""events"" if owned by the tree ;  ; virtual void * AddXMLTo (void ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:23162,Testability,test,testing,23162," having the importance 1. ; The importance in evaluated as the total separation-gain that this variable had in the decision trees (weighted by the number of events) ; Definition at line 2745 of file DecisionTree.cxx. ◆ GetVariableImportance() [2/2]. Double_t TMVA::DecisionTree::GetVariableImportance ; (; UInt_t ; ivar). returns the relative importance of variable ivar ; Definition at line 2766 of file DecisionTree.cxx. ◆ IsA(). virtual TClass * TMVA::DecisionTree::IsA ; (; ); const. inlinevirtual . ReturnsTClass describing current object ; Reimplemented from TMVA::BinaryTree.; Definition at line 243 of file DecisionTree.h. ◆ PruneNode(). void TMVA::DecisionTree::PruneNode ; (; TMVA::DecisionTreeNode * ; node). prune away the subtree below the node ; Definition at line 1194 of file DecisionTree.cxx. ◆ PruneNodeInPlace(). void TMVA::DecisionTree::PruneNodeInPlace ; (; TMVA::DecisionTreeNode * ; node). prune a node temporarily (without actually deleting its descendants which allows testing the pruned tree quality for many different pruning stages without ""touching"" the tree. ; Definition at line 1217 of file DecisionTree.cxx. ◆ PruneTree(). Double_t TMVA::DecisionTree::PruneTree ; (; const EventConstList * ; validationSample = nullptr). prune (get rid of internal nodes) the Decision tree to avoid overtraining several different pruning methods can be applied as selected by the variable ""fPruneMethod"". ; Definition at line 964 of file DecisionTree.cxx. ◆ SamplePurity(). Double_t TMVA::DecisionTree::SamplePurity ; (; EventList ; eventSample). private . calculates the purity S/(S+B) of a given event sample ; Definition at line 2722 of file DecisionTree.cxx. ◆ SetAnalysisType(). void TMVA::DecisionTree::SetAnalysisType ; (; Types::EAnalysisType ; t). inline . Definition at line 189 of file DecisionTree.h. ◆ SetMinLinCorrForFisher(). void TMVA::DecisionTree::SetMinLinCorrForFisher ; (; Double_t ; min). inline . Definition at line 192 of file DecisionTree.h. ◆ SetNodePurityLi",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:25997,Testability,test,testing,25997,"isionTree::SetTreeID ; (; Int_t ; treeID). inline . Definition at line 185 of file DecisionTree.h. ◆ SetUseExclusiveVars(). void TMVA::DecisionTree::SetUseExclusiveVars ; (; Bool_t ; t = kTRUE). inline . Definition at line 193 of file DecisionTree.h. ◆ SetUseFisherCuts(). void TMVA::DecisionTree::SetUseFisherCuts ; (; Bool_t ; t = kTRUE). inline . Definition at line 191 of file DecisionTree.h. ◆ Streamer(). virtual void TMVA::DecisionTree::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::BinaryTree. ◆ StreamerNVirtual(). void TMVA::DecisionTree::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 243 of file DecisionTree.h. ◆ TestPrunedTreeQuality(). Double_t TMVA::DecisionTree::TestPrunedTreeQuality ; (; const DecisionTreeNode * ; dt = nullptr, . Int_t ; mode = 0 . ); const. return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFu",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:26028,Testability,test,test,26028,"isionTree::SetTreeID ; (; Int_t ; treeID). inline . Definition at line 185 of file DecisionTree.h. ◆ SetUseExclusiveVars(). void TMVA::DecisionTree::SetUseExclusiveVars ; (; Bool_t ; t = kTRUE). inline . Definition at line 193 of file DecisionTree.h. ◆ SetUseFisherCuts(). void TMVA::DecisionTree::SetUseFisherCuts ; (; Bool_t ; t = kTRUE). inline . Definition at line 191 of file DecisionTree.h. ◆ Streamer(). virtual void TMVA::DecisionTree::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::BinaryTree. ◆ StreamerNVirtual(). void TMVA::DecisionTree::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 243 of file DecisionTree.h. ◆ TestPrunedTreeQuality(). Double_t TMVA::DecisionTree::TestPrunedTreeQuality ; (; const DecisionTreeNode * ; dt = nullptr, . Int_t ; mode = 0 . ); const. return the misclassification rate of a pruned tree a ""pruned tree"" may have set the variable ""IsTerminal"" to ""arbitrary"" at any node, hence this tree quality testing will stop there, hence test the pruned tree (while the full tree is still in place for normal/later use) ; Definition at line 1043 of file DecisionTree.cxx. ◆ TrainNode(). Double_t TMVA::DecisionTree::TrainNode ; (; const EventConstList & ; eventSample, . DecisionTreeNode * ; node . ). inline . Definition at line 108 of file DecisionTree.h. ◆ TrainNodeFast(). Double_t TMVA::DecisionTree::TrainNodeFast ; (; const EventConstList & ; eventSample, . TMVA::DecisionTreeNode * ; node . ). Decide how to split a node using one of the variables that gives the best separation of signal/background. ; In order to do this, for each variable a scan of the different cut values in a grid (grid = fNCuts) is performed and the resulting separation gains are compared. in addition to the individual variables, one can also ask for a fisher discriminant being built out of (some) of the variables and used as a possible multivariate split. ; Definition at line 1374 of file DecisionTree.cxx. ◆ TrainNodeFu",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:4039,Usability,clear,clear,4039,"round) of the final node (basket) in which the given events ends up. ;  ; void CheckEventWithPrunedTree (const TMVA::Event *) const;  pass a single validation event through a pruned decision tree on the way down the tree, fill in all the ""intermediate"" information that would normally be there from training. ;  ; virtual const char * ClassName () const;  ; UInt_t CleanTree (DecisionTreeNode *node=nullptr);  remove those last splits that result in two leaf nodes that are both of the type (i.e. ;  ; void ClearTree ();  clear the tree nodes (their S/N, Nevents etc), just keep the structure of the tree ;  ; UInt_t CountLeafNodes (TMVA::Node *n=nullptr);  return the number of terminal nodes in the sub-tree below Node n ;  ; virtual DecisionTreeNode * CreateNode (UInt_t) const;  ; virtual BinaryTree * CreateTree () const;  ; void DescendTree (Node *n=nullptr);  descend a tree to find all its leaf nodes ;  ; Bool_t DoRegression () const;  ; void FillEvent (const TMVA::Event &event, TMVA::DecisionTreeNode *node);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; void FillTree (const EventList &eventSample);  fill the existing the decision tree structure by filling event in from the top node and see where they happen to end up ;  ; Types::EAnalysisType GetAnalysisType (void);  ; TMVA::DecisionTreeNode * GetEventNode (const TMVA::Event &e) const;  get the pointer to the leaf node where a particular event ends up in... (used in gradient boosting) ;  ; std::vector< Double_t > GetFisherCoefficients (const EventConstList &eventSample, UInt_t nFisherVars, UInt_t *mapVarInFisher);  calculate the fisher coefficients for the event sample and the variables used ;  ; Int_t GetNNodesBeforePruning ();  ; Node * GetNode (ULong_t sequence, UInt_t depth);  retrieve node from the tree. ;  ; Double_t GetNodePurityLimit () const;  ; Double_t GetPruneStrength () const;  ; void GetRandomisedVariables (Bool_t *useVariable,",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:12043,Usability,simpl,simple,12043,"EPruneMethod fPruneMethod;  method used for pruning ;  ; Double_t fPruneStrength;  a parameter to set the ""amount"" of pruning..needs to be adjusted ;  ; Bool_t fRandomisedTree;  choose at each node splitting a random set of variables ;  ; RegressionVariance * fRegType;  the separation criteria used in Regression ;  ; SeparationBase * fSepType;  the separation criteria ;  ; UInt_t fSigClass;  class which is treated as signal when building the tree ;  ; Int_t fTreeID;  just an ID number given to the tree.. makes debugging easier as tree knows who he is. ;  ; Bool_t fUseExclusiveVars;  individual variables already used in fisher criterium are not anymore analysed individually for node splitting ;  ; Bool_t fUseFisherCuts;  use multivariate splits using the Fisher criterium ;  ; Int_t fUseNvars;  the number of variables used in randomised trees; ;  ; Bool_t fUsePoissonNvars;  use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ;  ; Bool_t fUseSearchTree;  cut scan done with binary trees or simple event loop. ;  ; std::vector< Double_t > fVariableImportance;  the relative importance of the different variables ;  . Static Private Attributes; static const Int_t fgDebugLevel = 0;  debug level determining some printout/control plots etc. ;  ; static const Int_t fgRandomSeed = 0;  . Additional Inherited Members;  Protected Member Functions inherited from TMVA::BinaryTree; void DeleteNode (Node *);  protected, recursive, function used by the class destructor and when Pruning ;  ; MsgLogger & Log () const;  ;  Protected Attributes inherited from TMVA::BinaryTree; UInt_t fDepth;  maximal depth in tree reached ;  ; UInt_t fNNodes;  total number of nodes in the tree (counted) ;  ; Node * fRoot;  the root node of the tree the tree only has it's root node, the ""daughters"" are taken care of by the ""node"" properties of the ""root"" ;  . #include <TMVA/DecisionTree.h>. Inheritance diagram for TMVA::DecisionTree:. This browser is not able to show SVG: try Fire",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:17684,Usability,clear,clear,17684,"tic . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DecisionTree::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 243 of file DecisionTree.h. ◆ ClassName(). virtual const char * TMVA::DecisionTree::ClassName ; (; ); const. inlinevirtual . Implements TMVA::BinaryTree.; Definition at line 98 of file DecisionTree.h. ◆ CleanTree(). UInt_t TMVA::DecisionTree::CleanTree ; (; DecisionTreeNode * ; node = nullptr). remove those last splits that result in two leaf nodes that are both of the type (i.e. ; both signal or both background) this of course is only a reasonable thing to do when you use ""YesOrNo"" leafs, while it might loose s.th. if you use the purity information in the nodes. --> hence I don't call it automatically in the tree building ; Definition at line 937 of file DecisionTree.cxx. ◆ ClearTree(). void TMVA::DecisionTree::ClearTree ; (; ). clear the tree nodes (their S/N, Nevents etc), just keep the structure of the tree ; Definition at line 923 of file DecisionTree.cxx. ◆ CountLeafNodes(). UInt_t TMVA::DecisionTree::CountLeafNodes ; (; TMVA::Node * ; n = nullptr). return the number of terminal nodes in the sub-tree below Node n ; Definition at line 1131 of file DecisionTree.cxx. ◆ CreateFromXML(). TMVA::DecisionTree * TMVA::DecisionTree::CreateFromXML ; (; void * ; node, . UInt_t ; tmva_Version_Code = 262657 . ). static . re-create a new tree (decision tree or search tree) from XML ; Definition at line 281 of file DecisionTree.cxx. ◆ CreateNode(). virtual DecisionTreeNode * TMVA::DecisionTree::CreateNode ; (; UInt_t ; ); const. inlinevirtual . Implements TMVA::BinaryTree.; Definition at line 95 of file DecisionTree.h. ◆ CreateTree(). virtual BinaryTree * TMVA::DecisionTree::CreateTree ; (; ); const. inlinevirtual . Implements TMVA::BinaryTree.; Definition at line 96 of file DecisionTree.h. ◆ DeclFileName(). static const char * TMVA::DecisionTree::DeclFileName ; (; ). inlinestatic . ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTree.html:31689,Usability,simpl,simple,31689,"; Definition at line 212 of file DecisionTree.h. ◆ fSepType. SeparationBase* TMVA::DecisionTree::fSepType. private . the separation criteria ; Definition at line 211 of file DecisionTree.h. ◆ fSigClass. UInt_t TMVA::DecisionTree::fSigClass. private . class which is treated as signal when building the tree ; Definition at line 235 of file DecisionTree.h. ◆ fTreeID. Int_t TMVA::DecisionTree::fTreeID. private . just an ID number given to the tree.. makes debugging easier as tree knows who he is. ; Definition at line 237 of file DecisionTree.h. ◆ fUseExclusiveVars. Bool_t TMVA::DecisionTree::fUseExclusiveVars. private . individual variables already used in fisher criterium are not anymore analysed individually for node splitting ; Definition at line 209 of file DecisionTree.h. ◆ fUseFisherCuts. Bool_t TMVA::DecisionTree::fUseFisherCuts. private . use multivariate splits using the Fisher criterium ; Definition at line 207 of file DecisionTree.h. ◆ fUseNvars. Int_t TMVA::DecisionTree::fUseNvars. private . the number of variables used in randomised trees; ; Definition at line 227 of file DecisionTree.h. ◆ fUsePoissonNvars. Bool_t TMVA::DecisionTree::fUsePoissonNvars. private . use ""fUseNvars"" not as fixed number but as mean of a poisson distr. in each split ; Definition at line 228 of file DecisionTree.h. ◆ fUseSearchTree. Bool_t TMVA::DecisionTree::fUseSearchTree. private . cut scan done with binary trees or simple event loop. ; Definition at line 218 of file DecisionTree.h. ◆ fVariableImportance. std::vector< Double_t > TMVA::DecisionTree::fVariableImportance. private . the relative importance of the different variables ; Definition at line 232 of file DecisionTree.h. Libraries for TMVA::DecisionTree:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DecisionTree.h; tmva/tmva/src/DecisionTree.cxx. TMVADecisionTree. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTree.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTree.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:4307,Modifiability,variab,variable,4307,"SigEvents (void) const;  return the sum of the signal weights in the node, or -1 if traininfo undefined ;  ; Float_t GetNSigEvents_unboosted (void) const;  return the sum of unboosted signal weights in the node, or -1 if traininfo undefined ;  ; Float_t GetNSigEvents_unweighted (void) const;  ; Double_t GetNSValidation () const;  return number of signal events from the pruning validation sample, or -1 if traininfo undefined ;  ; Int_t GetNTerminal () const;  return number of terminal nodes in the subtree rooted here, or -1 if traininfo undefined ;  ; virtual DecisionTreeNode * GetParent () const;  ; Float_t GetPurity (void) const;  return S/(S+B) (purity) at this node (from training) ;  ; Float_t GetResponse (void) const;  return the response of the node (for regression) ;  ; virtual DecisionTreeNode * GetRight () const;  ; Float_t GetRMS (void) const;  return the RMS of the response of the node (for regression) ;  ; Float_t GetSampleMax (UInt_t ivar) const;  return the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return 9999 ;  ; Float_t GetSampleMin (UInt_t ivar) const;  return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ;  ; Short_t GetSelector () const;  return index of variable used for discrimination at this node ;  ; Float_t GetSeparationGain (void) const;  return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ;  ; Float_t GetSeparationIndex (void) const;  return the separation index AT this node, or 0 if traininfo undefined ;  ; Double_t GetSubTreeR () const;  return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ;  ; Float_t GetSumTarget () const;  return sum target, or -9999 if traininfo undefined ;  ; Float_t GetSumTarget2 () const;  return sum target 2, or -9999 if traininfo undefined ;",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:4505,Modifiability,variab,variable,4505," in the node, or -1 if traininfo undefined ;  ; Float_t GetNSigEvents_unweighted (void) const;  ; Double_t GetNSValidation () const;  return number of signal events from the pruning validation sample, or -1 if traininfo undefined ;  ; Int_t GetNTerminal () const;  return number of terminal nodes in the subtree rooted here, or -1 if traininfo undefined ;  ; virtual DecisionTreeNode * GetParent () const;  ; Float_t GetPurity (void) const;  return S/(S+B) (purity) at this node (from training) ;  ; Float_t GetResponse (void) const;  return the response of the node (for regression) ;  ; virtual DecisionTreeNode * GetRight () const;  ; Float_t GetRMS (void) const;  return the RMS of the response of the node (for regression) ;  ; Float_t GetSampleMax (UInt_t ivar) const;  return the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return 9999 ;  ; Float_t GetSampleMin (UInt_t ivar) const;  return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ;  ; Short_t GetSelector () const;  return index of variable used for discrimination at this node ;  ; Float_t GetSeparationGain (void) const;  return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ;  ; Float_t GetSeparationIndex (void) const;  return the separation index AT this node, or 0 if traininfo undefined ;  ; Double_t GetSubTreeR () const;  return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ;  ; Float_t GetSumTarget () const;  return sum target, or -9999 if traininfo undefined ;  ; Float_t GetSumTarget2 () const;  return sum target 2, or -9999 if traininfo undefined ;  ; virtual Bool_t GoesLeft (const Event &) const;  test event if it descends the tree at this node to the left ;  ; virtual Bool_t GoesRight (const Event &) const;  test event if it descends the tr",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:4686,Modifiability,variab,variable,4686," validation sample, or -1 if traininfo undefined ;  ; Int_t GetNTerminal () const;  return number of terminal nodes in the subtree rooted here, or -1 if traininfo undefined ;  ; virtual DecisionTreeNode * GetParent () const;  ; Float_t GetPurity (void) const;  return S/(S+B) (purity) at this node (from training) ;  ; Float_t GetResponse (void) const;  return the response of the node (for regression) ;  ; virtual DecisionTreeNode * GetRight () const;  ; Float_t GetRMS (void) const;  return the RMS of the response of the node (for regression) ;  ; Float_t GetSampleMax (UInt_t ivar) const;  return the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return 9999 ;  ; Float_t GetSampleMin (UInt_t ivar) const;  return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ;  ; Short_t GetSelector () const;  return index of variable used for discrimination at this node ;  ; Float_t GetSeparationGain (void) const;  return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ;  ; Float_t GetSeparationIndex (void) const;  return the separation index AT this node, or 0 if traininfo undefined ;  ; Double_t GetSubTreeR () const;  return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ;  ; Float_t GetSumTarget () const;  return sum target, or -9999 if traininfo undefined ;  ; Float_t GetSumTarget2 () const;  return sum target 2, or -9999 if traininfo undefined ;  ; virtual Bool_t GoesLeft (const Event &) const;  test event if it descends the tree at this node to the left ;  ; virtual Bool_t GoesRight (const Event &) const;  test event if it descends the tree at this node to the right ;  ; void IncrementNBkgEvents (Float_t b);  increment the sum of the backgr weights in the node, if traininfo defined ;  ; void IncrementNBkgEvents_unwe",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:7634,Modifiability,variab,variable,7634,,MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:10134,Modifiability,variab,variable,10134,,MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:10297,Modifiability,variab,variable,10297,,MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:10437,Modifiability,variab,variable,10437,,MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:11222,Modifiability,inherit,inherited,11222," ; void SetSampleMin (UInt_t ivar, Float_t xmin);  set the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined ;  ; void SetSelector (Short_t i);  set index of variable used for discrimination at this node ;  ; void SetSeparationGain (Float_t sep);  set the separation, or information gained BY this node's selection, if traininfo defined ;  ; void SetSeparationIndex (Float_t sep);  set the chosen index, measure of ""purity"" (separation between S and B) AT this node, if traininfo defined ;  ; void SetSubTreeR (Double_t r);  set the resubstitution estimate, R(T_t), of the tree rooted at this node, if traininfo defined ;  ; void SetSumTarget (Float_t t);  set sum target, if traininfo defined ;  ; void SetSumTarget2 (Float_t t2);  set sum target 2, if traininfo defined ;  ; void SetTerminal (Bool_t s=kTRUE);  ; virtual void Streamer (TBuffer &);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  ;  Public Member Functions inherited from TMVA::Node;  Node ();  ;  Node (const Node &n);  copy constructor, make sure you don't just copy the pointer to the node, but that the parents/daughters are initialized to 0 (and set by the copy constructors of the derived classes ;  ;  Node (Node *p, char pos);  constructor of a daughter node as a daughter of 'p' ;  ; virtual ~Node ();  node destructor ;  ; void * AddXMLTo (void *parent) const;  add attributes to XML ;  ; Int_t CountMeAndAllDaughters () const;  recursively go through the part of the tree below this node and count all daughters ;  ; int GetCount ();  returns the global number of instantiated nodes ;  ; UInt_t GetDepth () const;  ; virtual TMVA::BinaryTree * GetParentTree () const;  ; char GetPos () const;  ; void ReadXML (void *node, UInt_t tmva_Version_Code=262657);  read attributes from XML ;  ; void SetDepth (UInt_t d);  ; virtual void SetParentTree (TMVA::BinaryTree *t);  ; void SetPos (char s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:12613,Modifiability,inherit,inherited,12613,"f a daughter node as a daughter of 'p' ;  ; virtual ~Node ();  node destructor ;  ; void * AddXMLTo (void *parent) const;  add attributes to XML ;  ; Int_t CountMeAndAllDaughters () const;  recursively go through the part of the tree below this node and count all daughters ;  ; int GetCount ();  returns the global number of instantiated nodes ;  ; UInt_t GetDepth () const;  ; virtual TMVA::BinaryTree * GetParentTree () const;  ; char GetPos () const;  ; void ReadXML (void *node, UInt_t tmva_Version_Code=262657);  read attributes from XML ;  ; void SetDepth (UInt_t d);  ; virtual void SetParentTree (TMVA::BinaryTree *t);  ; void SetPos (char s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static UInt_t GetTmvaVersionCode ();  ; static bool IsTraining ();  ; static void SetIsTraining (bool on);  ; static void SetTmvaVersionCode (UInt_t code);  ;  Static Public Member Functions inherited from TMVA::Node; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Static Protected Member Functions; static MsgLogger & Log ();  . Protected Attributes; Bool_t fCutType;  true: if event variable > cutValue ==> signal , false otherwise ;  ; Float_t fCutValue;  cut value applied on this node to discriminate bkg against sig ;  ; std::vector< Double_t > fFisherCoeff;  the fisher coeff (offset at the last element) ;  ; Bool_t fIsTerminalNode;  ! flag to set node as terminal (i.e., without deleting its descendants) ;  ; Int_t fNodeType;  Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal. ;  ; Float_t fPurity;  the node purity ;  ; Float_t fResponse;  response value in case of regression ;  ; Float_t fRMS;  response RMS of the regression node ;  ; Short_t fSelector;  ind",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:12915,Modifiability,variab,variable,12915," read attributes from XML ;  ; void SetDepth (UInt_t d);  ; virtual void SetParentTree (TMVA::BinaryTree *t);  ; void SetPos (char s);  ; void StreamerNVirtual (TBuffer &ClassDef_StreamerNVirtual_b);  . Static Public Member Functions; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  ; static UInt_t GetTmvaVersionCode ();  ; static bool IsTraining ();  ; static void SetIsTraining (bool on);  ; static void SetTmvaVersionCode (UInt_t code);  ;  Static Public Member Functions inherited from TMVA::Node; static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Static Protected Member Functions; static MsgLogger & Log ();  . Protected Attributes; Bool_t fCutType;  true: if event variable > cutValue ==> signal , false otherwise ;  ; Float_t fCutValue;  cut value applied on this node to discriminate bkg against sig ;  ; std::vector< Double_t > fFisherCoeff;  the fisher coeff (offset at the last element) ;  ; Bool_t fIsTerminalNode;  ! flag to set node as terminal (i.e., without deleting its descendants) ;  ; Int_t fNodeType;  Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal. ;  ; Float_t fPurity;  the node purity ;  ; Float_t fResponse;  response value in case of regression ;  ; Float_t fRMS;  response RMS of the regression node ;  ; Short_t fSelector;  index of variable used in node selection (decision tree) ;  ; DTNodeTrainingInfo * fTrainInfo;  ;  Protected Attributes inherited from TMVA::Node; UInt_t fDepth;  depth of the node within the tree (seen from root node) ;  ; Node * fLeft;  pointers to the two ""daughter"" nodes ;  ; Node * fParent;  the previous (parent) node ;  ; BinaryTree * fParentTree;  pointer to the parent tree to which the Node belongs ;  ; char fPos;  position, i.e. it is a left (l) or right (r) daughter ;  ; Node * fRight;  pointers to the two ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:13520,Modifiability,variab,variable,13520," static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Static Protected Member Functions; static MsgLogger & Log ();  . Protected Attributes; Bool_t fCutType;  true: if event variable > cutValue ==> signal , false otherwise ;  ; Float_t fCutValue;  cut value applied on this node to discriminate bkg against sig ;  ; std::vector< Double_t > fFisherCoeff;  the fisher coeff (offset at the last element) ;  ; Bool_t fIsTerminalNode;  ! flag to set node as terminal (i.e., without deleting its descendants) ;  ; Int_t fNodeType;  Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal. ;  ; Float_t fPurity;  the node purity ;  ; Float_t fResponse;  response value in case of regression ;  ; Float_t fRMS;  response RMS of the regression node ;  ; Short_t fSelector;  index of variable used in node selection (decision tree) ;  ; DTNodeTrainingInfo * fTrainInfo;  ;  Protected Attributes inherited from TMVA::Node; UInt_t fDepth;  depth of the node within the tree (seen from root node) ;  ; Node * fLeft;  pointers to the two ""daughter"" nodes ;  ; Node * fParent;  the previous (parent) node ;  ; BinaryTree * fParentTree;  pointer to the parent tree to which the Node belongs ;  ; char fPos;  position, i.e. it is a left (l) or right (r) daughter ;  ; Node * fRight;  pointers to the two ""daughter"" nodes ;  . Static Protected Attributes; static bool fgIsTraining = false;  static variable to flag training phase in which we need fTrainInfo ;  ; static UInt_t fgTmva_Version_Code = 0;  set only when read from weightfile ;  . #include <TMVA/DecisionTreeNode.h>. Inheritance diagram for TMVA::DecisionTreeNode:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DecisionTreeNode() [1/3]. TMVA::DecisionTreeNode::DecisionTreeNode ; (; ). constructor of an essentially ""empty"" node floating in space ; Definit",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:13631,Modifiability,inherit,inherited,13631," static TClass * Class ();  ; static const char * Class_Name ();  ; static constexpr Version_t Class_Version ();  ; static const char * DeclFileName ();  . Static Protected Member Functions; static MsgLogger & Log ();  . Protected Attributes; Bool_t fCutType;  true: if event variable > cutValue ==> signal , false otherwise ;  ; Float_t fCutValue;  cut value applied on this node to discriminate bkg against sig ;  ; std::vector< Double_t > fFisherCoeff;  the fisher coeff (offset at the last element) ;  ; Bool_t fIsTerminalNode;  ! flag to set node as terminal (i.e., without deleting its descendants) ;  ; Int_t fNodeType;  Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal. ;  ; Float_t fPurity;  the node purity ;  ; Float_t fResponse;  response value in case of regression ;  ; Float_t fRMS;  response RMS of the regression node ;  ; Short_t fSelector;  index of variable used in node selection (decision tree) ;  ; DTNodeTrainingInfo * fTrainInfo;  ;  Protected Attributes inherited from TMVA::Node; UInt_t fDepth;  depth of the node within the tree (seen from root node) ;  ; Node * fLeft;  pointers to the two ""daughter"" nodes ;  ; Node * fParent;  the previous (parent) node ;  ; BinaryTree * fParentTree;  pointer to the parent tree to which the Node belongs ;  ; char fPos;  position, i.e. it is a left (l) or right (r) daughter ;  ; Node * fRight;  pointers to the two ""daughter"" nodes ;  . Static Protected Attributes; static bool fgIsTraining = false;  static variable to flag training phase in which we need fTrainInfo ;  ; static UInt_t fgTmva_Version_Code = 0;  set only when read from weightfile ;  . #include <TMVA/DecisionTreeNode.h>. Inheritance diagram for TMVA::DecisionTreeNode:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DecisionTreeNode() [1/3]. TMVA::DecisionTreeNode::DecisionTreeNode ; (; ). constructor of an essentially ""empty"" node floating in space ; Definit",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:14126,Modifiability,variab,variable,14126,"nalNode;  ! flag to set node as terminal (i.e., without deleting its descendants) ;  ; Int_t fNodeType;  Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal. ;  ; Float_t fPurity;  the node purity ;  ; Float_t fResponse;  response value in case of regression ;  ; Float_t fRMS;  response RMS of the regression node ;  ; Short_t fSelector;  index of variable used in node selection (decision tree) ;  ; DTNodeTrainingInfo * fTrainInfo;  ;  Protected Attributes inherited from TMVA::Node; UInt_t fDepth;  depth of the node within the tree (seen from root node) ;  ; Node * fLeft;  pointers to the two ""daughter"" nodes ;  ; Node * fParent;  the previous (parent) node ;  ; BinaryTree * fParentTree;  pointer to the parent tree to which the Node belongs ;  ; char fPos;  position, i.e. it is a left (l) or right (r) daughter ;  ; Node * fRight;  pointers to the two ""daughter"" nodes ;  . Static Protected Attributes; static bool fgIsTraining = false;  static variable to flag training phase in which we need fTrainInfo ;  ; static UInt_t fgTmva_Version_Code = 0;  set only when read from weightfile ;  . #include <TMVA/DecisionTreeNode.h>. Inheritance diagram for TMVA::DecisionTreeNode:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ DecisionTreeNode() [1/3]. TMVA::DecisionTreeNode::DecisionTreeNode ; (; ). constructor of an essentially ""empty"" node floating in space ; Definition at line 67 of file DecisionTreeNode.cxx. ◆ DecisionTreeNode() [2/3]. TMVA::DecisionTreeNode::DecisionTreeNode ; (; TMVA::Node * ; p, . char ; pos . ). constructor of a daughter node as a daughter of 'p' ; Definition at line 91 of file DecisionTreeNode.cxx. ◆ DecisionTreeNode() [3/3]. TMVA::DecisionTreeNode::DecisionTreeNode ; (; const DecisionTreeNode & ; n, . DecisionTreeNode * ; parent = nullptr . ). copy constructor of a node. ; It will result in an explicit copy of the node and recursively all it's daughters ;",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:23295,Modifiability,variab,variable,23295,"t line 285 of file DecisionTreeNode.h. ◆ GetPurity(). Float_t TMVA::DecisionTreeNode::GetPurity ; (; void ; ); const. inline . return S/(S+B) (purity) at this node (from training) ; Definition at line 168 of file DecisionTreeNode.h. ◆ GetResponse(). Float_t TMVA::DecisionTreeNode::GetResponse ; (; void ; ); const. inline . return the response of the node (for regression) ; Definition at line 176 of file DecisionTreeNode.h. ◆ GetRight(). virtual DecisionTreeNode * TMVA::DecisionTreeNode::GetRight ; (; ); const. inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 284 of file DecisionTreeNode.h. ◆ GetRMS(). Float_t TMVA::DecisionTreeNode::GetRMS ; (; void ; ); const. inline . return the RMS of the response of the node (for regression) ; Definition at line 182 of file DecisionTreeNode.h. ◆ GetSampleMax(). Float_t TMVA::DecisionTreeNode::GetSampleMax ; (; UInt_t ; ivar); const. return the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return 9999 ; Definition at line 427 of file DecisionTreeNode.cxx. ◆ GetSampleMin(). Float_t TMVA::DecisionTreeNode::GetSampleMin ; (; UInt_t ; ivar); const. return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ; Definition at line 415 of file DecisionTreeNode.cxx. ◆ GetSelector(). Short_t TMVA::DecisionTreeNode::GetSelector ; (; ); const. inline . return index of variable used for discrimination at this node ; Definition at line 150 of file DecisionTreeNode.h. ◆ GetSeparationGain(). Float_t TMVA::DecisionTreeNode::GetSeparationGain ; (; void ; ); const. inline . return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ; Definition at line 266 of file DecisionTreeNode.h. ◆ GetSeparationIndex(). Float_t TMVA::DecisionTreeNode::GetSeparationIndex ; (; void ; ); const. inline . return the separation index AT this node,",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:23591,Modifiability,variab,variable,23591,"; void ; ); const. inline . return the response of the node (for regression) ; Definition at line 176 of file DecisionTreeNode.h. ◆ GetRight(). virtual DecisionTreeNode * TMVA::DecisionTreeNode::GetRight ; (; ); const. inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 284 of file DecisionTreeNode.h. ◆ GetRMS(). Float_t TMVA::DecisionTreeNode::GetRMS ; (; void ; ); const. inline . return the RMS of the response of the node (for regression) ; Definition at line 182 of file DecisionTreeNode.h. ◆ GetSampleMax(). Float_t TMVA::DecisionTreeNode::GetSampleMax ; (; UInt_t ; ivar); const. return the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return 9999 ; Definition at line 427 of file DecisionTreeNode.cxx. ◆ GetSampleMin(). Float_t TMVA::DecisionTreeNode::GetSampleMin ; (; UInt_t ; ivar); const. return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ; Definition at line 415 of file DecisionTreeNode.cxx. ◆ GetSelector(). Short_t TMVA::DecisionTreeNode::GetSelector ; (; ); const. inline . return index of variable used for discrimination at this node ; Definition at line 150 of file DecisionTreeNode.h. ◆ GetSeparationGain(). Float_t TMVA::DecisionTreeNode::GetSeparationGain ; (; void ; ); const. inline . return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ; Definition at line 266 of file DecisionTreeNode.h. ◆ GetSeparationIndex(). Float_t TMVA::DecisionTreeNode::GetSeparationIndex ; (; void ; ); const. inline . return the separation index AT this node, or 0 if traininfo undefined ; Definition at line 260 of file DecisionTreeNode.h. ◆ GetSubTreeR(). Double_t TMVA::DecisionTreeNode::GetSubTreeR ; (; ); const. inline . return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ; Definition at line 3",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:23876,Modifiability,variab,variable,23876,"ed from TMVA::Node.; Definition at line 284 of file DecisionTreeNode.h. ◆ GetRMS(). Float_t TMVA::DecisionTreeNode::GetRMS ; (; void ; ); const. inline . return the RMS of the response of the node (for regression) ; Definition at line 182 of file DecisionTreeNode.h. ◆ GetSampleMax(). Float_t TMVA::DecisionTreeNode::GetSampleMax ; (; UInt_t ; ivar); const. return the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return 9999 ; Definition at line 427 of file DecisionTreeNode.cxx. ◆ GetSampleMin(). Float_t TMVA::DecisionTreeNode::GetSampleMin ; (; UInt_t ; ivar); const. return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ; Definition at line 415 of file DecisionTreeNode.cxx. ◆ GetSelector(). Short_t TMVA::DecisionTreeNode::GetSelector ; (; ); const. inline . return index of variable used for discrimination at this node ; Definition at line 150 of file DecisionTreeNode.h. ◆ GetSeparationGain(). Float_t TMVA::DecisionTreeNode::GetSeparationGain ; (; void ; ); const. inline . return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ; Definition at line 266 of file DecisionTreeNode.h. ◆ GetSeparationIndex(). Float_t TMVA::DecisionTreeNode::GetSeparationIndex ; (; void ; ); const. inline . return the separation index AT this node, or 0 if traininfo undefined ; Definition at line 260 of file DecisionTreeNode.h. ◆ GetSubTreeR(). Double_t TMVA::DecisionTreeNode::GetSubTreeR ; (; ); const. inline . return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ; Definition at line 300 of file DecisionTreeNode.h. ◆ GetSumTarget(). Float_t TMVA::DecisionTreeNode::GetSumTarget ; (; ); const. inline . return sum target, or -9999 if traininfo undefined ; Definition at line 340 of file DecisionTreeNode.h. ◆ GetSumTarget2(). Float_",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:30306,Modifiability,variab,variable,30306,"alidationData(). void TMVA::DecisionTreeNode::ResetValidationData ; (; ). temporary stored node values (number of events, etc.) that originate not from the training but from the validation data (used in pruning) ; Definition at line 366 of file DecisionTreeNode.cxx. ◆ SetAlpha(). void TMVA::DecisionTreeNode::SetAlpha ; (; Double_t ; alpha). inline . set the critical point alpha, if traininfo defined ; Definition at line 306 of file DecisionTreeNode.h. ◆ SetAlphaMinSubtree(). void TMVA::DecisionTreeNode::SetAlphaMinSubtree ; (; Double_t ; g). inline . set the minimum alpha in the tree rooted at this node, if traininfo defined ; Definition at line 311 of file DecisionTreeNode.h. ◆ SetCC(). void TMVA::DecisionTreeNode::SetCC ; (; Double_t ; cc). Set CC, if traininfo defined, otherwise Log Fatal. ; Definition at line 404 of file DecisionTreeNode.cxx. ◆ SetCutType(). void TMVA::DecisionTreeNode::SetCutType ; (; Bool_t ; t). inline . set true: if event variable > cutValue ==> signal , false otherwise ; Definition at line 158 of file DecisionTreeNode.h. ◆ SetCutValue(). void TMVA::DecisionTreeNode::SetCutValue ; (; Float_t ; c). inline . set the cut value applied at this node ; Definition at line 153 of file DecisionTreeNode.h. ◆ SetFisherCoeff(). void TMVA::DecisionTreeNode::SetFisherCoeff ; (; Int_t ; ivar, . Double_t ; coeff . ). set fisher coefficients ; Definition at line 518 of file DecisionTreeNode.cxx. ◆ SetIsTraining(). void TMVA::DecisionTreeNode::SetIsTraining ; (; bool ; on). static . Definition at line 549 of file DecisionTreeNode.cxx. ◆ SetLeft(). virtual void TMVA::DecisionTreeNode::SetLeft ; (; Node * ; l). inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 288 of file DecisionTreeNode.h. ◆ SetNBkgEvents(). void TMVA::DecisionTreeNode::SetNBkgEvents ; (; Float_t ; b). inline . set the sum of the backgr weights in the node, if traininfo defined ; Definition at line 188 of file DecisionTreeNode.h. ◆ SetNBkgEvents_unboosted(). void TMVA::Decisio",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:35605,Modifiability,variab,variable,35605,"urity(). void TMVA::DecisionTreeNode::SetPurity ; (; void ; ). return the S/(S+B) (purity) for the node REM: even if nodes with purity 0.01 are very PURE background nodes, they still get a small value of the purity. ; Definition at line 191 of file DecisionTreeNode.cxx. ◆ SetResponse(). void TMVA::DecisionTreeNode::SetResponse ; (; Float_t ; r). inline . set the response of the node (for regression) ; Definition at line 173 of file DecisionTreeNode.h. ◆ SetRight(). virtual void TMVA::DecisionTreeNode::SetRight ; (; Node * ; r). inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 289 of file DecisionTreeNode.h. ◆ SetRMS(). void TMVA::DecisionTreeNode::SetRMS ; (; Float_t ; r). inline . set the RMS of the response of the node (for regression) ; Definition at line 179 of file DecisionTreeNode.h. ◆ SetSampleMax(). void TMVA::DecisionTreeNode::SetSampleMax ; (; UInt_t ; ivar, . Float_t ; xmax . ). set the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined ; Definition at line 449 of file DecisionTreeNode.cxx. ◆ SetSampleMin(). void TMVA::DecisionTreeNode::SetSampleMin ; (; UInt_t ; ivar, . Float_t ; xmin . ). set the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined ; Definition at line 438 of file DecisionTreeNode.cxx. ◆ SetSelector(). void TMVA::DecisionTreeNode::SetSelector ; (; Short_t ; i). inline . set index of variable used for discrimination at this node ; Definition at line 148 of file DecisionTreeNode.h. ◆ SetSeparationGain(). void TMVA::DecisionTreeNode::SetSeparationGain ; (; Float_t ; sep). inline . set the separation, or information gained BY this node's selection, if traininfo defined ; Definition at line 263 of file DecisionTreeNode.h. ◆ SetSeparationIndex(). void TMVA::DecisionTreeNode::SetSeparationIndex ; (; Float_t ; sep). inline . set the chosen index, measure of ""purity"" (separation between S and B) AT this node, if traininfo defined ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:35872,Modifiability,variab,variable,35872,"xx. ◆ SetResponse(). void TMVA::DecisionTreeNode::SetResponse ; (; Float_t ; r). inline . set the response of the node (for regression) ; Definition at line 173 of file DecisionTreeNode.h. ◆ SetRight(). virtual void TMVA::DecisionTreeNode::SetRight ; (; Node * ; r). inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 289 of file DecisionTreeNode.h. ◆ SetRMS(). void TMVA::DecisionTreeNode::SetRMS ; (; Float_t ; r). inline . set the RMS of the response of the node (for regression) ; Definition at line 179 of file DecisionTreeNode.h. ◆ SetSampleMax(). void TMVA::DecisionTreeNode::SetSampleMax ; (; UInt_t ; ivar, . Float_t ; xmax . ). set the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined ; Definition at line 449 of file DecisionTreeNode.cxx. ◆ SetSampleMin(). void TMVA::DecisionTreeNode::SetSampleMin ; (; UInt_t ; ivar, . Float_t ; xmin . ). set the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined ; Definition at line 438 of file DecisionTreeNode.cxx. ◆ SetSelector(). void TMVA::DecisionTreeNode::SetSelector ; (; Short_t ; i). inline . set index of variable used for discrimination at this node ; Definition at line 148 of file DecisionTreeNode.h. ◆ SetSeparationGain(). void TMVA::DecisionTreeNode::SetSeparationGain ; (; Float_t ; sep). inline . set the separation, or information gained BY this node's selection, if traininfo defined ; Definition at line 263 of file DecisionTreeNode.h. ◆ SetSeparationIndex(). void TMVA::DecisionTreeNode::SetSeparationIndex ; (; Float_t ; sep). inline . set the chosen index, measure of ""purity"" (separation between S and B) AT this node, if traininfo defined ; Definition at line 257 of file DecisionTreeNode.h. ◆ SetSubTreeR(). void TMVA::DecisionTreeNode::SetSubTreeR ; (; Double_t ; r). inline . set the resubstitution estimate, R(T_t), of the tree rooted at this node, if traininfo defined ; Definition at line 298 of file",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:36117,Modifiability,variab,variable,36117,"sionTreeNode::SetRight ; (; Node * ; r). inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 289 of file DecisionTreeNode.h. ◆ SetRMS(). void TMVA::DecisionTreeNode::SetRMS ; (; Float_t ; r). inline . set the RMS of the response of the node (for regression) ; Definition at line 179 of file DecisionTreeNode.h. ◆ SetSampleMax(). void TMVA::DecisionTreeNode::SetSampleMax ; (; UInt_t ; ivar, . Float_t ; xmax . ). set the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined ; Definition at line 449 of file DecisionTreeNode.cxx. ◆ SetSampleMin(). void TMVA::DecisionTreeNode::SetSampleMin ; (; UInt_t ; ivar, . Float_t ; xmin . ). set the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined ; Definition at line 438 of file DecisionTreeNode.cxx. ◆ SetSelector(). void TMVA::DecisionTreeNode::SetSelector ; (; Short_t ; i). inline . set index of variable used for discrimination at this node ; Definition at line 148 of file DecisionTreeNode.h. ◆ SetSeparationGain(). void TMVA::DecisionTreeNode::SetSeparationGain ; (; Float_t ; sep). inline . set the separation, or information gained BY this node's selection, if traininfo defined ; Definition at line 263 of file DecisionTreeNode.h. ◆ SetSeparationIndex(). void TMVA::DecisionTreeNode::SetSeparationIndex ; (; Float_t ; sep). inline . set the chosen index, measure of ""purity"" (separation between S and B) AT this node, if traininfo defined ; Definition at line 257 of file DecisionTreeNode.h. ◆ SetSubTreeR(). void TMVA::DecisionTreeNode::SetSubTreeR ; (; Double_t ; r). inline . set the resubstitution estimate, R(T_t), of the tree rooted at this node, if traininfo defined ; Definition at line 298 of file DecisionTreeNode.h. ◆ SetSumTarget(). void TMVA::DecisionTreeNode::SetSumTarget ; (; Float_t ; t). inline . set sum target, if traininfo defined ; Definition at line 330 of file DecisionTreeNode.h. ◆ SetSumTarget2(). void TM",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:38017,Modifiability,variab,variable,38017,"if traininfo defined ; Definition at line 330 of file DecisionTreeNode.h. ◆ SetSumTarget2(). void TMVA::DecisionTreeNode::SetSumTarget2 ; (; Float_t ; t2). inline . set sum target 2, if traininfo defined ; Definition at line 332 of file DecisionTreeNode.h. ◆ SetTerminal(). void TMVA::DecisionTreeNode::SetTerminal ; (; Bool_t ; s = kTRUE). inline . Definition at line 350 of file DecisionTreeNode.h. ◆ SetTmvaVersionCode(). void TMVA::DecisionTreeNode::SetTmvaVersionCode ; (; UInt_t ; code). static . Definition at line 553 of file DecisionTreeNode.cxx. ◆ Streamer(). virtual void TMVA::DecisionTreeNode::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::Node. ◆ StreamerNVirtual(). void TMVA::DecisionTreeNode::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 397 of file DecisionTreeNode.h. Member Data Documentation. ◆ fCutType. Bool_t TMVA::DecisionTreeNode::fCutType. protected . true: if event variable > cutValue ==> signal , false otherwise ; Definition at line 383 of file DecisionTreeNode.h. ◆ fCutValue. Float_t TMVA::DecisionTreeNode::fCutValue. protected . cut value applied on this node to discriminate bkg against sig ; Definition at line 382 of file DecisionTreeNode.h. ◆ fFisherCoeff. std::vector<Double_t> TMVA::DecisionTreeNode::fFisherCoeff. protected . the fisher coeff (offset at the last element) ; Definition at line 380 of file DecisionTreeNode.h. ◆ fgIsTraining. Bool_t TMVA::DecisionTreeNode::fgIsTraining = false. staticprotected . static variable to flag training phase in which we need fTrainInfo ; Definition at line 377 of file DecisionTreeNode.h. ◆ fgTmva_Version_Code. UInt_t TMVA::DecisionTreeNode::fgTmva_Version_Code = 0. staticprotected . set only when read from weightfile ; Definition at line 378 of file DecisionTreeNode.h. ◆ fIsTerminalNode. Bool_t TMVA::DecisionTreeNode::fIsTerminalNode. protected . ! flag to set node as terminal (i.e., without deleting its descendants) ; Definition at line 391 o",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:38584,Modifiability,variab,variable,38584," void TMVA::DecisionTreeNode::Streamer ; (; TBuffer & ; ). virtual . Reimplemented from TMVA::Node. ◆ StreamerNVirtual(). void TMVA::DecisionTreeNode::StreamerNVirtual ; (; TBuffer & ; ClassDef_StreamerNVirtual_b). inline . Definition at line 397 of file DecisionTreeNode.h. Member Data Documentation. ◆ fCutType. Bool_t TMVA::DecisionTreeNode::fCutType. protected . true: if event variable > cutValue ==> signal , false otherwise ; Definition at line 383 of file DecisionTreeNode.h. ◆ fCutValue. Float_t TMVA::DecisionTreeNode::fCutValue. protected . cut value applied on this node to discriminate bkg against sig ; Definition at line 382 of file DecisionTreeNode.h. ◆ fFisherCoeff. std::vector<Double_t> TMVA::DecisionTreeNode::fFisherCoeff. protected . the fisher coeff (offset at the last element) ; Definition at line 380 of file DecisionTreeNode.h. ◆ fgIsTraining. Bool_t TMVA::DecisionTreeNode::fgIsTraining = false. staticprotected . static variable to flag training phase in which we need fTrainInfo ; Definition at line 377 of file DecisionTreeNode.h. ◆ fgTmva_Version_Code. UInt_t TMVA::DecisionTreeNode::fgTmva_Version_Code = 0. staticprotected . set only when read from weightfile ; Definition at line 378 of file DecisionTreeNode.h. ◆ fIsTerminalNode. Bool_t TMVA::DecisionTreeNode::fIsTerminalNode. protected . ! flag to set node as terminal (i.e., without deleting its descendants) ; Definition at line 391 of file DecisionTreeNode.h. ◆ fNodeType. Int_t TMVA::DecisionTreeNode::fNodeType. protected . Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal. ; Definition at line 388 of file DecisionTreeNode.h. ◆ fPurity. Float_t TMVA::DecisionTreeNode::fPurity. protected . the node purity ; Definition at line 389 of file DecisionTreeNode.h. ◆ fResponse. Float_t TMVA::DecisionTreeNode::fResponse. protected . response value in case of regression ; Definition at line 386 of file DecisionTreeNode.h. ◆ fRMS. Float_t TMVA::DecisionTreeNode::fRMS. protected . response RMS of the",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:39782,Modifiability,variab,variable,39782,"DecisionTreeNode::fFisherCoeff. protected . the fisher coeff (offset at the last element) ; Definition at line 380 of file DecisionTreeNode.h. ◆ fgIsTraining. Bool_t TMVA::DecisionTreeNode::fgIsTraining = false. staticprotected . static variable to flag training phase in which we need fTrainInfo ; Definition at line 377 of file DecisionTreeNode.h. ◆ fgTmva_Version_Code. UInt_t TMVA::DecisionTreeNode::fgTmva_Version_Code = 0. staticprotected . set only when read from weightfile ; Definition at line 378 of file DecisionTreeNode.h. ◆ fIsTerminalNode. Bool_t TMVA::DecisionTreeNode::fIsTerminalNode. protected . ! flag to set node as terminal (i.e., without deleting its descendants) ; Definition at line 391 of file DecisionTreeNode.h. ◆ fNodeType. Int_t TMVA::DecisionTreeNode::fNodeType. protected . Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal. ; Definition at line 388 of file DecisionTreeNode.h. ◆ fPurity. Float_t TMVA::DecisionTreeNode::fPurity. protected . the node purity ; Definition at line 389 of file DecisionTreeNode.h. ◆ fResponse. Float_t TMVA::DecisionTreeNode::fResponse. protected . response value in case of regression ; Definition at line 386 of file DecisionTreeNode.h. ◆ fRMS. Float_t TMVA::DecisionTreeNode::fRMS. protected . response RMS of the regression node ; Definition at line 387 of file DecisionTreeNode.h. ◆ fSelector. Short_t TMVA::DecisionTreeNode::fSelector. protected . index of variable used in node selection (decision tree) ; Definition at line 384 of file DecisionTreeNode.h. ◆ fTrainInfo. DTNodeTrainingInfo* TMVA::DecisionTreeNode::fTrainInfo. mutableprotected . Definition at line 393 of file DecisionTreeNode.h. Libraries for TMVA::DecisionTreeNode:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/DecisionTreeNode.h; tmva/tmva/src/DecisionTreeNode.cxx. TMVADecisionTreeNode. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:31 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:2513,Security,validat,validation,2513,"ed ;  ; Double_t GetAlphaMinSubtree () const;  return the minimum alpha in the tree rooted at this node, or -1 if traininfo undefined ;  ; Double_t GetCC () const;  return CC, or -1 if traininfo undefined ;  ; Bool_t GetCutType (void) const;  return kTRUE: Cuts select signal, kFALSE: Cuts select bkg ;  ; Float_t GetCutValue (void) const;  return the cut value applied at this node ;  ; Double_t GetFisherCoeff (Int_t ivar) const;  get fisher coefficients ;  ; virtual DecisionTreeNode * GetLeft () const;  ; Float_t GetNBkgEvents (void) const;  return the sum of the backgr weights in the node, or -1 if traininfo undefined ;  ; Float_t GetNBkgEvents_unboosted (void) const;  return the sum of unboosted backgr weights in the node, or -1 if traininfo undefined ;  ; Float_t GetNBkgEvents_unweighted (void) const;  return the sum of unweighted backgr weights in the node, or -1 if traininfo undefined ;  ; Double_t GetNBValidation () const;  return number of background events from the pruning validation sample, or -1 if traininfo undefined ;  ; Float_t GetNEvents (void) const;  return the number of events that entered the node (during training), or -1 if traininfo undefined ;  ; Float_t GetNEvents_unboosted (void) const;  return the number of unboosted events that entered the node (during training), or -1 if traininfo undefined ;  ; Float_t GetNEvents_unweighted (void) const;  return the number of unweighted events that entered the node (during training), or -1 if traininfo undefined ;  ; UInt_t GetNFisherCoeff () const;  ; Double_t GetNodeR () const;  return the node resubstitution estimate, R(t), for Cost Complexity pruning, or -1 if traininfo undefined ;  ; Int_t GetNodeType (void) const;  return node type: 1 signal node, -1 bkg leave, 0 intermediate Node ;  ; Float_t GetNSigEvents (void) const;  return the sum of the signal weights in the node, or -1 if traininfo undefined ;  ; Float_t GetNSigEvents_unboosted (void) const;  return the sum of unboosted signal weights in the no",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:3691,Security,validat,validation,3691,"d ;  ; Float_t GetNEvents_unboosted (void) const;  return the number of unboosted events that entered the node (during training), or -1 if traininfo undefined ;  ; Float_t GetNEvents_unweighted (void) const;  return the number of unweighted events that entered the node (during training), or -1 if traininfo undefined ;  ; UInt_t GetNFisherCoeff () const;  ; Double_t GetNodeR () const;  return the node resubstitution estimate, R(t), for Cost Complexity pruning, or -1 if traininfo undefined ;  ; Int_t GetNodeType (void) const;  return node type: 1 signal node, -1 bkg leave, 0 intermediate Node ;  ; Float_t GetNSigEvents (void) const;  return the sum of the signal weights in the node, or -1 if traininfo undefined ;  ; Float_t GetNSigEvents_unboosted (void) const;  return the sum of unboosted signal weights in the node, or -1 if traininfo undefined ;  ; Float_t GetNSigEvents_unweighted (void) const;  ; Double_t GetNSValidation () const;  return number of signal events from the pruning validation sample, or -1 if traininfo undefined ;  ; Int_t GetNTerminal () const;  return number of terminal nodes in the subtree rooted here, or -1 if traininfo undefined ;  ; virtual DecisionTreeNode * GetParent () const;  ; Float_t GetPurity (void) const;  return S/(S+B) (purity) at this node (from training) ;  ; Float_t GetResponse (void) const;  return the response of the node (for regression) ;  ; virtual DecisionTreeNode * GetRight () const;  ; Float_t GetRMS (void) const;  return the RMS of the response of the node (for regression) ;  ; Float_t GetSampleMax (UInt_t ivar) const;  return the maximum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return 9999 ;  ; Float_t GetSampleMin (UInt_t ivar) const;  return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ;  ; Short_t GetSelector () const;  return index of variable us",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:7255,Security,validat,validation,7255,"(std::ostream &os) const;  printout of the node (can be read in with ReadDataRecord) ;  ; virtual void PrintRec (std::ostream &os) const;  recursively print the node and its daughters (--> print the 'tree') ;  ; void PrintRecPrune (std::ostream &os) const;  recursive printout of the node and its daughters ;  ; virtual void ReadAttributes (void *node, UInt_t tmva_Version_Code=262657);  ; virtual void ReadContent (std::stringstream &s);  reading attributes from tree node (well, was used in BinarySearchTree, and somehow I guess someone programmed it such that we need this in this tree too, although we don't..) ;  ; virtual Bool_t ReadDataRecord (std::istream &is, UInt_t tmva_Version_Code=262657);  Read the data block. ;  ; void ResetValidationData ();  temporary stored node values (number of events, etc.) that originate not from the training but from the validation data (used in pruning) ;  ; void SetAlpha (Double_t alpha);  set the critical point alpha, if traininfo defined ;  ; void SetAlphaMinSubtree (Double_t g);  set the minimum alpha in the tree rooted at this node, if traininfo defined ;  ; void SetCC (Double_t cc);  Set CC, if traininfo defined, otherwise Log Fatal. ;  ; void SetCutType (Bool_t t);  set true: if event variable > cutValue ==> signal , false otherwise ;  ; void SetCutValue (Float_t c);  set the cut value applied at this node ;  ; void SetFisherCoeff (Int_t ivar, Double_t coeff);  set fisher coefficients ;  ; virtual void SetLeft (Node *l);  ; void SetNBkgEvents (Float_t b);  set the sum of the backgr weights in the node, if traininfo defined ;  ; void SetNBkgEvents_unboosted (Float_t b);  set the sum of the unboosted backgr events in the node, if traininfo defined ;  ; void SetNBkgEvents_unweighted (Float_t b);  set the sum of the unweighted backgr events in the node, if traininfo defined ;  ; void SetNBValidation (Double_t b);  set number of background events from the pruning validation sample, if traininfo defined ;  ; void SetNEvents (Float_t n",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:8321,Security,validat,validation,8321,,MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:9484,Security,validat,validation,9484,"rCoeff (Int_t ivar, Double_t coeff);  set fisher coefficients ;  ; virtual void SetLeft (Node *l);  ; void SetNBkgEvents (Float_t b);  set the sum of the backgr weights in the node, if traininfo defined ;  ; void SetNBkgEvents_unboosted (Float_t b);  set the sum of the unboosted backgr events in the node, if traininfo defined ;  ; void SetNBkgEvents_unweighted (Float_t b);  set the sum of the unweighted backgr events in the node, if traininfo defined ;  ; void SetNBValidation (Double_t b);  set number of background events from the pruning validation sample, if traininfo defined ;  ; void SetNEvents (Float_t nev);  set the number of events that entered the node (during training), if traininfo defined ;  ; void SetNEvents_unboosted (Float_t nev);  set the number of unboosted events that entered the node (during training), if traininfo defined ;  ; void SetNEvents_unweighted (Float_t nev);  set the number of unweighted events that entered the node (during training), if traininfo defined ;  ; void SetNFisherCoeff (Int_t nvars);  ; void SetNodeR (Double_t r);  set the node resubstitution estimate, R(t), for Cost Complexity pruning, if traininfo defined ;  ; void SetNodeType (Int_t t);  set node type: 1 signal node, -1 bkg leave, 0 intermediate Node ;  ; void SetNSigEvents (Float_t s);  set the sum of the signal weights in the node, if traininfo defined ;  ; void SetNSigEvents_unboosted (Float_t s);  set the sum of the unboosted signal events in the node, if traininfo defined ;  ; void SetNSigEvents_unweighted (Float_t s);  set the sum of the unweighted signal events in the node, if traininfo defined ;  ; void SetNSValidation (Double_t s);  set number of signal events from the pruning validation sample, if traininfo defined ;  ; void SetNTerminal (Int_t n);  set number of terminal nodes in the subtree rooted here, if traininfo defined ;  ; virtual void SetParent (Node *p);  ; void SetPurity (void);  return the S/(S+B) (purity) for the node REM: even if nodes with purity 0.",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:19622,Security,validat,validation,19622,"virtual . Reimplemented from TMVA::Node.; Definition at line 283 of file DecisionTreeNode.h. ◆ GetNBkgEvents(). Float_t TMVA::DecisionTreeNode::GetNBkgEvents ; (; void ; ); const. inline . return the sum of the backgr weights in the node, or -1 if traininfo undefined ; Definition at line 233 of file DecisionTreeNode.h. ◆ GetNBkgEvents_unboosted(). Float_t TMVA::DecisionTreeNode::GetNBkgEvents_unboosted ; (; void ; ); const. inline . return the sum of unboosted backgr weights in the node, or -1 if traininfo undefined ; Definition at line 251 of file DecisionTreeNode.h. ◆ GetNBkgEvents_unweighted(). Float_t TMVA::DecisionTreeNode::GetNBkgEvents_unweighted ; (; void ; ); const. inline . return the sum of unweighted backgr weights in the node, or -1 if traininfo undefined ; Definition at line 242 of file DecisionTreeNode.h. ◆ GetNBValidation(). Double_t TMVA::DecisionTreeNode::GetNBValidation ; (; ); const. inline . return number of background events from the pruning validation sample, or -1 if traininfo undefined ; Definition at line 325 of file DecisionTreeNode.h. ◆ GetNEvents(). Float_t TMVA::DecisionTreeNode::GetNEvents ; (; void ; ); const. inline . return the number of events that entered the node (during training), or -1 if traininfo undefined ; Definition at line 236 of file DecisionTreeNode.h. ◆ GetNEvents_unboosted(). Float_t TMVA::DecisionTreeNode::GetNEvents_unboosted ; (; void ; ); const. inline . return the number of unboosted events that entered the node (during training), or -1 if traininfo undefined ; Definition at line 254 of file DecisionTreeNode.h. ◆ GetNEvents_unweighted(). Float_t TMVA::DecisionTreeNode::GetNEvents_unweighted ; (; void ; ); const. inline . return the number of unweighted events that entered the node (during training), or -1 if traininfo undefined ; Definition at line 245 of file DecisionTreeNode.h. ◆ GetNFisherCoeff(). UInt_t TMVA::DecisionTreeNode::GetNFisherCoeff ; (; ); const. inline . Definition at line 135 of file DecisionTreeN",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:21893,Security,validat,validation,21893,"::DecisionTreeNode::GetNodeType ; (; void ; ); const. inline . return node type: 1 signal node, -1 bkg leave, 0 intermediate Node ; Definition at line 165 of file DecisionTreeNode.h. ◆ GetNSigEvents(). Float_t TMVA::DecisionTreeNode::GetNSigEvents ; (; void ; ); const. inline . return the sum of the signal weights in the node, or -1 if traininfo undefined ; Definition at line 230 of file DecisionTreeNode.h. ◆ GetNSigEvents_unboosted(). Float_t TMVA::DecisionTreeNode::GetNSigEvents_unboosted ; (; void ; ); const. inline . return the sum of unboosted signal weights in the node, or -1 if traininfo undefined ; Definition at line 248 of file DecisionTreeNode.h. ◆ GetNSigEvents_unweighted(). Float_t TMVA::DecisionTreeNode::GetNSigEvents_unweighted ; (; void ; ); const. inline . Definition at line 239 of file DecisionTreeNode.h. ◆ GetNSValidation(). Double_t TMVA::DecisionTreeNode::GetNSValidation ; (; ); const. inline . return number of signal events from the pruning validation sample, or -1 if traininfo undefined ; Definition at line 327 of file DecisionTreeNode.h. ◆ GetNTerminal(). Int_t TMVA::DecisionTreeNode::GetNTerminal ; (; ); const. inline . return number of terminal nodes in the subtree rooted here, or -1 if traininfo undefined ; Definition at line 318 of file DecisionTreeNode.h. ◆ GetParent(). virtual DecisionTreeNode * TMVA::DecisionTreeNode::GetParent ; (; ); const. inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 285 of file DecisionTreeNode.h. ◆ GetPurity(). Float_t TMVA::DecisionTreeNode::GetPurity ; (; void ; ); const. inline . return S/(S+B) (purity) at this node (from training) ; Definition at line 168 of file DecisionTreeNode.h. ◆ GetResponse(). Float_t TMVA::DecisionTreeNode::GetResponse ; (; void ; ); const. inline . return the response of the node (for regression) ; Definition at line 176 of file DecisionTreeNode.h. ◆ GetRight(). virtual DecisionTreeNode * TMVA::DecisionTreeNode::GetRight ; (; ); const. inlinevirtual . Reimplemented ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:29523,Security,validat,validation,29523,"es(). void TMVA::DecisionTreeNode::ReadAttributes ; (; void * ; node, . UInt_t ; tmva_Version_Code = 262657 . ). virtual . Implements TMVA::Node.; Definition at line 458 of file DecisionTreeNode.cxx. ◆ ReadContent(). void TMVA::DecisionTreeNode::ReadContent ; (; std::stringstream & ; s). virtual . reading attributes from tree node (well, was used in BinarySearchTree, and somehow I guess someone programmed it such that we need this in this tree too, although we don't..) ; Implements TMVA::Node.; Definition at line 538 of file DecisionTreeNode.cxx. ◆ ReadDataRecord(). Bool_t TMVA::DecisionTreeNode::ReadDataRecord ; (; std::istream & ; is, . UInt_t ; tmva_Version_Code = 262657 . ). virtual . Read the data block. ; Implements TMVA::Node.; Definition at line 272 of file DecisionTreeNode.cxx. ◆ ResetValidationData(). void TMVA::DecisionTreeNode::ResetValidationData ; (; ). temporary stored node values (number of events, etc.) that originate not from the training but from the validation data (used in pruning) ; Definition at line 366 of file DecisionTreeNode.cxx. ◆ SetAlpha(). void TMVA::DecisionTreeNode::SetAlpha ; (; Double_t ; alpha). inline . set the critical point alpha, if traininfo defined ; Definition at line 306 of file DecisionTreeNode.h. ◆ SetAlphaMinSubtree(). void TMVA::DecisionTreeNode::SetAlphaMinSubtree ; (; Double_t ; g). inline . set the minimum alpha in the tree rooted at this node, if traininfo defined ; Definition at line 311 of file DecisionTreeNode.h. ◆ SetCC(). void TMVA::DecisionTreeNode::SetCC ; (; Double_t ; cc). Set CC, if traininfo defined, otherwise Log Fatal. ; Definition at line 404 of file DecisionTreeNode.cxx. ◆ SetCutType(). void TMVA::DecisionTreeNode::SetCutType ; (; Bool_t ; t). inline . set true: if event variable > cutValue ==> signal , false otherwise ; Definition at line 158 of file DecisionTreeNode.h. ◆ SetCutValue(). void TMVA::DecisionTreeNode::SetCutValue ; (; Float_t ; c). inline . set the cut value applied at this node ; Defi",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:31926,Security,validat,validation,31926,"ecisionTreeNode::SetLeft ; (; Node * ; l). inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 288 of file DecisionTreeNode.h. ◆ SetNBkgEvents(). void TMVA::DecisionTreeNode::SetNBkgEvents ; (; Float_t ; b). inline . set the sum of the backgr weights in the node, if traininfo defined ; Definition at line 188 of file DecisionTreeNode.h. ◆ SetNBkgEvents_unboosted(). void TMVA::DecisionTreeNode::SetNBkgEvents_unboosted ; (; Float_t ; b). inline . set the sum of the unboosted backgr events in the node, if traininfo defined ; Definition at line 206 of file DecisionTreeNode.h. ◆ SetNBkgEvents_unweighted(). void TMVA::DecisionTreeNode::SetNBkgEvents_unweighted ; (; Float_t ; b). inline . set the sum of the unweighted backgr events in the node, if traininfo defined ; Definition at line 197 of file DecisionTreeNode.h. ◆ SetNBValidation(). void TMVA::DecisionTreeNode::SetNBValidation ; (; Double_t ; b). inline . set number of background events from the pruning validation sample, if traininfo defined ; Definition at line 321 of file DecisionTreeNode.h. ◆ SetNEvents(). void TMVA::DecisionTreeNode::SetNEvents ; (; Float_t ; nev). inline . set the number of events that entered the node (during training), if traininfo defined ; Definition at line 191 of file DecisionTreeNode.h. ◆ SetNEvents_unboosted(). void TMVA::DecisionTreeNode::SetNEvents_unboosted ; (; Float_t ; nev). inline . set the number of unboosted events that entered the node (during training), if traininfo defined ; Definition at line 209 of file DecisionTreeNode.h. ◆ SetNEvents_unweighted(). void TMVA::DecisionTreeNode::SetNEvents_unweighted ; (; Float_t ; nev). inline . set the number of unweighted events that entered the node (during training), if traininfo defined ; Definition at line 200 of file DecisionTreeNode.h. ◆ SetNFisherCoeff(). void TMVA::DecisionTreeNode::SetNFisherCoeff ; (; Int_t ; nvars). inline . Definition at line 134 of file DecisionTreeNode.h. ◆ SetNodeR(). void TMVA::DecisionTreeNo",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:34170,Security,validat,validation,34170," ; (; Int_t ; t). inline . set node type: 1 signal node, -1 bkg leave, 0 intermediate Node ; Definition at line 163 of file DecisionTreeNode.h. ◆ SetNSigEvents(). void TMVA::DecisionTreeNode::SetNSigEvents ; (; Float_t ; s). inline . set the sum of the signal weights in the node, if traininfo defined ; Definition at line 185 of file DecisionTreeNode.h. ◆ SetNSigEvents_unboosted(). void TMVA::DecisionTreeNode::SetNSigEvents_unboosted ; (; Float_t ; s). inline . set the sum of the unboosted signal events in the node, if traininfo defined ; Definition at line 203 of file DecisionTreeNode.h. ◆ SetNSigEvents_unweighted(). void TMVA::DecisionTreeNode::SetNSigEvents_unweighted ; (; Float_t ; s). inline . set the sum of the unweighted signal events in the node, if traininfo defined ; Definition at line 194 of file DecisionTreeNode.h. ◆ SetNSValidation(). void TMVA::DecisionTreeNode::SetNSValidation ; (; Double_t ; s). inline . set number of signal events from the pruning validation sample, if traininfo defined ; Definition at line 323 of file DecisionTreeNode.h. ◆ SetNTerminal(). void TMVA::DecisionTreeNode::SetNTerminal ; (; Int_t ; n). inline . set number of terminal nodes in the subtree rooted here, if traininfo defined ; Definition at line 316 of file DecisionTreeNode.h. ◆ SetParent(). virtual void TMVA::DecisionTreeNode::SetParent ; (; Node * ; p). inlinevirtual . Reimplemented from TMVA::Node.; Definition at line 290 of file DecisionTreeNode.h. ◆ SetPurity(). void TMVA::DecisionTreeNode::SetPurity ; (; void ; ). return the S/(S+B) (purity) for the node REM: even if nodes with purity 0.01 are very PURE background nodes, they still get a small value of the purity. ; Definition at line 191 of file DecisionTreeNode.cxx. ◆ SetResponse(). void TMVA::DecisionTreeNode::SetResponse ; (; Float_t ; r). inline . set the response of the node (for regression) ; Definition at line 173 of file DecisionTreeNode.h. ◆ SetRight(). virtual void TMVA::DecisionTreeNode::SetRight ; (; Node *",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:5364,Testability,test,test,5364,"this node, if traininfo defined, otherwise Log Fatal and return 9999 ;  ; Float_t GetSampleMin (UInt_t ivar) const;  return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ;  ; Short_t GetSelector () const;  return index of variable used for discrimination at this node ;  ; Float_t GetSeparationGain (void) const;  return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ;  ; Float_t GetSeparationIndex (void) const;  return the separation index AT this node, or 0 if traininfo undefined ;  ; Double_t GetSubTreeR () const;  return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ;  ; Float_t GetSumTarget () const;  return sum target, or -9999 if traininfo undefined ;  ; Float_t GetSumTarget2 () const;  return sum target 2, or -9999 if traininfo undefined ;  ; virtual Bool_t GoesLeft (const Event &) const;  test event if it descends the tree at this node to the left ;  ; virtual Bool_t GoesRight (const Event &) const;  test event if it descends the tree at this node to the right ;  ; void IncrementNBkgEvents (Float_t b);  increment the sum of the backgr weights in the node, if traininfo defined ;  ; void IncrementNBkgEvents_unweighted ();  increment the sum of the backgr weights in the node, if traininfo defined ;  ; void IncrementNEvents (Float_t nev);  ; void IncrementNEvents_unweighted ();  increment the number of events that entered the node (during training), if traininfo defined ;  ; void IncrementNSigEvents (Float_t s);  increment the sum of the signal weights in the node, if traininfo defined ;  ; void IncrementNSigEvents_unweighted ();  increment the sum of the signal weights in the node, if traininfo defined ;  ; virtual TClass * IsA () const;  ; Bool_t IsTerminal () const;  flag indicates whether this node is terminal ;  ; virtual void Print (std::ostream &os) const;  print the no",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:5478,Testability,test,test,5478,";  return the minimum of variable ivar from the training sample that pass/end up in this node, if traininfo defined, otherwise Log Fatal and return -9999 ;  ; Short_t GetSelector () const;  return index of variable used for discrimination at this node ;  ; Float_t GetSeparationGain (void) const;  return the gain in separation obtained by this node's selection, or -1 if traininfo undefined ;  ; Float_t GetSeparationIndex (void) const;  return the separation index AT this node, or 0 if traininfo undefined ;  ; Double_t GetSubTreeR () const;  return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ;  ; Float_t GetSumTarget () const;  return sum target, or -9999 if traininfo undefined ;  ; Float_t GetSumTarget2 () const;  return sum target 2, or -9999 if traininfo undefined ;  ; virtual Bool_t GoesLeft (const Event &) const;  test event if it descends the tree at this node to the left ;  ; virtual Bool_t GoesRight (const Event &) const;  test event if it descends the tree at this node to the right ;  ; void IncrementNBkgEvents (Float_t b);  increment the sum of the backgr weights in the node, if traininfo defined ;  ; void IncrementNBkgEvents_unweighted ();  increment the sum of the backgr weights in the node, if traininfo defined ;  ; void IncrementNEvents (Float_t nev);  ; void IncrementNEvents_unweighted ();  increment the number of events that entered the node (during training), if traininfo defined ;  ; void IncrementNSigEvents (Float_t s);  increment the sum of the signal weights in the node, if traininfo defined ;  ; void IncrementNSigEvents_unweighted ();  increment the sum of the signal weights in the node, if traininfo defined ;  ; virtual TClass * IsA () const;  ; Bool_t IsTerminal () const;  flag indicates whether this node is terminal ;  ; virtual void Print (std::ostream &os) const;  print the node ;  ; void PrintPrune (std::ostream &os) const;  printout of the node (can be read in with ReadDataRecord) ;  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:25326,Testability,test,test,25326," node, or 0 if traininfo undefined ; Definition at line 260 of file DecisionTreeNode.h. ◆ GetSubTreeR(). Double_t TMVA::DecisionTreeNode::GetSubTreeR ; (; ); const. inline . return the resubstitution estimate, R(T_t), of the tree rooted at this node, or -1 if traininfo undefined ; Definition at line 300 of file DecisionTreeNode.h. ◆ GetSumTarget(). Float_t TMVA::DecisionTreeNode::GetSumTarget ; (; ); const. inline . return sum target, or -9999 if traininfo undefined ; Definition at line 340 of file DecisionTreeNode.h. ◆ GetSumTarget2(). Float_t TMVA::DecisionTreeNode::GetSumTarget2 ; (; ); const. inline . return sum target 2, or -9999 if traininfo undefined ; Definition at line 342 of file DecisionTreeNode.h. ◆ GetTmvaVersionCode(). UInt_t TMVA::DecisionTreeNode::GetTmvaVersionCode ; (; ). static . Definition at line 561 of file DecisionTreeNode.cxx. ◆ GoesLeft(). Bool_t TMVA::DecisionTreeNode::GoesLeft ; (; const Event & ; e); const. virtual . test event if it descends the tree at this node to the left ; Implements TMVA::Node.; Definition at line 179 of file DecisionTreeNode.cxx. ◆ GoesRight(). Bool_t TMVA::DecisionTreeNode::GoesRight ; (; const Event & ; e); const. virtual . test event if it descends the tree at this node to the right ; Implements TMVA::Node.; Definition at line 155 of file DecisionTreeNode.cxx. ◆ IncrementNBkgEvents(). void TMVA::DecisionTreeNode::IncrementNBkgEvents ; (; Float_t ; b). inline . increment the sum of the backgr weights in the node, if traininfo defined ; Definition at line 215 of file DecisionTreeNode.h. ◆ IncrementNBkgEvents_unweighted(). void TMVA::DecisionTreeNode::IncrementNBkgEvents_unweighted ; (; ). inline . increment the sum of the backgr weights in the node, if traininfo defined ; Definition at line 224 of file DecisionTreeNode.h. ◆ IncrementNEvents(). void TMVA::DecisionTreeNode::IncrementNEvents ; (; Float_t ; nev). inline . Definition at line 218 of file DecisionTreeNode.h. ◆ IncrementNEvents_unweighted(). void TMVA::Dec",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:25563,Testability,test,test,25563,"t this node, or -1 if traininfo undefined ; Definition at line 300 of file DecisionTreeNode.h. ◆ GetSumTarget(). Float_t TMVA::DecisionTreeNode::GetSumTarget ; (; ); const. inline . return sum target, or -9999 if traininfo undefined ; Definition at line 340 of file DecisionTreeNode.h. ◆ GetSumTarget2(). Float_t TMVA::DecisionTreeNode::GetSumTarget2 ; (; ); const. inline . return sum target 2, or -9999 if traininfo undefined ; Definition at line 342 of file DecisionTreeNode.h. ◆ GetTmvaVersionCode(). UInt_t TMVA::DecisionTreeNode::GetTmvaVersionCode ; (; ). static . Definition at line 561 of file DecisionTreeNode.cxx. ◆ GoesLeft(). Bool_t TMVA::DecisionTreeNode::GoesLeft ; (; const Event & ; e); const. virtual . test event if it descends the tree at this node to the left ; Implements TMVA::Node.; Definition at line 179 of file DecisionTreeNode.cxx. ◆ GoesRight(). Bool_t TMVA::DecisionTreeNode::GoesRight ; (; const Event & ; e); const. virtual . test event if it descends the tree at this node to the right ; Implements TMVA::Node.; Definition at line 155 of file DecisionTreeNode.cxx. ◆ IncrementNBkgEvents(). void TMVA::DecisionTreeNode::IncrementNBkgEvents ; (; Float_t ; b). inline . increment the sum of the backgr weights in the node, if traininfo defined ; Definition at line 215 of file DecisionTreeNode.h. ◆ IncrementNBkgEvents_unweighted(). void TMVA::DecisionTreeNode::IncrementNBkgEvents_unweighted ; (; ). inline . increment the sum of the backgr weights in the node, if traininfo defined ; Definition at line 224 of file DecisionTreeNode.h. ◆ IncrementNEvents(). void TMVA::DecisionTreeNode::IncrementNEvents ; (; Float_t ; nev). inline . Definition at line 218 of file DecisionTreeNode.h. ◆ IncrementNEvents_unweighted(). void TMVA::DecisionTreeNode::IncrementNEvents_unweighted ; (; ). inline . increment the number of events that entered the node (during training), if traininfo defined ; Definition at line 227 of file DecisionTreeNode.h. ◆ IncrementNSigEvents(). void T",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:1308,Usability,clear,clear,1308,") ;  ; void AddToSumTarget (Float_t t);  add to sum target, if traininfo defined ;  ; void AddToSumTarget2 (Float_t t2);  add to sum target 2, if traininfo defined ;  ; void ClearNodeAndAllDaughters ();  clear the nodes (their S/N, Nevents etc), just keep the structure of the tree ;  ; virtual Node * CreateNode () const;  ; Double_t GetAlpha () const;  return the critical point alpha, or -1 if traininfo undefined ;  ; Double_t GetAlphaMinSubtree () const;  return the minimum alpha in the tree rooted at this node, or -1 if traininfo undefined ;  ; Double_t GetCC () const;  return CC, or -1 if traininfo undefined ;  ; Bool_t GetCutType (void) const;  return kTRUE: Cuts select signal, kFALSE: Cuts select bkg ;  ; Float_t GetCutValue (void) const;  return the cut value applied at this node ;  ; Double_t GetFisherCoeff (Int_t ivar) const;  get fisher coefficients ;  ; virtual DecisionTreeN",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html:16868,Usability,clear,clear,16868,"n at line 529 of file DecisionTreeNode.cxx. ◆ AddToSumTarget(). void TMVA::DecisionTreeNode::AddToSumTarget ; (; Float_t ; t). inline . add to sum target, if traininfo defined ; Definition at line 335 of file DecisionTreeNode.h. ◆ AddToSumTarget2(). void TMVA::DecisionTreeNode::AddToSumTarget2 ; (; Float_t ; t2). inline . add to sum target 2, if traininfo defined ; Definition at line 337 of file DecisionTreeNode.h. ◆ Class(). static TClass * TMVA::DecisionTreeNode::Class ; (; ). static . ReturnsTClass describing this class . ◆ Class_Name(). static const char * TMVA::DecisionTreeNode::Class_Name ; (; ). static . ReturnsName of this class . ◆ Class_Version(). static constexpr Version_t TMVA::DecisionTreeNode::Class_Version ; (; ). inlinestaticconstexpr . ReturnsVersion of this class ; Definition at line 397 of file DecisionTreeNode.h. ◆ ClearNodeAndAllDaughters(). void TMVA::DecisionTreeNode::ClearNodeAndAllDaughters ; (; ). clear the nodes (their S/N, Nevents etc), just keep the structure of the tree ; Definition at line 346 of file DecisionTreeNode.cxx. ◆ CreateNode(). virtual Node * TMVA::DecisionTreeNode::CreateNode ; (; ); const. inlinevirtual . Implements TMVA::Node.; Definition at line 132 of file DecisionTreeNode.h. ◆ DeclFileName(). static const char * TMVA::DecisionTreeNode::DeclFileName ; (; ). inlinestatic . ReturnsName of the file containing the class declaration ; Definition at line 397 of file DecisionTreeNode.h. ◆ GetAlpha(). Double_t TMVA::DecisionTreeNode::GetAlpha ; (; ); const. inline . return the critical point alpha, or -1 if traininfo undefined ; Definition at line 308 of file DecisionTreeNode.h. ◆ GetAlphaMinSubtree(). Double_t TMVA::DecisionTreeNode::GetAlphaMinSubtree ; (; ); const. inline . return the minimum alpha in the tree rooted at this node, or -1 if traininfo undefined ; Definition at line 313 of file DecisionTreeNode.h. ◆ GetCC(). Double_t TMVA::DecisionTreeNode::GetCC ; (; ); const. inline . return CC, or -1 if traininfo undefined ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DecisionTreeNode.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DecisionTreeNode.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1749,Availability,error,error,1749,"Type _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3834,Availability,error,error,3834,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9261,Availability,error,error,9261,"; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::ClassificationSettings::m_input. Definition at line 995 of file NeuralNet.h. ◆ m_output. std::vector<double> TMVA::DNN::ClassificationSettings::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4466,Deployability,configurat,configuration,4466,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:7458,Deployability,update,update,7458,"tructor Documentation. ◆ ClassificationSettings(). TMVA::DNN::ClassificationSettings::ClassificationSettings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . EnumRegularization ; _regularization = EnumRegularization::NONE, . size_t ; _scaleToNumEvents = 0, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _useMultithreading = true . ). inline . c'tor ; Definition at line 901 of file NeuralNet.h. ◆ ~ClassificationSettings(). virtual TMVA::DNN::ClassificationSettings::~ClassificationSettings ; (; ). inlinevirtual . d'tor ; Definition at line 924 of file NeuralNet.h. Member Function Documentation. ◆ endTestCycle(). void TMVA::DNN::ClassificationSettings::endTestCycle ; (; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 326 of file NeuralNet.cxx. ◆ endTrainCycle(). void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:7737,Deployability,update,update,7737,"ation ; _regularization = EnumRegularization::NONE, . size_t ; _scaleToNumEvents = 0, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _useMultithreading = true . ). inline . c'tor ; Definition at line 901 of file NeuralNet.h. ◆ ~ClassificationSettings(). virtual TMVA::DNN::ClassificationSettings::~ClassificationSettings ; (; ). inlinevirtual . d'tor ; Definition at line 924 of file NeuralNet.h. Member Function Documentation. ◆ endTestCycle(). void TMVA::DNN::ClassificationSettings::endTestCycle ; (; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 326 of file NeuralNet.cxx. ◆ endTrainCycle(). void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virt",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8563,Deployability,update,update,8563,"void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::Classif",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8839,Deployability,update,update,8839,"tation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Def",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9404,Deployability,update,update,9404,"TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::ClassificationSettings::m_input. Definition at line 995 of file NeuralNet.h. ◆ m_output. std::vector<double> TMVA::DNN::ClassificationSettings::m_output. Definition at line 996 of file NeuralNet.h. ◆ m_pResultPatternContainer. std::vector<Pattern>* TMVA::DNN::ClassificationSettings::m_pResultPatternContainer. Definition at line 1009 of file NeuralNe",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1305,Energy Efficiency,monitor,monitoring,1305,"unction signatures. ; contains additional settings if the DNN problem is classification ; Definition at line 894 of file NeuralNet.h. Public Member Functions;  ClassificationSettings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, EnumRegularization _regularization=EnumRegularization::NONE, size_t _scaleToNumEvents=0, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1452,Energy Efficiency,monitor,monitoring,1452,"unction signatures. ; contains additional settings if the DNN problem is classification ; Definition at line 894 of file NeuralNet.h. Public Member Functions;  ClassificationSettings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, EnumRegularization _regularization=EnumRegularization::NONE, size_t _scaleToNumEvents=0, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1696,Energy Efficiency,monitor,monitoring,1696,"Type _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:2403,Energy Efficiency,monitor,monitoring,2403,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:2483,Energy Efficiency,monitor,monitoring,2483,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:2589,Energy Efficiency,monitor,monitoring,2589,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:2686,Energy Efficiency,monitor,monitoring,2686,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:2966,Energy Efficiency,monitor,monitoring,2966,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3095,Energy Efficiency,monitor,monitoring,3095,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3307,Energy Efficiency,monitor,monitoring,3307,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3470,Energy Efficiency,monitor,monitoring,3470,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4043,Energy Efficiency,monitor,monitoring,4043,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4143,Energy Efficiency,monitor,monitoring,4143,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4578,Energy Efficiency,monitor,monitoring,4578,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:6111,Energy Efficiency,monitor,monitoring,6111,"ise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::Settings; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::ClassificationSettings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor &",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:7470,Energy Efficiency,monitor,monitoring,7470,"tructor Documentation. ◆ ClassificationSettings(). TMVA::DNN::ClassificationSettings::ClassificationSettings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . EnumRegularization ; _regularization = EnumRegularization::NONE, . size_t ; _scaleToNumEvents = 0, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _useMultithreading = true . ). inline . c'tor ; Definition at line 901 of file NeuralNet.h. ◆ ~ClassificationSettings(). virtual TMVA::DNN::ClassificationSettings::~ClassificationSettings ; (; ). inlinevirtual . d'tor ; Definition at line 924 of file NeuralNet.h. Member Function Documentation. ◆ endTestCycle(). void TMVA::DNN::ClassificationSettings::endTestCycle ; (; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 326 of file NeuralNet.cxx. ◆ endTrainCycle(). void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:7749,Energy Efficiency,monitor,monitoring,7749,"ation ; _regularization = EnumRegularization::NONE, . size_t ; _scaleToNumEvents = 0, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _useMultithreading = true . ). inline . c'tor ; Definition at line 901 of file NeuralNet.h. ◆ ~ClassificationSettings(). virtual TMVA::DNN::ClassificationSettings::~ClassificationSettings ; (; ). inlinevirtual . d'tor ; Definition at line 924 of file NeuralNet.h. Member Function Documentation. ◆ endTestCycle(). void TMVA::DNN::ClassificationSettings::endTestCycle ; (; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 326 of file NeuralNet.cxx. ◆ endTrainCycle(). void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virt",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8090,Energy Efficiency,monitor,monitoring,8090,"assificationSettings(). virtual TMVA::DNN::ClassificationSettings::~ClassificationSettings ; (; ). inlinevirtual . d'tor ; Definition at line 924 of file NeuralNet.h. Member Function Documentation. ◆ endTestCycle(). void TMVA::DNN::ClassificationSettings::endTestCycle ; (; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 326 of file NeuralNet.cxx. ◆ endTrainCycle(). void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented fro",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8346,Energy Efficiency,monitor,monitoring,8346,"ificationSettings::endTestCycle ; (; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 326 of file NeuralNet.cxx. ◆ endTrainCycle(). void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to b",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8575,Energy Efficiency,monitor,monitoring,8575,"void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::Classif",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8851,Energy Efficiency,monitor,monitoring,8851,"tation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Def",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9069,Energy Efficiency,monitor,monitoring,9069,"toring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::Classif",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9416,Energy Efficiency,monitor,monitoring,9416,"TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::ClassificationSettings::m_input. Definition at line 995 of file NeuralNet.h. ◆ m_output. std::vector<double> TMVA::DNN::ClassificationSettings::m_output. Definition at line 996 of file NeuralNet.h. ◆ m_pResultPatternContainer. std::vector<Pattern>* TMVA::DNN::ClassificationSettings::m_pResultPatternContainer. Definition at line 1009 of file NeuralNe",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1896,Modifiability,inherit,inherited,1896,"void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bin",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4466,Modifiability,config,configuration,4466,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:5338,Modifiability,inherit,inherited,5338,"ise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::Settings; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::ClassificationSettings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor &",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:6179,Modifiability,inherit,inherited,6179,"eights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::Settings; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::ClassificationSettings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ ClassificationSettings(). TMVA::DNN::ClassificationSettings::ClassificationSettings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . EnumRegularization ; _regularization = EnumRegularization::NONE, . size_t ; _scaleToNumEvents = 0, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _useMultithreading = true . ). inline . c'tor ; Definition at line 901 of file NeuralNet.h. ◆ ~ClassificationSettings(). virtual TMVA::DNN::ClassificationSettings::~ClassificationSettings ; (; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1536,Testability,test,test,1536,"unction signatures. ; contains additional settings if the DNN problem is classification ; Definition at line 894 of file NeuralNet.h. Public Member Functions;  ClassificationSettings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, EnumRegularization _regularization=EnumRegularization::NONE, size_t _scaleToNumEvents=0, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1664,Testability,test,testIteration,1664,"Type _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1711,Testability,log,loggging,1711,"Type _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1730,Testability,test,testSample,1730,"Type _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:1848,Testability,test,test,1848,"Type _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _useMultithreading=true);  c'tor ;  ; virtual ~ClassificationSettings ();  d'tor ;  ; virtual void endTestCycle ();  action to be done when the training cycle is ended (e.g. ;  ; void endTrainCycle (double);  action to be done when the training cycle is ended (e.g. ;  ; void setResultComputation (std::string _fileNameNetConfig, std::string _fileNameResult, std::vector< Pattern > *_resultPatternContainer);  preparation for monitoring output ;  ; void setWeightSums (double sumOfSigWeights, double sumOfBkgWeights);  set the weight sums to be scaled to (preparations for monitoring output) ;  ; virtual void startTestCycle ();  action to be done when the test cycle is started (e.g. ;  ; void startTrainCycle ();  action to be done when the training cycle is started (e.g. ;  ; void testIteration ();  callback for monitoring and loggging ;  ; void testSample (double error, double output, double target, double weight);  action to be done after the computation of a test sample (e.g. ;  ;  Public Member Functions inherited from TMVA::DNN::Settings;  Settings (TString name, size_t _convergenceSteps=15, size_t _batchSize=10, size_t _testRepetitions=7, double _factorWeightDecay=1e-5, TMVA::DNN::EnumRegularization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:2701,Testability,log,logging,2701,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3322,Testability,log,logging,3322,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3589,Testability,test,testError,3589,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4717,Testability,test,testRepetitions,4717,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4761,Testability,test,test,4761,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:4771,Testability,test,tested,4771,"const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8533,Testability,test,test,8533," Definition at line 326 of file NeuralNet.cxx. ◆ endTrainCycle(). void TMVA::DNN::ClassificationSettings::endTrainCycle ; (; double ; ). virtual . action to be done when the training cycle is ended (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 296 of file NeuralNet.cxx. ◆ setResultComputation(). void TMVA::DNN::ClassificationSettings::setResultComputation ; (; std::string ; _fileNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member D",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:8961,Testability,test,testIteration,8961,"eNameNetConfig, . std::string ; _fileNameResult, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::Classific",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9018,Testability,test,testIteration,9018,"Result, . std::vector< Pattern > * ; _resultPatternContainer . ). preparation for monitoring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definit",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9084,Testability,log,loggging,9084,"toring output ; Definition at line 520 of file NeuralNet.cxx. ◆ setWeightSums(). void TMVA::DNN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::Classif",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9182,Testability,test,testSample,9182,"NN::ClassificationSettings::setWeightSums ; (; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::ClassificationSettings::m_input. Definition at line 995 of file NeuralNet.h. ◆ m_output. std::vecto",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9236,Testability,test,testSample,9236,"; double ; sumOfSigWeights, . double ; sumOfBkgWeights . ). set the weight sums to be scaled to (preparations for monitoring output) ; Definition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::ClassificationSettings::m_input. Definition at line 995 of file NeuralNet.h. ◆ m_output. std::vector<double> TMVA::DNN::ClassificationSettings::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:9384,Testability,test,test,9384,"efinition at line 512 of file NeuralNet.cxx. ◆ startTestCycle(). void TMVA::DNN::ClassificationSettings::startTestCycle ; (; ). virtual . action to be done when the test cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 316 of file NeuralNet.cxx. ◆ startTrainCycle(). void TMVA::DNN::ClassificationSettings::startTrainCycle ; (; ). virtual . action to be done when the training cycle is started (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 281 of file NeuralNet.cxx. ◆ testIteration(). void TMVA::DNN::ClassificationSettings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented from TMVA::DNN::Settings.; Definition at line 930 of file NeuralNet.h. ◆ testSample(). void TMVA::DNN::ClassificationSettings::testSample ; (; double ; error, . double ; output, . double ; target, . double ; weight . ). virtual . action to be done after the computation of a test sample (e.g. ; update some monitoring output) ; Reimplemented from TMVA::DNN::Settings.; Definition at line 304 of file NeuralNet.cxx. Member Data Documentation. ◆ m_ams. std::vector<double> TMVA::DNN::ClassificationSettings::m_ams. Definition at line 1000 of file NeuralNet.h. ◆ m_cutValue. double TMVA::DNN::ClassificationSettings::m_cutValue. Definition at line 1008 of file NeuralNet.h. ◆ m_fileNameNetConfig. std::string TMVA::DNN::ClassificationSettings::m_fileNameNetConfig. Definition at line 1011 of file NeuralNet.h. ◆ m_fileNameResult. std::string TMVA::DNN::ClassificationSettings::m_fileNameResult. Definition at line 1010 of file NeuralNet.h. ◆ m_input. std::vector<double> TMVA::DNN::ClassificationSettings::m_input. Definition at line 995 of file NeuralNet.h. ◆ m_output. std::vector<double> TMVA::DNN::ClassificationSettings::m_output. Definition at line 996 of file NeuralNet.h. ◆ m_pResultPatternContainer. std::vector<Pattern>* TMVA::DNN::ClassificationSetting",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:2553,Usability,clear,clear,2553,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3651,Usability,learn,learningRate,3651,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:3683,Usability,learn,learning,3683,"rization _regularization=TMVA::DNN::EnumRegularization::NONE, MinimizerType _eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:5912,Usability,progress bar,progress bar,5912,"ise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::Settings; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::ClassificationSettings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor &",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html:5998,Usability,progress bar,progress bar,5998,"ise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTraining ();  ; size_t testRepetitions () const;  how often is the test data tested ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; std::vector< double > m_ams;  ; double m_cutValue;  ; std::string m_fileNameNetConfig;  ; std::string m_fileNameResult;  ; std::vector< double > m_input;  ; std::vector< double > m_output;  ; std::vector< Pattern > * m_pResultPatternContainer;  ; size_t m_scaleToNumEvents;  ; std::vector< double > m_significances;  ; double m_sumOfBkgWeights;  ; double m_sumOfSigWeights;  ; std::vector< double > m_targets;  ; std::vector< double > m_weights;  ;  Public Attributes inherited from TMVA::DNN::Settings; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::Settings; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::ClassificationSettings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor &",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1ClassificationSettings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:8613,Modifiability,layers,layers,8613," applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 81 of file NeuralNet.cxx. ◆ LayerData() [2/6]. TMVA::DNN::LayerData::LayerData ; (; size_t ; inputSize). c'tor of LayerData ; C'tor of LayerData for the input layer; Parameters. inputSizeinput size of this layer . Definition at line 68 of file NeuralNet.cxx. ◆ ~LayerData(). TMVA::DNN::LayerData::~LayerData ; (; ). inline . Definition at line 471 of file NeuralNet.h. ◆ LayerData() [3/6]. TMVA::DNN::LayerData::LayerData ; (; size_t ; size, . const_iterator_type ; itWeightBegin, . iterator_type ; itGradientBegin, . std::shared_ptr< std::function< double(double)> > ; activationFunction, . std::shared_ptr< std::function< double(double)> > ; inverseActivationFunction, . ModeOutputValues ; eModeOutput = ModeOutputValues::DIRECT . ). c'tor of LayerData ; C'tor of LayerData for all layers which are not the input layer; Used during the training of the DNN; Parameters. sizesize of the layer ; itWeightBeginindicates the start of the weights for this layer on the weight vector ; itGradientBeginindicates the start of the gradients for this layer on the gradient vector ; activationFunctionindicates activation functions for this layer ; inverseActivationFunctionindicates the inverse activation functions for this layer ; eModeOutputindicates a potential tranformation of the output values before further computation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 97 of file NeuralNet.cxx. ◆ LayerData() [4/6]. TMVA::DNN::LayerData::LayerData ; (; size_t ; size, . const_iterator_type ; itWeightBegin, . std::shared_ptr< std::function< double(double)> > ; activationFunction, . ModeOutputValues ; eModeO",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:9718,Modifiability,layers,layers,9718,"ndicates the start of the weights for this layer on the weight vector ; itGradientBeginindicates the start of the gradients for this layer on the gradient vector ; activationFunctionindicates activation functions for this layer ; inverseActivationFunctionindicates the inverse activation functions for this layer ; eModeOutputindicates a potential tranformation of the output values before further computation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 97 of file NeuralNet.cxx. ◆ LayerData() [4/6]. TMVA::DNN::LayerData::LayerData ; (; size_t ; size, . const_iterator_type ; itWeightBegin, . std::shared_ptr< std::function< double(double)> > ; activationFunction, . ModeOutputValues ; eModeOutput = ModeOutputValues::DIRECT . ). c'tor of LayerData ; C'tor of LayerData for all layers which are not the input layer; Used during the application of the DNN; Parameters. sizesize of the layer ; itWeightBeginindicates the start of the weights for this layer on the weight vector ; activationFunctionindicates the activation function for this layer ; eModeOutputindicates a potential tranformation of the output values before further computation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 122 of file NeuralNet.cxx. ◆ LayerData() [5/6]. TMVA::DNN::LayerData::LayerData ; (; const LayerData & ; other). inline . copy c'tor of LayerData ; Definition at line 515 of file NeuralNet.h. ◆ LayerData() [6/6]. TMVA::DNN::LayerData::LayerData ; (; LayerData && ; other). inline . move c'tor of LayerData ; Definition at line 538 of file NeuralNet.h. Member Function Documentation. ◆ activationFunction(). std:",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:1929,Usability,clear,clear,1929,"ember Functions;  LayerData (const LayerData &other);  copy c'tor of LayerData ;  ;  LayerData (const_iterator_type itInputBegin, const_iterator_type itInputEnd, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (LayerData &&other);  move c'tor of LayerData ;  ;  LayerData (size_t inputSize);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, iterator_type itGradientBegin, std::shared_ptr< std::function< double(double)> > activationFunction, std::shared_ptr< std::function< double(double)> > inverseActivationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, std::shared_ptr< std::function< double(double)> > activationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  ~LayerData ();  ; std::shared_ptr< std::function< double(double)> > activationFunction () const;  ; void clear ();  clear the values and the deltas ;  ; void clearDropOut ();  clear the drop-out-data for this layer ;  ; iterator_type deltasBegin ();  returns iterator to the begin of the deltas (back-propagation) ;  ; const_iterator_type deltasBegin () const;  returns const iterator to the begin of the deltas (back-propagation) ;  ; iterator_type deltasEnd ();  returns iterator to the end of the deltas (back-propagation) ;  ; const_iterator_type deltasEnd () const;  returns const iterator to the end of the deltas (back-propagation) ;  ; const_dropout_iterator dropOut () const;  return the begin of the drop-out information ;  ; iterator_type gradientsBegin ();  returns iterator to the begin of the gradients ;  ; const_iterator_type gradientsBegin () const;  returns const iterator to the begin of the gradients ;  ; bool hasDropOut () const;  has this layer drop-out turned on? ;  ; std::shared_ptr< std::function< double(double)> > inverseActivationFunction () const;  ; ModeOutputValues outputMod",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:1940,Usability,clear,clear,1940,"ions;  LayerData (const LayerData &other);  copy c'tor of LayerData ;  ;  LayerData (const_iterator_type itInputBegin, const_iterator_type itInputEnd, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (LayerData &&other);  move c'tor of LayerData ;  ;  LayerData (size_t inputSize);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, iterator_type itGradientBegin, std::shared_ptr< std::function< double(double)> > activationFunction, std::shared_ptr< std::function< double(double)> > inverseActivationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, std::shared_ptr< std::function< double(double)> > activationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  ~LayerData ();  ; std::shared_ptr< std::function< double(double)> > activationFunction () const;  ; void clear ();  clear the values and the deltas ;  ; void clearDropOut ();  clear the drop-out-data for this layer ;  ; iterator_type deltasBegin ();  returns iterator to the begin of the deltas (back-propagation) ;  ; const_iterator_type deltasBegin () const;  returns const iterator to the begin of the deltas (back-propagation) ;  ; iterator_type deltasEnd ();  returns iterator to the end of the deltas (back-propagation) ;  ; const_iterator_type deltasEnd () const;  returns const iterator to the end of the deltas (back-propagation) ;  ; const_dropout_iterator dropOut () const;  return the begin of the drop-out information ;  ; iterator_type gradientsBegin ();  returns iterator to the begin of the gradients ;  ; const_iterator_type gradientsBegin () const;  returns const iterator to the begin of the gradients ;  ; bool hasDropOut () const;  has this layer drop-out turned on? ;  ; std::shared_ptr< std::function< double(double)> > inverseActivationFunction () const;  ; ModeOutputValues outputMode () const;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:1982,Usability,clear,clearDropOut,1982,"opy c'tor of LayerData ;  ;  LayerData (const_iterator_type itInputBegin, const_iterator_type itInputEnd, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (LayerData &&other);  move c'tor of LayerData ;  ;  LayerData (size_t inputSize);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, iterator_type itGradientBegin, std::shared_ptr< std::function< double(double)> > activationFunction, std::shared_ptr< std::function< double(double)> > inverseActivationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, std::shared_ptr< std::function< double(double)> > activationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  ~LayerData ();  ; std::shared_ptr< std::function< double(double)> > activationFunction () const;  ; void clear ();  clear the values and the deltas ;  ; void clearDropOut ();  clear the drop-out-data for this layer ;  ; iterator_type deltasBegin ();  returns iterator to the begin of the deltas (back-propagation) ;  ; const_iterator_type deltasBegin () const;  returns const iterator to the begin of the deltas (back-propagation) ;  ; iterator_type deltasEnd ();  returns iterator to the end of the deltas (back-propagation) ;  ; const_iterator_type deltasEnd () const;  returns const iterator to the end of the deltas (back-propagation) ;  ; const_dropout_iterator dropOut () const;  return the begin of the drop-out information ;  ; iterator_type gradientsBegin ();  returns iterator to the begin of the gradients ;  ; const_iterator_type gradientsBegin () const;  returns const iterator to the begin of the gradients ;  ; bool hasDropOut () const;  has this layer drop-out turned on? ;  ; std::shared_ptr< std::function< double(double)> > inverseActivationFunction () const;  ; ModeOutputValues outputMode () const;  returns the output mode ;  ; container_type ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:2000,Usability,clear,clear,2000,"yerData ;  ;  LayerData (const_iterator_type itInputBegin, const_iterator_type itInputEnd, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (LayerData &&other);  move c'tor of LayerData ;  ;  LayerData (size_t inputSize);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, iterator_type itGradientBegin, std::shared_ptr< std::function< double(double)> > activationFunction, std::shared_ptr< std::function< double(double)> > inverseActivationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  LayerData (size_t size, const_iterator_type itWeightBegin, std::shared_ptr< std::function< double(double)> > activationFunction, ModeOutputValues eModeOutput=ModeOutputValues::DIRECT);  c'tor of LayerData ;  ;  ~LayerData ();  ; std::shared_ptr< std::function< double(double)> > activationFunction () const;  ; void clear ();  clear the values and the deltas ;  ; void clearDropOut ();  clear the drop-out-data for this layer ;  ; iterator_type deltasBegin ();  returns iterator to the begin of the deltas (back-propagation) ;  ; const_iterator_type deltasBegin () const;  returns const iterator to the begin of the deltas (back-propagation) ;  ; iterator_type deltasEnd ();  returns iterator to the end of the deltas (back-propagation) ;  ; const_iterator_type deltasEnd () const;  returns const iterator to the end of the deltas (back-propagation) ;  ; const_dropout_iterator dropOut () const;  return the begin of the drop-out information ;  ; iterator_type gradientsBegin ();  returns iterator to the begin of the gradients ;  ; const_iterator_type gradientsBegin () const;  returns const iterator to the begin of the gradients ;  ; bool hasDropOut () const;  has this layer drop-out turned on? ;  ; std::shared_ptr< std::function< double(double)> > inverseActivationFunction () const;  ; ModeOutputValues outputMode () const;  returns the output mode ;  ; container_type probabilities ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:10895,Usability,clear,clear,10895, the weight vector ; activationFunctionindicates the activation function for this layer ; eModeOutputindicates a potential tranformation of the output values before further computation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 122 of file NeuralNet.cxx. ◆ LayerData() [5/6]. TMVA::DNN::LayerData::LayerData ; (; const LayerData & ; other). inline . copy c'tor of LayerData ; Definition at line 515 of file NeuralNet.h. ◆ LayerData() [6/6]. TMVA::DNN::LayerData::LayerData ; (; LayerData && ; other). inline . move c'tor of LayerData ; Definition at line 538 of file NeuralNet.h. Member Function Documentation. ◆ activationFunction(). std::shared_ptr< std::function< double(double)> > TMVA::DNN::LayerData::activationFunction ; (; ); const. inline . Definition at line 607 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::LayerData::clear ; (; ). inline . clear the values and the deltas ; Definition at line 576 of file NeuralNet.h. ◆ clearDropOut(). void TMVA::DNN::LayerData::clearDropOut ; (; ). inline . clear the drop-out-data for this layer ; Definition at line 620 of file NeuralNet.h. ◆ computeProbabilities(). LayerData::container_type TMVA::DNN::LayerData::computeProbabilities ; (; ); const. private . compute the probabilities from the node values ; Definition at line 140 of file NeuralNet.cxx. ◆ deltasBegin() [1/2]. iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ). inline . returns iterator to the begin of the deltas (back-propagation) ; Definition at line 591 of file NeuralNet.h. ◆ deltasBegin() [2/2]. const_iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ); const. inline . returns const iterator to the begin of the deltas (back-propagation) ; Definition at line 594 of file NeuralNet.h. ◆ deltasEnd() [1/2]. iterator_type TMVA::DNN::LayerData::deltasEnd ; (; ). inl,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:10931,Usability,clear,clear,10931,ationFunctionindicates the activation function for this layer ; eModeOutputindicates a potential tranformation of the output values before further computation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 122 of file NeuralNet.cxx. ◆ LayerData() [5/6]. TMVA::DNN::LayerData::LayerData ; (; const LayerData & ; other). inline . copy c'tor of LayerData ; Definition at line 515 of file NeuralNet.h. ◆ LayerData() [6/6]. TMVA::DNN::LayerData::LayerData ; (; LayerData && ; other). inline . move c'tor of LayerData ; Definition at line 538 of file NeuralNet.h. Member Function Documentation. ◆ activationFunction(). std::shared_ptr< std::function< double(double)> > TMVA::DNN::LayerData::activationFunction ; (; ); const. inline . Definition at line 607 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::LayerData::clear ; (; ). inline . clear the values and the deltas ; Definition at line 576 of file NeuralNet.h. ◆ clearDropOut(). void TMVA::DNN::LayerData::clearDropOut ; (; ). inline . clear the drop-out-data for this layer ; Definition at line 620 of file NeuralNet.h. ◆ computeProbabilities(). LayerData::container_type TMVA::DNN::LayerData::computeProbabilities ; (; ); const. private . compute the probabilities from the node values ; Definition at line 140 of file NeuralNet.cxx. ◆ deltasBegin() [1/2]. iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ). inline . returns iterator to the begin of the deltas (back-propagation) ; Definition at line 591 of file NeuralNet.h. ◆ deltasBegin() [2/2]. const_iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ); const. inline . returns const iterator to the begin of the deltas (back-propagation) ; Definition at line 594 of file NeuralNet.h. ◆ deltasEnd() [1/2]. iterator_type TMVA::DNN::LayerData::deltasEnd ; (; ). inline . returns iterator to ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:10954,Usability,clear,clear,10954,eOutputindicates a potential tranformation of the output values before further computation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 122 of file NeuralNet.cxx. ◆ LayerData() [5/6]. TMVA::DNN::LayerData::LayerData ; (; const LayerData & ; other). inline . copy c'tor of LayerData ; Definition at line 515 of file NeuralNet.h. ◆ LayerData() [6/6]. TMVA::DNN::LayerData::LayerData ; (; LayerData && ; other). inline . move c'tor of LayerData ; Definition at line 538 of file NeuralNet.h. Member Function Documentation. ◆ activationFunction(). std::shared_ptr< std::function< double(double)> > TMVA::DNN::LayerData::activationFunction ; (; ); const. inline . Definition at line 607 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::LayerData::clear ; (; ). inline . clear the values and the deltas ; Definition at line 576 of file NeuralNet.h. ◆ clearDropOut(). void TMVA::DNN::LayerData::clearDropOut ; (; ). inline . clear the drop-out-data for this layer ; Definition at line 620 of file NeuralNet.h. ◆ computeProbabilities(). LayerData::container_type TMVA::DNN::LayerData::computeProbabilities ; (; ); const. private . compute the probabilities from the node values ; Definition at line 140 of file NeuralNet.cxx. ◆ deltasBegin() [1/2]. iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ). inline . returns iterator to the begin of the deltas (back-propagation) ; Definition at line 591 of file NeuralNet.h. ◆ deltasBegin() [2/2]. const_iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ); const. inline . returns const iterator to the begin of the deltas (back-propagation) ; Definition at line 594 of file NeuralNet.h. ◆ deltasEnd() [1/2]. iterator_type TMVA::DNN::LayerData::deltasEnd ; (; ). inline . returns iterator to the end of the deltas (back-propagation) ; Definition at line 592 o,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:11034,Usability,clear,clearDropOut,11034, output values before further computation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 122 of file NeuralNet.cxx. ◆ LayerData() [5/6]. TMVA::DNN::LayerData::LayerData ; (; const LayerData & ; other). inline . copy c'tor of LayerData ; Definition at line 515 of file NeuralNet.h. ◆ LayerData() [6/6]. TMVA::DNN::LayerData::LayerData ; (; LayerData && ; other). inline . move c'tor of LayerData ; Definition at line 538 of file NeuralNet.h. Member Function Documentation. ◆ activationFunction(). std::shared_ptr< std::function< double(double)> > TMVA::DNN::LayerData::activationFunction ; (; ); const. inline . Definition at line 607 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::LayerData::clear ; (; ). inline . clear the values and the deltas ; Definition at line 576 of file NeuralNet.h. ◆ clearDropOut(). void TMVA::DNN::LayerData::clearDropOut ; (; ). inline . clear the drop-out-data for this layer ; Definition at line 620 of file NeuralNet.h. ◆ computeProbabilities(). LayerData::container_type TMVA::DNN::LayerData::computeProbabilities ; (; ); const. private . compute the probabilities from the node values ; Definition at line 140 of file NeuralNet.cxx. ◆ deltasBegin() [1/2]. iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ). inline . returns iterator to the begin of the deltas (back-propagation) ; Definition at line 591 of file NeuralNet.h. ◆ deltasBegin() [2/2]. const_iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ); const. inline . returns const iterator to the begin of the deltas (back-propagation) ; Definition at line 594 of file NeuralNet.h. ◆ deltasEnd() [1/2]. iterator_type TMVA::DNN::LayerData::deltasEnd ; (; ). inline . returns iterator to the end of the deltas (back-propagation) ; Definition at line 592 of file NeuralNet.h. ◆ deltasEnd() [2/2]. const_it,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:11077,Usability,clear,clearDropOut,11077,putation DIRECT does not further transformation; SIGMOID applies a sigmoid transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 122 of file NeuralNet.cxx. ◆ LayerData() [5/6]. TMVA::DNN::LayerData::LayerData ; (; const LayerData & ; other). inline . copy c'tor of LayerData ; Definition at line 515 of file NeuralNet.h. ◆ LayerData() [6/6]. TMVA::DNN::LayerData::LayerData ; (; LayerData && ; other). inline . move c'tor of LayerData ; Definition at line 538 of file NeuralNet.h. Member Function Documentation. ◆ activationFunction(). std::shared_ptr< std::function< double(double)> > TMVA::DNN::LayerData::activationFunction ; (; ); const. inline . Definition at line 607 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::LayerData::clear ; (; ). inline . clear the values and the deltas ; Definition at line 576 of file NeuralNet.h. ◆ clearDropOut(). void TMVA::DNN::LayerData::clearDropOut ; (; ). inline . clear the drop-out-data for this layer ; Definition at line 620 of file NeuralNet.h. ◆ computeProbabilities(). LayerData::container_type TMVA::DNN::LayerData::computeProbabilities ; (; ); const. private . compute the probabilities from the node values ; Definition at line 140 of file NeuralNet.cxx. ◆ deltasBegin() [1/2]. iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ). inline . returns iterator to the begin of the deltas (back-propagation) ; Definition at line 591 of file NeuralNet.h. ◆ deltasBegin() [2/2]. const_iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ); const. inline . returns const iterator to the begin of the deltas (back-propagation) ; Definition at line 594 of file NeuralNet.h. ◆ deltasEnd() [1/2]. iterator_type TMVA::DNN::LayerData::deltasEnd ; (; ). inline . returns iterator to the end of the deltas (back-propagation) ; Definition at line 592 of file NeuralNet.h. ◆ deltasEnd() [2/2]. const_iterator_type TMVA::DNN::LayerData:,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html:11107,Usability,clear,clear,11107, transformation to each output value (to create a probability); SOFTMAX applies a softmax transformation to all output values (mutually exclusive probability) . Definition at line 122 of file NeuralNet.cxx. ◆ LayerData() [5/6]. TMVA::DNN::LayerData::LayerData ; (; const LayerData & ; other). inline . copy c'tor of LayerData ; Definition at line 515 of file NeuralNet.h. ◆ LayerData() [6/6]. TMVA::DNN::LayerData::LayerData ; (; LayerData && ; other). inline . move c'tor of LayerData ; Definition at line 538 of file NeuralNet.h. Member Function Documentation. ◆ activationFunction(). std::shared_ptr< std::function< double(double)> > TMVA::DNN::LayerData::activationFunction ; (; ); const. inline . Definition at line 607 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::LayerData::clear ; (; ). inline . clear the values and the deltas ; Definition at line 576 of file NeuralNet.h. ◆ clearDropOut(). void TMVA::DNN::LayerData::clearDropOut ; (; ). inline . clear the drop-out-data for this layer ; Definition at line 620 of file NeuralNet.h. ◆ computeProbabilities(). LayerData::container_type TMVA::DNN::LayerData::computeProbabilities ; (; ); const. private . compute the probabilities from the node values ; Definition at line 140 of file NeuralNet.cxx. ◆ deltasBegin() [1/2]. iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ). inline . returns iterator to the begin of the deltas (back-propagation) ; Definition at line 591 of file NeuralNet.h. ◆ deltasBegin() [2/2]. const_iterator_type TMVA::DNN::LayerData::deltasBegin ; (; ); const. inline . returns const iterator to the begin of the deltas (back-propagation) ; Definition at line 594 of file NeuralNet.h. ◆ deltasEnd() [1/2]. iterator_type TMVA::DNN::LayerData::deltasEnd ; (; ). inline . returns iterator to the end of the deltas (back-propagation) ; Definition at line 592 of file NeuralNet.h. ◆ deltasEnd() [2/2]. const_iterator_type TMVA::DNN::LayerData::deltasEnd ; (; ); const. inline . returns const iterator to the end of the,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1LayerData.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1LayerData.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1MeanVariance.html:506,Usability,clear,clear,506,". ROOT: TMVA::DNN::MeanVariance Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::MeanVariance Class Reference. . Definition at line 74 of file NeuralNet.h. Public Member Functions;  MeanVariance ();  ; template<typename ITERATOR > ; void add (ITERATOR itBegin, ITERATOR itEnd);  ; template<typename T > ; void add (T value, double weight=1.0);  ; void clear ();  ; int count () const;  ; double mean () const;  ; double stdDev () const;  ; double stdDev_corr () const;  ; double var () const;  ; double var_corr () const;  ; double weights () const;  . Private Attributes; double m_mean;  ; size_t m_n;  ; double m_squared;  ; double m_sumWeights;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ MeanVariance(). TMVA::DNN::MeanVariance::MeanVariance ; (; ). inline . Definition at line 77 of file NeuralNet.h. Member Function Documentation. ◆ add() [1/2]. template<typename ITERATOR > . void TMVA::DNN::MeanVariance::add ; (; ITERATOR ; itBegin, . ITERATOR ; itEnd . ). inline . Definition at line 116 of file NeuralNet.h. ◆ add() [2/2]. template<typename T > . void TMVA::DNN::MeanVariance::add ; (; T ; value, . double ; weight = 1.0 . ). inline . Definition at line 93 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::MeanVariance::clear ; (; ). inline . Definition at line 84 of file NeuralNet.h. ◆ count(). int TMVA::DNN::MeanVariance::count ; (; ); const. inline . Definition at line 124 of file NeuralNet.h. ◆ mean(). double TMVA::DNN::MeanVariance::mean ; (; ); const. inline . Definition at line 126 of file NeuralNet.h. ◆ stdDev(). double TMVA::DNN::MeanVariance::stdDev ; (; ); const. inline . Definition at line 145 of file NeuralNet.h. ◆ stdDev_corr(). double TMVA::DNN::MeanVariance::stdDev_corr ; (; ); const. inline . Definition at line 144 of file NeuralNet.h. ◆ var(). double TMVA::DNN::MeanVariance::var",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1MeanVariance.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1MeanVariance.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1MeanVariance.html:1375,Usability,clear,clear,1375,"RATOR > ; void add (ITERATOR itBegin, ITERATOR itEnd);  ; template<typename T > ; void add (T value, double weight=1.0);  ; void clear ();  ; int count () const;  ; double mean () const;  ; double stdDev () const;  ; double stdDev_corr () const;  ; double var () const;  ; double var_corr () const;  ; double weights () const;  . Private Attributes; double m_mean;  ; size_t m_n;  ; double m_squared;  ; double m_sumWeights;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ MeanVariance(). TMVA::DNN::MeanVariance::MeanVariance ; (; ). inline . Definition at line 77 of file NeuralNet.h. Member Function Documentation. ◆ add() [1/2]. template<typename ITERATOR > . void TMVA::DNN::MeanVariance::add ; (; ITERATOR ; itBegin, . ITERATOR ; itEnd . ). inline . Definition at line 116 of file NeuralNet.h. ◆ add() [2/2]. template<typename T > . void TMVA::DNN::MeanVariance::add ; (; T ; value, . double ; weight = 1.0 . ). inline . Definition at line 93 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::MeanVariance::clear ; (; ). inline . Definition at line 84 of file NeuralNet.h. ◆ count(). int TMVA::DNN::MeanVariance::count ; (; ); const. inline . Definition at line 124 of file NeuralNet.h. ◆ mean(). double TMVA::DNN::MeanVariance::mean ; (; ); const. inline . Definition at line 126 of file NeuralNet.h. ◆ stdDev(). double TMVA::DNN::MeanVariance::stdDev ; (; ); const. inline . Definition at line 145 of file NeuralNet.h. ◆ stdDev_corr(). double TMVA::DNN::MeanVariance::stdDev_corr ; (; ); const. inline . Definition at line 144 of file NeuralNet.h. ◆ var(). double TMVA::DNN::MeanVariance::var ; (; ); const. inline . Definition at line 127 of file NeuralNet.h. ◆ var_corr(). double TMVA::DNN::MeanVariance::var_corr ; (; ); const. inline . Definition at line 136 of file NeuralNet.h. ◆ weights(). double TMVA::DNN::MeanVariance::weights ; (; ); const. inline . Definition at line 125 of file NeuralNet.h. Member Data Documentation. ◆ m_mean. double TMVA::DNN::MeanVaria",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1MeanVariance.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1MeanVariance.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1MeanVariance.html:1414,Usability,clear,clear,1414," itBegin, ITERATOR itEnd);  ; template<typename T > ; void add (T value, double weight=1.0);  ; void clear ();  ; int count () const;  ; double mean () const;  ; double stdDev () const;  ; double stdDev_corr () const;  ; double var () const;  ; double var_corr () const;  ; double weights () const;  . Private Attributes; double m_mean;  ; size_t m_n;  ; double m_squared;  ; double m_sumWeights;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ MeanVariance(). TMVA::DNN::MeanVariance::MeanVariance ; (; ). inline . Definition at line 77 of file NeuralNet.h. Member Function Documentation. ◆ add() [1/2]. template<typename ITERATOR > . void TMVA::DNN::MeanVariance::add ; (; ITERATOR ; itBegin, . ITERATOR ; itEnd . ). inline . Definition at line 116 of file NeuralNet.h. ◆ add() [2/2]. template<typename T > . void TMVA::DNN::MeanVariance::add ; (; T ; value, . double ; weight = 1.0 . ). inline . Definition at line 93 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::MeanVariance::clear ; (; ). inline . Definition at line 84 of file NeuralNet.h. ◆ count(). int TMVA::DNN::MeanVariance::count ; (; ); const. inline . Definition at line 124 of file NeuralNet.h. ◆ mean(). double TMVA::DNN::MeanVariance::mean ; (; ); const. inline . Definition at line 126 of file NeuralNet.h. ◆ stdDev(). double TMVA::DNN::MeanVariance::stdDev ; (; ); const. inline . Definition at line 145 of file NeuralNet.h. ◆ stdDev_corr(). double TMVA::DNN::MeanVariance::stdDev_corr ; (; ); const. inline . Definition at line 144 of file NeuralNet.h. ◆ var(). double TMVA::DNN::MeanVariance::var ; (; ); const. inline . Definition at line 127 of file NeuralNet.h. ◆ var_corr(). double TMVA::DNN::MeanVariance::var_corr ; (; ); const. inline . Definition at line 136 of file NeuralNet.h. ◆ weights(). double TMVA::DNN::MeanVariance::weights ; (; ); const. inline . Definition at line 125 of file NeuralNet.h. Member Data Documentation. ◆ m_mean. double TMVA::DNN::MeanVariance::m_mean. private . Defi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1MeanVariance.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1MeanVariance.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:1783,Availability,error,errorFunction,1783," addLayer (Layer &&layer);  ; void addLayer (Layer &layer);  add a layer (layout) ;  ; template<typename Settings > ; void backPropagate (std::vector< std::vector< LayerData > > &layerPatternData, const Settings &settings, size_t trainFromLayer, size_t totalNumWeights) const;  ; void clear ();  ; template<typename Weights > ; std::vector< double > compute (const std::vector< double > &input, const Weights &weights) const;  compute the net with the given input and the given weights ;  ; template<typename ItWeight > ; std::tuple< double, double > computeError (const Settings &settings, std::vector< LayerData > &lastLayerData, Batch &batch, ItWeight itWeightBegin, ItWeight itWeightEnd) const;  ; void dE ();  ; template<typename WeightsType , typename DropProbabilities > ; void dropOutWeightFactor (WeightsType &weights, const DropProbabilities &drops, bool inverse=false);  set the drop out configuration ;  ; double E ();  ; template<typename Container , typename ItWeight > ; double errorFunction (LayerData &layerData, Container truth, ItWeight itWeight, ItWeight itWeightEnd, double patternWeight, double factorWeightDecay, EnumRegularization eRegularization) const;  computes the error of the DNN ;  ; template<typename OutputContainer > ; void fetchOutput (const LayerData &lastLayerData, OutputContainer &outputContainer) const;  ; template<typename OutputContainer > ; void fetchOutput (const std::vector< LayerData > &layerPatternData, OutputContainer &outputContainer) const;  ; template<typename LayerContainer , typename PassThrough , typename ItWeight , typename ItGradient , typename OutContainer > ; double forward_backward (LayerContainer &layers, PassThrough &settingsAndBatch, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t trainFromLayer, OutContainer &outputContainer, bool fetchOutput) const;  main NN computation function ;  ; template<typename LayerContainer , typename LayerPatternContainer > ; void forwardB",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:1983,Availability,error,error,1983," const Settings &settings, size_t trainFromLayer, size_t totalNumWeights) const;  ; void clear ();  ; template<typename Weights > ; std::vector< double > compute (const std::vector< double > &input, const Weights &weights) const;  compute the net with the given input and the given weights ;  ; template<typename ItWeight > ; std::tuple< double, double > computeError (const Settings &settings, std::vector< LayerData > &lastLayerData, Batch &batch, ItWeight itWeightBegin, ItWeight itWeightEnd) const;  ; void dE ();  ; template<typename WeightsType , typename DropProbabilities > ; void dropOutWeightFactor (WeightsType &weights, const DropProbabilities &drops, bool inverse=false);  set the drop out configuration ;  ; double E ();  ; template<typename Container , typename ItWeight > ; double errorFunction (LayerData &layerData, Container truth, ItWeight itWeight, ItWeight itWeightEnd, double patternWeight, double factorWeightDecay, EnumRegularization eRegularization) const;  computes the error of the DNN ;  ; template<typename OutputContainer > ; void fetchOutput (const LayerData &lastLayerData, OutputContainer &outputContainer) const;  ; template<typename OutputContainer > ; void fetchOutput (const std::vector< LayerData > &layerPatternData, OutputContainer &outputContainer) const;  ; template<typename LayerContainer , typename PassThrough , typename ItWeight , typename ItGradient , typename OutContainer > ; double forward_backward (LayerContainer &layers, PassThrough &settingsAndBatch, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t trainFromLayer, OutContainer &outputContainer, bool fetchOutput) const;  main NN computation function ;  ; template<typename LayerContainer , typename LayerPatternContainer > ; void forwardBatch (const LayerContainer &_layers, LayerPatternContainer &layerPatternData, std::vector< double > &valuesMean, std::vector< double > &valuesStdDev, size_t trainFromLayer) const;  ; template<type",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:5522,Availability,error,error,5522,"e PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients) const;  execute computation of the DNN for one mini-batch (used by the minimizer); returns gradients as well ;  ; template<typename Weights , typename Gradients , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients, ModeOutput eFetch, OutContainer &outputContainer) const;  ; size_t outputSize () const;  output size of the DNN ;  ; template<typename LayerContainer , typename DropContainer , typename ItWeight , typename ItGradient > ; std::vector< std::vector< LayerData > > prepareLayerData (LayerContainer &layers, Batch &batch, const DropContainer &dropContainer, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t &totalNumWeights) const;  ; template<typename Minimizer > ; void preTrain (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  pre-training for future use ;  ; void removeLayer ();  remove one layer ;  ; void setErrorFunction (ModeErrorFunction eErrorFunction);  which error function is to be used ;  ; void setInputSize (size_t sizeInput);  set the input size of the DNN ;  ; void SetIpythonInteractive (IPythonInteractive *fI, bool *fE, UInt_t *M, UInt_t *C);  ; void setOutputSize (size_t sizeOutput);  set the output size of the DNN ;  ; template<typename Minimizer > ; double train (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  start the training ;  ; template<typename Iterator , typename Minimizer > ; double trainCycle (Minimizer &minimizer, std::vector< double > &weights, Iterator itPatternBegin, Iterator itPatternEnd, Settings &settings, DropContainer &dropContainer);  executes one training cycle ;  .",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:6743,Availability,error,error,6743,"emplate<typename Minimizer > ; double train (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  start the training ;  ; template<typename Iterator , typename Minimizer > ; double trainCycle (Minimizer &minimizer, std::vector< double > &weights, Iterator itPatternBegin, Iterator itPatternEnd, Settings &settings, DropContainer &dropContainer);  executes one training cycle ;  . Protected Member Functions; void fillDropContainer (DropContainer &dropContainer, double dropFraction, size_t numNodes) const;  prepare the drop-out-container (select the nodes which are to be dropped out) ;  . Protected Attributes; bool * fExitFromTraining = nullptr;  ; IPythonInteractive * fInteractive = nullptr;  ; UInt_t * fIPyCurrentIter = nullptr;  ; UInt_t * fIPyMaxIter = nullptr;  . Private Attributes; ModeErrorFunction m_eErrorFunction;  denotes the error function ;  ; std::vector< Layer > m_layers;  layer-structure-data ;  ; size_t m_sizeInput;  input size of this DNN ;  ; size_t m_sizeOutput;  output size of this DNN ;  . #include <TMVA/NeuralNet.h>; Member Typedef Documentation. ◆ begin_end_type. typedef std::pair<iterator_type,iterator_type> TMVA::DNN::Net::begin_end_type. Definition at line 1067 of file NeuralNet.h. ◆ container_type. typedef std::vector<double> TMVA::DNN::Net::container_type. Definition at line 1065 of file NeuralNet.h. ◆ iterator_type. typedef container_type::iterator TMVA::DNN::Net::iterator_type. Definition at line 1066 of file NeuralNet.h. Constructor & Destructor Documentation. ◆ Net() [1/2]. TMVA::DNN::Net::Net ; (; ). inline . c'tor ; Definition at line 1074 of file NeuralNet.h. ◆ Net() [2/2]. TMVA::DNN::Net::Net ; (; const Net & ; other). inline . d'tor ; Definition at line 1085 of file NeuralNet.h. Member Function Documentation. ◆ addLayer() [1/2]. void TMVA::DNN::Net::addLayer ; (; Layer && ; layer). inline . Definition at line 1095 of file NeuralNet.h",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:9779,Availability,error,errorFunction,9779,"> TMVA::DNN::Net::computeError ; (; const Settings & ; settings, . std::vector< LayerData > & ; lastLayerData, . Batch & ; batch, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd . ); const. Definition at line 1321 of file NeuralNet.icc. ◆ dE(). void TMVA::DNN::Net::dE ; (; ). ◆ dropOutWeightFactor(). template<typename WeightsType , typename DropProbabilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; inverse = false . ). set the drop out configuration ; compute the drop-out-weight factor; when using drop-out a fraction of the nodes is turned off at each cycle of the computation once all nodes are turned on again (for instances when the test samples are evaluated), the weights have to be adjusted to account for the different number of active nodes this function computes the factor and applies it to the weights ; Definition at line 652 of file NeuralNet.icc. ◆ E(). double TMVA::DNN::Net::E ; (; ). ◆ errorFunction(). template<typename Container , typename ItWeight > . double TMVA::DNN::Net::errorFunction ; (; LayerData & ; layerData, . Container ; truth, . ItWeight ; itWeight, . ItWeight ; itWeightEnd, . double ; patternWeight, . double ; factorWeightDecay, . EnumRegularization ; eRegularization . ); const. computes the error of the DNN ; compute the error function; Definition at line 1579 of file NeuralNet.icc. ◆ fetchOutput() [1/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const LayerData & ; lastLayerData, . OutputContainer & ; outputContainer . ); const. Definition at line 1291 of file NeuralNet.icc. ◆ fetchOutput() [2/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const std::vector< LayerData > & ; layerPatternData, . OutputContainer & ; outputContainer . ); const. Definition at line 1312 of file NeuralNet.icc. ◆ fillDropContainer(). void TMVA::DNN::Net::fillDropContainer ; (; DropContainer & ; dropContainer, . doubl",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:9871,Availability,error,errorFunction,9871,"lastLayerData, . Batch & ; batch, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd . ); const. Definition at line 1321 of file NeuralNet.icc. ◆ dE(). void TMVA::DNN::Net::dE ; (; ). ◆ dropOutWeightFactor(). template<typename WeightsType , typename DropProbabilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; inverse = false . ). set the drop out configuration ; compute the drop-out-weight factor; when using drop-out a fraction of the nodes is turned off at each cycle of the computation once all nodes are turned on again (for instances when the test samples are evaluated), the weights have to be adjusted to account for the different number of active nodes this function computes the factor and applies it to the weights ; Definition at line 652 of file NeuralNet.icc. ◆ E(). double TMVA::DNN::Net::E ; (; ). ◆ errorFunction(). template<typename Container , typename ItWeight > . double TMVA::DNN::Net::errorFunction ; (; LayerData & ; layerData, . Container ; truth, . ItWeight ; itWeight, . ItWeight ; itWeightEnd, . double ; patternWeight, . double ; factorWeightDecay, . EnumRegularization ; eRegularization . ); const. computes the error of the DNN ; compute the error function; Definition at line 1579 of file NeuralNet.icc. ◆ fetchOutput() [1/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const LayerData & ; lastLayerData, . OutputContainer & ; outputContainer . ); const. Definition at line 1291 of file NeuralNet.icc. ◆ fetchOutput() [2/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const std::vector< LayerData > & ; layerPatternData, . OutputContainer & ; outputContainer . ); const. Definition at line 1312 of file NeuralNet.icc. ◆ fillDropContainer(). void TMVA::DNN::Net::fillDropContainer ; (; DropContainer & ; dropContainer, . double ; dropFraction, . size_t ; _numNodes . ); const. protected . prepare the drop-out-container (s",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:10105,Availability,error,error,10105,"bilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; inverse = false . ). set the drop out configuration ; compute the drop-out-weight factor; when using drop-out a fraction of the nodes is turned off at each cycle of the computation once all nodes are turned on again (for instances when the test samples are evaluated), the weights have to be adjusted to account for the different number of active nodes this function computes the factor and applies it to the weights ; Definition at line 652 of file NeuralNet.icc. ◆ E(). double TMVA::DNN::Net::E ; (; ). ◆ errorFunction(). template<typename Container , typename ItWeight > . double TMVA::DNN::Net::errorFunction ; (; LayerData & ; layerData, . Container ; truth, . ItWeight ; itWeight, . ItWeight ; itWeightEnd, . double ; patternWeight, . double ; factorWeightDecay, . EnumRegularization ; eRegularization . ); const. computes the error of the DNN ; compute the error function; Definition at line 1579 of file NeuralNet.icc. ◆ fetchOutput() [1/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const LayerData & ; lastLayerData, . OutputContainer & ; outputContainer . ); const. Definition at line 1291 of file NeuralNet.icc. ◆ fetchOutput() [2/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const std::vector< LayerData > & ; layerPatternData, . OutputContainer & ; outputContainer . ); const. Definition at line 1312 of file NeuralNet.icc. ◆ fillDropContainer(). void TMVA::DNN::Net::fillDropContainer ; (; DropContainer & ; dropContainer, . double ; dropFraction, . size_t ; _numNodes . ); const. protected . prepare the drop-out-container (select the nodes which are to be dropped out) ; prepare the drop-out container given the provided drop-fractions; Definition at line 572 of file NeuralNet.cxx. ◆ forward_backward(). template<typename LayerContainer , typename PassThrough , typename ItWeight , ty",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:10136,Availability,error,error,10136,"bilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; inverse = false . ). set the drop out configuration ; compute the drop-out-weight factor; when using drop-out a fraction of the nodes is turned off at each cycle of the computation once all nodes are turned on again (for instances when the test samples are evaluated), the weights have to be adjusted to account for the different number of active nodes this function computes the factor and applies it to the weights ; Definition at line 652 of file NeuralNet.icc. ◆ E(). double TMVA::DNN::Net::E ; (; ). ◆ errorFunction(). template<typename Container , typename ItWeight > . double TMVA::DNN::Net::errorFunction ; (; LayerData & ; layerData, . Container ; truth, . ItWeight ; itWeight, . ItWeight ; itWeightEnd, . double ; patternWeight, . double ; factorWeightDecay, . EnumRegularization ; eRegularization . ); const. computes the error of the DNN ; compute the error function; Definition at line 1579 of file NeuralNet.icc. ◆ fetchOutput() [1/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const LayerData & ; lastLayerData, . OutputContainer & ; outputContainer . ); const. Definition at line 1291 of file NeuralNet.icc. ◆ fetchOutput() [2/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const std::vector< LayerData > & ; layerPatternData, . OutputContainer & ; outputContainer . ); const. Definition at line 1312 of file NeuralNet.icc. ◆ fillDropContainer(). void TMVA::DNN::Net::fillDropContainer ; (; DropContainer & ; dropContainer, . double ; dropFraction, . size_t ; _numNodes . ); const. protected . prepare the drop-out-container (select the nodes which are to be dropped out) ; prepare the drop-out container given the provided drop-fractions; Definition at line 572 of file NeuralNet.cxx. ◆ forward_backward(). template<typename LayerContainer , typename PassThrough , typename ItWeight , ty",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:16096,Availability,error,error,16096,"pContainer , typename ItWeight , typename ItGradient > . std::vector< std::vector< LayerData > > TMVA::DNN::Net::prepareLayerData ; (; LayerContainer & ; layers, . Batch & ; batch, . const DropContainer & ; dropContainer, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd, . ItGradient ; itGradientBegin, . ItGradient ; itGradientEnd, . size_t & ; totalNumWeights . ); const. Definition at line 1111 of file NeuralNet.icc. ◆ preTrain(). template<typename Minimizer > . void TMVA::DNN::Net::preTrain ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). pre-training for future use . ◆ removeLayer(). void TMVA::DNN::Net::removeLayer ; (; ). inline . remove one layer ; Definition at line 1248 of file NeuralNet.h. ◆ setErrorFunction(). void TMVA::DNN::Net::setErrorFunction ; (; ModeErrorFunction ; eErrorFunction). inline . which error function is to be used ; Definition at line 1096 of file NeuralNet.h. ◆ setInputSize(). void TMVA::DNN::Net::setInputSize ; (; size_t ; sizeInput). inline . set the input size of the DNN ; Definition at line 1092 of file NeuralNet.h. ◆ SetIpythonInteractive(). void TMVA::DNN::Net::SetIpythonInteractive ; (; IPythonInteractive * ; fI, . bool * ; fE, . UInt_t * ; M, . UInt_t * ; C . ). inline . Definition at line 1283 of file NeuralNet.h. ◆ setOutputSize(). void TMVA::DNN::Net::setOutputSize ; (; size_t ; sizeOutput). inline . set the output size of the DNN ; Definition at line 1093 of file NeuralNet.h. ◆ train(). template<typename Minimizer > . double TMVA::DNN::Net::train ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). start the training ; execute the training until convergence emerges; Parameters. weightsweight vector ; trainPatterntraining pattern ; testPatterntest pattern ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:19298,Availability,error,error,19298,"ndthe pattern to be trained with ; settingsthe settings for the training ; dropContainerthe configuration for DNN drop-out. uses multithreading if turned on; Parameters. minimizerthe minimizer to be used (e.g. SGD) ; weightsthe weight container with all the synapse weights ; itPatternBeginbegin of the pattern container ; itPatternEndthe end of the pattern container ; settingsthe settings for this training (e.g. multithreading or not, regularization, etc.) ; dropContainerthe data for dropping-out nodes (regularization technique) . Definition at line 939 of file NeuralNet.icc. Member Data Documentation. ◆ fExitFromTraining. bool* TMVA::DNN::Net::fExitFromTraining = nullptr. protected . Definition at line 1277 of file NeuralNet.h. ◆ fInteractive. IPythonInteractive* TMVA::DNN::Net::fInteractive = nullptr. protected . Definition at line 1276 of file NeuralNet.h. ◆ fIPyCurrentIter. UInt_t * TMVA::DNN::Net::fIPyCurrentIter = nullptr. protected . Definition at line 1278 of file NeuralNet.h. ◆ fIPyMaxIter. UInt_t* TMVA::DNN::Net::fIPyMaxIter = nullptr. protected . Definition at line 1278 of file NeuralNet.h. ◆ m_eErrorFunction. ModeErrorFunction TMVA::DNN::Net::m_eErrorFunction. private . denotes the error function ; Definition at line 1269 of file NeuralNet.h. ◆ m_layers. std::vector<Layer> TMVA::DNN::Net::m_layers. private . layer-structure-data ; Definition at line 1272 of file NeuralNet.h. ◆ m_sizeInput. size_t TMVA::DNN::Net::m_sizeInput. private . input size of this DNN ; Definition at line 1270 of file NeuralNet.h. ◆ m_sizeOutput. size_t TMVA::DNN::Net::m_sizeOutput. private . output size of this DNN ; Definition at line 1271 of file NeuralNet.h. Libraries for TMVA::DNN::Net:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/NeuralNet.h; tmva/tmva/inc/TMVA/NeuralNet.icc; tmva/tmva/src/NeuralNet.cxx. TMVADNNNet. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:45 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:1689,Deployability,configurat,configuration,1689," Public Member Functions;  Net ();  c'tor ;  ;  Net (const Net &other);  d'tor ;  ; void addLayer (Layer &&layer);  ; void addLayer (Layer &layer);  add a layer (layout) ;  ; template<typename Settings > ; void backPropagate (std::vector< std::vector< LayerData > > &layerPatternData, const Settings &settings, size_t trainFromLayer, size_t totalNumWeights) const;  ; void clear ();  ; template<typename Weights > ; std::vector< double > compute (const std::vector< double > &input, const Weights &weights) const;  compute the net with the given input and the given weights ;  ; template<typename ItWeight > ; std::tuple< double, double > computeError (const Settings &settings, std::vector< LayerData > &lastLayerData, Batch &batch, ItWeight itWeightBegin, ItWeight itWeightEnd) const;  ; void dE ();  ; template<typename WeightsType , typename DropProbabilities > ; void dropOutWeightFactor (WeightsType &weights, const DropProbabilities &drops, bool inverse=false);  set the drop out configuration ;  ; double E ();  ; template<typename Container , typename ItWeight > ; double errorFunction (LayerData &layerData, Container truth, ItWeight itWeight, ItWeight itWeightEnd, double patternWeight, double factorWeightDecay, EnumRegularization eRegularization) const;  computes the error of the DNN ;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:9310,Deployability,configurat,configuration,9310,"onst. compute the net with the given input and the given weights ; compute the neural net; Parameters. inputthe input data ; weightsthe weight data . Definition at line 1037 of file NeuralNet.icc. ◆ computeError(). template<typename ItWeight > . std::tuple< double, double > TMVA::DNN::Net::computeError ; (; const Settings & ; settings, . std::vector< LayerData > & ; lastLayerData, . Batch & ; batch, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd . ); const. Definition at line 1321 of file NeuralNet.icc. ◆ dE(). void TMVA::DNN::Net::dE ; (; ). ◆ dropOutWeightFactor(). template<typename WeightsType , typename DropProbabilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; inverse = false . ). set the drop out configuration ; compute the drop-out-weight factor; when using drop-out a fraction of the nodes is turned off at each cycle of the computation once all nodes are turned on again (for instances when the test samples are evaluated), the weights have to be adjusted to account for the different number of active nodes this function computes the factor and applies it to the weights ; Definition at line 652 of file NeuralNet.icc. ◆ E(). double TMVA::DNN::Net::E ; (; ). ◆ errorFunction(). template<typename Container , typename ItWeight > . double TMVA::DNN::Net::errorFunction ; (; LayerData & ; layerData, . Container ; truth, . ItWeight ; itWeight, . ItWeight ; itWeightEnd, . double ; patternWeight, . double ; factorWeightDecay, . EnumRegularization ; eRegularization . ); const. computes the error of the DNN ; compute the error function; Definition at line 1579 of file NeuralNet.icc. ◆ fetchOutput() [1/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const LayerData & ; lastLayerData, . OutputContainer & ; outputContainer . ); const. Definition at line 1291 of file NeuralNet.icc. ◆ fetchOutput() [2/2]. template<typename OutputContainer > . void TMVA::DNN::Net::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:18178,Deployability,configurat,configuration,18178,"tterntraining pattern ; testPatterntest pattern ; minimizeruse this minimizer for training (e.g. SGD) ; settingssettings used for this training run. Parameters. weightsthe container with the weights (synapses) ; trainPatternthe pattern for the training ; testPatternthe pattern for the testing ; minimizerthe minimizer (e.g. steepest gradient descent) to be used ; settingsthe settings for the training (e.g. multithreading or not, regularization etc.) . Definition at line 712 of file NeuralNet.icc. ◆ trainCycle(). template<typename Iterator , typename Minimizer > . double TMVA::DNN::Net::trainCycle ; (; Minimizer & ; minimizer, . std::vector< double > & ; weights, . Iterator ; itPatternBegin, . Iterator ; itPatternEnd, . Settings & ; settings, . DropContainer & ; dropContainer . ). inline . executes one training cycle ; execute a single training cycle; Parameters. minimizerthe minimizer to be used ; weightsthe weight vector to be used ; itPatternBeginthe pattern to be trained with ; itPatternEndthe pattern to be trained with ; settingsthe settings for the training ; dropContainerthe configuration for DNN drop-out. uses multithreading if turned on; Parameters. minimizerthe minimizer to be used (e.g. SGD) ; weightsthe weight container with all the synapse weights ; itPatternBeginbegin of the pattern container ; itPatternEndthe end of the pattern container ; settingsthe settings for this training (e.g. multithreading or not, regularization, etc.) ; dropContainerthe data for dropping-out nodes (regularization technique) . Definition at line 939 of file NeuralNet.icc. Member Data Documentation. ◆ fExitFromTraining. bool* TMVA::DNN::Net::fExitFromTraining = nullptr. protected . Definition at line 1277 of file NeuralNet.h. ◆ fInteractive. IPythonInteractive* TMVA::DNN::Net::fInteractive = nullptr. protected . Definition at line 1276 of file NeuralNet.h. ◆ fIPyCurrentIter. UInt_t * TMVA::DNN::Net::fIPyCurrentIter = nullptr. protected . Definition at line 1278 of file NeuralNet.",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:359,Modifiability,layers,layers,359,". ROOT: TMVA::DNN::Net Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; Private Attributes |; List of all members ; TMVA::DNN::Net Class Reference. ; neural net ; holds the structure of all layers and some data for the whole net does not know the layer data though (i.e. values of the nodes and weights) ; Definition at line 1061 of file NeuralNet.h. Public Types; typedef std::pair< iterator_type, iterator_type > begin_end_type;  ; typedef std::vector< double > container_type;  ; typedef container_type::iterator iterator_type;  . Public Member Functions;  Net ();  c'tor ;  ;  Net (const Net &other);  d'tor ;  ; void addLayer (Layer &&layer);  ; void addLayer (Layer &layer);  add a layer (layout) ;  ; template<typename Settings > ; void backPropagate (std::vector< std::vector< LayerData > > &layerPatternData, const Settings &settings, size_t trainFromLayer, size_t totalNumWeights) const;  ; void clear ();  ; template<typename Weights > ; std::vector< double > compute (const std::vector< double > &input, const Weights &weights) const;  compute the net with the given input and the given weights ;  ; template<typename ItWeight > ; std::tuple< double, double > computeError (const Settings &settings, std::vector< LayerData > &lastLayerData, Batch &batch, ItWeight itWeightBegin, ItWeight itWeightEnd) const;  ; void dE ();  ; template<typename WeightsType , typename DropProbabilities > ; void dropOutWeightFactor (WeightsType &weights, const DropProbabilities &drops, bool inverse=false);  set the drop out configuration ;  ; double E ();  ; template<typename Container , typename ItWeight > ; double errorFunction (LayerData &layerData, Container truth, ItWeight itWeight, ItWeight itWeightEnd, double patternWeight, double factorWeightDecay, EnumRegularization eRegularization) const;  computes the error of the DNN ;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:1689,Modifiability,config,configuration,1689," Public Member Functions;  Net ();  c'tor ;  ;  Net (const Net &other);  d'tor ;  ; void addLayer (Layer &&layer);  ; void addLayer (Layer &layer);  add a layer (layout) ;  ; template<typename Settings > ; void backPropagate (std::vector< std::vector< LayerData > > &layerPatternData, const Settings &settings, size_t trainFromLayer, size_t totalNumWeights) const;  ; void clear ();  ; template<typename Weights > ; std::vector< double > compute (const std::vector< double > &input, const Weights &weights) const;  compute the net with the given input and the given weights ;  ; template<typename ItWeight > ; std::tuple< double, double > computeError (const Settings &settings, std::vector< LayerData > &lastLayerData, Batch &batch, ItWeight itWeightBegin, ItWeight itWeightEnd) const;  ; void dE ();  ; template<typename WeightsType , typename DropProbabilities > ; void dropOutWeightFactor (WeightsType &weights, const DropProbabilities &drops, bool inverse=false);  set the drop out configuration ;  ; double E ();  ; template<typename Container , typename ItWeight > ; double errorFunction (LayerData &layerData, Container truth, ItWeight itWeight, ItWeight itWeightEnd, double patternWeight, double factorWeightDecay, EnumRegularization eRegularization) const;  computes the error of the DNN ;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:2454,Modifiability,layers,layers,2454,"n, ItWeight itWeightEnd) const;  ; void dE ();  ; template<typename WeightsType , typename DropProbabilities > ; void dropOutWeightFactor (WeightsType &weights, const DropProbabilities &drops, bool inverse=false);  set the drop out configuration ;  ; double E ();  ; template<typename Container , typename ItWeight > ; double errorFunction (LayerData &layerData, Container truth, ItWeight itWeight, ItWeight itWeightEnd, double patternWeight, double factorWeightDecay, EnumRegularization eRegularization) const;  computes the error of the DNN ;  ; template<typename OutputContainer > ; void fetchOutput (const LayerData &lastLayerData, OutputContainer &outputContainer) const;  ; template<typename OutputContainer > ; void fetchOutput (const std::vector< LayerData > &layerPatternData, OutputContainer &outputContainer) const;  ; template<typename LayerContainer , typename PassThrough , typename ItWeight , typename ItGradient , typename OutContainer > ; double forward_backward (LayerContainer &layers, PassThrough &settingsAndBatch, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t trainFromLayer, OutContainer &outputContainer, bool fetchOutput) const;  main NN computation function ;  ; template<typename LayerContainer , typename LayerPatternContainer > ; void forwardBatch (const LayerContainer &_layers, LayerPatternContainer &layerPatternData, std::vector< double > &valuesMean, std::vector< double > &valuesStdDev, size_t trainFromLayer) const;  ; template<typename LayerContainer > ; void forwardPattern (const LayerContainer &_layers, std::vector< LayerData > &layerData) const;  ; template<typename OutIterator > ; void initializeWeights (WeightInitializationStrategy eInitStrategy, OutIterator itWeight);  initialize the weights with the given strategy ;  ; size_t inputSize () const;  input size of the DNN ;  ; std::vector< Layer > & layers ();  returns the layers (structure) ;  ; const std::vector< Layer > & layers () const;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:3365,Modifiability,layers,layers,3365,"ename ItGradient , typename OutContainer > ; double forward_backward (LayerContainer &layers, PassThrough &settingsAndBatch, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t trainFromLayer, OutContainer &outputContainer, bool fetchOutput) const;  main NN computation function ;  ; template<typename LayerContainer , typename LayerPatternContainer > ; void forwardBatch (const LayerContainer &_layers, LayerPatternContainer &layerPatternData, std::vector< double > &valuesMean, std::vector< double > &valuesStdDev, size_t trainFromLayer) const;  ; template<typename LayerContainer > ; void forwardPattern (const LayerContainer &_layers, std::vector< LayerData > &layerData) const;  ; template<typename OutIterator > ; void initializeWeights (WeightInitializationStrategy eInitStrategy, OutIterator itWeight);  initialize the weights with the given strategy ;  ; size_t inputSize () const;  input size of the DNN ;  ; std::vector< Layer > & layers ();  returns the layers (structure) ;  ; const std::vector< Layer > & layers () const;  returns the layers (structure) ;  ; size_t numNodes (size_t trainingStartLayer=0) const;  returns the number of nodes in this net ;  ; size_t numWeights (size_t trainingStartLayer=0) const;  returns the number of weights in this net ;  ; template<typename Weights , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights) const;  execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ;  ; template<typename Weights , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights, ModeOutput eFetch, OutContainer &outputContainer) const;  execute computation of the DNN for one mini-batch; helper function ;  ; template<typename Weights , typename Gradients , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:3389,Modifiability,layers,layers,3389,"ame OutContainer > ; double forward_backward (LayerContainer &layers, PassThrough &settingsAndBatch, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t trainFromLayer, OutContainer &outputContainer, bool fetchOutput) const;  main NN computation function ;  ; template<typename LayerContainer , typename LayerPatternContainer > ; void forwardBatch (const LayerContainer &_layers, LayerPatternContainer &layerPatternData, std::vector< double > &valuesMean, std::vector< double > &valuesStdDev, size_t trainFromLayer) const;  ; template<typename LayerContainer > ; void forwardPattern (const LayerContainer &_layers, std::vector< LayerData > &layerData) const;  ; template<typename OutIterator > ; void initializeWeights (WeightInitializationStrategy eInitStrategy, OutIterator itWeight);  initialize the weights with the given strategy ;  ; size_t inputSize () const;  input size of the DNN ;  ; std::vector< Layer > & layers ();  returns the layers (structure) ;  ; const std::vector< Layer > & layers () const;  returns the layers (structure) ;  ; size_t numNodes (size_t trainingStartLayer=0) const;  returns the number of nodes in this net ;  ; size_t numWeights (size_t trainingStartLayer=0) const;  returns the number of weights in this net ;  ; template<typename Weights , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights) const;  execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ;  ; template<typename Weights , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights, ModeOutput eFetch, OutContainer &outputContainer) const;  execute computation of the DNN for one mini-batch; helper function ;  ; template<typename Weights , typename Gradients , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients) const; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:3442,Modifiability,layers,layers,3442,"ntainer &layers, PassThrough &settingsAndBatch, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t trainFromLayer, OutContainer &outputContainer, bool fetchOutput) const;  main NN computation function ;  ; template<typename LayerContainer , typename LayerPatternContainer > ; void forwardBatch (const LayerContainer &_layers, LayerPatternContainer &layerPatternData, std::vector< double > &valuesMean, std::vector< double > &valuesStdDev, size_t trainFromLayer) const;  ; template<typename LayerContainer > ; void forwardPattern (const LayerContainer &_layers, std::vector< LayerData > &layerData) const;  ; template<typename OutIterator > ; void initializeWeights (WeightInitializationStrategy eInitStrategy, OutIterator itWeight);  initialize the weights with the given strategy ;  ; size_t inputSize () const;  input size of the DNN ;  ; std::vector< Layer > & layers ();  returns the layers (structure) ;  ; const std::vector< Layer > & layers () const;  returns the layers (structure) ;  ; size_t numNodes (size_t trainingStartLayer=0) const;  returns the number of nodes in this net ;  ; size_t numWeights (size_t trainingStartLayer=0) const;  returns the number of weights in this net ;  ; template<typename Weights , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights) const;  execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ;  ; template<typename Weights , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights, ModeOutput eFetch, OutContainer &outputContainer) const;  execute computation of the DNN for one mini-batch; helper function ;  ; template<typename Weights , typename Gradients , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients) const;  execute computation of the DNN for one mini-batch (u",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:3472,Modifiability,layers,layers,3472,"settingsAndBatch, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t trainFromLayer, OutContainer &outputContainer, bool fetchOutput) const;  main NN computation function ;  ; template<typename LayerContainer , typename LayerPatternContainer > ; void forwardBatch (const LayerContainer &_layers, LayerPatternContainer &layerPatternData, std::vector< double > &valuesMean, std::vector< double > &valuesStdDev, size_t trainFromLayer) const;  ; template<typename LayerContainer > ; void forwardPattern (const LayerContainer &_layers, std::vector< LayerData > &layerData) const;  ; template<typename OutIterator > ; void initializeWeights (WeightInitializationStrategy eInitStrategy, OutIterator itWeight);  initialize the weights with the given strategy ;  ; size_t inputSize () const;  input size of the DNN ;  ; std::vector< Layer > & layers ();  returns the layers (structure) ;  ; const std::vector< Layer > & layers () const;  returns the layers (structure) ;  ; size_t numNodes (size_t trainingStartLayer=0) const;  returns the number of nodes in this net ;  ; size_t numWeights (size_t trainingStartLayer=0) const;  returns the number of weights in this net ;  ; template<typename Weights , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights) const;  execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ;  ; template<typename Weights , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights, ModeOutput eFetch, OutContainer &outputContainer) const;  execute computation of the DNN for one mini-batch; helper function ;  ; template<typename Weights , typename Gradients , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients) const;  execute computation of the DNN for one mini-batch (used by the minimizer); returns",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:4983,Modifiability,layers,layers,4983,"ugh , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, const Weights &weights, ModeOutput eFetch, OutContainer &outputContainer) const;  execute computation of the DNN for one mini-batch; helper function ;  ; template<typename Weights , typename Gradients , typename PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients) const;  execute computation of the DNN for one mini-batch (used by the minimizer); returns gradients as well ;  ; template<typename Weights , typename Gradients , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients, ModeOutput eFetch, OutContainer &outputContainer) const;  ; size_t outputSize () const;  output size of the DNN ;  ; template<typename LayerContainer , typename DropContainer , typename ItWeight , typename ItGradient > ; std::vector< std::vector< LayerData > > prepareLayerData (LayerContainer &layers, Batch &batch, const DropContainer &dropContainer, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t &totalNumWeights) const;  ; template<typename Minimizer > ; void preTrain (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  pre-training for future use ;  ; void removeLayer ();  remove one layer ;  ; void setErrorFunction (ModeErrorFunction eErrorFunction);  which error function is to be used ;  ; void setInputSize (size_t sizeInput);  set the input size of the DNN ;  ; void SetIpythonInteractive (IPythonInteractive *fI, bool *fE, UInt_t *M, UInt_t *C);  ; void setOutputSize (size_t sizeOutput);  set the output size of the DNN ;  ; template<typename Minimizer > ; double train (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &s",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:9310,Modifiability,config,configuration,9310,"onst. compute the net with the given input and the given weights ; compute the neural net; Parameters. inputthe input data ; weightsthe weight data . Definition at line 1037 of file NeuralNet.icc. ◆ computeError(). template<typename ItWeight > . std::tuple< double, double > TMVA::DNN::Net::computeError ; (; const Settings & ; settings, . std::vector< LayerData > & ; lastLayerData, . Batch & ; batch, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd . ); const. Definition at line 1321 of file NeuralNet.icc. ◆ dE(). void TMVA::DNN::Net::dE ; (; ). ◆ dropOutWeightFactor(). template<typename WeightsType , typename DropProbabilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; inverse = false . ). set the drop out configuration ; compute the drop-out-weight factor; when using drop-out a fraction of the nodes is turned off at each cycle of the computation once all nodes are turned on again (for instances when the test samples are evaluated), the weights have to be adjusted to account for the different number of active nodes this function computes the factor and applies it to the weights ; Definition at line 652 of file NeuralNet.icc. ◆ E(). double TMVA::DNN::Net::E ; (; ). ◆ errorFunction(). template<typename Container , typename ItWeight > . double TMVA::DNN::Net::errorFunction ; (; LayerData & ; layerData, . Container ; truth, . ItWeight ; itWeight, . ItWeight ; itWeightEnd, . double ; patternWeight, . double ; factorWeightDecay, . EnumRegularization ; eRegularization . ); const. computes the error of the DNN ; compute the error function; Definition at line 1579 of file NeuralNet.icc. ◆ fetchOutput() [1/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const LayerData & ; lastLayerData, . OutputContainer & ; outputContainer . ); const. Definition at line 1291 of file NeuralNet.icc. ◆ fetchOutput() [2/2]. template<typename OutputContainer > . void TMVA::DNN::Net::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:12689,Modifiability,layers,layers,12689,"me LayerPatternContainer > . void TMVA::DNN::Net::forwardBatch ; (; const LayerContainer & ; _layers, . LayerPatternContainer & ; layerPatternData, . std::vector< double > & ; valuesMean, . std::vector< double > & ; valuesStdDev, . size_t ; trainFromLayer . ); const. Definition at line 1240 of file NeuralNet.icc. ◆ forwardPattern(). template<typename LayerContainer > . void TMVA::DNN::Net::forwardPattern ; (; const LayerContainer & ; _layers, . std::vector< LayerData > & ; layerData . ); const. Definition at line 1221 of file NeuralNet.icc. ◆ initializeWeights(). template<typename OutIterator > . void TMVA::DNN::Net::initializeWeights ; (; WeightInitializationStrategy ; eInitStrategy, . OutIterator ; itWeight . ). initialize the weights with the given strategy ; initialization of the weights; Definition at line 1470 of file NeuralNet.icc. ◆ inputSize(). size_t TMVA::DNN::Net::inputSize ; (; ); const. inline . input size of the DNN ; Definition at line 1098 of file NeuralNet.h. ◆ layers() [1/2]. std::vector< Layer > & TMVA::DNN::Net::layers ; (; ). inline . returns the layers (structure) ; Definition at line 1246 of file NeuralNet.h. ◆ layers() [2/2]. const std::vector< Layer > & TMVA::DNN::Net::layers ; (; ); const. inline . returns the layers (structure) ; Definition at line 1245 of file NeuralNet.h. ◆ numNodes(). size_t TMVA::DNN::Net::numNodes ; (; size_t ; trainingStartLayer = 0); const. returns the number of nodes in this net ; Definition at line 556 of file NeuralNet.cxx. ◆ numWeights(). size_t TMVA::DNN::Net::numWeights ; (; size_t ; trainingStartLayer = 0); const. returns the number of weights in this net ; compute the number of weights given the size of the input layer; Definition at line 540 of file NeuralNet.cxx. ◆ operator()() [1/4]. template<typename Weights , typename PassThrough > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . const Weights & ; weights . ); const. execute computation of the DNN for one mini-batch (used by",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:12744,Modifiability,layers,layers,12744,"VA::DNN::Net::forwardBatch ; (; const LayerContainer & ; _layers, . LayerPatternContainer & ; layerPatternData, . std::vector< double > & ; valuesMean, . std::vector< double > & ; valuesStdDev, . size_t ; trainFromLayer . ); const. Definition at line 1240 of file NeuralNet.icc. ◆ forwardPattern(). template<typename LayerContainer > . void TMVA::DNN::Net::forwardPattern ; (; const LayerContainer & ; _layers, . std::vector< LayerData > & ; layerData . ); const. Definition at line 1221 of file NeuralNet.icc. ◆ initializeWeights(). template<typename OutIterator > . void TMVA::DNN::Net::initializeWeights ; (; WeightInitializationStrategy ; eInitStrategy, . OutIterator ; itWeight . ). initialize the weights with the given strategy ; initialization of the weights; Definition at line 1470 of file NeuralNet.icc. ◆ inputSize(). size_t TMVA::DNN::Net::inputSize ; (; ); const. inline . input size of the DNN ; Definition at line 1098 of file NeuralNet.h. ◆ layers() [1/2]. std::vector< Layer > & TMVA::DNN::Net::layers ; (; ). inline . returns the layers (structure) ; Definition at line 1246 of file NeuralNet.h. ◆ layers() [2/2]. const std::vector< Layer > & TMVA::DNN::Net::layers ; (; ); const. inline . returns the layers (structure) ; Definition at line 1245 of file NeuralNet.h. ◆ numNodes(). size_t TMVA::DNN::Net::numNodes ; (; size_t ; trainingStartLayer = 0); const. returns the number of nodes in this net ; Definition at line 556 of file NeuralNet.cxx. ◆ numWeights(). size_t TMVA::DNN::Net::numWeights ; (; size_t ; trainingStartLayer = 0); const. returns the number of weights in this net ; compute the number of weights given the size of the input layer; Definition at line 540 of file NeuralNet.cxx. ◆ operator()() [1/4]. template<typename Weights , typename PassThrough > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . const Weights & ; weights . ); const. execute computation of the DNN for one mini-batch (used by the minimizer); no computation of g",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:12780,Modifiability,layers,layers,12780,"atternContainer & ; layerPatternData, . std::vector< double > & ; valuesMean, . std::vector< double > & ; valuesStdDev, . size_t ; trainFromLayer . ); const. Definition at line 1240 of file NeuralNet.icc. ◆ forwardPattern(). template<typename LayerContainer > . void TMVA::DNN::Net::forwardPattern ; (; const LayerContainer & ; _layers, . std::vector< LayerData > & ; layerData . ); const. Definition at line 1221 of file NeuralNet.icc. ◆ initializeWeights(). template<typename OutIterator > . void TMVA::DNN::Net::initializeWeights ; (; WeightInitializationStrategy ; eInitStrategy, . OutIterator ; itWeight . ). initialize the weights with the given strategy ; initialization of the weights; Definition at line 1470 of file NeuralNet.icc. ◆ inputSize(). size_t TMVA::DNN::Net::inputSize ; (; ); const. inline . input size of the DNN ; Definition at line 1098 of file NeuralNet.h. ◆ layers() [1/2]. std::vector< Layer > & TMVA::DNN::Net::layers ; (; ). inline . returns the layers (structure) ; Definition at line 1246 of file NeuralNet.h. ◆ layers() [2/2]. const std::vector< Layer > & TMVA::DNN::Net::layers ; (; ); const. inline . returns the layers (structure) ; Definition at line 1245 of file NeuralNet.h. ◆ numNodes(). size_t TMVA::DNN::Net::numNodes ; (; size_t ; trainingStartLayer = 0); const. returns the number of nodes in this net ; Definition at line 556 of file NeuralNet.cxx. ◆ numWeights(). size_t TMVA::DNN::Net::numWeights ; (; size_t ; trainingStartLayer = 0); const. returns the number of weights in this net ; compute the number of weights given the size of the input layer; Definition at line 540 of file NeuralNet.cxx. ◆ operator()() [1/4]. template<typename Weights , typename PassThrough > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . const Weights & ; weights . ); const. execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ; Definition at line 1070 of file NeuralNet.icc. ◆ operator()() [",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:12848,Modifiability,layers,layers,12848,"or< double > & ; valuesMean, . std::vector< double > & ; valuesStdDev, . size_t ; trainFromLayer . ); const. Definition at line 1240 of file NeuralNet.icc. ◆ forwardPattern(). template<typename LayerContainer > . void TMVA::DNN::Net::forwardPattern ; (; const LayerContainer & ; _layers, . std::vector< LayerData > & ; layerData . ); const. Definition at line 1221 of file NeuralNet.icc. ◆ initializeWeights(). template<typename OutIterator > . void TMVA::DNN::Net::initializeWeights ; (; WeightInitializationStrategy ; eInitStrategy, . OutIterator ; itWeight . ). initialize the weights with the given strategy ; initialization of the weights; Definition at line 1470 of file NeuralNet.icc. ◆ inputSize(). size_t TMVA::DNN::Net::inputSize ; (; ); const. inline . input size of the DNN ; Definition at line 1098 of file NeuralNet.h. ◆ layers() [1/2]. std::vector< Layer > & TMVA::DNN::Net::layers ; (; ). inline . returns the layers (structure) ; Definition at line 1246 of file NeuralNet.h. ◆ layers() [2/2]. const std::vector< Layer > & TMVA::DNN::Net::layers ; (; ); const. inline . returns the layers (structure) ; Definition at line 1245 of file NeuralNet.h. ◆ numNodes(). size_t TMVA::DNN::Net::numNodes ; (; size_t ; trainingStartLayer = 0); const. returns the number of nodes in this net ; Definition at line 556 of file NeuralNet.cxx. ◆ numWeights(). size_t TMVA::DNN::Net::numWeights ; (; size_t ; trainingStartLayer = 0); const. returns the number of weights in this net ; compute the number of weights given the size of the input layer; Definition at line 540 of file NeuralNet.cxx. ◆ operator()() [1/4]. template<typename Weights , typename PassThrough > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . const Weights & ; weights . ); const. execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ; Definition at line 1070 of file NeuralNet.icc. ◆ operator()() [2/4]. template<typename Weights , typename PassTh",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:12909,Modifiability,layers,layers,12909,"< double > & ; valuesStdDev, . size_t ; trainFromLayer . ); const. Definition at line 1240 of file NeuralNet.icc. ◆ forwardPattern(). template<typename LayerContainer > . void TMVA::DNN::Net::forwardPattern ; (; const LayerContainer & ; _layers, . std::vector< LayerData > & ; layerData . ); const. Definition at line 1221 of file NeuralNet.icc. ◆ initializeWeights(). template<typename OutIterator > . void TMVA::DNN::Net::initializeWeights ; (; WeightInitializationStrategy ; eInitStrategy, . OutIterator ; itWeight . ). initialize the weights with the given strategy ; initialization of the weights; Definition at line 1470 of file NeuralNet.icc. ◆ inputSize(). size_t TMVA::DNN::Net::inputSize ; (; ); const. inline . input size of the DNN ; Definition at line 1098 of file NeuralNet.h. ◆ layers() [1/2]. std::vector< Layer > & TMVA::DNN::Net::layers ; (; ). inline . returns the layers (structure) ; Definition at line 1246 of file NeuralNet.h. ◆ layers() [2/2]. const std::vector< Layer > & TMVA::DNN::Net::layers ; (; ); const. inline . returns the layers (structure) ; Definition at line 1245 of file NeuralNet.h. ◆ numNodes(). size_t TMVA::DNN::Net::numNodes ; (; size_t ; trainingStartLayer = 0); const. returns the number of nodes in this net ; Definition at line 556 of file NeuralNet.cxx. ◆ numWeights(). size_t TMVA::DNN::Net::numWeights ; (; size_t ; trainingStartLayer = 0); const. returns the number of weights in this net ; compute the number of weights given the size of the input layer; Definition at line 540 of file NeuralNet.cxx. ◆ operator()() [1/4]. template<typename Weights , typename PassThrough > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . const Weights & ; weights . ); const. execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ; Definition at line 1070 of file NeuralNet.icc. ◆ operator()() [2/4]. template<typename Weights , typename PassThrough , typename OutContainer > . double TM",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:12952,Modifiability,layers,layers,12952,"line 1240 of file NeuralNet.icc. ◆ forwardPattern(). template<typename LayerContainer > . void TMVA::DNN::Net::forwardPattern ; (; const LayerContainer & ; _layers, . std::vector< LayerData > & ; layerData . ); const. Definition at line 1221 of file NeuralNet.icc. ◆ initializeWeights(). template<typename OutIterator > . void TMVA::DNN::Net::initializeWeights ; (; WeightInitializationStrategy ; eInitStrategy, . OutIterator ; itWeight . ). initialize the weights with the given strategy ; initialization of the weights; Definition at line 1470 of file NeuralNet.icc. ◆ inputSize(). size_t TMVA::DNN::Net::inputSize ; (; ); const. inline . input size of the DNN ; Definition at line 1098 of file NeuralNet.h. ◆ layers() [1/2]. std::vector< Layer > & TMVA::DNN::Net::layers ; (; ). inline . returns the layers (structure) ; Definition at line 1246 of file NeuralNet.h. ◆ layers() [2/2]. const std::vector< Layer > & TMVA::DNN::Net::layers ; (; ); const. inline . returns the layers (structure) ; Definition at line 1245 of file NeuralNet.h. ◆ numNodes(). size_t TMVA::DNN::Net::numNodes ; (; size_t ; trainingStartLayer = 0); const. returns the number of nodes in this net ; Definition at line 556 of file NeuralNet.cxx. ◆ numWeights(). size_t TMVA::DNN::Net::numWeights ; (; size_t ; trainingStartLayer = 0); const. returns the number of weights in this net ; compute the number of weights given the size of the input layer; Definition at line 540 of file NeuralNet.cxx. ◆ operator()() [1/4]. template<typename Weights , typename PassThrough > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . const Weights & ; weights . ); const. execute computation of the DNN for one mini-batch (used by the minimizer); no computation of gradients ; Definition at line 1070 of file NeuralNet.icc. ◆ operator()() [2/4]. template<typename Weights , typename PassThrough , typename OutContainer > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . const Weights ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:15283,Modifiability,layers,layers,15283,"dients , typename PassThrough > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . Weights & ; weights, . Gradients & ; gradients . ); const. execute computation of the DNN for one mini-batch (used by the minimizer); returns gradients as well ; Definition at line 1089 of file NeuralNet.icc. ◆ operator()() [4/4]. template<typename Weights , typename Gradients , typename PassThrough , typename OutContainer > . double TMVA::DNN::Net::operator() ; (; PassThrough & ; settingsAndBatch, . Weights & ; weights, . Gradients & ; gradients, . ModeOutput ; eFetch, . OutContainer & ; outputContainer . ); const. Definition at line 1099 of file NeuralNet.icc. ◆ outputSize(). size_t TMVA::DNN::Net::outputSize ; (; ); const. inline . output size of the DNN ; Definition at line 1099 of file NeuralNet.h. ◆ prepareLayerData(). template<typename LayerContainer , typename DropContainer , typename ItWeight , typename ItGradient > . std::vector< std::vector< LayerData > > TMVA::DNN::Net::prepareLayerData ; (; LayerContainer & ; layers, . Batch & ; batch, . const DropContainer & ; dropContainer, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd, . ItGradient ; itGradientBegin, . ItGradient ; itGradientEnd, . size_t & ; totalNumWeights . ); const. Definition at line 1111 of file NeuralNet.icc. ◆ preTrain(). template<typename Minimizer > . void TMVA::DNN::Net::preTrain ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). pre-training for future use . ◆ removeLayer(). void TMVA::DNN::Net::removeLayer ; (; ). inline . remove one layer ; Definition at line 1248 of file NeuralNet.h. ◆ setErrorFunction(). void TMVA::DNN::Net::setErrorFunction ; (; ModeErrorFunction ; eErrorFunction). inline . which error function is to be used ; Definition at line 1096 of file NeuralNet.h. ◆ setInputSize(). void TMVA::DNN::Net::setInputSize ; (; size_t ; s",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:18178,Modifiability,config,configuration,18178,"tterntraining pattern ; testPatterntest pattern ; minimizeruse this minimizer for training (e.g. SGD) ; settingssettings used for this training run. Parameters. weightsthe container with the weights (synapses) ; trainPatternthe pattern for the training ; testPatternthe pattern for the testing ; minimizerthe minimizer (e.g. steepest gradient descent) to be used ; settingsthe settings for the training (e.g. multithreading or not, regularization etc.) . Definition at line 712 of file NeuralNet.icc. ◆ trainCycle(). template<typename Iterator , typename Minimizer > . double TMVA::DNN::Net::trainCycle ; (; Minimizer & ; minimizer, . std::vector< double > & ; weights, . Iterator ; itPatternBegin, . Iterator ; itPatternEnd, . Settings & ; settings, . DropContainer & ; dropContainer . ). inline . executes one training cycle ; execute a single training cycle; Parameters. minimizerthe minimizer to be used ; weightsthe weight vector to be used ; itPatternBeginthe pattern to be trained with ; itPatternEndthe pattern to be trained with ; settingsthe settings for the training ; dropContainerthe configuration for DNN drop-out. uses multithreading if turned on; Parameters. minimizerthe minimizer to be used (e.g. SGD) ; weightsthe weight container with all the synapse weights ; itPatternBeginbegin of the pattern container ; itPatternEndthe end of the pattern container ; settingsthe settings for this training (e.g. multithreading or not, regularization, etc.) ; dropContainerthe data for dropping-out nodes (regularization technique) . Definition at line 939 of file NeuralNet.icc. Member Data Documentation. ◆ fExitFromTraining. bool* TMVA::DNN::Net::fExitFromTraining = nullptr. protected . Definition at line 1277 of file NeuralNet.h. ◆ fInteractive. IPythonInteractive* TMVA::DNN::Net::fInteractive = nullptr. protected . Definition at line 1276 of file NeuralNet.h. ◆ fIPyCurrentIter. UInt_t * TMVA::DNN::Net::fIPyCurrentIter = nullptr. protected . Definition at line 1278 of file NeuralNet.",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:5323,Testability,test,testPattern,5323,"e PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients) const;  execute computation of the DNN for one mini-batch (used by the minimizer); returns gradients as well ;  ; template<typename Weights , typename Gradients , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients, ModeOutput eFetch, OutContainer &outputContainer) const;  ; size_t outputSize () const;  output size of the DNN ;  ; template<typename LayerContainer , typename DropContainer , typename ItWeight , typename ItGradient > ; std::vector< std::vector< LayerData > > prepareLayerData (LayerContainer &layers, Batch &batch, const DropContainer &dropContainer, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t &totalNumWeights) const;  ; template<typename Minimizer > ; void preTrain (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  pre-training for future use ;  ; void removeLayer ();  remove one layer ;  ; void setErrorFunction (ModeErrorFunction eErrorFunction);  which error function is to be used ;  ; void setInputSize (size_t sizeInput);  set the input size of the DNN ;  ; void SetIpythonInteractive (IPythonInteractive *fI, bool *fE, UInt_t *M, UInt_t *C);  ; void setOutputSize (size_t sizeOutput);  set the output size of the DNN ;  ; template<typename Minimizer > ; double train (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  start the training ;  ; template<typename Iterator , typename Minimizer > ; double trainCycle (Minimizer &minimizer, std::vector< double > &weights, Iterator itPatternBegin, Iterator itPatternEnd, Settings &settings, DropContainer &dropContainer);  executes one training cycle ;  .",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:5941,Testability,test,testPattern,5941,"e PassThrough > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients) const;  execute computation of the DNN for one mini-batch (used by the minimizer); returns gradients as well ;  ; template<typename Weights , typename Gradients , typename PassThrough , typename OutContainer > ; double operator() (PassThrough &settingsAndBatch, Weights &weights, Gradients &gradients, ModeOutput eFetch, OutContainer &outputContainer) const;  ; size_t outputSize () const;  output size of the DNN ;  ; template<typename LayerContainer , typename DropContainer , typename ItWeight , typename ItGradient > ; std::vector< std::vector< LayerData > > prepareLayerData (LayerContainer &layers, Batch &batch, const DropContainer &dropContainer, ItWeight itWeightBegin, ItWeight itWeightEnd, ItGradient itGradientBegin, ItGradient itGradientEnd, size_t &totalNumWeights) const;  ; template<typename Minimizer > ; void preTrain (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  pre-training for future use ;  ; void removeLayer ();  remove one layer ;  ; void setErrorFunction (ModeErrorFunction eErrorFunction);  which error function is to be used ;  ; void setInputSize (size_t sizeInput);  set the input size of the DNN ;  ; void SetIpythonInteractive (IPythonInteractive *fI, bool *fE, UInt_t *M, UInt_t *C);  ; void setOutputSize (size_t sizeOutput);  set the output size of the DNN ;  ; template<typename Minimizer > ; double train (std::vector< double > &weights, std::vector< Pattern > &trainPattern, const std::vector< Pattern > &testPattern, Minimizer &minimizer, Settings &settings);  start the training ;  ; template<typename Iterator , typename Minimizer > ; double trainCycle (Minimizer &minimizer, std::vector< double > &weights, Iterator itPatternBegin, Iterator itPatternEnd, Settings &settings, DropContainer &dropContainer);  executes one training cycle ;  .",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:9512,Testability,test,test,9512,"onst. compute the net with the given input and the given weights ; compute the neural net; Parameters. inputthe input data ; weightsthe weight data . Definition at line 1037 of file NeuralNet.icc. ◆ computeError(). template<typename ItWeight > . std::tuple< double, double > TMVA::DNN::Net::computeError ; (; const Settings & ; settings, . std::vector< LayerData > & ; lastLayerData, . Batch & ; batch, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd . ); const. Definition at line 1321 of file NeuralNet.icc. ◆ dE(). void TMVA::DNN::Net::dE ; (; ). ◆ dropOutWeightFactor(). template<typename WeightsType , typename DropProbabilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; inverse = false . ). set the drop out configuration ; compute the drop-out-weight factor; when using drop-out a fraction of the nodes is turned off at each cycle of the computation once all nodes are turned on again (for instances when the test samples are evaluated), the weights have to be adjusted to account for the different number of active nodes this function computes the factor and applies it to the weights ; Definition at line 652 of file NeuralNet.icc. ◆ E(). double TMVA::DNN::Net::E ; (; ). ◆ errorFunction(). template<typename Container , typename ItWeight > . double TMVA::DNN::Net::errorFunction ; (; LayerData & ; layerData, . Container ; truth, . ItWeight ; itWeight, . ItWeight ; itWeightEnd, . double ; patternWeight, . double ; factorWeightDecay, . EnumRegularization ; eRegularization . ); const. computes the error of the DNN ; compute the error function; Definition at line 1579 of file NeuralNet.icc. ◆ fetchOutput() [1/2]. template<typename OutputContainer > . void TMVA::DNN::Net::fetchOutput ; (; const LayerData & ; lastLayerData, . OutputContainer & ; outputContainer . ); const. Definition at line 1291 of file NeuralNet.icc. ◆ fetchOutput() [2/2]. template<typename OutputContainer > . void TMVA::DNN::Net::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:15749,Testability,test,testPattern,15749,"AndBatch, . Weights & ; weights, . Gradients & ; gradients, . ModeOutput ; eFetch, . OutContainer & ; outputContainer . ); const. Definition at line 1099 of file NeuralNet.icc. ◆ outputSize(). size_t TMVA::DNN::Net::outputSize ; (; ); const. inline . output size of the DNN ; Definition at line 1099 of file NeuralNet.h. ◆ prepareLayerData(). template<typename LayerContainer , typename DropContainer , typename ItWeight , typename ItGradient > . std::vector< std::vector< LayerData > > TMVA::DNN::Net::prepareLayerData ; (; LayerContainer & ; layers, . Batch & ; batch, . const DropContainer & ; dropContainer, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd, . ItGradient ; itGradientBegin, . ItGradient ; itGradientEnd, . size_t & ; totalNumWeights . ); const. Definition at line 1111 of file NeuralNet.icc. ◆ preTrain(). template<typename Minimizer > . void TMVA::DNN::Net::preTrain ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). pre-training for future use . ◆ removeLayer(). void TMVA::DNN::Net::removeLayer ; (; ). inline . remove one layer ; Definition at line 1248 of file NeuralNet.h. ◆ setErrorFunction(). void TMVA::DNN::Net::setErrorFunction ; (; ModeErrorFunction ; eErrorFunction). inline . which error function is to be used ; Definition at line 1096 of file NeuralNet.h. ◆ setInputSize(). void TMVA::DNN::Net::setInputSize ; (; size_t ; sizeInput). inline . set the input size of the DNN ; Definition at line 1092 of file NeuralNet.h. ◆ SetIpythonInteractive(). void TMVA::DNN::Net::SetIpythonInteractive ; (; IPythonInteractive * ; fI, . bool * ; fE, . UInt_t * ; M, . UInt_t * ; C . ). inline . Definition at line 1283 of file NeuralNet.h. ◆ setOutputSize(). void TMVA::DNN::Net::setOutputSize ; (; size_t ; sizeOutput). inline . set the output size of the DNN ; Definition at line 1093 of file NeuralNet.h. ◆ train(). template<typename",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:16901,Testability,test,testPattern,16901,"eLayer ; (; ). inline . remove one layer ; Definition at line 1248 of file NeuralNet.h. ◆ setErrorFunction(). void TMVA::DNN::Net::setErrorFunction ; (; ModeErrorFunction ; eErrorFunction). inline . which error function is to be used ; Definition at line 1096 of file NeuralNet.h. ◆ setInputSize(). void TMVA::DNN::Net::setInputSize ; (; size_t ; sizeInput). inline . set the input size of the DNN ; Definition at line 1092 of file NeuralNet.h. ◆ SetIpythonInteractive(). void TMVA::DNN::Net::SetIpythonInteractive ; (; IPythonInteractive * ; fI, . bool * ; fE, . UInt_t * ; M, . UInt_t * ; C . ). inline . Definition at line 1283 of file NeuralNet.h. ◆ setOutputSize(). void TMVA::DNN::Net::setOutputSize ; (; size_t ; sizeOutput). inline . set the output size of the DNN ; Definition at line 1093 of file NeuralNet.h. ◆ train(). template<typename Minimizer > . double TMVA::DNN::Net::train ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). start the training ; execute the training until convergence emerges; Parameters. weightsweight vector ; trainPatterntraining pattern ; testPatterntest pattern ; minimizeruse this minimizer for training (e.g. SGD) ; settingssettings used for this training run. Parameters. weightsthe container with the weights (synapses) ; trainPatternthe pattern for the training ; testPatternthe pattern for the testing ; minimizerthe minimizer (e.g. steepest gradient descent) to be used ; settingsthe settings for the training (e.g. multithreading or not, regularization etc.) . Definition at line 712 of file NeuralNet.icc. ◆ trainCycle(). template<typename Iterator , typename Minimizer > . double TMVA::DNN::Net::trainCycle ; (; Minimizer & ; minimizer, . std::vector< double > & ; weights, . Iterator ; itPatternBegin, . Iterator ; itPatternEnd, . Settings & ; settings, . DropContainer & ; dropContainer . ). inline . executes on",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:17105,Testability,test,testPatterntest,17105,"s to be used ; Definition at line 1096 of file NeuralNet.h. ◆ setInputSize(). void TMVA::DNN::Net::setInputSize ; (; size_t ; sizeInput). inline . set the input size of the DNN ; Definition at line 1092 of file NeuralNet.h. ◆ SetIpythonInteractive(). void TMVA::DNN::Net::SetIpythonInteractive ; (; IPythonInteractive * ; fI, . bool * ; fE, . UInt_t * ; M, . UInt_t * ; C . ). inline . Definition at line 1283 of file NeuralNet.h. ◆ setOutputSize(). void TMVA::DNN::Net::setOutputSize ; (; size_t ; sizeOutput). inline . set the output size of the DNN ; Definition at line 1093 of file NeuralNet.h. ◆ train(). template<typename Minimizer > . double TMVA::DNN::Net::train ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). start the training ; execute the training until convergence emerges; Parameters. weightsweight vector ; trainPatterntraining pattern ; testPatterntest pattern ; minimizeruse this minimizer for training (e.g. SGD) ; settingssettings used for this training run. Parameters. weightsthe container with the weights (synapses) ; trainPatternthe pattern for the training ; testPatternthe pattern for the testing ; minimizerthe minimizer (e.g. steepest gradient descent) to be used ; settingsthe settings for the training (e.g. multithreading or not, regularization etc.) . Definition at line 712 of file NeuralNet.icc. ◆ trainCycle(). template<typename Iterator , typename Minimizer > . double TMVA::DNN::Net::trainCycle ; (; Minimizer & ; minimizer, . std::vector< double > & ; weights, . Iterator ; itPatternBegin, . Iterator ; itPatternEnd, . Settings & ; settings, . DropContainer & ; dropContainer . ). inline . executes one training cycle ; execute a single training cycle; Parameters. minimizerthe minimizer to be used ; weightsthe weight vector to be used ; itPatternBeginthe pattern to be trained with ; itPatternEndthe pattern to be trained",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:17336,Testability,test,testPatternthe,17336," NeuralNet.h. ◆ SetIpythonInteractive(). void TMVA::DNN::Net::SetIpythonInteractive ; (; IPythonInteractive * ; fI, . bool * ; fE, . UInt_t * ; M, . UInt_t * ; C . ). inline . Definition at line 1283 of file NeuralNet.h. ◆ setOutputSize(). void TMVA::DNN::Net::setOutputSize ; (; size_t ; sizeOutput). inline . set the output size of the DNN ; Definition at line 1093 of file NeuralNet.h. ◆ train(). template<typename Minimizer > . double TMVA::DNN::Net::train ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). start the training ; execute the training until convergence emerges; Parameters. weightsweight vector ; trainPatterntraining pattern ; testPatterntest pattern ; minimizeruse this minimizer for training (e.g. SGD) ; settingssettings used for this training run. Parameters. weightsthe container with the weights (synapses) ; trainPatternthe pattern for the training ; testPatternthe pattern for the testing ; minimizerthe minimizer (e.g. steepest gradient descent) to be used ; settingsthe settings for the training (e.g. multithreading or not, regularization etc.) . Definition at line 712 of file NeuralNet.icc. ◆ trainCycle(). template<typename Iterator , typename Minimizer > . double TMVA::DNN::Net::trainCycle ; (; Minimizer & ; minimizer, . std::vector< double > & ; weights, . Iterator ; itPatternBegin, . Iterator ; itPatternEnd, . Settings & ; settings, . DropContainer & ; dropContainer . ). inline . executes one training cycle ; execute a single training cycle; Parameters. minimizerthe minimizer to be used ; weightsthe weight vector to be used ; itPatternBeginthe pattern to be trained with ; itPatternEndthe pattern to be trained with ; settingsthe settings for the training ; dropContainerthe configuration for DNN drop-out. uses multithreading if turned on; Parameters. minimizerthe minimizer to be used (e.g. SGD) ; weightsthe weight c",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:17367,Testability,test,testing,17367," NeuralNet.h. ◆ SetIpythonInteractive(). void TMVA::DNN::Net::SetIpythonInteractive ; (; IPythonInteractive * ; fI, . bool * ; fE, . UInt_t * ; M, . UInt_t * ; C . ). inline . Definition at line 1283 of file NeuralNet.h. ◆ setOutputSize(). void TMVA::DNN::Net::setOutputSize ; (; size_t ; sizeOutput). inline . set the output size of the DNN ; Definition at line 1093 of file NeuralNet.h. ◆ train(). template<typename Minimizer > . double TMVA::DNN::Net::train ; (; std::vector< double > & ; weights, . std::vector< Pattern > & ; trainPattern, . const std::vector< Pattern > & ; testPattern, . Minimizer & ; minimizer, . Settings & ; settings . ). start the training ; execute the training until convergence emerges; Parameters. weightsweight vector ; trainPatterntraining pattern ; testPatterntest pattern ; minimizeruse this minimizer for training (e.g. SGD) ; settingssettings used for this training run. Parameters. weightsthe container with the weights (synapses) ; trainPatternthe pattern for the training ; testPatternthe pattern for the testing ; minimizerthe minimizer (e.g. steepest gradient descent) to be used ; settingsthe settings for the training (e.g. multithreading or not, regularization etc.) . Definition at line 712 of file NeuralNet.icc. ◆ trainCycle(). template<typename Iterator , typename Minimizer > . double TMVA::DNN::Net::trainCycle ; (; Minimizer & ; minimizer, . std::vector< double > & ; weights, . Iterator ; itPatternBegin, . Iterator ; itPatternEnd, . Settings & ; settings, . DropContainer & ; dropContainer . ). inline . executes one training cycle ; execute a single training cycle; Parameters. minimizerthe minimizer to be used ; weightsthe weight vector to be used ; itPatternBeginthe pattern to be trained with ; itPatternEndthe pattern to be trained with ; settingsthe settings for the training ; dropContainerthe configuration for DNN drop-out. uses multithreading if turned on; Parameters. minimizerthe minimizer to be used (e.g. SGD) ; weightsthe weight c",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:1075,Usability,clear,clear,1075," Public Member Functions;  Net ();  c'tor ;  ;  Net (const Net &other);  d'tor ;  ; void addLayer (Layer &&layer);  ; void addLayer (Layer &layer);  add a layer (layout) ;  ; template<typename Settings > ; void backPropagate (std::vector< std::vector< LayerData > > &layerPatternData, const Settings &settings, size_t trainFromLayer, size_t totalNumWeights) const;  ; void clear ();  ; template<typename Weights > ; std::vector< double > compute (const std::vector< double > &input, const Weights &weights) const;  compute the net with the given input and the given weights ;  ; template<typename ItWeight > ; std::tuple< double, double > computeError (const Settings &settings, std::vector< LayerData > &lastLayerData, Batch &batch, ItWeight itWeightBegin, ItWeight itWeightEnd) const;  ; void dE ();  ; template<typename WeightsType , typename DropProbabilities > ; void dropOutWeightFactor (WeightsType &weights, const DropProbabilities &drops, bool inverse=false);  set the drop out configuration ;  ; double E ();  ; template<typename Container , typename ItWeight > ; double errorFunction (LayerData &layerData, Container truth, ItWeight itWeight, ItWeight itWeightEnd, double patternWeight, double factorWeightDecay, EnumRegularization eRegularization) const;  computes the error of the DNN ;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:8247,Usability,clear,clear,8247,"erator_type. typedef container_type::iterator TMVA::DNN::Net::iterator_type. Definition at line 1066 of file NeuralNet.h. Constructor & Destructor Documentation. ◆ Net() [1/2]. TMVA::DNN::Net::Net ; (; ). inline . c'tor ; Definition at line 1074 of file NeuralNet.h. ◆ Net() [2/2]. TMVA::DNN::Net::Net ; (; const Net & ; other). inline . d'tor ; Definition at line 1085 of file NeuralNet.h. Member Function Documentation. ◆ addLayer() [1/2]. void TMVA::DNN::Net::addLayer ; (; Layer && ; layer). inline . Definition at line 1095 of file NeuralNet.h. ◆ addLayer() [2/2]. void TMVA::DNN::Net::addLayer ; (; Layer & ; layer). inline . add a layer (layout) ; Definition at line 1094 of file NeuralNet.h. ◆ backPropagate(). template<typename Settings > . void TMVA::DNN::Net::backPropagate ; (; std::vector< std::vector< LayerData > > & ; layerPatternData, . const Settings & ; settings, . size_t ; trainFromLayer, . size_t ; totalNumWeights . ); const. Definition at line 1355 of file NeuralNet.icc. ◆ clear(). void TMVA::DNN::Net::clear ; (; ). inline . Definition at line 1251 of file NeuralNet.h. ◆ compute(). template<typename Weights > . std::vector< double > TMVA::DNN::Net::compute ; (; const std::vector< double > & ; input, . const Weights & ; weights . ); const. compute the net with the given input and the given weights ; compute the neural net; Parameters. inputthe input data ; weightsthe weight data . Definition at line 1037 of file NeuralNet.icc. ◆ computeError(). template<typename ItWeight > . std::tuple< double, double > TMVA::DNN::Net::computeError ; (; const Settings & ; settings, . std::vector< LayerData > & ; lastLayerData, . Batch & ; batch, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd . ); const. Definition at line 1321 of file NeuralNet.icc. ◆ dE(). void TMVA::DNN::Net::dE ; (; ). ◆ dropOutWeightFactor(). template<typename WeightsType , typename DropProbabilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilitie",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html:8277,Usability,clear,clear,8277,"ntainer_type::iterator TMVA::DNN::Net::iterator_type. Definition at line 1066 of file NeuralNet.h. Constructor & Destructor Documentation. ◆ Net() [1/2]. TMVA::DNN::Net::Net ; (; ). inline . c'tor ; Definition at line 1074 of file NeuralNet.h. ◆ Net() [2/2]. TMVA::DNN::Net::Net ; (; const Net & ; other). inline . d'tor ; Definition at line 1085 of file NeuralNet.h. Member Function Documentation. ◆ addLayer() [1/2]. void TMVA::DNN::Net::addLayer ; (; Layer && ; layer). inline . Definition at line 1095 of file NeuralNet.h. ◆ addLayer() [2/2]. void TMVA::DNN::Net::addLayer ; (; Layer & ; layer). inline . add a layer (layout) ; Definition at line 1094 of file NeuralNet.h. ◆ backPropagate(). template<typename Settings > . void TMVA::DNN::Net::backPropagate ; (; std::vector< std::vector< LayerData > > & ; layerPatternData, . const Settings & ; settings, . size_t ; trainFromLayer, . size_t ; totalNumWeights . ); const. Definition at line 1355 of file NeuralNet.icc. ◆ clear(). void TMVA::DNN::Net::clear ; (; ). inline . Definition at line 1251 of file NeuralNet.h. ◆ compute(). template<typename Weights > . std::vector< double > TMVA::DNN::Net::compute ; (; const std::vector< double > & ; input, . const Weights & ; weights . ); const. compute the net with the given input and the given weights ; compute the neural net; Parameters. inputthe input data ; weightsthe weight data . Definition at line 1037 of file NeuralNet.icc. ◆ computeError(). template<typename ItWeight > . std::tuple< double, double > TMVA::DNN::Net::computeError ; (; const Settings & ; settings, . std::vector< LayerData > & ; lastLayerData, . Batch & ; batch, . ItWeight ; itWeightBegin, . ItWeight ; itWeightEnd . ); const. Definition at line 1321 of file NeuralNet.icc. ◆ dE(). void TMVA::DNN::Net::dE ; (; ). ◆ dropOutWeightFactor(). template<typename WeightsType , typename DropProbabilities > . void TMVA::DNN::Net::dropOutWeightFactor ; (; WeightsType & ; weights, . const DropProbabilities & ; drops, . bool ; i",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Net.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Net.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2452,Availability,error,error,2452,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:9429,Availability,error,error,9429,"tion at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer shall be used (e.g. SGD) ; Definition at line 774 of file NeuralNet.h. ◆ momentum(). double TMVA::DNN::Settings::momentum ; (; ); const. inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ; (; std::string ; histoName, . std::string ; options, . int ; pad, . EColor ; color . ). inline . for monitoring ; Definition at line 823 of file NeuralNet.h. ◆ regularization(). EnumRegularization TMVA::DNN::Settings::regularization ; (; ); const. inline . some regularization of the DNN is turned on? ; Definition at line 813 of file NeuralNet.h. ◆ repetitions(). int TMVA::DNN::Settings::repetitions ; (; ); const. inline . how many step",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3084,Deployability,configurat,configuration,3084,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:10721,Deployability,configurat,configuration,10721," inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ; (; std::string ; histoName, . std::string ; options, . int ; pad, . EColor ; color . ). inline . for monitoring ; Definition at line 823 of file NeuralNet.h. ◆ regularization(). EnumRegularization TMVA::DNN::Settings::regularization ; (; ); const. inline . some regularization of the DNN is turned on? ; Definition at line 813 of file NeuralNet.h. ◆ repetitions(). int TMVA::DNN::Settings::repetitions ; (; ); const. inline . how many steps have to be gone until the batch is changed ; Definition at line 773 of file NeuralNet.h. ◆ setDropOut(). template<typename Iterator > . void TMVA::DNN::Settings::setDropOut ; (; Iterator ; begin, . Iterator ; end, . size_t ; _dropRepetitions . ). inline . set the drop-out configuration (layer-wise) ; Parameters. beginbegin of an array or vector denoting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::Settings::setMonitoring ; (; std::shared_ptr< Monitoring > ; ptrMonitoring). inline . prepared for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:869,Energy Efficiency,monitor,monitoring,869,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:949,Energy Efficiency,monitor,monitoring,949,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1055,Energy Efficiency,monitor,monitoring,1055,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1152,Energy Efficiency,monitor,monitoring,1152,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1432,Energy Efficiency,monitor,monitoring,1432,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1561,Energy Efficiency,monitor,monitoring,1561,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1773,Energy Efficiency,monitor,monitoring,1773,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1938,Energy Efficiency,monitor,monitoring,1938,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2018,Energy Efficiency,monitor,monitoring,2018,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2088,Energy Efficiency,monitor,monitoring,2088,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2661,Energy Efficiency,monitor,monitoring,2661,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2761,Energy Efficiency,monitor,monitoring,2761,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3196,Energy Efficiency,monitor,monitoring,3196,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3340,Energy Efficiency,monitor,monitoring,3340,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3484,Energy Efficiency,monitor,monitoring,3484,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3677,Energy Efficiency,monitor,monitoring,3677,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:4528,Energy Efficiency,monitor,monitoring,4528,"toring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Protected Attributes; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::Settings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ Settings(). TMVA::DNN::Settings::Settings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . TMVA::DNN::EnumRegularization ; _regularization = TMVA::DNN::EnumRegularization::NONE, . MinimizerType ;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:5645,Energy Efficiency,monitor,monitoring,5645," Inheritance diagram for TMVA::DNN::Settings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ Settings(). TMVA::DNN::Settings::Settings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . TMVA::DNN::EnumRegularization ; _regularization = TMVA::DNN::EnumRegularization::NONE, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _multithreading = true . ). c'tor ; Definition at line 232 of file NeuralNet.cxx. ◆ ~Settings(). TMVA::DNN::Settings::~Settings ; (; ). virtual . d'tor ; Definition at line 261 of file NeuralNet.cxx. Member Function Documentation. ◆ addPoint() [1/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x . ). inline . for monitoring ; Definition at line 821 of file NeuralNet.h. ◆ addPoint() [2/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x, . double ; y . ). inline . for monitoring ; Definition at line 822 of file NeuralNet.h. ◆ batchSize(). size_t TMVA::DNN::Settings::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many ste",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:5832,Energy Efficiency,monitor,monitoring,5832,"ttings(). TMVA::DNN::Settings::Settings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . TMVA::DNN::EnumRegularization ; _regularization = TMVA::DNN::EnumRegularization::NONE, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _multithreading = true . ). c'tor ; Definition at line 232 of file NeuralNet.cxx. ◆ ~Settings(). TMVA::DNN::Settings::~Settings ; (; ). virtual . d'tor ; Definition at line 261 of file NeuralNet.cxx. Member Function Documentation. ◆ addPoint() [1/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x . ). inline . for monitoring ; Definition at line 821 of file NeuralNet.h. ◆ addPoint() [2/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x, . double ; y . ). inline . for monitoring ; Definition at line 822 of file NeuralNet.h. ◆ batchSize(). size_t TMVA::DNN::Settings::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:6115,Energy Efficiency,monitor,monitoring,6115," . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _multithreading = true . ). c'tor ; Definition at line 232 of file NeuralNet.cxx. ◆ ~Settings(). TMVA::DNN::Settings::~Settings ; (; ). virtual . d'tor ; Definition at line 261 of file NeuralNet.cxx. Member Function Documentation. ◆ addPoint() [1/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x . ). inline . for monitoring ; Definition at line 821 of file NeuralNet.h. ◆ addPoint() [2/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x, . double ; y . ). inline . for monitoring ; Definition at line 822 of file NeuralNet.h. ◆ batchSize(). size_t TMVA::DNN::Settings::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:6323,Energy Efficiency,monitor,monitoring,6323,"ine 232 of file NeuralNet.cxx. ◆ ~Settings(). TMVA::DNN::Settings::~Settings ; (; ). virtual . d'tor ; Definition at line 261 of file NeuralNet.cxx. Member Function Documentation. ◆ addPoint() [1/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x . ). inline . for monitoring ; Definition at line 821 of file NeuralNet.h. ◆ addPoint() [2/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x, . double ; y . ). inline . for monitoring ; Definition at line 822 of file NeuralNet.h. ◆ batchSize(). size_t TMVA::DNN::Settings::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ). inline . for monitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:6906,Energy Efficiency,monitor,monitoring,6906,"::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ). inline . for monitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance on the progress bar; progressthe new value; texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::S",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:7156,Energy Efficiency,monitor,monitoring,7156,"eResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ). inline . for monitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance on the progress bar; progressthe new value; texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::C",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:7670,Energy Efficiency,monitor,monitoring,7670,"d to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ). inline . for monitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance on the progress bar; progressthe new value; texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDec",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:8126,Energy Efficiency,monitor,monitoring,8126,"onitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance on the progress bar; progressthe new value; texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConv",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:8362,Energy Efficiency,monitor,monitoring,8362," texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); c",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:8573,Energy Efficiency,monitor,monitoring,8573,"td::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer sh",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:9907,Energy Efficiency,monitor,monitoring,9907,"? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer shall be used (e.g. SGD) ; Definition at line 774 of file NeuralNet.h. ◆ momentum(). double TMVA::DNN::Settings::momentum ; (; ); const. inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ; (; std::string ; histoName, . std::string ; options, . int ; pad, . EColor ; color . ). inline . for monitoring ; Definition at line 823 of file NeuralNet.h. ◆ regularization(). EnumRegularization TMVA::DNN::Settings::regularization ; (; ); const. inline . some regularization of the DNN is turned on? ; Definition at line 813 of file NeuralNet.h. ◆ repetitions(). int TMVA::DNN::Settings::repetitions ; (; ); const. inline . how many steps have to be gone until the batch is changed ; Definition at line 773 of file NeuralNet.h. ◆ setDropOut(). template<typename Iterator > . void TMVA::DNN::Settings::setDropOut ; (; Iterator ; begin, . Iterator ; end, . size_t ; _dropRepetitions . ). inline . set the drop-out configuration (layer-wise) ; Parameters. beginbegin of an array or vector denoting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each laye",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:10108,Energy Efficiency,monitor,monitoring,10108,"ile NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer shall be used (e.g. SGD) ; Definition at line 774 of file NeuralNet.h. ◆ momentum(). double TMVA::DNN::Settings::momentum ; (; ); const. inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ; (; std::string ; histoName, . std::string ; options, . int ; pad, . EColor ; color . ). inline . for monitoring ; Definition at line 823 of file NeuralNet.h. ◆ regularization(). EnumRegularization TMVA::DNN::Settings::regularization ; (; ); const. inline . some regularization of the DNN is turned on? ; Definition at line 813 of file NeuralNet.h. ◆ repetitions(). int TMVA::DNN::Settings::repetitions ; (; ); const. inline . how many steps have to be gone until the batch is changed ; Definition at line 773 of file NeuralNet.h. ◆ setDropOut(). template<typename Iterator > . void TMVA::DNN::Settings::setDropOut ; (; Iterator ; begin, . Iterator ; end, . size_t ; _dropRepetitions . ). inline . set the drop-out configuration (layer-wise) ; Parameters. beginbegin of an array or vector denoting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::S",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:11231,Energy Efficiency,monitor,monitoring,11231,"st. inline . some regularization of the DNN is turned on? ; Definition at line 813 of file NeuralNet.h. ◆ repetitions(). int TMVA::DNN::Settings::repetitions ; (; ); const. inline . how many steps have to be gone until the batch is changed ; Definition at line 773 of file NeuralNet.h. ◆ setDropOut(). template<typename Iterator > . void TMVA::DNN::Settings::setDropOut ; (; Iterator ; begin, . Iterator ; end, . size_t ; _dropRepetitions . ). inline . set the drop-out configuration (layer-wise) ; Parameters. beginbegin of an array or vector denoting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::Settings::setMonitoring ; (; std::shared_ptr< Monitoring > ; ptrMonitoring). inline . prepared for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMV",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:11471,Energy Efficiency,monitor,monitoring,11471,". ◆ setDropOut(). template<typename Iterator > . void TMVA::DNN::Settings::setDropOut ; (; Iterator ; begin, . Iterator ; end, . size_t ; _dropRepetitions . ). inline . set the drop-out configuration (layer-wise) ; Parameters. beginbegin of an array or vector denoting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::Settings::setMonitoring ; (; std::shared_ptr< Monitoring > ; ptrMonitoring). inline . prepared for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:11766,Energy Efficiency,monitor,monitoring,11766,"noting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::Settings::setMonitoring ; (; std::shared_ptr< Monitoring > ; ptrMonitoring). inline . prepared for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callba",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12320,Energy Efficiency,monitor,monitoring,12320,"gressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at li",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12780,Energy Efficiency,monitor,monitoring,12780,"nted in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at line 845 of file NeuralNet.h. ◆ count_mb_E. size_t TMVA::DNN::Settings::count_mb_E. Definition at line 844 of file NeuralNet.h. ◆ fLearningRate. double TMVA::DNN::Settings::fLearningRate. Definition at line 852 of file NeuralNet.h. ◆ fMinimizerType. MinimizerType TMVA::DNN::Settings::fMinimizerType. Definition at line 855 of file NeuralNet.h. ◆ fMomentum. double TMVA::DNN::Settings::fMomentum. Definition at line 853 of file NeuralNet.h. ◆ fMonitor",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:15575,Energy Efficiency,monitor,monitoring,15575,ings::m_batchSize. mini-batch size ; Definition at line 838 of file NeuralNet.h. ◆ m_convergenceCount. size_t TMVA::DNN::Settings::m_convergenceCount. Definition at line 857 of file NeuralNet.h. ◆ m_convergenceSteps. size_t TMVA::DNN::Settings::m_convergenceSteps. number of steps without improvement to consider the DNN to have converged ; Definition at line 837 of file NeuralNet.h. ◆ m_dropOut. std::vector<double> TMVA::DNN::Settings::m_dropOut. Definition at line 850 of file NeuralNet.h. ◆ m_dropRepetitions. double TMVA::DNN::Settings::m_dropRepetitions. Definition at line 849 of file NeuralNet.h. ◆ m_factorWeightDecay. double TMVA::DNN::Settings::m_factorWeightDecay. Definition at line 840 of file NeuralNet.h. ◆ m_maxConvergenceCount. size_t TMVA::DNN::Settings::m_maxConvergenceCount. Definition at line 858 of file NeuralNet.h. ◆ m_maxProgress. double TMVA::DNN::Settings::m_maxProgress. current limits for the progress bar ; Definition at line 834 of file NeuralNet.h. ◆ m_minError. double TMVA::DNN::Settings::m_minError. Definition at line 859 of file NeuralNet.h. ◆ m_minProgress. double TMVA::DNN::Settings::m_minProgress. current limits for the progress bar ; Definition at line 833 of file NeuralNet.h. ◆ m_regularization. EnumRegularization TMVA::DNN::Settings::m_regularization. Definition at line 847 of file NeuralNet.h. ◆ m_testRepetitions. size_t TMVA::DNN::Settings::m_testRepetitions. Definition at line 839 of file NeuralNet.h. ◆ m_timer. Timer TMVA::DNN::Settings::m_timer. timer for monitoring ; Definition at line 832 of file NeuralNet.h. ◆ m_useMultithreading. bool TMVA::DNN::Settings::m_useMultithreading. protected . Definition at line 863 of file NeuralNet.h. Libraries for TMVA::DNN::Settings:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/NeuralNet.h; tmva/tmva/src/NeuralNet.cxx. TMVADNNSettings. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:45 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3084,Modifiability,config,configuration,3084,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:10721,Modifiability,config,configuration,10721," inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ; (; std::string ; histoName, . std::string ; options, . int ; pad, . EColor ; color . ). inline . for monitoring ; Definition at line 823 of file NeuralNet.h. ◆ regularization(). EnumRegularization TMVA::DNN::Settings::regularization ; (; ); const. inline . some regularization of the DNN is turned on? ; Definition at line 813 of file NeuralNet.h. ◆ repetitions(). int TMVA::DNN::Settings::repetitions ; (; ); const. inline . how many steps have to be gone until the batch is changed ; Definition at line 773 of file NeuralNet.h. ◆ setDropOut(). template<typename Iterator > . void TMVA::DNN::Settings::setDropOut ; (; Iterator ; begin, . Iterator ; end, . size_t ; _dropRepetitions . ). inline . set the drop-out configuration (layer-wise) ; Parameters. beginbegin of an array or vector denoting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::Settings::setMonitoring ; (; std::shared_ptr< Monitoring > ; ptrMonitoring). inline . prepared for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1167,Testability,log,logging,1167,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1788,Testability,log,logging,1788,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1953,Testability,log,loggging,1953,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2033,Testability,log,logging,2033,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2207,Testability,test,testError,2207,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3355,Testability,log,loggging,3355,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3452,Testability,test,testIteration,3452,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3499,Testability,log,loggging,3499,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3520,Testability,test,testRepetitions,3520,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3564,Testability,test,test,3564,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3574,Testability,test,tested,3574,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:3599,Testability,test,testSample,3599,"asConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (int numPads);  preparation for monitoring ;  ; void plot (std::string histoName, std::string options, int pad, EColor color);  for monitoring ;  ; EnumRegularization regularization () const;  some regularization of the DNN is turned on? ;  ; int repetitions () const;  how many steps have to be gone until the batch is changed ;  ; template<typename Iterator > ; void setDropOut (Iterator begin, Iterator end, size_t _dropRepetitions);  set the drop-out configuration (layer-wise) ;  ; void setMonitoring (std::shared_ptr< Monitoring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:6338,Testability,log,logging,6338,"ine 232 of file NeuralNet.cxx. ◆ ~Settings(). TMVA::DNN::Settings::~Settings ; (; ). virtual . d'tor ; Definition at line 261 of file NeuralNet.cxx. Member Function Documentation. ◆ addPoint() [1/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x . ). inline . for monitoring ; Definition at line 821 of file NeuralNet.h. ◆ addPoint() [2/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x, . double ; y . ). inline . for monitoring ; Definition at line 822 of file NeuralNet.h. ◆ batchSize(). size_t TMVA::DNN::Settings::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ). inline . for monitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:7685,Testability,log,logging,7685,"d to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ). inline . for monitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance on the progress bar; progressthe new value; texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDec",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:8141,Testability,log,loggging,8141,"onitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance on the progress bar; progressthe new value; texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConv",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:8377,Testability,log,logging,8377," texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); c",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:8868,Testability,test,testError,8868, Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer shall be used (e.g. SGD) ; Definition at line 774 of file NeuralNet.h. ◆ momentum(). double TMVA::DNN::Settings::momentum ; (; ); const. inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::S,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:11486,Testability,log,logging,11486,". ◆ setDropOut(). template<typename Iterator > . void TMVA::DNN::Settings::setDropOut ; (; Iterator ; begin, . Iterator ; end, . size_t ; _dropRepetitions . ). inline . set the drop-out configuration (layer-wise) ; Parameters. beginbegin of an array or vector denoting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::Settings::setMonitoring ; (; std::shared_ptr< Monitoring > ; ptrMonitoring). inline . prepared for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:11781,Testability,log,loggging,11781,"noting the drop-out probabilities for each layer ; endend of an array or vector denoting the drop-out probabilities for each layer ; _dropRepetitionsdenotes after how many repetitions the drop-out setting (which nodes are dropped out exactly) is changed . Definition at line 759 of file NeuralNet.h. ◆ setMonitoring(). void TMVA::DNN::Settings::setMonitoring ; (; std::shared_ptr< Monitoring > ; ptrMonitoring). inline . prepared for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callba",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12218,Testability,test,testIteration,12218,"ed for monitoring ; Definition at line 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12269,Testability,test,testIteration,12269," 764 of file NeuralNet.h. ◆ setProgressLimits(). virtual void TMVA::DNN::Settings::setProgressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12335,Testability,log,loggging,12335,"gressLimits ; (; double ; minProgress = 0, . double ; maxProgress = 100 . ). inlinevirtual . Parameters. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at li",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12445,Testability,test,testRepetitions,12445,"rs. maxProgressfor monitoring and logging (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at line 845 of file NeuralNet.h. ◆ count_mb_E. size_t TMVA::DNN::Settings::count_mb_E. Definition at line ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12492,Testability,test,testRepetitions,12492,"g (set the current ""progress"" limits for the display of the progress); minProgressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at line 845 of file NeuralNet.h. ◆ count_mb_E. size_t TMVA::DNN::Settings::count_mb_E. Definition at line 844 of file NeuralNet.h. ◆ fLearningRate",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12549,Testability,test,test,12549,"ressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at line 845 of file NeuralNet.h. ◆ count_mb_E. size_t TMVA::DNN::Settings::count_mb_E. Definition at line 844 of file NeuralNet.h. ◆ fLearningRate. double TMVA::DNN::Settings::fLearningRate. Definition at line 852 of file N",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12559,Testability,test,tested,12559,"ressminimum value; maxProgressmaximum value . Definition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at line 845 of file NeuralNet.h. ◆ count_mb_E. size_t TMVA::DNN::Settings::count_mb_E. Definition at line 844 of file NeuralNet.h. ◆ fLearningRate. double TMVA::DNN::Settings::fLearningRate. Definition at line 852 of file N",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12614,Testability,test,testSample,12614,"inition at line 790 of file NeuralNet.h. ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at line 845 of file NeuralNet.h. ◆ count_mb_E. size_t TMVA::DNN::Settings::count_mb_E. Definition at line 844 of file NeuralNet.h. ◆ fLearningRate. double TMVA::DNN::Settings::fLearningRate. Definition at line 852 of file NeuralNet.h. ◆ fMinimizerType. MinimizerType TMVA:",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:12662,Testability,test,testSample,12662,". ◆ startTestCycle(). virtual void TMVA::DNN::Settings::startTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 804 of file NeuralNet.h. ◆ startTrainCycle(). virtual void TMVA::DNN::Settings::startTrainCycle ; (; ). inlinevirtual . Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 782 of file NeuralNet.h. ◆ startTraining(). virtual void TMVA::DNN::Settings::startTraining ; (; ). inlinevirtual . Definition at line 795 of file NeuralNet.h. ◆ testIteration(). virtual void TMVA::DNN::Settings::testIteration ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 806 of file NeuralNet.h. ◆ testRepetitions(). size_t TMVA::DNN::Settings::testRepetitions ; (; ); const. inline . how often is the test data tested ; Definition at line 768 of file NeuralNet.h. ◆ testSample(). virtual void TMVA::DNN::Settings::testSample ; (; double ; , . double ; , . double ; , . double ;  . ). inlinevirtual . virtual function to be used for monitoring (callback) ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 781 of file NeuralNet.h. ◆ useMultithreading(). bool TMVA::DNN::Settings::useMultithreading ; (; ); const. inline . is multithreading turned on? ; Definition at line 815 of file NeuralNet.h. Member Data Documentation. ◆ count_dE. size_t TMVA::DNN::Settings::count_dE. Definition at line 843 of file NeuralNet.h. ◆ count_E. size_t TMVA::DNN::Settings::count_E. Definition at line 842 of file NeuralNet.h. ◆ count_mb_dE. size_t TMVA::DNN::Settings::count_mb_dE. Definition at line 845 of file NeuralNet.h. ◆ count_mb_E. size_t TMVA::DNN::Settings::count_mb_E. Definition at line 844 of file NeuralNet.h. ◆ fLearningRate. double TMVA::DNN::Settings::fLearningRate. Definition at line 852 of file NeuralNet.h. ◆ fMinimizerType. MinimizerType TMVA::DNN::Settings::fMinimizerType. Definiti",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:1019,Usability,clear,clear,1019,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2269,Usability,learn,learningRate,2269,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:2301,Usability,learn,learning,2301,"_eMinimizerType=MinimizerType::fSteepest, double _learningRate=1e-5, double _momentum=0.3, int _repetitions=3, bool _multithreading=true);  c'tor ;  ; virtual ~Settings ();  d'tor ;  ; void addPoint (std::string histoName, double x);  for monitoring ;  ; void addPoint (std::string histoName, double x, double y);  for monitoring ;  ; size_t batchSize () const;  mini-batch size ;  ; void clear (std::string histoName);  for monitoring ;  ; virtual void computeResult (const Net &, std::vector< double > &);  callback for monitoring and logging ;  ; size_t convergenceCount () const;  returns the current convergence count ;  ; size_t convergenceSteps () const;  how many steps until training is deemed to have converged ;  ; void create (std::string histoName, int bins, double min, double max);  for monitoring ;  ; void create (std::string histoName, int bins, double min, double max, int bins2, double min2, double max2);  for monitoring ;  ; virtual void cycle (double progress, TString text);  ; virtual void drawSample (const std::vector< double > &, const std::vector< double > &, const std::vector< double > &, double);  callback for monitoring and logging ;  ; const std::vector< double > & dropFractions () const;  ; size_t dropRepetitions () const;  ; virtual void endTestCycle ();  callback for monitoring and loggging ;  ; virtual void endTrainCycle (double);  callback for monitoring and logging ;  ; bool exists (std::string histoName);  for monitoring ;  ; double factorWeightDecay () const;  get the weight-decay factor ;  ; virtual bool hasConverged (double testError);  has this training converged already? ;  ; double learningRate () const;  get the learning rate ;  ; size_t maxConvergenceCount () const;  returns the max convergence count so far ;  ; size_t minError () const;  returns the smallest error so far ;  ; MinimizerType minimizerType () const;  which minimizer shall be used (e.g. SGD) ;  ; double momentum () const;  get the momentum (e.g. for SGD) ;  ; void pads (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:4329,Usability,progress bar,progress bar,4329,"toring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Protected Attributes; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::Settings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ Settings(). TMVA::DNN::Settings::Settings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . TMVA::DNN::EnumRegularization ; _regularization = TMVA::DNN::EnumRegularization::NONE, . MinimizerType ;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:4415,Usability,progress bar,progress bar,4415,"toring > ptrMonitoring);  prepared for monitoring ;  ; virtual void setProgressLimits (double minProgress=0, double maxProgress=100);  ; virtual void startTestCycle ();  callback for monitoring and loggging ;  ; virtual void startTrainCycle ();  ; virtual void startTraining ();  ; virtual void testIteration ();  callback for monitoring and loggging ;  ; size_t testRepetitions () const;  how often is the test data tested ;  ; virtual void testSample (double, double, double, double);  virtual function to be used for monitoring (callback) ;  ; bool useMultithreading () const;  is multithreading turned on? ;  . Public Attributes; size_t count_dE;  ; size_t count_E;  ; size_t count_mb_dE;  ; size_t count_mb_E;  ; double fLearningRate;  ; MinimizerType fMinimizerType;  ; double fMomentum;  ; int fRepetitions;  ; size_t m_batchSize;  mini-batch size ;  ; size_t m_convergenceCount;  ; size_t m_convergenceSteps;  number of steps without improvement to consider the DNN to have converged ;  ; std::vector< double > m_dropOut;  ; double m_dropRepetitions;  ; double m_factorWeightDecay;  ; size_t m_maxConvergenceCount;  ; double m_maxProgress;  current limits for the progress bar ;  ; double m_minError;  ; double m_minProgress;  current limits for the progress bar ;  ; EnumRegularization m_regularization;  ; size_t m_testRepetitions;  ; Timer m_timer;  timer for monitoring ;  . Protected Attributes; std::shared_ptr< Monitoring > fMonitoring;  ; bool m_useMultithreading;  . #include <TMVA/NeuralNet.h>. Inheritance diagram for TMVA::DNN::Settings:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Constructor & Destructor Documentation. ◆ Settings(). TMVA::DNN::Settings::Settings ; (; TString ; name, . size_t ; _convergenceSteps = 15, . size_t ; _batchSize = 10, . size_t ; _testRepetitions = 7, . double ; _factorWeightDecay = 1e-5, . TMVA::DNN::EnumRegularization ; _regularization = TMVA::DNN::EnumRegularization::NONE, . MinimizerType ;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:6030,Usability,clear,clear,6030,"eightDecay = 1e-5, . TMVA::DNN::EnumRegularization ; _regularization = TMVA::DNN::EnumRegularization::NONE, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _multithreading = true . ). c'tor ; Definition at line 232 of file NeuralNet.cxx. ◆ ~Settings(). TMVA::DNN::Settings::~Settings ; (; ). virtual . d'tor ; Definition at line 261 of file NeuralNet.cxx. Member Function Documentation. ◆ addPoint() [1/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x . ). inline . for monitoring ; Definition at line 821 of file NeuralNet.h. ◆ addPoint() [2/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x, . double ; y . ). inline . for monitoring ; Definition at line 822 of file NeuralNet.h. ◆ batchSize(). size_t TMVA::DNN::Settings::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:6065,Usability,clear,clear,6065,"egularization ; _regularization = TMVA::DNN::EnumRegularization::NONE, . MinimizerType ; _eMinimizerType = MinimizerType::fSteepest, . double ; _learningRate = 1e-5, . double ; _momentum = 0.3, . int ; _repetitions = 3, . bool ; _multithreading = true . ). c'tor ; Definition at line 232 of file NeuralNet.cxx. ◆ ~Settings(). TMVA::DNN::Settings::~Settings ; (; ). virtual . d'tor ; Definition at line 261 of file NeuralNet.cxx. Member Function Documentation. ◆ addPoint() [1/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x . ). inline . for monitoring ; Definition at line 821 of file NeuralNet.h. ◆ addPoint() [2/2]. void TMVA::DNN::Settings::addPoint ; (; std::string ; histoName, . double ; x, . double ; y . ). inline . for monitoring ; Definition at line 822 of file NeuralNet.h. ◆ batchSize(). size_t TMVA::DNN::Settings::batchSize ; (; ); const. inline . mini-batch size ; Definition at line 767 of file NeuralNet.h. ◆ clear(). void TMVA::DNN::Settings::clear ; (; std::string ; histoName). inline . for monitoring ; Definition at line 824 of file NeuralNet.h. ◆ computeResult(). virtual void TMVA::DNN::Settings::computeResult ; (; const Net & ; , . std::vector< double > & ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 809 of file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; m",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:7357,Usability,progress bar,progress bar,7357,"f file NeuralNet.h. ◆ convergenceCount(). size_t TMVA::DNN::Settings::convergenceCount ; (; ); const. inline . returns the current convergence count ; Definition at line 827 of file NeuralNet.h. ◆ convergenceSteps(). size_t TMVA::DNN::Settings::convergenceSteps ; (; ); const. inline . how many steps until training is deemed to have converged ; Definition at line 766 of file NeuralNet.h. ◆ create() [1/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max . ). inline . for monitoring ; Definition at line 819 of file NeuralNet.h. ◆ create() [2/2]. void TMVA::DNN::Settings::create ; (; std::string ; histoName, . int ; bins, . double ; min, . double ; max, . int ; bins2, . double ; min2, . double ; max2 . ). inline . for monitoring ; Definition at line 820 of file NeuralNet.h. ◆ cycle(). virtual void TMVA::DNN::Settings::cycle ; (; double ; progress, . TString ; text . ). inlinevirtual . Parameters. textadvance on the progress bar; progressthe new value; texta label . Definition at line 799 of file NeuralNet.h. ◆ drawSample(). virtual void TMVA::DNN::Settings::drawSample ; (; const std::vector< double > & ; , . const std::vector< double > & ; , . const std::vector< double > & ; , . double ;  . ). inlinevirtual . callback for monitoring and logging ; Definition at line 807 of file NeuralNet.h. ◆ dropFractions(). const std::vector< double > & TMVA::DNN::Settings::dropFractions ; (; ); const. inline . Definition at line 762 of file NeuralNet.h. ◆ dropRepetitions(). size_t TMVA::DNN::Settings::dropRepetitions ; (; ); const. inline . Definition at line 761 of file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:9000,Usability,learn,learningRate,9000,file NeuralNet.h. ◆ endTestCycle(). virtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer shall be used (e.g. SGD) ; Definition at line 774 of file NeuralNet.h. ◆ momentum(). double TMVA::DNN::Settings::momentum ; (; ); const. inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ;,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:9044,Usability,learn,learningRate,9044,"irtual void TMVA::DNN::Settings::endTestCycle ; (; ). inlinevirtual . callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer shall be used (e.g. SGD) ; Definition at line 774 of file NeuralNet.h. ◆ momentum(). double TMVA::DNN::Settings::momentum ; (; ); const. inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ; (; std::string ; histoName, . std::s",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:9089,Usability,learn,learning,9089,"callback for monitoring and loggging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 805 of file NeuralNet.h. ◆ endTrainCycle(). virtual void TMVA::DNN::Settings::endTrainCycle ; (; double ; ). inlinevirtual . callback for monitoring and logging ; Reimplemented in TMVA::DNN::ClassificationSettings.; Definition at line 788 of file NeuralNet.h. ◆ exists(). bool TMVA::DNN::Settings::exists ; (; std::string ; histoName). inline . for monitoring ; Definition at line 825 of file NeuralNet.h. ◆ factorWeightDecay(). double TMVA::DNN::Settings::factorWeightDecay ; (; ); const. inline . get the weight-decay factor ; Definition at line 769 of file NeuralNet.h. ◆ hasConverged(). bool TMVA::DNN::Settings::hasConverged ; (; double ; testError). virtual . has this training converged already? ; check for convergence; Definition at line 485 of file NeuralNet.cxx. ◆ learningRate(). double TMVA::DNN::Settings::learningRate ; (; ); const. inline . get the learning rate ; Definition at line 771 of file NeuralNet.h. ◆ maxConvergenceCount(). size_t TMVA::DNN::Settings::maxConvergenceCount ; (; ); const. inline . returns the max convergence count so far ; Definition at line 828 of file NeuralNet.h. ◆ minError(). size_t TMVA::DNN::Settings::minError ; (; ); const. inline . returns the smallest error so far ; Definition at line 829 of file NeuralNet.h. ◆ minimizerType(). MinimizerType TMVA::DNN::Settings::minimizerType ; (; ); const. inline . which minimizer shall be used (e.g. SGD) ; Definition at line 774 of file NeuralNet.h. ◆ momentum(). double TMVA::DNN::Settings::momentum ; (; ); const. inline . get the momentum (e.g. for SGD) ; Definition at line 772 of file NeuralNet.h. ◆ pads(). void TMVA::DNN::Settings::pads ; (; int ; numPads). inline . preparation for monitoring ; Definition at line 818 of file NeuralNet.h. ◆ plot(). void TMVA::DNN::Settings::plot ; (; std::string ; histoName, . std::string ; options, . int ; pad, . EColor ; color . ). inline . for monit",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:14985,Usability,progress bar,progress bar,14985, of file NeuralNet.h. ◆ m_batchSize. size_t TMVA::DNN::Settings::m_batchSize. mini-batch size ; Definition at line 838 of file NeuralNet.h. ◆ m_convergenceCount. size_t TMVA::DNN::Settings::m_convergenceCount. Definition at line 857 of file NeuralNet.h. ◆ m_convergenceSteps. size_t TMVA::DNN::Settings::m_convergenceSteps. number of steps without improvement to consider the DNN to have converged ; Definition at line 837 of file NeuralNet.h. ◆ m_dropOut. std::vector<double> TMVA::DNN::Settings::m_dropOut. Definition at line 850 of file NeuralNet.h. ◆ m_dropRepetitions. double TMVA::DNN::Settings::m_dropRepetitions. Definition at line 849 of file NeuralNet.h. ◆ m_factorWeightDecay. double TMVA::DNN::Settings::m_factorWeightDecay. Definition at line 840 of file NeuralNet.h. ◆ m_maxConvergenceCount. size_t TMVA::DNN::Settings::m_maxConvergenceCount. Definition at line 858 of file NeuralNet.h. ◆ m_maxProgress. double TMVA::DNN::Settings::m_maxProgress. current limits for the progress bar ; Definition at line 834 of file NeuralNet.h. ◆ m_minError. double TMVA::DNN::Settings::m_minError. Definition at line 859 of file NeuralNet.h. ◆ m_minProgress. double TMVA::DNN::Settings::m_minProgress. current limits for the progress bar ; Definition at line 833 of file NeuralNet.h. ◆ m_regularization. EnumRegularization TMVA::DNN::Settings::m_regularization. Definition at line 847 of file NeuralNet.h. ◆ m_testRepetitions. size_t TMVA::DNN::Settings::m_testRepetitions. Definition at line 839 of file NeuralNet.h. ◆ m_timer. Timer TMVA::DNN::Settings::m_timer. timer for monitoring ; Definition at line 832 of file NeuralNet.h. ◆ m_useMultithreading. bool TMVA::DNN::Settings::m_useMultithreading. protected . Definition at line 863 of file NeuralNet.h. Libraries for TMVA::DNN::Settings:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/NeuralNet.h; tmva/tmva/src/NeuralNet.cxx. TMVADNNSettings. ROOT master - Reference Guide Generated on Tu,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html:15225,Usability,progress bar,progress bar,15225,ings::m_batchSize. mini-batch size ; Definition at line 838 of file NeuralNet.h. ◆ m_convergenceCount. size_t TMVA::DNN::Settings::m_convergenceCount. Definition at line 857 of file NeuralNet.h. ◆ m_convergenceSteps. size_t TMVA::DNN::Settings::m_convergenceSteps. number of steps without improvement to consider the DNN to have converged ; Definition at line 837 of file NeuralNet.h. ◆ m_dropOut. std::vector<double> TMVA::DNN::Settings::m_dropOut. Definition at line 850 of file NeuralNet.h. ◆ m_dropRepetitions. double TMVA::DNN::Settings::m_dropRepetitions. Definition at line 849 of file NeuralNet.h. ◆ m_factorWeightDecay. double TMVA::DNN::Settings::m_factorWeightDecay. Definition at line 840 of file NeuralNet.h. ◆ m_maxConvergenceCount. size_t TMVA::DNN::Settings::m_maxConvergenceCount. Definition at line 858 of file NeuralNet.h. ◆ m_maxProgress. double TMVA::DNN::Settings::m_maxProgress. current limits for the progress bar ; Definition at line 834 of file NeuralNet.h. ◆ m_minError. double TMVA::DNN::Settings::m_minError. Definition at line 859 of file NeuralNet.h. ◆ m_minProgress. double TMVA::DNN::Settings::m_minProgress. current limits for the progress bar ; Definition at line 833 of file NeuralNet.h. ◆ m_regularization. EnumRegularization TMVA::DNN::Settings::m_regularization. Definition at line 847 of file NeuralNet.h. ◆ m_testRepetitions. size_t TMVA::DNN::Settings::m_testRepetitions. Definition at line 839 of file NeuralNet.h. ◆ m_timer. Timer TMVA::DNN::Settings::m_timer. timer for monitoring ; Definition at line 832 of file NeuralNet.h. ◆ m_useMultithreading. bool TMVA::DNN::Settings::m_useMultithreading. protected . Definition at line 863 of file NeuralNet.h. Libraries for TMVA::DNN::Settings:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/NeuralNet.h; tmva/tmva/src/NeuralNet.cxx. TMVADNNSettings. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:45 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Settings.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Settings.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html:453,Usability,learn,learningRate,453,". ROOT: TMVA::DNN::Steepest Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Public Attributes |; List of all members ; TMVA::DNN::Steepest Class Reference. ; Steepest Gradient Descent algorithm (SGD) ; Implements a steepest gradient descent minimization algorithm ; Definition at line 333 of file NeuralNet.h. Public Member Functions;  Steepest (double learningRate=1e-4, double momentum=0.5, size_t repetitions=10);  c'tor ;  ; template<typename Function , typename Weights , typename PassThrough > ; double operator() (Function &fitnessFunction, Weights &weights, PassThrough &passThrough);  operator to call the steepest gradient descent algorithm ;  . Public Attributes; double m_alpha;  internal parameter (learningRate) ;  ; double m_beta;  internal parameter (momentum) ;  ; std::vector< double > m_localGradients;  local gradients for reuse in thread. ;  ; std::vector< double > m_localWeights;  local weights for reuse in thread. ;  ; std::vector< double > m_prevGradients;  vector remembers the gradients of the previous step ;  ; size_t m_repetitions;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ Steepest(). TMVA::DNN::Steepest::Steepest ; (; double ; learningRate = 1e-4, . double ; momentum = 0.5, . size_t ; repetitions = 10 . ). inline . c'tor ; C'tor; Parameters. learningRatedenotes the learning rate for the SGD algorithm ; momentumfraction of the velocity which is taken over from the last step ; repetitionsre-compute the gradients each ""repetitions"" steps . Definition at line 348 of file NeuralNet.h. Member Function Documentation. ◆ operator()(). template<typename Function , typename Weights , typename PassThrough > . double TMVA::DNN::Steepest::operator() ; (; Function & ; fitnessFunction, . Weights & ; weights, . PassThrough & ; passThrough . ). operator to call the steepest gradient descent algorithm ; implementation of the steepest gradie",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Steepest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html:812,Usability,learn,learningRate,812,". ROOT: TMVA::DNN::Steepest Class Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Public Attributes |; List of all members ; TMVA::DNN::Steepest Class Reference. ; Steepest Gradient Descent algorithm (SGD) ; Implements a steepest gradient descent minimization algorithm ; Definition at line 333 of file NeuralNet.h. Public Member Functions;  Steepest (double learningRate=1e-4, double momentum=0.5, size_t repetitions=10);  c'tor ;  ; template<typename Function , typename Weights , typename PassThrough > ; double operator() (Function &fitnessFunction, Weights &weights, PassThrough &passThrough);  operator to call the steepest gradient descent algorithm ;  . Public Attributes; double m_alpha;  internal parameter (learningRate) ;  ; double m_beta;  internal parameter (momentum) ;  ; std::vector< double > m_localGradients;  local gradients for reuse in thread. ;  ; std::vector< double > m_localWeights;  local weights for reuse in thread. ;  ; std::vector< double > m_prevGradients;  vector remembers the gradients of the previous step ;  ; size_t m_repetitions;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ Steepest(). TMVA::DNN::Steepest::Steepest ; (; double ; learningRate = 1e-4, . double ; momentum = 0.5, . size_t ; repetitions = 10 . ). inline . c'tor ; C'tor; Parameters. learningRatedenotes the learning rate for the SGD algorithm ; momentumfraction of the velocity which is taken over from the last step ; repetitionsre-compute the gradients each ""repetitions"" steps . Definition at line 348 of file NeuralNet.h. Member Function Documentation. ◆ operator()(). template<typename Function , typename Weights , typename PassThrough > . double TMVA::DNN::Steepest::operator() ; (; Function & ; fitnessFunction, . Weights & ; weights, . PassThrough & ; passThrough . ). operator to call the steepest gradient descent algorithm ; implementation of the steepest gradie",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Steepest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html:1293,Usability,learn,learningRate,1293,"nt algorithm (SGD) ; Implements a steepest gradient descent minimization algorithm ; Definition at line 333 of file NeuralNet.h. Public Member Functions;  Steepest (double learningRate=1e-4, double momentum=0.5, size_t repetitions=10);  c'tor ;  ; template<typename Function , typename Weights , typename PassThrough > ; double operator() (Function &fitnessFunction, Weights &weights, PassThrough &passThrough);  operator to call the steepest gradient descent algorithm ;  . Public Attributes; double m_alpha;  internal parameter (learningRate) ;  ; double m_beta;  internal parameter (momentum) ;  ; std::vector< double > m_localGradients;  local gradients for reuse in thread. ;  ; std::vector< double > m_localWeights;  local weights for reuse in thread. ;  ; std::vector< double > m_prevGradients;  vector remembers the gradients of the previous step ;  ; size_t m_repetitions;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ Steepest(). TMVA::DNN::Steepest::Steepest ; (; double ; learningRate = 1e-4, . double ; momentum = 0.5, . size_t ; repetitions = 10 . ). inline . c'tor ; C'tor; Parameters. learningRatedenotes the learning rate for the SGD algorithm ; momentumfraction of the velocity which is taken over from the last step ; repetitionsre-compute the gradients each ""repetitions"" steps . Definition at line 348 of file NeuralNet.h. Member Function Documentation. ◆ operator()(). template<typename Function , typename Weights , typename PassThrough > . double TMVA::DNN::Steepest::operator() ; (; Function & ; fitnessFunction, . Weights & ; weights, . PassThrough & ; passThrough . ). operator to call the steepest gradient descent algorithm ; implementation of the steepest gradient descent algorithm; entry point to start the minimization procedure; Parameters. fitnessFunction(templated) function which has to be provided. This function is minimized ; weights(templated) a reference to a container of weights. The result of the minimization procedure is retur",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Steepest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html:1410,Usability,learn,learningRatedenotes,1410,"ons=10);  c'tor ;  ; template<typename Function , typename Weights , typename PassThrough > ; double operator() (Function &fitnessFunction, Weights &weights, PassThrough &passThrough);  operator to call the steepest gradient descent algorithm ;  . Public Attributes; double m_alpha;  internal parameter (learningRate) ;  ; double m_beta;  internal parameter (momentum) ;  ; std::vector< double > m_localGradients;  local gradients for reuse in thread. ;  ; std::vector< double > m_localWeights;  local weights for reuse in thread. ;  ; std::vector< double > m_prevGradients;  vector remembers the gradients of the previous step ;  ; size_t m_repetitions;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ Steepest(). TMVA::DNN::Steepest::Steepest ; (; double ; learningRate = 1e-4, . double ; momentum = 0.5, . size_t ; repetitions = 10 . ). inline . c'tor ; C'tor; Parameters. learningRatedenotes the learning rate for the SGD algorithm ; momentumfraction of the velocity which is taken over from the last step ; repetitionsre-compute the gradients each ""repetitions"" steps . Definition at line 348 of file NeuralNet.h. Member Function Documentation. ◆ operator()(). template<typename Function , typename Weights , typename PassThrough > . double TMVA::DNN::Steepest::operator() ; (; Function & ; fitnessFunction, . Weights & ; weights, . PassThrough & ; passThrough . ). operator to call the steepest gradient descent algorithm ; implementation of the steepest gradient descent algorithm; entry point to start the minimization procedure; Parameters. fitnessFunction(templated) function which has to be provided. This function is minimized ; weights(templated) a reference to a container of weights. The result of the minimization procedure is returned via this reference (needs to support std::begin and std::end ; passThrough(templated) object which can hold any data which the fitness function needs. This object is not touched by the minimizer; This object is provided to ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Steepest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html:1434,Usability,learn,learning,1434,"ons=10);  c'tor ;  ; template<typename Function , typename Weights , typename PassThrough > ; double operator() (Function &fitnessFunction, Weights &weights, PassThrough &passThrough);  operator to call the steepest gradient descent algorithm ;  . Public Attributes; double m_alpha;  internal parameter (learningRate) ;  ; double m_beta;  internal parameter (momentum) ;  ; std::vector< double > m_localGradients;  local gradients for reuse in thread. ;  ; std::vector< double > m_localWeights;  local weights for reuse in thread. ;  ; std::vector< double > m_prevGradients;  vector remembers the gradients of the previous step ;  ; size_t m_repetitions;  . #include <TMVA/NeuralNet.h>; Constructor & Destructor Documentation. ◆ Steepest(). TMVA::DNN::Steepest::Steepest ; (; double ; learningRate = 1e-4, . double ; momentum = 0.5, . size_t ; repetitions = 10 . ). inline . c'tor ; C'tor; Parameters. learningRatedenotes the learning rate for the SGD algorithm ; momentumfraction of the velocity which is taken over from the last step ; repetitionsre-compute the gradients each ""repetitions"" steps . Definition at line 348 of file NeuralNet.h. Member Function Documentation. ◆ operator()(). template<typename Function , typename Weights , typename PassThrough > . double TMVA::DNN::Steepest::operator() ; (; Function & ; fitnessFunction, . Weights & ; weights, . PassThrough & ; passThrough . ). operator to call the steepest gradient descent algorithm ; implementation of the steepest gradient descent algorithm; entry point to start the minimization procedure; Parameters. fitnessFunction(templated) function which has to be provided. This function is minimized ; weights(templated) a reference to a container of weights. The result of the minimization procedure is returned via this reference (needs to support std::begin and std::end ; passThrough(templated) object which can hold any data which the fitness function needs. This object is not touched by the minimizer; This object is provided to ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Steepest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html:2766,Usability,learn,learningRate,2766,"::DNN::Steepest::operator() ; (; Function & ; fitnessFunction, . Weights & ; weights, . PassThrough & ; passThrough . ). operator to call the steepest gradient descent algorithm ; implementation of the steepest gradient descent algorithm; entry point to start the minimization procedure; Parameters. fitnessFunction(templated) function which has to be provided. This function is minimized ; weights(templated) a reference to a container of weights. The result of the minimization procedure is returned via this reference (needs to support std::begin and std::end ; passThrough(templated) object which can hold any data which the fitness function needs. This object is not touched by the minimizer; This object is provided to the fitness function when called. Can be used with multithreading (i.e. ""HogWild!"" style); see call in trainCycle ; Definition at line 271 of file NeuralNet.icc. Member Data Documentation. ◆ m_alpha. double TMVA::DNN::Steepest::m_alpha. internal parameter (learningRate) ; Definition at line 371 of file NeuralNet.h. ◆ m_beta. double TMVA::DNN::Steepest::m_beta. internal parameter (momentum) ; Definition at line 372 of file NeuralNet.h. ◆ m_localGradients. std::vector<double> TMVA::DNN::Steepest::m_localGradients. local gradients for reuse in thread. ; Definition at line 376 of file NeuralNet.h. ◆ m_localWeights. std::vector<double> TMVA::DNN::Steepest::m_localWeights. local weights for reuse in thread. ; Definition at line 375 of file NeuralNet.h. ◆ m_prevGradients. std::vector<double> TMVA::DNN::Steepest::m_prevGradients. vector remembers the gradients of the previous step ; Definition at line 373 of file NeuralNet.h. ◆ m_repetitions. size_t TMVA::DNN::Steepest::m_repetitions. Definition at line 337 of file NeuralNet.h. Libraries for TMVA::DNN::Steepest:. [legend]; The documentation for this class was generated from the following files:; tmva/tmva/inc/TMVA/NeuralNet.h; tmva/tmva/inc/TMVA/NeuralNet.icc. TMVADNNSteepest. ROOT master - Reference Guide Genera",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1Steepest.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1Steepest.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:3236,Deployability,update,updates,3236,"_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The accumulation of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasUpdates;  The accumulation of the square of the past bias updates associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The accumulation of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightUpdates;  The accumulation of the square of the past weight updates associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or wei",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:3560,Deployability,update,updates,3560,"ases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The accumulation of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasUpdates;  The accumulation of the square of the past bias updates associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The accumulation of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightUpdates;  The accumulation of the square of the past weight updates associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #inclu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:11695,Deployability,update,updates,11695,"t<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 52 of file Adadelta.h. ◆ fPastSquaredBiasGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasGradients. protected . The accumulation of the square of the past bias gradients associated with the deep net. ; Definition at line 55 of file Adadelta.h. ◆ fPastSquaredBiasUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasUpdates. protected . The accumulation of the square of the past bias updates associated with the deep net. ; Definition at line 60 of file Adadelta.h. ◆ fPastSquaredWeightGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightGradients. protected . The accumulation of the square of the past weight gradients associated with the deep net. ; Definition at line 53 of file Adadelta.h. ◆ fPastSquaredWeightUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightUpdates. protected . The accumulation of the square of the past weight updates associated with the deep net. ; Definition at line 58 of file Adadelta.h. ◆ fRho. template<typename Ar",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:12579,Deployability,update,updates,12579,"Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasUpdates. protected . The accumulation of the square of the past bias updates associated with the deep net. ; Definition at line 60 of file Adadelta.h. ◆ fPastSquaredWeightGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightGradients. protected . The accumulation of the square of the past weight gradients associated with the deep net. ; Definition at line 53 of file Adadelta.h. ◆ fPastSquaredWeightUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightUpdates. protected . The accumulation of the square of the past weight updates associated with the deep net. ; Definition at line 58 of file Adadelta.h. ◆ fRho. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fRho. protected . The Rho constant used by the optimizer. ; Definition at line 51 of file Adadelta.h. ◆ fWorkBiasTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor1. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 63 of file Adadelta.h. ◆ fWorkBiasTensor2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vecto",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:864,Modifiability,inherit,inherited,864,". ROOT: TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >Adadelta Optimizer class. ; This class represents the Adadelta Optimizer. ; Definition at line 45 of file Adadelta.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdadelta (DeepNet_t &deepNet, Scalar_t learningRate=1.0, Scalar_t rho=0.95, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdadelta ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredBiasUpdatesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredWeightUpdatesAt (size_t i);  ; Scalar_t GetRho () const;  Getters. ;  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:1925,Modifiability,inherit,inherited,1925,"Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdadelta (DeepNet_t &deepNet, Scalar_t learningRate=1.0, Scalar_t rho=0.95, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdadelta ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredBiasUpdatesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredWeightUpdatesAt (size_t i);  ; Scalar_t GetRho () const;  Getters. ;  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::ve",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:4236,Modifiability,inherit,inherited,4236,"redBiasGradients;  The accumulation of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasUpdates;  The accumulation of the square of the past bias updates associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The accumulation of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightUpdates;  The accumulation of the square of the past weight updates associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adadelta.h>. Inheritance diagram for TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::M",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:2457,Performance,optimiz,optimization,2457,"< Matrix_t > > & GetPastSquaredBiasUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredBiasUpdatesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredWeightUpdatesAt (size_t i);  ; Scalar_t GetRho () const;  Getters. ;  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The accumulation of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasUpdates;  The accumulation of the square of the past bias updates associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The accumulation of the square of the past weight gradients associated with the deep net. ;  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:3648,Performance,optimiz,optimizer,3648,"te the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The accumulation of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasUpdates;  The accumulation of the square of the past bias updates associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The accumulation of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightUpdates;  The accumulation of the square of the past weight updates associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adadelta.h>. Inheritance diagram for TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >:. ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:12928,Performance,optimiz,optimizer,12928,"pNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightGradients. protected . The accumulation of the square of the past weight gradients associated with the deep net. ; Definition at line 53 of file Adadelta.h. ◆ fPastSquaredWeightUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightUpdates. protected . The accumulation of the square of the past weight updates associated with the deep net. ; Definition at line 58 of file Adadelta.h. ◆ fRho. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fRho. protected . The Rho constant used by the optimizer. ; Definition at line 51 of file Adadelta.h. ◆ fWorkBiasTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor1. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 63 of file Adadelta.h. ◆ fWorkBiasTensor2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor2. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 65 of file Adadelta.h. ◆ fWorkWeightTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t =",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:2934,Safety,avoid,avoid,2934,"mber Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The accumulation of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasUpdates;  The accumulation of the square of the past bias updates associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The accumulation of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightUpdates;  The accumulation of the square of the past weight updates associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:10837,Safety,avoid,avoid,10837,"& ; biasGradients . ). protectedvirtual . Update the biases, given the current bias gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 206 of file Adadelta.h. ◆ UpdateWeights(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . auto TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::UpdateWeights ; (; size_t ; layerIndex, . std::vector< Matrix_t > & ; weights, . const std::vector< Matrix_t > & ; weightGradients . ). protectedvirtual . Update the weights, given the current weight gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 147 of file Adadelta.h. Member Data Documentation. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 52 of file Adadelta.h. ◆ fPastSquaredBiasGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasGradients. protected . The accumulation of the square of the past bias gradients associated with the deep net. ; Definition at line 55 of file Adadelta.h. ◆ fPastSquaredBiasUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasUpdates. protected . The accumulation of the square of the past bias updates associated with the deep net. ; Definition at line 60 of file Adadelta.h. ◆ fPastSquaredWeightGradients. template<typename Architect",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:1116,Usability,learn,learningRate,1116,"e. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >Adadelta Optimizer class. ; This class represents the Adadelta Optimizer. ; Definition at line 45 of file Adadelta.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdadelta (DeepNet_t &deepNet, Scalar_t learningRate=1.0, Scalar_t rho=0.95, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdadelta ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredBiasUpdatesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredWeightUpdatesAt (size_t i);  ; Scalar_t GetRho () const;  Getters. ;  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOpt",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:2023,Usability,learn,learningRate,2023,"Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdadelta (DeepNet_t &deepNet, Scalar_t learningRate=1.0, Scalar_t rho=0.95, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdadelta ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredBiasUpdatesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredWeightUpdatesAt (size_t i);  ; Scalar_t GetRho () const;  Getters. ;  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::ve",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:2391,Usability,learn,learningRate,2391,"edBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredBiasUpdatesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightUpdates ();  ; std::vector< Matrix_t > & GetPastSquaredWeightUpdatesAt (size_t i);  ; Scalar_t GetRho () const;  Getters. ;  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The accumulation of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasUpdates;  The accumulation of the square of the past bias updates associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The accumulation of the square of the ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:4473,Usability,learn,learning,4473,"trix_t > > fPastSquaredWeightUpdates;  The accumulation of the square of the past weight updates associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adadelta.h>. Inheritance diagram for TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adadelta.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file Adadelta.h. Constructor & Destructor Documentation. ◆ TAdadelta(). template<typename Architecture_t , typename L",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html:5615,Usability,learn,learningRate,5615,"pNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adadelta.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file Adadelta.h. Constructor & Destructor Documentation. ◆ TAdadelta(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::TAdadelta ; (; DeepNet_t & ; deepNet, . Scalar_t ; learningRate = 1.0, . Scalar_t ; rho = 0.95, . Scalar_t ; epsilon = 1e-8 . ). Constructor. ; Definition at line 102 of file Adadelta.h. ◆ ~TAdadelta(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::~TAdadelta ; (; ). default . Destructor. . Member Function Documentation. ◆ GetEpsilon(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepNet_t >::GetEpsilon ; (; ); const. inline . Definition at line 82 of file Adadelta.h. ◆ GetPastSquaredBiasGradients(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector< std::vector< Matrix_t > > & TMVA::DNN::TAdadelta< Architecture_t, Layer_t, DeepN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdadelta.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdadelta.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:858,Modifiability,inherit,inherited,858,". ROOT: TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >Adagrad Optimizer class. ; This class represents the Adagrad Optimizer. ; Definition at line 45 of file Adagrad.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdagrad (DeepNet_t &deepNet, Scalar_t learningRate=0.01, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdagrad ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the gl",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:1580,Modifiability,inherit,inherited,1580,"ename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >Adagrad Optimizer class. ; This class represents the Adagrad Optimizer. ; Definition at line 45 of file Adagrad.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdagrad (DeepNet_t &deepNet, Scalar_t learningRate=0.01, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdagrad ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:3219,Modifiability,inherit,inherited,3219,"otected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adagrad.h>. Inheritance diagram for TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adagrad.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architectur",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:2112,Performance,optimiz,optimization,2112,"t, Scalar_t learningRate=0.01, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdagrad ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fW",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:2589,Safety,avoid,avoid,2589,"mber Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adagrad.h>. Inheritance diagram for TMVA::DNN::TAdagr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:8024,Safety,avoid,avoid,8024,"t > & ; biasGradients . ). protectedvirtual . Update the biases, given the current bias gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 158 of file Adagrad.h. ◆ UpdateWeights(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . auto TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::UpdateWeights ; (; size_t ; layerIndex, . std::vector< Matrix_t > & ; weights, . const std::vector< Matrix_t > & ; weightGradients . ). protectedvirtual . Update the weights, given the current weight gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 126 of file Adagrad.h. Member Data Documentation. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 51 of file Adagrad.h. ◆ fPastSquaredBiasGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasGradients. protected . The sum of the square of the past bias gradients associated with the deep net. ; Definition at line 56 of file Adagrad.h. ◆ fPastSquaredWeightGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightGradients. protected . The sum of the square of the past weight gradients associated with the deep net. ; Definition at line 54 of file Adagrad.h. ◆ fWorkBiasTensor. template<typename Architecture_t , typename Layer_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:1109,Usability,learn,learningRate,1109,"ference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >Adagrad Optimizer class. ; This class represents the Adagrad Optimizer. ; Definition at line 45 of file Adagrad.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdagrad (DeepNet_t &deepNet, Scalar_t learningRate=0.01, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdagrad ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Ste",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:1678,Usability,learn,learningRate,1678,"ename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >Adagrad Optimizer class. ; This class represents the Adagrad Optimizer. ; Definition at line 45 of file Adagrad.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdagrad (DeepNet_t &deepNet, Scalar_t learningRate=0.01, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdagrad ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:2046,Usability,learn,learningRate,2046,"  . Public Member Functions;  TAdagrad (DeepNet_t &deepNet, Scalar_t learningRate=0.01, Scalar_t epsilon=1e-8);  Constructor. ;  ;  ~TAdagrad ()=default;  Destructor. ;  ; Scalar_t GetEpsilon () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor;  working tensor used to keep a temporary copy of bias or bias",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:3456,Usability,learn,learning,3456,"s);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adagrad.h>. Inheritance diagram for TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adagrad.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file Adagrad.h. Constructor & Destructor Documentation. ◆ TAdagrad(). template<typename Architecture_t , typename Layer_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html:4589,Usability,learn,learningRate,4589,"_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adagrad.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file Adagrad.h. Constructor & Destructor Documentation. ◆ TAdagrad(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::TAdagrad ; (; DeepNet_t & ; deepNet, . Scalar_t ; learningRate = 0.01, . Scalar_t ; epsilon = 1e-8 . ). Constructor. ; Definition at line 90 of file Adagrad.h. ◆ ~TAdagrad(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::~TAdagrad ; (; ). default . Destructor. . Member Function Documentation. ◆ GetEpsilon(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::GetEpsilon ; (; ); const. inline . Getters. ; Definition at line 76 of file Adagrad.h. ◆ GetPastSquaredBiasGradients(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector< std::vector< Matrix_t > > & TMVA::DNN::TAdagrad< Architecture_t, Layer_t, DeepNet_t >::GetPastSquare",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdagrad.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdagrad.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:840,Modifiability,inherit,inherited,840,". ROOT: TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >Adam Optimizer class. ; This class represents the Adam Optimizer. ; Definition at line 45 of file Adam.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdam (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t beta1=0.9, Scalar_t beta2=0.999, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TAdam ()=default;  Destructor. ;  ; Scalar_t GetBeta1 () const;  Getters. ;  ; Scalar_t GetBeta2 () const;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentBiases ();  ; std::vector< Matrix_t > & GetFirstMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentWeights ();  ; std::vector< Matrix_t > & GetFirstMomentWeightsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentBiases ();  ; std::vector< Matrix_t > & GetSecondMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentWeights ();  ; std::vector< Matrix_t > & GetSecondMomentWeightsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:1903,Modifiability,inherit,inherited,1903,"er. ; Definition at line 45 of file Adam.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdam (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t beta1=0.9, Scalar_t beta2=0.999, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TAdam ()=default;  Destructor. ;  ; Scalar_t GetBeta1 () const;  Getters. ;  ; Scalar_t GetBeta2 () const;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentBiases ();  ; std::vector< Matrix_t > & GetFirstMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentWeights ();  ; std::vector< Matrix_t > & GetFirstMomentWeightsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentBiases ();  ; std::vector< Matrix_t > & GetSecondMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentWeights ();  ; std::vector< Matrix_t > & GetSecondMomentWeightsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:3755,Modifiability,inherit,inherited,3755,"en the current weight gradients. ;  . Protected Attributes; Scalar_t fBeta1;  The Beta1 constant used by the optimizer. ;  ; Scalar_t fBeta2;  The Beta2 constant used by the optimizer. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentBiases;  The decaying average of the first moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentWeights;  The decaying average of the first moment of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentBiases;  The decaying average of the second moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentWeights;  The decaying average of the second moment of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adam.h>. Inheritance diagram for TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adam.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Arc",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:2435,Performance,optimiz,optimization,2435,"Matrix_t > & GetFirstMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentWeights ();  ; std::vector< Matrix_t > & GetFirstMomentWeightsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentBiases ();  ; std::vector< Matrix_t > & GetSecondMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentWeights ();  ; std::vector< Matrix_t > & GetSecondMomentWeightsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fBeta1;  The Beta1 constant used by the optimizer. ;  ; Scalar_t fBeta2;  The Beta2 constant used by the optimizer. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentBiases;  The decaying average of the first moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentWeights;  The decaying average of the first moment of the past weight gradients associated with the deep net. ;  ; std::vector< std::vec",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:2914,Performance,optimiz,optimizer,2914,"ic Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fBeta1;  The Beta1 constant used by the optimizer. ;  ; Scalar_t fBeta2;  The Beta2 constant used by the optimizer. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentBiases;  The decaying average of the first moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentWeights;  The decaying average of the first moment of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentBiases;  The decaying average of the second moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentWeights;  The decaying average of the second moment of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:2979,Performance,optimiz,optimizer,2979," Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fBeta1;  The Beta1 constant used by the optimizer. ;  ; Scalar_t fBeta2;  The Beta2 constant used by the optimizer. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentBiases;  The decaying average of the first moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentWeights;  The decaying average of the first moment of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentBiases;  The decaying average of the second moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentWeights;  The decaying average of the second moment of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training.",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:10436,Performance,optimiz,optimizer,10436,"std::vector< Matrix_t > & ; biasGradients . ). protectedvirtual . Update the biases, given the current bias gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 169 of file Adam.h. ◆ UpdateWeights(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . auto TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::UpdateWeights ; (; size_t ; layerIndex, . std::vector< Matrix_t > & ; weights, . const std::vector< Matrix_t > & ; weightGradients . ). protectedvirtual . Update the weights, given the current weight gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 141 of file Adam.h. Member Data Documentation. ◆ fBeta1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fBeta1. protected . The Beta1 constant used by the optimizer. ; Definition at line 51 of file Adam.h. ◆ fBeta2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fBeta2. protected . The Beta2 constant used by the optimizer. ; Definition at line 52 of file Adam.h. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 53 of file Adam.h. ◆ fFirstMomentBiases. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fFirstMomentBiases. protecte",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:10756,Performance,optimiz,optimizer,10756,"et_t > . auto TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::UpdateWeights ; (; size_t ; layerIndex, . std::vector< Matrix_t > & ; weights, . const std::vector< Matrix_t > & ; weightGradients . ). protectedvirtual . Update the weights, given the current weight gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 141 of file Adam.h. Member Data Documentation. ◆ fBeta1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fBeta1. protected . The Beta1 constant used by the optimizer. ; Definition at line 51 of file Adam.h. ◆ fBeta2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fBeta2. protected . The Beta2 constant used by the optimizer. ; Definition at line 52 of file Adam.h. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 53 of file Adam.h. ◆ fFirstMomentBiases. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fFirstMomentBiases. protected . The decaying average of the first moment of the past bias gradients associated with the deep net. ; Definition at line 57 of file Adam.h. ◆ fFirstMomentWeights. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:3042,Safety,avoid,avoid,3042,"eepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fBeta1;  The Beta1 constant used by the optimizer. ;  ; Scalar_t fBeta2;  The Beta2 constant used by the optimizer. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentBiases;  The decaying average of the first moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentWeights;  The decaying average of the first moment of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentBiases;  The decaying average of the second moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentWeights;  The decaying average of the second moment of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:11076,Safety,avoid,avoid,11076,"e_t, Layer_t, DeepNet_t >.; Definition at line 141 of file Adam.h. Member Data Documentation. ◆ fBeta1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fBeta1. protected . The Beta1 constant used by the optimizer. ; Definition at line 51 of file Adam.h. ◆ fBeta2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fBeta2. protected . The Beta2 constant used by the optimizer. ; Definition at line 52 of file Adam.h. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 53 of file Adam.h. ◆ fFirstMomentBiases. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fFirstMomentBiases. protected . The decaying average of the first moment of the past bias gradients associated with the deep net. ; Definition at line 57 of file Adam.h. ◆ fFirstMomentWeights. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::fFirstMomentWeights. protected . The decaying average of the first moment of the past weight gradients associated with the deep net. ; Definition at line 55 of file Adam.h. ◆ fSecondMomentBiases. template<typename Architecture_t , typename Layer_t =",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:1088,Usability,learn,learningRate,1088,"lass Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >Adam Optimizer class. ; This class represents the Adam Optimizer. ; Definition at line 45 of file Adam.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdam (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t beta1=0.9, Scalar_t beta2=0.999, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TAdam ()=default;  Destructor. ;  ; Scalar_t GetBeta1 () const;  Getters. ;  ; Scalar_t GetBeta2 () const;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentBiases ();  ; std::vector< Matrix_t > & GetFirstMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentWeights ();  ; std::vector< Matrix_t > & GetFirstMomentWeightsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentBiases ();  ; std::vector< Matrix_t > & GetSecondMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentWeights ();  ; std::vector< Matrix_t > & GetSecondMomentWeightsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:2001,Usability,learn,learningRate,2001,"er. ; Definition at line 45 of file Adam.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TAdam (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t beta1=0.9, Scalar_t beta2=0.999, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TAdam ()=default;  Destructor. ;  ; Scalar_t GetBeta1 () const;  Getters. ;  ; Scalar_t GetBeta2 () const;  ; Scalar_t GetEpsilon () const;  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentBiases ();  ; std::vector< Matrix_t > & GetFirstMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentWeights ();  ; std::vector< Matrix_t > & GetFirstMomentWeightsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentBiases ();  ; std::vector< Matrix_t > & GetSecondMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentWeights ();  ; std::vector< Matrix_t > & GetSecondMomentWeightsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:2369,Usability,learn,learningRate,2369," Matrix_t > > & GetFirstMomentBiases ();  ; std::vector< Matrix_t > & GetFirstMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetFirstMomentWeights ();  ; std::vector< Matrix_t > & GetFirstMomentWeightsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentBiases ();  ; std::vector< Matrix_t > & GetSecondMomentBiasesAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetSecondMomentWeights ();  ; std::vector< Matrix_t > & GetSecondMomentWeightsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fBeta1;  The Beta1 constant used by the optimizer. ;  ; Scalar_t fBeta2;  The Beta2 constant used by the optimizer. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentBiases;  The decaying average of the first moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentWeights;  The decaying average of the first moment of the past weight gradients",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:3992,Usability,learn,learning,3992,";  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentBiases;  The decaying average of the first moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fFirstMomentWeights;  The decaying average of the first moment of the past weight gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentBiases;  The decaying average of the second moment of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fSecondMomentWeights;  The decaying average of the second moment of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/Adam.h>. Inheritance diagram for TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adam.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file Adam.h. Constructor & Destructor Documentation. ◆ TAdam(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html:5098,Usability,learn,learningRate,5098,"Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file Adam.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file Adam.h. Constructor & Destructor Documentation. ◆ TAdam(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::TAdam ; (; DeepNet_t & ; deepNet, . Scalar_t ; learningRate = 0.001, . Scalar_t ; beta1 = 0.9, . Scalar_t ; beta2 = 0.999, . Scalar_t ; epsilon = 1e-7 . ). Constructor. ; Definition at line 102 of file Adam.h. ◆ ~TAdam(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::~TAdam ; (; ). default . Destructor. . Member Function Documentation. ◆ GetBeta1(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::GetBeta1 ; (; ); const. inline . Getters. ; Definition at line 80 of file Adam.h. ◆ GetBeta2(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TAdam< Architecture_t, Layer_t, DeepNet_t >::GetBeta2 ; (; ); const. inline . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TAdam.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TAdam.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatch.html:543,Security,access,accessed,543,". ROOT: TMVA::DNN::TBatch< AArchitecture > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Private Types |; Private Attributes |; List of all members ; TMVA::DNN::TBatch< AArchitecture > Class Template Reference. ; template<typename AArchitecture>; class TMVA::DNN::TBatch< AArchitecture >TBatch. ; Class representing training batches consisting of a matrix of input data and a matrix of output data. The input and output data can be accessed using the GetInput() and GetOutput() member functions.; Template Parameters. AArchitectureThe underlying architecture. . Definition at line 54 of file DataLoader.h. Public Member Functions;  TBatch (const TBatch &)=default;  ;  TBatch (Matrix_t &, Matrix_t &, Matrix_t &);  ;  TBatch (TBatch &&)=default;  ; Matrix_t & GetInput ();  Return the matrix representing the input data. ;  ; Matrix_t & GetOutput ();  Return the matrix representing the output data. ;  ; Matrix_t & GetWeights ();  Return the matrix holding the event weights. ;  ; TBatch & operator= (const TBatch &)=default;  ; TBatch & operator= (TBatch &&)=default;  . Private Types; using Matrix_t = typename AArchitecture::Matrix_t;  . Private Attributes; Matrix_t fInputMatrix;  ; Matrix_t fOutputMatrix;  ; Matrix_t fWeightMatrix;  . #include <TMVA/DNN/DataLoader.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename AArchitecture > . using TMVA::DNN::TBatch< AArchitecture >::Matrix_t = typename AArchitecture::Matrix_t. private . Definition at line 58 of file DataLoader.h. Constructor & Destructor Documentation. ◆ TBatch() [1/3]. template<typename AArchitecture > . TMVA::DNN::TBatch< AArchitecture >::TBatch ; (; Matrix_t & ; inputMatrix, . Matrix_t & ; outputMatrix, . Matrix_t & ; weightMatrix . ). Definition at line 192 of file DataLoader.h. ◆ TBatch() [2/3]. template<typename AArchitecture > . TMVA::DNN::TBatch< AArchitecture >::TBatch ; (; const TBatch< AArc",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatch.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatch.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:668,Modifiability,variab,variable,668,". ROOT: TMVA::DNN::TBatchNormLayer< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Static Protected Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TBatchNormLayer< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TBatchNormLayer< Architecture_t >Layer implementing Batch Normalization. ; The input from each batch are normalized during training to have zero mean and unit variance and they are then scaled by two parameter, different for each input variable:; a scale factor gamma; an offset beta. In addition a running batch mean and variance is computed and stored in the class During inference the inputs are not normalized using the batch mean but the previously computed at running mean and variance If momentum is in [0,1) the running mean and variances are the exponential averages using the momentum value running_mean = momentum * running_mean + (1-momentum) * batch_mean If instead momentum<1 the cumulative average is computed running_mean = (nb/(nb+1) * running_mean + 1/(nb+1) * batch_mean; See more at [https://arxiv.org/pdf/1502.03167v3.pdf] ; Definition at line 64 of file BatchNormLayer.h. Public Types; using BNormDescriptors_t = typename Architecture_t::BNormDescriptors_t;  ; using HelperDescriptor_t = typename Architecture_t::TensorDescriptor_t;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Public Member Functions;  TBatchNormLayer (const TBatchNormLayer &);  Copy Constructor. ;  ;  TBatchNormLayer (size_t batchSize, size_t inputDepth, size_t inputHeight, size_t inputWidth, const std::vector< size_t > &shape, int axis=-1, Scalar_t momentum=-1., Scalar_t epsilon=0.0001);  Constructor. ;  ;  TBatchNormLayer (TBatchNormLayer< Archite",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:3652,Modifiability,inherit,inherited,3652," & GetIVariance ();  ; const Matrix_t & GetIVariance () const;  ; Scalar_t GetMomentum () const;  ; Matrix_t & GetMuVector ();  ; const Matrix_t & GetMuVector () const;  ; Scalar_t GetNormAxis () const;  ; int & GetNTrainedBatches ();  ; const int & GetNTrainedBatches () const;  ; Matrix_t & GetReshapedData ();  ; const Matrix_t & GetReshapedData () const;  ; Matrix_t & GetVariance ();  ; const Matrix_t & GetVariance () const;  ; Matrix_t & GetVarVector ();  ; const Matrix_t & GetVarVector () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; void Print () const;  Printing the layer info. ;  ; virtual void ReadWeightsFromXML (void *parent);  Read the information and the weights about the layer from XML node. ;  ; void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetExtraLayerParameters (const std::vector< Matrix_t > &params);  ;  Public Member Functions inherited from TMVA::DNN::VGeneralLayer< Architecture_t >;  VGeneralLayer (const VGeneralLayer &);  Copy Constructor. ;  ;  VGeneralLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t WeightsNSlices, size_t WeightsNRows, size_t WeightsNCols, size_t BiasesNSlices, size_t BiasesNRows, size_t BiasesNCols, size_t OutputNSlices, size_t OutputNRows, size_t OutputNCols, EInitialization Init);  Constructor. ;  ;  VGeneralLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t WeightsNSlices, std::vector< size_t > WeightsNRows, std::vector< size_t > WeightsNCols, size_t BiasesNSlices, std::vector< size_t > BiasesNRows, std::vector< size_t > BiasesNCols, size_t OutputNSlices, size_t OutputNRows, size_t OutputNCols, EInitialization Init);  General Constructor with different weights dimension. ;  ;  VGeneralLayer (VGeneralLayer< Architecture_t > *la",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:8936,Modifiability,inherit,inherited,8936," Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives;  First fDerivatives of the activations of this layer. ;  ; TDescriptors * fDescriptors = nullptr;  ; Scalar_t fEpsilon;  ; Matrix_t fIVar;  ; Scalar_t fMomentum;  The weight decay. ;  ; Matrix_t fMu;  ; Matrix_t fMu_Training;  ; int fNormAxis;  Normalization axis. For each element of this axis we will compute mean and stddev. ;  ; Tensor_t fReshapedData;  ; int fTrainedBatches = 0;  ; Matrix_t fVar;  ; Matrix_t fVar_Training;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; std::vector< Matrix_t > fBiases;  The biases associated to the layer. ;  ; std::vector< Matrix_t > fBiasGradients;  Gradients w.r.t. the bias values of the layer. ;  ; size_t fDepth;  The depth of the layer. ;  ; size_t fHeight;  The height of the layer. ;  ; EInitialization fInit;  The initialization method. ;  ; size_t fInputDepth;  The depth of the previous layer or input. ;  ; size_t fInputHeight;  The height of the previous layer or input. ;  ; size_t fInputWidth;  The width of the previous layer or input. ;  ; bool fIsTraining;  Flag indicating the mode. ;  ; Tensor_t fOutput;  Activations of this layer. ;  ; std::vector< Matrix_t > fWeightGradients;  Gradients w.r.t. the weights of the layer. ;  ; std::vector< Matrix_t > fWeights;  The weights associated to the layer. ;  ; size_t fWidth;  The ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7297,Usability,learn,learningRate,7297," const Tensor_t & GetOutput () const;  ; Matrix_t GetOutputAt (size_t i);  ; const Matrix_t & GetOutputAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeightGradients ();  ; const std::vector< Matrix_t > & GetWeightGradients () const;  ; Matrix_t & GetWeightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7355,Usability,learn,learning,7355," const Tensor_t & GetOutput () const;  ; Matrix_t GetOutputAt (size_t i);  ; const Matrix_t & GetOutputAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeightGradients ();  ; const std::vector< Matrix_t > & GetWeightGradients () const;  ; Matrix_t & GetWeightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7455,Usability,learn,learningRate,7455,"etWeightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7519,Usability,learn,learning,7519,"etWeightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7626,Usability,learn,learningRate,7626,"ghts () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives;  First fDerivatives of the activations of this layer. ;  ; TDescriptors * fDescriptors = nullptr;  ; Scalar_t fEpsilon;  ; Matrix_t fIVar;  ; Scalar_t fMomentum;  The weight dec",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7708,Usability,learn,learning,7708,"ghts () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives;  First fDerivatives of the activations of this layer. ;  ; TDescriptors * fDescriptors = nullptr;  ; Scalar_t fEpsilon;  ; Matrix_t fIVar;  ; Scalar_t fMomentum;  The weight dec",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7819,Usability,learn,learningRate,7819,"node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives;  First fDerivatives of the activations of this layer. ;  ; TDescriptors * fDescriptors = nullptr;  ; Scalar_t fEpsilon;  ; Matrix_t fIVar;  ; Scalar_t fMomentum;  The weight decay. ;  ; Matrix_t fMu;  ; Matrix_t fMu_Training;  ; int fNormAxis;  Normalization axis. For each element of this axis we will compute mean and stddev. ;  ; Tensor_t fReshapedData;  ; int fTrai",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:7903,Usability,learn,learning,7903,"node, const char *name, Matrix_t &matrix);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives;  First fDerivatives of the activations of this layer. ;  ; TDescriptors * fDescriptors = nullptr;  ; Scalar_t fEpsilon;  ; Matrix_t fIVar;  ; Scalar_t fMomentum;  The weight decay. ;  ; Matrix_t fMu;  ; Matrix_t fMu_Training;  ; int fNormAxis;  Normalization axis. For each element of this axis we will compute mean and stddev. ;  ; Tensor_t fReshapedData;  ; int fTrai",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:8006,Usability,learn,learningRate,8006,"t Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives;  First fDerivatives of the activations of this layer. ;  ; TDescriptors * fDescriptors = nullptr;  ; Scalar_t fEpsilon;  ; Matrix_t fIVar;  ; Scalar_t fMomentum;  The weight decay. ;  ; Matrix_t fMu;  ; Matrix_t fMu_Training;  ; int fNormAxis;  Normalization axis. For each element of this axis we will compute mean and stddev. ;  ; Tensor_t fReshapedData;  ; int fTrainedBatches = 0;  ; Matrix_t fVar;  ; Matrix_t fVar_Training;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html:8071,Usability,learn,learning,8071,"t Dropout probability. ;  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Static Protected Member Functions; static size_t CalculateNormDim (int axis, size_t c, size_t h, size_t w);  . Private Attributes; Tensor_t fDerivatives;  First fDerivatives of the activations of this layer. ;  ; TDescriptors * fDescriptors = nullptr;  ; Scalar_t fEpsilon;  ; Matrix_t fIVar;  ; Scalar_t fMomentum;  The weight decay. ;  ; Matrix_t fMu;  ; Matrix_t fMu_Training;  ; int fNormAxis;  Normalization axis. For each element of this axis we will compute mean and stddev. ;  ; Tensor_t fReshapedData;  ; int fTrainedBatches = 0;  ; Matrix_t fVar;  ; Matrix_t fVar_Training;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TBatchNormLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:22691,Availability,error,errors,22691,"atrix_t &C, const Matrix_t &A, const Matrix_t &B);  Standard multiplication of two matrices A and B with the result being written into C. ;  ; static void TransposeMultiply (Matrix_t &output, const Matrix_t &input, const Matrix_t &Weights, Scalar_t alpha=1.0, Scalar_t beta=0.);  Matrix multiplication of two matrices A and B^T (transposed) with the result being written into C. ;  ; static void Hadamard (Tensor_t &A, const Tensor_t &B);  In-place Hadamard (element-wise) product of matrices A and B with the result being written into A. ;  ; static void Hadamard (Matrix_t &A, const Matrix_t &B);  ; static void SumColumns (Matrix_t &B, const Matrix_t &A, Scalar_t alpha=1.0, Scalar_t beta=0.);  Sum columns of (m x n) matrix A and write the results into the first m elements in A. ;  ; static Scalar_t Sum (const Matrix_t &A);  Compute the sum of all elements in A. ;  ; static bool AlmostEquals (const Matrix_t &A, const Matrix_t &B, double epsilon=0.1);  Check two matrices for equality, taking floating point arithmetic errors into account. ;  ; static void ConstAdd (Matrix_t &A, Scalar_t beta);  Add the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConstMult (Matrix_t &A, Scalar_t beta);  Multiply the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ReciprocalElementWise (Matrix_t &A);  Reciprocal each element of the matrix A and write the result into A. ;  ; static void SquareElementWise (Matrix_t &A);  Square each element of the matrix A and write the result into A. ;  ; static void SqrtElementWise (Matrix_t &A);  Square root each element of the matrix A and write the result into A. ;  ; static void AdamUpdate (Matrix_t &A, const Matrix_t &M, const Matrix_t &V, Scalar_t alpha, Scalar_t eps);  Adam updates. ;  ; static void AdamUpdateFirstMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void AdamUpdateSecondMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:31948,Availability,error,errors,31948,"static . Definition at line 60 of file Regularization.hxx. ◆ AddL2RegularizationGradients(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::AddL2RegularizationGradients ; (; Matrix_t & ; A, . const Matrix_t & ; W, . Scalar_t ; weightDecay . ). static . Definition at line 132 of file Regularization.hxx. ◆ AddRowWise() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::AddRowWise ; (; Matrix_t & ; output, . const Matrix_t & ; biases . ). static . Add the vectors biases row-wise to the matrix output. . ◆ AddRowWise() [2/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::AddRowWise ; (; Tensor_t & ; output, . const Matrix_t & ; biases . ). inlinestatic . Definition at line 203 of file Cpu.h. ◆ AlmostEquals(). template<typename AReal > . bool TMVA::DNN::TCpu< AReal >::AlmostEquals ; (; const Matrix_t & ; A, . const Matrix_t & ; B, . double ; epsilon = 0.1 . ). static . Check two matrices for equality, taking floating point arithmetic errors into account. ; Checks two matrices for element-wise equality.; Template Parameters. ARealAn architecture-specific floating point number type. . Parameters. AThe first matrix. ; BThe second matrix. ; epsilonEquality tolerance, needed to address floating point arithmetic. . ReturnsWhether the two matrices can be considered equal element-wise ; Definition at line 194 of file Arithmetic.hxx. ◆ Backward(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::Backward ; (; Tensor_t & ; activationGradientsBackward, . Matrix_t & ; weightGradients, . Matrix_t & ; biasGradients, . const Tensor_t & ; df, . const Tensor_t & ; activationGradients, . const Matrix_t & ; weights, . const Tensor_t & ; activationBackward . ). static . Perform the complete backward propagation step. ; If the provided activationGradientsBackward matrix is not empty, compute the gradients of the objective function with respect to the activations of the previous layer (backward",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:32171,Availability,toler,tolerance,32171," static . Definition at line 132 of file Regularization.hxx. ◆ AddRowWise() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::AddRowWise ; (; Matrix_t & ; output, . const Matrix_t & ; biases . ). static . Add the vectors biases row-wise to the matrix output. . ◆ AddRowWise() [2/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::AddRowWise ; (; Tensor_t & ; output, . const Matrix_t & ; biases . ). inlinestatic . Definition at line 203 of file Cpu.h. ◆ AlmostEquals(). template<typename AReal > . bool TMVA::DNN::TCpu< AReal >::AlmostEquals ; (; const Matrix_t & ; A, . const Matrix_t & ; B, . double ; epsilon = 0.1 . ). static . Check two matrices for equality, taking floating point arithmetic errors into account. ; Checks two matrices for element-wise equality.; Template Parameters. ARealAn architecture-specific floating point number type. . Parameters. AThe first matrix. ; BThe second matrix. ; epsilonEquality tolerance, needed to address floating point arithmetic. . ReturnsWhether the two matrices can be considered equal element-wise ; Definition at line 194 of file Arithmetic.hxx. ◆ Backward(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::Backward ; (; Tensor_t & ; activationGradientsBackward, . Matrix_t & ; weightGradients, . Matrix_t & ; biasGradients, . const Tensor_t & ; df, . const Tensor_t & ; activationGradients, . const Matrix_t & ; weights, . const Tensor_t & ; activationBackward . ). static . Perform the complete backward propagation step. ; If the provided activationGradientsBackward matrix is not empty, compute the gradients of the objective function with respect to the activations of the previous layer (backward direction). Also compute the weight and the bias gradients. Modifies the values in df and thus produces only a valid result, if it is applied the first time after the corresponding forward propagation has been per- formed. ; Definition at line 100 of file Propaga",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:23474,Deployability,update,updates,23474,"to the first m elements in A. ;  ; static Scalar_t Sum (const Matrix_t &A);  Compute the sum of all elements in A. ;  ; static bool AlmostEquals (const Matrix_t &A, const Matrix_t &B, double epsilon=0.1);  Check two matrices for equality, taking floating point arithmetic errors into account. ;  ; static void ConstAdd (Matrix_t &A, Scalar_t beta);  Add the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConstMult (Matrix_t &A, Scalar_t beta);  Multiply the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ReciprocalElementWise (Matrix_t &A);  Reciprocal each element of the matrix A and write the result into A. ;  ; static void SquareElementWise (Matrix_t &A);  Square each element of the matrix A and write the result into A. ;  ; static void SqrtElementWise (Matrix_t &A);  Square root each element of the matrix A and write the result into A. ;  ; static void AdamUpdate (Matrix_t &A, const Matrix_t &M, const Matrix_t &V, Scalar_t alpha, Scalar_t eps);  Adam updates. ;  ; static void AdamUpdateFirstMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void AdamUpdateSecondMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void PrintTensor (const Tensor_t &A, const std::string name=""Cpu-tensor"", bool truncate=false);  . Static Private Attributes; static TRandom * fgRandomGen = nullptr;  . #include <TMVA/DNN/Architectures/Cpu.h>; Member Typedef Documentation. ◆ ActivationDescriptor_t. template<typename AReal = Float_t> . using TMVA::DNN::TCpu< AReal >::ActivationDescriptor_t = DummyDescriptor. Definition at line 75 of file Cpu.h. ◆ AlgorithmBackward_t. template<typename AReal = Float_t> . using TMVA::DNN::TCpu< AReal >::AlgorithmBackward_t = DummyConvolutionBwdDataAlgo. Definition at line 83 of file Cpu.h. ◆ AlgorithmDataType_t. template<typename AReal = Float_t> . using TMVA::DNN::TCpu< AReal >::AlgorithmDataType_t = DummyDataType. Definition at line 85 of file Cpu.",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:29942,Deployability,update,updates,29942,"< AFloat >::ActivationFunctionBackward ; (; Tensor_t & ; dX, . const Tensor_t & ; Y, . const Tensor_t & ; dY, . const Tensor_t & ; X, . EActivationFunction ; activFunct, . const ActivationDescriptor_t ; activationDescr, . const Scalar_t ; alpha = 1, . const Scalar_t ; beta = 0 . ). static . Computes the gradient of the activation function. ; Definition at line 41 of file ActivationFunctions.hxx. ◆ ActivationFunctionForward(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::ActivationFunctionForward ; (; Tensor_t & ; X, . EActivationFunction ; activFunct, . const ActivationDescriptor_t ; activationDescr, . const double ; coef = 0.0, . const Scalar_t ; alpha = 1, . const Scalar_t ; beta = 0 . ). static . Definition at line 32 of file ActivationFunctions.hxx. ◆ AdamUpdate(). template<typename AReal > . void TMVA::DNN::TCpu< AReal >::AdamUpdate ; (; Matrix_t & ; A, . const Matrix_t & ; M, . const Matrix_t & ; V, . Scalar_t ; alpha, . Scalar_t ; eps . ). static . Adam updates. ; Definition at line 343 of file Arithmetic.hxx. ◆ AdamUpdateFirstMom(). template<typename AReal > . void TMVA::DNN::TCpu< AReal >::AdamUpdateFirstMom ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta . ). static . Definition at line 357 of file Arithmetic.hxx. ◆ AdamUpdateSecondMom(). template<typename AReal > . void TMVA::DNN::TCpu< AReal >::AdamUpdateSecondMom ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta . ). static . Definition at line 369 of file Arithmetic.hxx. ◆ AddConvBiases(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::AddConvBiases ; (; Matrix_t & ; output, . const Matrix_t & ; biases . ). static . Add the biases in the Convolutional Layer. . Definition at line 289 of file Propagation.hxx. ◆ AddL1RegularizationGradients(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::AddL1RegularizationGradients ; (; Matrix_t & ; A, . const Matrix_t & ; W, . Scalar_t ; weightDecay . ). static . Definition",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:64617,Deployability,update,update,64617,"t_t> . static void TMVA::DNN::TCpu< AReal >::RNNForward ; (; const Tensor_t & ; , . const Matrix_t & ; , . const Matrix_t & ; , . const Tensor_t & ; , . Tensor_t & ; , . Matrix_t & ; , . Matrix_t & ; , . const RNNDescriptors_t & ; , . RNNWorkspace_t & ; , . bool ;  . ). inlinestatic . Definition at line 631 of file Cpu.h. ◆ RotateWeights(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::RotateWeights ; (; Matrix_t & ; A, . const Matrix_t & ; B, . size_t ; filterDepth, . size_t ; filterHeight, . size_t ; filterWidth, . size_t ; numFilters . ). static . Rotates the matrix B, which is representing a weights, and stores them in the matrix A. ; Definition at line 273 of file Propagation.hxx. ◆ ScaleAdd() [1/2]. template<typename AReal > . void TMVA::DNN::TCpu< AReal >::ScaleAdd ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Adds a the elements in matrix B scaled by c to the elements in the matrix A. ; This is required for the weight update in the gradient descent step. ; Definition at line 248 of file Arithmetic.hxx. ◆ ScaleAdd() [2/2]. template<typename AReal > . void TMVA::DNN::TCpu< AReal >::ScaleAdd ; (; Tensor_t & ; A, . const Tensor_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Above functions extended to vectors. ; Definition at line 279 of file Arithmetic.hxx. ◆ SetRandomSeed(). template<typename AFloat > . void TMVA::DNN::TCpu< AFloat >::SetRandomSeed ; (; size_t ; seed). static . Definition at line 29 of file Initialization.hxx. ◆ Sigmoid() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::Sigmoid ; (; Matrix_t & ; YHat, . const Matrix_t & ;  . ). static . ◆ Sigmoid() [2/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::Sigmoid ; (; Tensor_t & ; B). static . ◆ SigmoidDerivative(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::SigmoidDerivative ; (; Tensor_t & ; B, . const Tensor_t & ; A . ). static . Definition at line 90",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:424,Integrability,interface,interface,424,". ROOT: TMVA::DNN::TCpu< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TCpu< AReal > Class Template Reference. ; template<typename AReal = Float_t>; class TMVA::DNN::TCpu< AReal >The TCpu architecture class. ; Low-level interface class for multi-threaded CPU architectures. Contains as public types the declaration of the scalar, matrix and data loader types for this architecture as well as the remaining functions in the low-level interface in the form of static members. ; Definition at line 64 of file Cpu.h. Public Types; using ActivationDescriptor_t = DummyDescriptor;  ; using AlgorithmBackward_t = DummyConvolutionBwdDataAlgo;  ; using AlgorithmDataType_t = DummyDataType;  ; using AlgorithmForward_t = DummyConvolutionFwdAlgo;  ; using AlgorithmHelper_t = DummyConvolutionBwdFilterAlgo;  ; using BNormDescriptors_t = TDNNGenDescriptors< BNormLayer_t >;  ; using BNormLayer_t = TBatchNormLayer< TCpu< AReal > >;  ; using ConvDescriptors_t = CNN::TCNNDescriptors< ConvLayer_t >;  ; using ConvLayer_t = CNN::TConvLayer< TCpu< AReal > >;  ; using ConvolutionDescriptor_t = DummyDescriptor;  ; using ConvWorkspace_t = CNN::TCNNWorkspace< ConvLayer_t >;  ; using DeviceBuffer_t = TCpuBuffer< AReal >;  ; using DropoutDescriptor_t = DummyDescriptor;  ; using EmptyDescriptor_t = DummyDescriptor;  ; using FilterDescriptor_t = DummyDescriptor;  ; using GenLayer_t = VGeneralLayer< TCpu< AReal > >;  ; using HostBuffer_t = TCpuBuffer< AReal >;  ; using Matrix_t = TCpuMatrix< AReal >;  ; using PoolingDescriptor_t = DummyDescriptor;  ; using PoolingDescriptors_t = CNN::TCNNDescriptors< PoolingLayer_t >;  ; using PoolingLayer_t = CNN::TMaxPoolLayer< TCpu< AReal > >;  ; using PoolingWorkspace_t = CNN::TCNNWorkspace< PoolingLayer_t >;  ; using RecurrentDescriptor_t = DummyDataType;  ; using Redu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:637,Integrability,interface,interface,637,". ROOT: TMVA::DNN::TCpu< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TCpu< AReal > Class Template Reference. ; template<typename AReal = Float_t>; class TMVA::DNN::TCpu< AReal >The TCpu architecture class. ; Low-level interface class for multi-threaded CPU architectures. Contains as public types the declaration of the scalar, matrix and data loader types for this architecture as well as the remaining functions in the low-level interface in the form of static members. ; Definition at line 64 of file Cpu.h. Public Types; using ActivationDescriptor_t = DummyDescriptor;  ; using AlgorithmBackward_t = DummyConvolutionBwdDataAlgo;  ; using AlgorithmDataType_t = DummyDataType;  ; using AlgorithmForward_t = DummyConvolutionFwdAlgo;  ; using AlgorithmHelper_t = DummyConvolutionBwdFilterAlgo;  ; using BNormDescriptors_t = TDNNGenDescriptors< BNormLayer_t >;  ; using BNormLayer_t = TBatchNormLayer< TCpu< AReal > >;  ; using ConvDescriptors_t = CNN::TCNNDescriptors< ConvLayer_t >;  ; using ConvLayer_t = CNN::TConvLayer< TCpu< AReal > >;  ; using ConvolutionDescriptor_t = DummyDescriptor;  ; using ConvWorkspace_t = CNN::TCNNWorkspace< ConvLayer_t >;  ; using DeviceBuffer_t = TCpuBuffer< AReal >;  ; using DropoutDescriptor_t = DummyDescriptor;  ; using EmptyDescriptor_t = DummyDescriptor;  ; using FilterDescriptor_t = DummyDescriptor;  ; using GenLayer_t = VGeneralLayer< TCpu< AReal > >;  ; using HostBuffer_t = TCpuBuffer< AReal >;  ; using Matrix_t = TCpuMatrix< AReal >;  ; using PoolingDescriptor_t = DummyDescriptor;  ; using PoolingDescriptors_t = CNN::TCNNDescriptors< PoolingLayer_t >;  ; using PoolingLayer_t = CNN::TMaxPoolLayer< TCpu< AReal > >;  ; using PoolingWorkspace_t = CNN::TCNNWorkspace< PoolingLayer_t >;  ; using RecurrentDescriptor_t = DummyDataType;  ; using Redu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:6900,Integrability,interface,interface,6900,"nse Layers); Low-level functions required for the forward propagation of activations through the network. . static void Backward (Tensor_t &activationGradientsBackward, Matrix_t &weightGradients, Matrix_t &biasGradients, const Tensor_t &df, const Tensor_t &activationGradients, const Matrix_t &weights, const Tensor_t &activationBackward);  Perform the complete backward propagation step. ;  ; static void ScaleAdd (Matrix_t &A, const Matrix_t &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (Matrix_t &B, const Matrix_t &A);  ; template<typename AMatrix_t > ; static void CopyDiffArch (Matrix_t &B, const AMatrix_t &A);  ; static void ScaleAdd (Tensor_t &A, const Tensor_t &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (Tensor_t &A, const Tensor_t &B);  ; template<typename ATensor_t > ; static void CopyDiffArch (Tensor_t &A, const ATensor_t &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< Matrix_t > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const Scalar_t alpha=1, const Scalar_t beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const Scalar_t alpha=1, const Scalar_t beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:6923,Integrability,rout,routines,6923,"nse Layers); Low-level functions required for the forward propagation of activations through the network. . static void Backward (Tensor_t &activationGradientsBackward, Matrix_t &weightGradients, Matrix_t &biasGradients, const Tensor_t &df, const Tensor_t &activationGradients, const Matrix_t &weights, const Tensor_t &activationBackward);  Perform the complete backward propagation step. ;  ; static void ScaleAdd (Matrix_t &A, const Matrix_t &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (Matrix_t &B, const Matrix_t &A);  ; template<typename AMatrix_t > ; static void CopyDiffArch (Matrix_t &B, const AMatrix_t &A);  ; static void ScaleAdd (Tensor_t &A, const Tensor_t &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (Tensor_t &A, const Tensor_t &B);  ; template<typename ATensor_t > ; static void CopyDiffArch (Tensor_t &A, const ATensor_t &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< Matrix_t > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const Scalar_t alpha=1, const Scalar_t beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const Scalar_t alpha=1, const Scalar_t beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:8660,Integrability,rout,routing,8660,"id Relu (Tensor_t &B);  ; static void ReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Sigmoid (Tensor_t &B);  ; static void SigmoidDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Tanh (Tensor_t &B);  ; static void TanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (Tensor_t &B);  ; static void SymmetricReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SoftSign (Tensor_t &B);  ; static void SoftSignDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Gauss (Tensor_t &B);  ; static void GaussDerivative (Tensor_t &B, const Tensor_t &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void CrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t SoftmaxCrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Softmax transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void SoftmaxCrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:10829,Integrability,interface,interface,10829,"ts (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; Output Functions; Output functions transform the activations output of the output layer in the network to a valid prediction YHat for the desired usage of the network, e.g.; the identity function for regression or the sigmoid transformation for two-class classification. . static void Sigmoid (Matrix_t &YHat, const Matrix_t &);  ; static void Softmax (Matrix_t &YHat, const Matrix_t &);  ; Regularization; For each regularization type two functions are required, one named <Type>Regularization that evaluates the corresponding regularization functional for a given weight matrix and the Add<Type>RegularizationGradients, that adds the regularization component in the gradients to the provided matrix. . static Scalar_t L1Regularization (const Matrix_t &W);  ; static void AddL1RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; static Scalar_t L2Regularization (const Matrix_t &W);  ; static void AddL2RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; Initialization; For each initialization method, one function in the low-level interface is provided.; The naming scheme is ; Initialize<Type>; for a given initialization method Type. . static void InitializeGauss (Matrix_t &A);  ; static void InitializeUniform (Matrix_t &A);  ; static void InitializeIdentity (Matrix_t &A);  ; static void InitializeZero (Matrix_t &A);  ; static void InitializeZero (Tensor_t &A);  ; static void InitializeGlorotNormal (Matrix_t &A);  Truncated normal initialization (Glorot, called also Xavier normal) The values are sample with a normal distribution with stddev = sqrt(2/N_input + N_output) and values larger than 2 * stddev are discarded See Glorot & Bengio, AISTATS 2010 - http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf. ;  ; static void InitializeGlorotUniform (Matrix_t &A);  Sample from a uniform distribution in range [ -lim,+lim] where",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:21629,Integrability,interface,interface,21629,"nst TCpuMatrix< Scalar_t > &weights_candidate, const TCpuMatrix< Scalar_t > &weights_reset_state, const TCpuMatrix< Scalar_t > &weights_update_state, const TCpuMatrix< Scalar_t > &weights_candidate_state, const TCpuMatrix< Scalar_t > &input, TCpuMatrix< Scalar_t > &input_gradient, bool resetGateAfter);  Backward pass for GRU Network. ;  ; Forward and Backward Propagation in Reshape Layer; static void Reshape (Matrix_t &A, const Matrix_t &B);  Transform the matrix B to a matrix with different dimensions A. ;  ; static void Flatten (Tensor_t &A, const Tensor_t &B);  Flattens the tensor B, such that each matrix, is stretched in one row, resulting with a matrix A. ;  ; static void Deflatten (Tensor_t &A, const Tensor_t &B);  Transforms each row of B to a matrix and stores it in the tensor B. ;  ; static void Rearrange (Tensor_t &out, const Tensor_t &in);  Rearrage data according to time fill B x T x D out with T x B x D matrix in. ;  ; Additional Arithmetic Functions; Additional arithmetic on CUDA matrices used to implement the low-level interface. . static void Multiply (Matrix_t &C, const Matrix_t &A, const Matrix_t &B);  Standard multiplication of two matrices A and B with the result being written into C. ;  ; static void TransposeMultiply (Matrix_t &output, const Matrix_t &input, const Matrix_t &Weights, Scalar_t alpha=1.0, Scalar_t beta=0.);  Matrix multiplication of two matrices A and B^T (transposed) with the result being written into C. ;  ; static void Hadamard (Tensor_t &A, const Tensor_t &B);  In-place Hadamard (element-wise) product of matrices A and B with the result being written into A. ;  ; static void Hadamard (Matrix_t &A, const Matrix_t &B);  ; static void SumColumns (Matrix_t &B, const Matrix_t &A, Scalar_t alpha=1.0, Scalar_t beta=0.);  Sum columns of (m x n) matrix A and write the results into the first m elements in A. ;  ; static Scalar_t Sum (const Matrix_t &A);  Compute the sum of all elements in A. ;  ; static bool AlmostEquals (const Matrix_t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:6534,Modifiability,extend,extended,6534," AddRowWise (Matrix_t &output, const Matrix_t &biases);  Add the vectors biases row-wise to the matrix output. ;  ; static void AddRowWise (Tensor_t &output, const Matrix_t &biases);  ; Backward Propagation (Dense Layers); Low-level functions required for the forward propagation of activations through the network. . static void Backward (Tensor_t &activationGradientsBackward, Matrix_t &weightGradients, Matrix_t &biasGradients, const Tensor_t &df, const Tensor_t &activationGradients, const Matrix_t &weights, const Tensor_t &activationBackward);  Perform the complete backward propagation step. ;  ; static void ScaleAdd (Matrix_t &A, const Matrix_t &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (Matrix_t &B, const Matrix_t &A);  ; template<typename AMatrix_t > ; static void CopyDiffArch (Matrix_t &B, const AMatrix_t &A);  ; static void ScaleAdd (Tensor_t &A, const Tensor_t &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (Tensor_t &A, const Tensor_t &B);  ; template<typename ATensor_t > ; static void CopyDiffArch (Tensor_t &A, const ATensor_t &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< Matrix_t > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const Scalar_t alpha=1, const Scalar_t beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:12643,Modifiability,variab,variable,12643,"/v9/glorot10a/glorot10a.pdf. ;  ; static void InitializeGlorotUniform (Matrix_t &A);  Sample from a uniform distribution in range [ -lim,+lim] where lim = sqrt(6/N_in+N_out). ;  ; static TRandom & GetRandomGenerator ();  ; static void SetRandomSeed (size_t seed);  ; Dropout; static void DropoutForward (Tensor_t &A, TDescriptors *descriptors, TWorkspace *workspace, Scalar_t p);  Apply dropout with activation probability p to the given tensor A and scale the result by reciprocal of p. ;  ; static void DropoutForward (Matrix_t &A, Scalar_t p);  ; static void DropoutBackward (Tensor_t &, TDescriptors *, TWorkspace *);  ; Batch Normalization Layer Propagation; static void BatchNormLayerForwardTraining (int axis, const Tensor_t &x, Tensor_t &y, Matrix_t &gamma, Matrix_t &beta, Matrix_t &mean, Matrix_t &, Matrix_t &iVariance, Matrix_t &runningMeans, Matrix_t &runningVars, Scalar_t nTrainedBatches, Scalar_t momentum, Scalar_t epsilon, const TensorDescriptor_t &bnParDescriptor);  The input from each batch are normalized during training to have zero mean and unit variance and they are then scaled by two parameter, different for each input variable: ;  ; static void BatchNormLayerForwardInference (int axis, const Tensor_t &x, Matrix_t &gamma, Matrix_t &beta, Tensor_t &y, const Matrix_t &runningMeans, const Matrix_t &runningVars, Scalar_t epsilon, const TensorDescriptor_t &);  During inference the inputs are not normalized using the batch mean but the previously computed at running mean and variance. ;  ; static void BatchNormLayerBackward (int axis, const Tensor_t &x, const Tensor_t &dy, Tensor_t &dx, Matrix_t &gamma, Matrix_t &dgamma, Matrix_t &dbeta, const Matrix_t &mean, const Matrix_t &variance, const Matrix_t &iVariance, Scalar_t epsilon, const TensorDescriptor_t &);  ; static Tensor_t BatchNormLayerReshapeTensor (int axis, const Tensor_t &x);  ; Forward Propagation in Convolutional Layer; static size_t calculateDimension (size_t imgDim, size_t fltDim, size_t padding, siz",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:34882,Modifiability,variab,variable,34882,"erence ; (; int ; axis, . const Tensor_t & ; x, . Matrix_t & ; gamma, . Matrix_t & ; beta, . Tensor_t & ; y, . const Matrix_t & ; runningMeans, . const Matrix_t & ; runningVars, . Scalar_t ; epsilon, . const TensorDescriptor_t & ;  . ). static . During inference the inputs are not normalized using the batch mean but the previously computed at running mean and variance. ; Definition at line 794 of file Propagation.hxx. ◆ BatchNormLayerForwardTraining(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::BatchNormLayerForwardTraining ; (; int ; axis, . const Tensor_t & ; x, . Tensor_t & ; y, . Matrix_t & ; gamma, . Matrix_t & ; beta, . Matrix_t & ; mean, . Matrix_t & ; , . Matrix_t & ; iVariance, . Matrix_t & ; runningMeans, . Matrix_t & ; runningVars, . Scalar_t ; nTrainedBatches, . Scalar_t ; momentum, . Scalar_t ; epsilon, . const TensorDescriptor_t & ; bnParDescriptor . ). static . The input from each batch are normalized during training to have zero mean and unit variance and they are then scaled by two parameter, different for each input variable: . a scale factor \gamma gamma; an offset \beta beta . Definition at line 709 of file Propagation.hxx. ◆ BatchNormLayerReshapeTensor(). template<typename AReal = Float_t> . TCpuTensor< AFloat > TMVA::DNN::TCpu< AFloat >::BatchNormLayerReshapeTensor ; (; int ; axis, . const Tensor_t & ; x . ). static . Definition at line 693 of file Propagation.hxx. ◆ CalculateConvActivationGradients(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::CalculateConvActivationGradients ; (; Tensor_t & ; activationGradientsBackward, . const Tensor_t & ; df, . const Matrix_t & ; weights, . size_t ; batchSize, . size_t ; inputHeight, . size_t ; inputWidth, . size_t ; depth, . size_t ; height, . size_t ; width, . size_t ; filterDepth, . size_t ; filterHeight, . size_t ; filterWidth . ). static . Utility function for calculating the activation gradients of the layer before the convolutional layer. ; Definiti",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:64890,Modifiability,extend,extended,64890,"inestatic . Definition at line 631 of file Cpu.h. ◆ RotateWeights(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::RotateWeights ; (; Matrix_t & ; A, . const Matrix_t & ; B, . size_t ; filterDepth, . size_t ; filterHeight, . size_t ; filterWidth, . size_t ; numFilters . ). static . Rotates the matrix B, which is representing a weights, and stores them in the matrix A. ; Definition at line 273 of file Propagation.hxx. ◆ ScaleAdd() [1/2]. template<typename AReal > . void TMVA::DNN::TCpu< AReal >::ScaleAdd ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Adds a the elements in matrix B scaled by c to the elements in the matrix A. ; This is required for the weight update in the gradient descent step. ; Definition at line 248 of file Arithmetic.hxx. ◆ ScaleAdd() [2/2]. template<typename AReal > . void TMVA::DNN::TCpu< AReal >::ScaleAdd ; (; Tensor_t & ; A, . const Tensor_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Above functions extended to vectors. ; Definition at line 279 of file Arithmetic.hxx. ◆ SetRandomSeed(). template<typename AFloat > . void TMVA::DNN::TCpu< AFloat >::SetRandomSeed ; (; size_t ; seed). static . Definition at line 29 of file Initialization.hxx. ◆ Sigmoid() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::Sigmoid ; (; Matrix_t & ; YHat, . const Matrix_t & ;  . ). static . ◆ Sigmoid() [2/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCpu< AReal >::Sigmoid ; (; Tensor_t & ; B). static . ◆ SigmoidDerivative(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::SigmoidDerivative ; (; Tensor_t & ; B, . const Tensor_t & ; A . ). static . Definition at line 90 of file ActivationFunctions.hxx. ◆ Softmax(). template<typename AReal = Float_t> . void TMVA::DNN::TCpu< AFloat >::Softmax ; (; Matrix_t & ; YHat, . const Matrix_t & ;  . ). static . Definition at line 33 of file OutputFunctions.hxx. ◆ SoftmaxCrossEntropy(). template<typen",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:444,Performance,multi-thread,multi-threaded,444,". ROOT: TMVA::DNN::TCpu< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TCpu< AReal > Class Template Reference. ; template<typename AReal = Float_t>; class TMVA::DNN::TCpu< AReal >The TCpu architecture class. ; Low-level interface class for multi-threaded CPU architectures. Contains as public types the declaration of the scalar, matrix and data loader types for this architecture as well as the remaining functions in the low-level interface in the form of static members. ; Definition at line 64 of file Cpu.h. Public Types; using ActivationDescriptor_t = DummyDescriptor;  ; using AlgorithmBackward_t = DummyConvolutionBwdDataAlgo;  ; using AlgorithmDataType_t = DummyDataType;  ; using AlgorithmForward_t = DummyConvolutionFwdAlgo;  ; using AlgorithmHelper_t = DummyConvolutionBwdFilterAlgo;  ; using BNormDescriptors_t = TDNNGenDescriptors< BNormLayer_t >;  ; using BNormLayer_t = TBatchNormLayer< TCpu< AReal > >;  ; using ConvDescriptors_t = CNN::TCNNDescriptors< ConvLayer_t >;  ; using ConvLayer_t = CNN::TConvLayer< TCpu< AReal > >;  ; using ConvolutionDescriptor_t = DummyDescriptor;  ; using ConvWorkspace_t = CNN::TCNNWorkspace< ConvLayer_t >;  ; using DeviceBuffer_t = TCpuBuffer< AReal >;  ; using DropoutDescriptor_t = DummyDescriptor;  ; using EmptyDescriptor_t = DummyDescriptor;  ; using FilterDescriptor_t = DummyDescriptor;  ; using GenLayer_t = VGeneralLayer< TCpu< AReal > >;  ; using HostBuffer_t = TCpuBuffer< AReal >;  ; using Matrix_t = TCpuMatrix< AReal >;  ; using PoolingDescriptor_t = DummyDescriptor;  ; using PoolingDescriptors_t = CNN::TCNNDescriptors< PoolingLayer_t >;  ; using PoolingLayer_t = CNN::TMaxPoolLayer< TCpu< AReal > >;  ; using PoolingWorkspace_t = CNN::TCNNWorkspace< PoolingLayer_t >;  ; using RecurrentDescriptor_t = DummyDataType;  ; using Redu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:550,Performance,load,loader,550,". ROOT: TMVA::DNN::TCpu< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TCpu< AReal > Class Template Reference. ; template<typename AReal = Float_t>; class TMVA::DNN::TCpu< AReal >The TCpu architecture class. ; Low-level interface class for multi-threaded CPU architectures. Contains as public types the declaration of the scalar, matrix and data loader types for this architecture as well as the remaining functions in the low-level interface in the form of static members. ; Definition at line 64 of file Cpu.h. Public Types; using ActivationDescriptor_t = DummyDescriptor;  ; using AlgorithmBackward_t = DummyConvolutionBwdDataAlgo;  ; using AlgorithmDataType_t = DummyDataType;  ; using AlgorithmForward_t = DummyConvolutionFwdAlgo;  ; using AlgorithmHelper_t = DummyConvolutionBwdFilterAlgo;  ; using BNormDescriptors_t = TDNNGenDescriptors< BNormLayer_t >;  ; using BNormLayer_t = TBatchNormLayer< TCpu< AReal > >;  ; using ConvDescriptors_t = CNN::TCNNDescriptors< ConvLayer_t >;  ; using ConvLayer_t = CNN::TConvLayer< TCpu< AReal > >;  ; using ConvolutionDescriptor_t = DummyDescriptor;  ; using ConvWorkspace_t = CNN::TCNNWorkspace< ConvLayer_t >;  ; using DeviceBuffer_t = TCpuBuffer< AReal >;  ; using DropoutDescriptor_t = DummyDescriptor;  ; using EmptyDescriptor_t = DummyDescriptor;  ; using FilterDescriptor_t = DummyDescriptor;  ; using GenLayer_t = VGeneralLayer< TCpu< AReal > >;  ; using HostBuffer_t = TCpuBuffer< AReal >;  ; using Matrix_t = TCpuMatrix< AReal >;  ; using PoolingDescriptor_t = DummyDescriptor;  ; using PoolingDescriptors_t = CNN::TCNNDescriptors< PoolingLayer_t >;  ; using PoolingLayer_t = CNN::TMaxPoolLayer< TCpu< AReal > >;  ; using PoolingWorkspace_t = CNN::TCNNWorkspace< PoolingLayer_t >;  ; using RecurrentDescriptor_t = DummyDataType;  ; using Redu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:8574,Safety,predict,prediction,8574,"x. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const Scalar_t alpha=1, const Scalar_t beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const Scalar_t alpha=1, const Scalar_t beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static void ReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Sigmoid (Tensor_t &B);  ; static void SigmoidDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Tanh (Tensor_t &B);  ; static void TanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (Tensor_t &B);  ; static void SymmetricReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SoftSign (Tensor_t &B);  ; static void SoftSignDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Gauss (Tensor_t &B);  ; static void GaussDerivative (Tensor_t &B, const Tensor_t &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:8622,Safety,predict,prediction,8622,"x. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const Scalar_t alpha=1, const Scalar_t beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const Scalar_t alpha=1, const Scalar_t beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static void ReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Sigmoid (Tensor_t &B);  ; static void SigmoidDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Tanh (Tensor_t &B);  ; static void TanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (Tensor_t &B);  ; static void SymmetricReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SoftSign (Tensor_t &B);  ; static void SoftSignDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Gauss (Tensor_t &B);  ; static void GaussDerivative (Tensor_t &B, const Tensor_t &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html:9847,Safety,predict,prediction,9847,"ng of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void CrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t SoftmaxCrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Softmax transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void SoftmaxCrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; Output Functions; Output functions transform the activations output of the output layer in the network to a valid prediction YHat for the desired usage of the network, e.g.; the identity function for regression or the sigmoid transformation for two-class classification. . static void Sigmoid (Matrix_t &YHat, const Matrix_t &);  ; static void Softmax (Matrix_t &YHat, const Matrix_t &);  ; Regularization; For each regularization type two functions are required, one named <Type>Regularization that evaluates the corresponding regularization functional for a given weight matrix and the Add<Type>RegularizationGradients, that adds the regularization component in the gradients to the provided matrix. . static Scalar_t L1Regularization (const Matrix_t &W);  ; static void AddL1RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; static Scalar_t L2Regularization (const Matrix_t &W);  ; static void AddL2RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpu.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpu.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html:792,Performance,perform,performs,792,". ROOT: TMVA::DNN::TCpuBuffer< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCpuBuffer< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCpuBuffer< AFloat >TCpuBuffer. ; Since the memory on the CPU is homogeneous, only one buffer class is required. The host and device buffer classes are the same and copying between the host and device buffer is achieved by simply swapping the memory pointers.; Memory is handled as a shared pointer to a pointer of type AFloat, which is the floating point type used for the implementation.; Copying and assignment of TCpuBuffer objects performs only a shallow copy meaning the underlying data is shared between those objects.; Template Parameters. AFloatThe floating point type used for the computations. . Definition at line 43 of file CpuBuffer.h. Classes; class  FakeIteratorBegin;  ; struct  TDestructor;  . Public Member Functions;  TCpuBuffer ()=default;  ;  TCpuBuffer (const TCpuBuffer &)=default;  ;  TCpuBuffer (size_t size);  Construct buffer to hold size numbers of type AFloat. ;  ;  TCpuBuffer (TCpuBuffer &&)=default;  ; FakeIteratorBegin begin ();  ; void CopyFrom (const TCpuBuffer &);  Copy data from another buffer. ;  ; void CopyTo (TCpuBuffer &) const;  Copy data to another buffer. ;  ; AFloat * data () const;  ; size_t GetSize () const;  copy pointer from an external ;  ; TCpuBuffer GetSubBuffer (size_t offset, size_t start) const;  Return sub-buffer of size start starting at element offset. ;  ; size_t GetUseCount () const;  ;  operator AFloat * () const;  ; TCpuBuffer & operator= (const TCpuBuffer &)=default;  ; TCpuBuffer & operator= (TCpuBuffer &&)=default;  ; AFloat & operator[] (size_t i);  ; AFloat operator[] (size_t i) const;  . Private Attributes; std::shared_ptr< AFloat * > fBuffer;  ; struct TMVA::DNN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html:3153,Performance,perform,performed,3153,"ion. ◆ TCpuBuffer() [1/4]. template<typename AReal > . TMVA::DNN::TCpuBuffer< AReal >::TCpuBuffer ; (; size_t ; size). Construct buffer to hold size numbers of type AFloat. ; Definition at line 38 of file CpuBuffer.cxx. ◆ TCpuBuffer() [2/4]. template<typename AFloat > . TMVA::DNN::TCpuBuffer< AFloat >::TCpuBuffer ; (; ). default . ◆ TCpuBuffer() [3/4]. template<typename AFloat > . TMVA::DNN::TCpuBuffer< AFloat >::TCpuBuffer ; (; const TCpuBuffer< AFloat > & ; ). default . ◆ TCpuBuffer() [4/4]. template<typename AFloat > . TMVA::DNN::TCpuBuffer< AFloat >::TCpuBuffer ; (; TCpuBuffer< AFloat > && ; ). default . Member Function Documentation. ◆ begin(). template<typename AFloat > . FakeIteratorBegin TMVA::DNN::TCpuBuffer< AFloat >::begin ; (; ). inline . Definition at line 80 of file CpuBuffer.h. ◆ CopyFrom(). template<typename AReal > . void TMVA::DNN::TCpuBuffer< AReal >::CopyFrom ; (; const TCpuBuffer< AFloat > & ; other). Copy data from another buffer. ; No real copying is performed, only the data pointers are swapped. ; Definition at line 57 of file CpuBuffer.cxx. ◆ CopyTo(). template<typename AReal > . void TMVA::DNN::TCpuBuffer< AReal >::CopyTo ; (; TCpuBuffer< AFloat > & ; other); const. Copy data to another buffer. ; No real copying is performed, only the data pointers are swapped. ; Definition at line 65 of file CpuBuffer.cxx. ◆ data(). template<typename AFloat > . AFloat * TMVA::DNN::TCpuBuffer< AFloat >::data ; (; ); const. inline . Definition at line 68 of file CpuBuffer.h. ◆ GetSize(). template<typename AFloat > . size_t TMVA::DNN::TCpuBuffer< AFloat >::GetSize ; (; ); const. inline . copy pointer from an external ; Definition at line 103 of file CpuBuffer.h. ◆ GetSubBuffer(). template<typename AReal > . TCpuBuffer< AReal > TMVA::DNN::TCpuBuffer< AReal >::GetSubBuffer ; (; size_t ; offset, . size_t ; start . ); const. Return sub-buffer of size start starting at element offset. ; Definition at line 47 of file CpuBuffer.cxx. ◆ GetUseCount(). template<typenam",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html:3426,Performance,perform,performed,3426,"VA::DNN::TCpuBuffer< AFloat >::TCpuBuffer ; (; ). default . ◆ TCpuBuffer() [3/4]. template<typename AFloat > . TMVA::DNN::TCpuBuffer< AFloat >::TCpuBuffer ; (; const TCpuBuffer< AFloat > & ; ). default . ◆ TCpuBuffer() [4/4]. template<typename AFloat > . TMVA::DNN::TCpuBuffer< AFloat >::TCpuBuffer ; (; TCpuBuffer< AFloat > && ; ). default . Member Function Documentation. ◆ begin(). template<typename AFloat > . FakeIteratorBegin TMVA::DNN::TCpuBuffer< AFloat >::begin ; (; ). inline . Definition at line 80 of file CpuBuffer.h. ◆ CopyFrom(). template<typename AReal > . void TMVA::DNN::TCpuBuffer< AReal >::CopyFrom ; (; const TCpuBuffer< AFloat > & ; other). Copy data from another buffer. ; No real copying is performed, only the data pointers are swapped. ; Definition at line 57 of file CpuBuffer.cxx. ◆ CopyTo(). template<typename AReal > . void TMVA::DNN::TCpuBuffer< AReal >::CopyTo ; (; TCpuBuffer< AFloat > & ; other); const. Copy data to another buffer. ; No real copying is performed, only the data pointers are swapped. ; Definition at line 65 of file CpuBuffer.cxx. ◆ data(). template<typename AFloat > . AFloat * TMVA::DNN::TCpuBuffer< AFloat >::data ; (; ); const. inline . Definition at line 68 of file CpuBuffer.h. ◆ GetSize(). template<typename AFloat > . size_t TMVA::DNN::TCpuBuffer< AFloat >::GetSize ; (; ); const. inline . copy pointer from an external ; Definition at line 103 of file CpuBuffer.h. ◆ GetSubBuffer(). template<typename AReal > . TCpuBuffer< AReal > TMVA::DNN::TCpuBuffer< AReal >::GetSubBuffer ; (; size_t ; offset, . size_t ; start . ); const. Return sub-buffer of size start starting at element offset. ; Definition at line 47 of file CpuBuffer.cxx. ◆ GetUseCount(). template<typename AFloat > . size_t TMVA::DNN::TCpuBuffer< AFloat >::GetUseCount ; (; ); const. inline . Definition at line 105 of file CpuBuffer.h. ◆ operator AFloat *(). template<typename AFloat > . TMVA::DNN::TCpuBuffer< AFloat >::operator AFloat * ; (; ); const. inline . Definition at",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html:579,Usability,simpl,simply,579,". ROOT: TMVA::DNN::TCpuBuffer< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCpuBuffer< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCpuBuffer< AFloat >TCpuBuffer. ; Since the memory on the CPU is homogeneous, only one buffer class is required. The host and device buffer classes are the same and copying between the host and device buffer is achieved by simply swapping the memory pointers.; Memory is handled as a shared pointer to a pointer of type AFloat, which is the floating point type used for the implementation.; Copying and assignment of TCpuBuffer objects performs only a shallow copy meaning the underlying data is shared between those objects.; Template Parameters. AFloatThe floating point type used for the computations. . Definition at line 43 of file CpuBuffer.h. Classes; class  FakeIteratorBegin;  ; struct  TDestructor;  . Public Member Functions;  TCpuBuffer ()=default;  ;  TCpuBuffer (const TCpuBuffer &)=default;  ;  TCpuBuffer (size_t size);  Construct buffer to hold size numbers of type AFloat. ;  ;  TCpuBuffer (TCpuBuffer &&)=default;  ; FakeIteratorBegin begin ();  ; void CopyFrom (const TCpuBuffer &);  Copy data from another buffer. ;  ; void CopyTo (TCpuBuffer &) const;  Copy data to another buffer. ;  ; AFloat * data () const;  ; size_t GetSize () const;  copy pointer from an external ;  ; TCpuBuffer GetSubBuffer (size_t offset, size_t start) const;  Return sub-buffer of size start starting at element offset. ;  ; size_t GetUseCount () const;  ;  operator AFloat * () const;  ; TCpuBuffer & operator= (const TCpuBuffer &)=default;  ; TCpuBuffer & operator= (TCpuBuffer &&)=default;  ; AFloat & operator[] (size_t i);  ; AFloat operator[] (size_t i) const;  . Private Attributes; std::shared_ptr< AFloat * > fBuffer;  ; struct TMVA::DNN",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuBuffer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html:1471,Energy Efficiency,allocate,allocate,1471,"DNN::TCpuMatrix< AFloat >The TCpuMatrix class. ; Matrix class for multi-threaded CPU architectures. Uses the TCpuBuffer class to store the matrices in column-major format for compatibility with BLAS. Provides Map and MapFrom member functions to simplify the application of activation functions and derivatives to matrices.; Copying and assignment of TCpuMatrix objects only performs shallow copies, i.e. copying is fast and the resulting objects share the element data.; Template Parameters. AFloatThe floating point type used to represent the matrix elements. . Definition at line 86 of file CpuMatrix.h. Public Member Functions;  TCpuMatrix ();  ;  TCpuMatrix (const TCpuBuffer< AFloat > &buffer, size_t m, size_t n);  Construct a m-times-n matrix from the given buffer. ;  ;  TCpuMatrix (const TCpuMatrix &)=default;  ;  TCpuMatrix (const TMatrixT< AFloat > &);  Construct a TCpuMatrix object by (deeply) copying from a TMatrixT<Double_t> matrix. ;  ;  TCpuMatrix (size_t nRows, size_t nCols);  Construct matrix and allocate space for its elements. ;  ;  TCpuMatrix (TCpuMatrix &&)=default;  ;  ~TCpuMatrix ()=default;  ; TCpuBuffer< AFloat > & GetBuffer ();  Returns pointer to a vector holding only ones with a guaranteed length of the number of columns of every instantiated CpuMatrix object. ;  ; const TCpuBuffer< AFloat > & GetBuffer () const;  ; TCpuBuffer< AFloat > & GetDeviceBuffer ();  ; const TCpuBuffer< AFloat > & GetDeviceBuffer () const;  ; size_t GetNcols () const;  ; size_t GetNoElements () const;  ; size_t GetNrows () const;  ; AFloat * GetRawDataPointer ();  Return raw pointer to the elements stored contiguously in column-major order. ;  ; const AFloat * GetRawDataPointer () const;  ; size_t GetSize () const;  ; template<typename Function_t > ; void Map (Function_t &f);  Map the given function over the matrix elements. ;  ; template<typename Function_t > ; void MapFrom (Function_t &f, const TCpuMatrix &A);  Same as maps but takes the input values from the matrix A an",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html:4014,Energy Efficiency,allocate,allocate,4014,"ic Public Member Functions; static size_t GetNWorkItems (size_t nelements);  ; static const AFloat * GetOnePointer ();  ; static size_t GetOnePointerSize ();  ; static Executor & GetThreadExecutor ();  ; static void InitializeOneVector (size_t n);  . Public Attributes; TCpuBuffer< AFloat > fBuffer;  The buffer holding the matrix elements in column-major format. ;  . Private Member Functions; void Initialize ();  . Private Attributes; size_t fNCols;  ; size_t fNRows;  . Static Private Attributes; static std::vector< AFloat > fOnes {};  Vector filled with ones used for BLAS calls. ;  . #include <TMVA/DNN/Architectures/Cpu/CpuMatrix.h>; Constructor & Destructor Documentation. ◆ TCpuMatrix() [1/6]. template<typename AFloat > . TMVA::DNN::TCpuMatrix< AFloat >::TCpuMatrix ; (; ). inline . Definition at line 116 of file CpuMatrix.h. ◆ TCpuMatrix() [2/6]. template<typename AReal > . TMVA::DNN::TCpuMatrix< AReal >::TCpuMatrix ; (; size_t ; nRows, . size_t ; nCols . ). Construct matrix and allocate space for its elements. ; Definition at line 23 of file CpuMatrix.cxx. ◆ TCpuMatrix() [3/6]. template<typename AFloat > . TMVA::DNN::TCpuMatrix< AFloat >::TCpuMatrix ; (; const TMatrixT< AFloat > & ; ). Construct a TCpuMatrix object by (deeply) copying from a TMatrixT<Double_t> matrix. . ◆ TCpuMatrix() [4/6]. template<typename AFloat > . TMVA::DNN::TCpuMatrix< AFloat >::TCpuMatrix ; (; const TCpuBuffer< AFloat > & ; buffer, . size_t ; m, . size_t ; n . ). Construct a m-times-n matrix from the given buffer. ; The size must of course match. . ◆ TCpuMatrix() [5/6]. template<typename AFloat > . TMVA::DNN::TCpuMatrix< AFloat >::TCpuMatrix ; (; const TCpuMatrix< AFloat > & ; ). default . ◆ TCpuMatrix() [6/6]. template<typename AFloat > . TMVA::DNN::TCpuMatrix< AFloat >::TCpuMatrix ; (; TCpuMatrix< AFloat > && ; ). default . ◆ ~TCpuMatrix(). template<typename AFloat > . TMVA::DNN::TCpuMatrix< AFloat >::~TCpuMatrix ; (; ). default . Member Function Documentation. ◆ GetBuffer() [1/2]. templ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html:518,Performance,multi-thread,multi-threaded,518,". ROOT: TMVA::DNN::TCpuMatrix< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCpuMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCpuMatrix< AFloat >The TCpuMatrix class. ; Matrix class for multi-threaded CPU architectures. Uses the TCpuBuffer class to store the matrices in column-major format for compatibility with BLAS. Provides Map and MapFrom member functions to simplify the application of activation functions and derivatives to matrices.; Copying and assignment of TCpuMatrix objects only performs shallow copies, i.e. copying is fast and the resulting objects share the element data.; Template Parameters. AFloatThe floating point type used to represent the matrix elements. . Definition at line 86 of file CpuMatrix.h. Public Member Functions;  TCpuMatrix ();  ;  TCpuMatrix (const TCpuBuffer< AFloat > &buffer, size_t m, size_t n);  Construct a m-times-n matrix from the given buffer. ;  ;  TCpuMatrix (const TCpuMatrix &)=default;  ;  TCpuMatrix (const TMatrixT< AFloat > &);  Construct a TCpuMatrix object by (deeply) copying from a TMatrixT<Double_t> matrix. ;  ;  TCpuMatrix (size_t nRows, size_t nCols);  Construct matrix and allocate space for its elements. ;  ;  TCpuMatrix (TCpuMatrix &&)=default;  ;  ~TCpuMatrix ()=default;  ; TCpuBuffer< AFloat > & GetBuffer ();  Returns pointer to a vector holding only ones with a guaranteed length of the number of columns of every instantiated CpuMatrix object. ;  ; const TCpuBuffer< AFloat > & GetBuffer () const;  ; TCpuBuffer< AFloat > & GetDeviceBuffer ();  ; const TCpuBuffer< AFloat > & GetDeviceBuffer () const;  ; size_t GetNcols () const;  ; size_t GetNoElements () const;  ; size_t GetNrows () const; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html:826,Performance,perform,performs,826,". ROOT: TMVA::DNN::TCpuMatrix< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCpuMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCpuMatrix< AFloat >The TCpuMatrix class. ; Matrix class for multi-threaded CPU architectures. Uses the TCpuBuffer class to store the matrices in column-major format for compatibility with BLAS. Provides Map and MapFrom member functions to simplify the application of activation functions and derivatives to matrices.; Copying and assignment of TCpuMatrix objects only performs shallow copies, i.e. copying is fast and the resulting objects share the element data.; Template Parameters. AFloatThe floating point type used to represent the matrix elements. . Definition at line 86 of file CpuMatrix.h. Public Member Functions;  TCpuMatrix ();  ;  TCpuMatrix (const TCpuBuffer< AFloat > &buffer, size_t m, size_t n);  Construct a m-times-n matrix from the given buffer. ;  ;  TCpuMatrix (const TCpuMatrix &)=default;  ;  TCpuMatrix (const TMatrixT< AFloat > &);  Construct a TCpuMatrix object by (deeply) copying from a TMatrixT<Double_t> matrix. ;  ;  TCpuMatrix (size_t nRows, size_t nCols);  Construct matrix and allocate space for its elements. ;  ;  TCpuMatrix (TCpuMatrix &&)=default;  ;  ~TCpuMatrix ()=default;  ; TCpuBuffer< AFloat > & GetBuffer ();  Returns pointer to a vector holding only ones with a guaranteed length of the number of columns of every instantiated CpuMatrix object. ;  ; const TCpuBuffer< AFloat > & GetBuffer () const;  ; TCpuBuffer< AFloat > & GetDeviceBuffer ();  ; const TCpuBuffer< AFloat > & GetDeviceBuffer () const;  ; size_t GetNcols () const;  ; size_t GetNoElements () const;  ; size_t GetNrows () const; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html:697,Usability,simpl,simplify,697,". ROOT: TMVA::DNN::TCpuMatrix< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCpuMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCpuMatrix< AFloat >The TCpuMatrix class. ; Matrix class for multi-threaded CPU architectures. Uses the TCpuBuffer class to store the matrices in column-major format for compatibility with BLAS. Provides Map and MapFrom member functions to simplify the application of activation functions and derivatives to matrices.; Copying and assignment of TCpuMatrix objects only performs shallow copies, i.e. copying is fast and the resulting objects share the element data.; Template Parameters. AFloatThe floating point type used to represent the matrix elements. . Definition at line 86 of file CpuMatrix.h. Public Member Functions;  TCpuMatrix ();  ;  TCpuMatrix (const TCpuBuffer< AFloat > &buffer, size_t m, size_t n);  Construct a m-times-n matrix from the given buffer. ;  ;  TCpuMatrix (const TCpuMatrix &)=default;  ;  TCpuMatrix (const TMatrixT< AFloat > &);  Construct a TCpuMatrix object by (deeply) copying from a TMatrixT<Double_t> matrix. ;  ;  TCpuMatrix (size_t nRows, size_t nCols);  Construct matrix and allocate space for its elements. ;  ;  TCpuMatrix (TCpuMatrix &&)=default;  ;  ~TCpuMatrix ()=default;  ; TCpuBuffer< AFloat > & GetBuffer ();  Returns pointer to a vector holding only ones with a guaranteed length of the number of columns of every instantiated CpuMatrix object. ;  ; const TCpuBuffer< AFloat > & GetBuffer () const;  ; TCpuBuffer< AFloat > & GetDeviceBuffer ();  ; const TCpuBuffer< AFloat > & GetDeviceBuffer () const;  ; size_t GetNcols () const;  ; size_t GetNoElements () const;  ; size_t GetNrows () const; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html:647,Modifiability,inherit,inherited,647,". ROOT: TMVA::DNN::TCpuTensor< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Friends |; List of all members ; TMVA::DNN::TCpuTensor< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCpuTensor< AFloat >; Definition at line 40 of file CpuTensor.h. Public Types; using Matrix_t = TCpuMatrix< AFloat >;  ; using MemoryLayout = TMVA::Experimental::MemoryLayout;  ; using Scalar_t = AFloat;  ; using Shape_t = typename TMVA::Experimental::RTensor< AFloat >::Shape_t;  ;  Public Types inherited from TMVA::Experimental::RTensor< AFloat, TCpuBuffer< AFloat > >; using Container_t = TCpuBuffer< AFloat >;  ; using Index_t = Shape_t;  ; using Shape_t = std::vector< std::size_t >;  ; using Slice_t = std::vector< Shape_t >;  ; using Value_t = AFloat;  . Public Member Functions;  TCpuTensor ();  ;  TCpuTensor (AFloat *data, const Shape_t &shape, MemoryLayout memlayout=MemoryLayout::ColumnMajor);  ;  TCpuTensor (const TCpuBuffer< AFloat > &buffer, Shape_t shape, MemoryLayout memlayout=MemoryLayout::ColumnMajor);  constructors from a TCpuBuffer and a shape ;  ;  TCpuTensor (const TCpuMatrix< AFloat > &matrix, size_t dim=3, MemoryLayout memlayout=MemoryLayout::ColumnMajor);  constructors from a TCpuMatrix. ;  ;  TCpuTensor (Shape_t shape, MemoryLayout memlayout=MemoryLayout::ColumnMajor);  constructors from a shape. ;  ;  TCpuTensor (size_t bsize, size_t depth, size_t height, size_t width, MemoryLayout memlayout=MemoryLayout::ColumnMajor);  constructors from batch size, depth, height, width ;  ;  TCpuTensor (size_t bsize, size_t depth, size_t hw, MemoryLayout memlayout=MemoryLayout::ColumnMajor);  constructors from batch size, depth, height*width ;  ;  TCpuTensor (size_t n, size_t m, MemoryLayout memlayout=MemoryLayout::ColumnMajor);  constructors from n m ;  ; TCpuTensor< AFloat > At (size_t i);  ; TCpuTensor< AFloat > At (",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html:3463,Modifiability,inherit,inherited,3463,"size_t GetNcols () const;  ; size_t GetNoElements () const;  ; size_t GetNrows () const;  ; AFloat * GetRawDataPointer ();  Return raw pointer to the elements stored contiguously in column-major order. ;  ; const AFloat * GetRawDataPointer () const;  ; size_t GetWSize () const;  ; template<typename Function_t > ; void Map (Function_t &f);  Map the given function over the matrix elements. ;  ; template<typename Function_t > ; void MapFrom (Function_t &f, const TCpuTensor< AFloat > &A);  Same as maps but takes the input values from the tensor A and writes the results in this tensor. ;  ;  operator TMatrixT () const;  Convert to a TMatrixT<AFloat_t> object. ;  ; AFloat & operator() (size_t i, size_t j);  ; AFloat operator() (size_t i, size_t j) const;  ; AFloat & operator() (size_t i, size_t j, size_t k);  ; AFloat operator() (size_t i, size_t j, size_t k) const;  ; TCpuMatrix< AFloat > operator[] (size_t i) const;  ; void Print (const char *name=""Tensor"") const;  ; void PrintShape (const char *name=""Tensor"") const;  ; TCpuTensor< AFloat > Reshape (Shape_t shape) const;  ; void Zero ();  ;  Public Member Functions inherited from TMVA::Experimental::RTensor< AFloat, TCpuBuffer< AFloat > >;  RTensor (Shape_t shape, MemoryLayout layout=MemoryLayout::RowMajor);  Construct a tensor owning data initialized with new container. ;  ;  RTensor (std::shared_ptr< Container_t > container, Shape_t shape, MemoryLayout layout=MemoryLayout::RowMajor);  Construct a tensor owning externally provided data. ;  ;  RTensor (Value_t *data, Shape_t shape, MemoryLayout layout=MemoryLayout::RowMajor);  Construct a tensor as view on data. ;  ;  RTensor (Value_t *data, Shape_t shape, Shape_t strides, MemoryLayout layout=MemoryLayout::RowMajor);  Construct a tensor as view on data. ;  ; Iterator begin () noexcept;  ; RTensor< Value_t, Container_t > Copy (MemoryLayout layout=MemoryLayout::RowMajor) const;  Copy RTensor to new object. ;  ; Iterator end () noexcept;  ; RTensor< Value_t, Container_t > ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html:5556,Modifiability,inherit,inherited,5556,"t;  ; const Shape_t & GetShape () const;  ; std::size_t GetSize () const;  ; const Shape_t & GetStrides () const;  ; bool IsOwner () const;  ; bool IsView () const;  ; Value_t & operator() (const Index_t &idx);  Access elements. ;  ; const Value_t & operator() (const Index_t &idx) const;  Access elements. ;  ; Value_t & operator() (Idx... idx);  Access elements. ;  ; const Value_t & operator() (Idx... idx) const;  Access elements. ;  ; RTensor< Value_t, Container_t > Reshape (const Shape_t &shape) const;  Reshape tensor. ;  ; RTensor< Value_t, Container_t > Resize (const Shape_t &shape);  Resize tensor. ;  ; RTensor< Value_t, Container_t > Slice (const Slice_t &slice);  Create a slice of the tensor. ;  ; RTensor< Value_t, Container_t > Squeeze () const;  Squeeze dimensions. ;  ; RTensor< Value_t, Container_t > Transpose () const;  Transpose. ;  . Friends; class TCpuMatrix< AFloat >;  . Additional Inherited Members;  Protected Member Functions inherited from TMVA::Experimental::RTensor< AFloat, TCpuBuffer< AFloat > >; void ReshapeInplace (const Shape_t &shape);  Reshape tensor in place. ;  . #include <TMVA/DNN/Architectures/Cpu/CpuTensor.h>. Inheritance diagram for TMVA::DNN::TCpuTensor< AFloat >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename AFloat > . using TMVA::DNN::TCpuTensor< AFloat >::Matrix_t = TCpuMatrix<AFloat>. Definition at line 49 of file CpuTensor.h. ◆ MemoryLayout. template<typename AFloat > . using TMVA::DNN::TCpuTensor< AFloat >::MemoryLayout = TMVA::Experimental::MemoryLayout. Definition at line 48 of file CpuTensor.h. ◆ Scalar_t. template<typename AFloat > . using TMVA::DNN::TCpuTensor< AFloat >::Scalar_t = AFloat. Definition at line 50 of file CpuTensor.h. ◆ Shape_t. template<typename AFloat > . using TMVA::DNN::TCpuTensor< AFloat >::Shape_t = typename TMVA::Experimental::RTensor<AFloat>::Shape_t. Definition at line 47 of file CpuTensor.h",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCpuTensor.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:22402,Availability,error,errors,22402,"atrix_t &C, const Matrix_t &A, const Matrix_t &B);  Standard multiplication of two matrices A and B with the result being written into C. ;  ; static void TransposeMultiply (Matrix_t &output, const Matrix_t &input, const Matrix_t &Weights, Scalar_t alpha=1.0, Scalar_t beta=0.);  Matrix multiplication of two matrices A and B^T (transposed) with the result being written into C. ;  ; static void Hadamard (Tensor_t &A, const Tensor_t &B);  In-place Hadamard (element-wise) product of matrices A and B with the result being written into A. ;  ; static void Hadamard (Matrix_t &A, const Matrix_t &B);  ; static void SumColumns (Matrix_t &B, const Matrix_t &A, Scalar_t alpha=1.0, Scalar_t beta=0.);  Sum columns of (m x n) matrix A and write the results into the first m elements in A. ;  ; static Scalar_t Sum (const Matrix_t &A);  Compute the sum of all elements in A. ;  ; static bool AlmostEquals (const Matrix_t &A, const Matrix_t &B, double epsilon=0.1);  Check two matrices for equality, taking floating point arithmetic errors into account. ;  ; static void ConstAdd (Matrix_t &A, Scalar_t beta);  Add the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConstMult (Matrix_t &A, Scalar_t beta);  Multiply the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ReciprocalElementWise (Matrix_t &A);  Reciprocal each element of the matrix A and write the result into A. ;  ; static void SquareElementWise (Matrix_t &A);  Square each element of the matrix A and write the result into A. ;  ; static void SqrtElementWise (Matrix_t &A);  Square root each element of the matrix A and write the result into A. ;  ; static void AdamUpdate (Matrix_t &A, const Matrix_t &M, const Matrix_t &V, Scalar_t alpha, Scalar_t eps);  Adam updates. ;  ; static void AdamUpdateFirstMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void AdamUpdateSecondMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:32057,Availability,error,errors,32057,"inition at line 43 of file Regularization.cu. ◆ AddL2RegularizationGradients(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::AddL2RegularizationGradients ; (; Matrix_t & ; A, . const Matrix_t & ; W, . Scalar_t ; weightDecay . ). static . Definition at line 76 of file Regularization.cu. ◆ AddRowWise() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::AddRowWise ; (; Matrix_t & ; output, . const Matrix_t & ; biases . ). static . Add the vectors biases row-wise to the matrix output. . ◆ AddRowWise() [2/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::AddRowWise ; (; Tensor_t & ; output, . const Matrix_t & ; biases . ). inlinestatic . Definition at line 217 of file Cuda.h. ◆ AlmostEquals(). template<typename AReal = Float_t> . bool TMVA::DNN::TCuda< AFloat >::AlmostEquals ; (; const Matrix_t & ; A, . const Matrix_t & ; B, . double ; epsilon = 0.1 . ). static . Check two matrices for equality, taking floating point arithmetic errors into account. ; Checks two matrices for element-wise equality.; Template Parameters. AFloatAn architecture-specific floating point number type. . Parameters. AThe first matrix. ; BThe second matrix. ; epsilonEquality tolerance, needed to address floating point arithmetic. . ReturnsWhether the two matrices can be considered equal element-wise ; Definition at line 291 of file Arithmetic.cu. ◆ Backward(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::Backward ; (; Tensor_t & ; activationGradientsBackward, . Matrix_t & ; weightGradients, . Matrix_t & ; biasGradients, . const Tensor_t & ; df, . const Tensor_t & ; activationGradients, . const Matrix_t & ; weights, . const Tensor_t & ; activationBackward . ). static . Perform the complete backward propagation step. ; If the provided activationGradientsBackward matrix is not empty, compute the gradients of the objective function with respect to the activations of the previous layer (backwar",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:32281,Availability,toler,tolerance,32281,"inition at line 76 of file Regularization.cu. ◆ AddRowWise() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::AddRowWise ; (; Matrix_t & ; output, . const Matrix_t & ; biases . ). static . Add the vectors biases row-wise to the matrix output. . ◆ AddRowWise() [2/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::AddRowWise ; (; Tensor_t & ; output, . const Matrix_t & ; biases . ). inlinestatic . Definition at line 217 of file Cuda.h. ◆ AlmostEquals(). template<typename AReal = Float_t> . bool TMVA::DNN::TCuda< AFloat >::AlmostEquals ; (; const Matrix_t & ; A, . const Matrix_t & ; B, . double ; epsilon = 0.1 . ). static . Check two matrices for equality, taking floating point arithmetic errors into account. ; Checks two matrices for element-wise equality.; Template Parameters. AFloatAn architecture-specific floating point number type. . Parameters. AThe first matrix. ; BThe second matrix. ; epsilonEquality tolerance, needed to address floating point arithmetic. . ReturnsWhether the two matrices can be considered equal element-wise ; Definition at line 291 of file Arithmetic.cu. ◆ Backward(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::Backward ; (; Tensor_t & ; activationGradientsBackward, . Matrix_t & ; weightGradients, . Matrix_t & ; biasGradients, . const Tensor_t & ; df, . const Tensor_t & ; activationGradients, . const Matrix_t & ; weights, . const Tensor_t & ; activationBackward . ). static . Perform the complete backward propagation step. ; If the provided activationGradientsBackward matrix is not empty, compute the gradients of the objective function with respect to the activations of the previous layer (backward direction). Also compute the weight and the bias gradients. Modifies the values in df and thus produces only a valid result, if it is applied the first time after the corresponding forward propagation has been per- formed. ; Definition at line 91 of file Propagat",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:23185,Deployability,update,updates,23185,"to the first m elements in A. ;  ; static Scalar_t Sum (const Matrix_t &A);  Compute the sum of all elements in A. ;  ; static bool AlmostEquals (const Matrix_t &A, const Matrix_t &B, double epsilon=0.1);  Check two matrices for equality, taking floating point arithmetic errors into account. ;  ; static void ConstAdd (Matrix_t &A, Scalar_t beta);  Add the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConstMult (Matrix_t &A, Scalar_t beta);  Multiply the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ReciprocalElementWise (Matrix_t &A);  Reciprocal each element of the matrix A and write the result into A. ;  ; static void SquareElementWise (Matrix_t &A);  Square each element of the matrix A and write the result into A. ;  ; static void SqrtElementWise (Matrix_t &A);  Square root each element of the matrix A and write the result into A. ;  ; static void AdamUpdate (Matrix_t &A, const Matrix_t &M, const Matrix_t &V, Scalar_t alpha, Scalar_t eps);  Adam updates. ;  ; static void AdamUpdateFirstMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void AdamUpdateSecondMom (Matrix_t &A, const Matrix_t &B, Scalar_t beta);  ; static void PrintTensor (const Tensor_t &A, const std::string name=""Cuda-tensor"", bool=false);  ; static void SumRows (Matrix_t &B, const Matrix_t &A);  extra functions defined only for CPU architecture !!! ;  . Static Private Attributes; static TRandom * fgRandomGen = nullptr;  . #include <TMVA/DNN/Architectures/Cuda.h>; Member Typedef Documentation. ◆ ActivationDescriptor_t. template<typename AReal = Float_t> . using TMVA::DNN::TCuda< AReal >::ActivationDescriptor_t = CudaActivationDescriptor. Definition at line 78 of file Cuda.h. ◆ AFloat. template<typename AReal = Float_t> . using TMVA::DNN::TCuda< AReal >::AFloat = AReal. Definition at line 70 of file Cuda.h. ◆ AlgorithmBackward_t. template<typename AReal = Float_t> . using TMVA::DNN::TCuda< AReal >",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:30019,Deployability,update,updates,30019,":TCuda< AFloat >::ActivationFunctionBackward ; (; Tensor_t & ; dX, . const Tensor_t & ; Y, . const Tensor_t & ; dY, . const Tensor_t & ; X, . EActivationFunction ; activFunct, . const ActivationDescriptor_t ; activationDescr, . const AFloat ; alpha = 1, . const AFloat ; beta = 0 . ). static . Computes the gradient of the activation function. ; Definition at line 37 of file ActivationFunctions.cu. ◆ ActivationFunctionForward(). template<typename AFloat > . void TMVA::DNN::TCuda< AFloat >::ActivationFunctionForward ; (; Tensor_t & ; X, . EActivationFunction ; activFunct, . const ActivationDescriptor_t ; activationDescr, . const double ; coef = 0.0, . const AFloat ; alpha = 1, . const AFloat ; beta = 0 . ). static . Definition at line 28 of file ActivationFunctions.cu. ◆ AdamUpdate(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::AdamUpdate ; (; Matrix_t & ; A, . const Matrix_t & ; M, . const Matrix_t & ; V, . Scalar_t ; alpha, . Scalar_t ; eps . ). static . Adam updates. ; Definition at line 425 of file Arithmetic.cu. ◆ AdamUpdateFirstMom(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::AdamUpdateFirstMom ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta . ). static . Definition at line 441 of file Arithmetic.cu. ◆ AdamUpdateSecondMom(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::AdamUpdateSecondMom ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta . ). static . Definition at line 455 of file Arithmetic.cu. ◆ AddConvBiases(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::AddConvBiases ; (; Matrix_t & ; output, . const Matrix_t & ; biases . ). static . Add the biases in the Convolutional Layer. . Definition at line 432 of file Propagation.cu. ◆ AddL1RegularizationGradients(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::AddL1RegularizationGradients ; (; Matrix_t & ; A, . const Matrix_t & ; W, . Scalar_t ; weightDecay . ).",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:66970,Deployability,update,update,66970,"TMVA::DNN::TCuda< AReal >::RNNForward ; (; const Tensor_t & ; , . const Matrix_t & ; , . const Matrix_t & ; , . const Tensor_t & ; , . Tensor_t & ; , . Matrix_t & ; , . Matrix_t & ; , . const RNNDescriptors_t & ; , . RNNWorkspace_t & ; , . bool ;  . ). inlinestatic . Definition at line 687 of file Cuda.h. ◆ RotateWeights(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::RotateWeights ; (; Matrix_t & ; A, . const Matrix_t & ; B, . size_t ; filterDepth, . size_t ; filterHeight, . size_t ; filterWidth, . size_t ; numFilters . ). static . Rotates the matrix B, which is representing a weights, and stores them in the matrix A. ; Definition at line 207 of file Propagation.cu. ◆ ScaleAdd() [1/4]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::ScaleAdd ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Adds a the elements in matrix B scaled by c to the elements in the matrix A. ; This is required for the weight update in the gradient descent step. . ◆ ScaleAdd() [2/4]. void TMVA::DNN::TCuda< double >::ScaleAdd ; (; TCudaMatrix< double > & ; B, . const TCudaMatrix< double > & ; A, . double ; alpha . ). Definition at line 330 of file Arithmetic.cu. ◆ ScaleAdd() [3/4]. void TMVA::DNN::TCuda< float >::ScaleAdd ; (; TCudaMatrix< float > & ; B, . const TCudaMatrix< float > & ; A, . float ; alpha . ). Definition at line 317 of file Arithmetic.cu. ◆ ScaleAdd() [4/4]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::ScaleAdd ; (; Tensor_t & ; A, . const Tensor_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Above functions extended to vectors. . ◆ SetRandomSeed(). template<typename AFloat > . void TMVA::DNN::TCuda< AFloat >::SetRandomSeed ; (; size_t ; seed). static . Definition at line 31 of file Initialization.cu. ◆ Sigmoid() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::Sigmoid ; (; Matrix_t & ; YHat, . const Matrix_t & ;  . ). static . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:455,Integrability,interface,interface,455,". ROOT: TMVA::DNN::TCuda< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TCuda< AReal > Class Template Reference. ; template<typename AReal = Float_t>; class TMVA::DNN::TCuda< AReal >The TCuda architecture class. ; Low-level interface class for CUDA computing architectures. Contains as public types the declaration of the scalar, matrix and buffer types for this architecture as well as the remaining functions in the low-level interface in the form of static members. ; Definition at line 64 of file Cuda.h. Public Types; using ActivationDescriptor_t = CudaActivationDescriptor;  ; using AFloat = AReal;  ; using AlgorithmBackward_t = CudaConvolutionBwdDataAlgo;  ; using AlgorithmDataType_t = DummyCudaDataType;  ; using AlgorithmForward_t = CudaConvolutionFwdAlgo;  ; using AlgorithmHelper_t = CudaConvolutionBwdFilterAlgo;  ; using BNormDescriptors_t = TDNNGenDescriptors< BNormLayer_t >;  ; using BNormLayer_t = TBatchNormLayer< TCuda< AReal > >;  ; using ConvDescriptors_t = CNN::TCNNDescriptors< ConvLayer_t >;  ; using ConvLayer_t = CNN::TConvLayer< TCuda< AReal > >;  ; using ConvolutionDescriptor_t = CudaConvolutionDescriptor;  ; using ConvWorkspace_t = CNN::TCNNWorkspace< ConvLayer_t >;  ; using DeviceBuffer_t = TCudaDeviceBuffer< AFloat >;  ; using DropoutDescriptor_t = CudaDropoutDescriptor;  ; using EmptyDescriptor_t = CudaEmptyDescriptor;  ; using FilterDescriptor_t = CudaFilterDescriptor;  ; using GenLayer_t = VGeneralLayer< TCuda< AReal > >;  ; using HostBuffer_t = TCudaHostBuffer< AFloat >;  ; using Matrix_t = TCudaMatrix< AFloat >;  ; using PoolingDescriptor_t = CudaPoolingDescriptor;  ; using PoolingDescriptors_t = CNN::TCNNDescriptors< PoolingLayer_t >;  ; using PoolingLayer_t = CNN::TMaxPoolLayer< TCuda< AReal > >;  ; using PoolingWork",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:659,Integrability,interface,interface,659,". ROOT: TMVA::DNN::TCuda< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TCuda< AReal > Class Template Reference. ; template<typename AReal = Float_t>; class TMVA::DNN::TCuda< AReal >The TCuda architecture class. ; Low-level interface class for CUDA computing architectures. Contains as public types the declaration of the scalar, matrix and buffer types for this architecture as well as the remaining functions in the low-level interface in the form of static members. ; Definition at line 64 of file Cuda.h. Public Types; using ActivationDescriptor_t = CudaActivationDescriptor;  ; using AFloat = AReal;  ; using AlgorithmBackward_t = CudaConvolutionBwdDataAlgo;  ; using AlgorithmDataType_t = DummyCudaDataType;  ; using AlgorithmForward_t = CudaConvolutionFwdAlgo;  ; using AlgorithmHelper_t = CudaConvolutionBwdFilterAlgo;  ; using BNormDescriptors_t = TDNNGenDescriptors< BNormLayer_t >;  ; using BNormLayer_t = TBatchNormLayer< TCuda< AReal > >;  ; using ConvDescriptors_t = CNN::TCNNDescriptors< ConvLayer_t >;  ; using ConvLayer_t = CNN::TConvLayer< TCuda< AReal > >;  ; using ConvolutionDescriptor_t = CudaConvolutionDescriptor;  ; using ConvWorkspace_t = CNN::TCNNWorkspace< ConvLayer_t >;  ; using DeviceBuffer_t = TCudaDeviceBuffer< AFloat >;  ; using DropoutDescriptor_t = CudaDropoutDescriptor;  ; using EmptyDescriptor_t = CudaEmptyDescriptor;  ; using FilterDescriptor_t = CudaFilterDescriptor;  ; using GenLayer_t = VGeneralLayer< TCuda< AReal > >;  ; using HostBuffer_t = TCudaHostBuffer< AFloat >;  ; using Matrix_t = TCudaMatrix< AFloat >;  ; using PoolingDescriptor_t = CudaPoolingDescriptor;  ; using PoolingDescriptors_t = CNN::TCNNDescriptors< PoolingLayer_t >;  ; using PoolingLayer_t = CNN::TMaxPoolLayer< TCuda< AReal > >;  ; using PoolingWork",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:8581,Integrability,interface,interface,8581,"nse Layers); Low-level functions required for the forward propagation of activations through the network. . static void Backward (Tensor_t &activationGradientsBackward, Matrix_t &weightGradients, Matrix_t &biasGradients, const Tensor_t &df, const Tensor_t &activationGradients, const Matrix_t &weights, const Tensor_t &activationBackward);  Perform the complete backward propagation step. ;  ; static void ScaleAdd (Matrix_t &A, const Matrix_t &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (Matrix_t &B, const Matrix_t &A);  ; template<typename AMatrix_t > ; static void CopyDiffArch (Matrix_t &B, const AMatrix_t &A);  ; static void ScaleAdd (Tensor_t &A, const Tensor_t &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (Tensor_t &A, const Tensor_t &B);  ; template<typename ATensor_t > ; static void CopyDiffArch (Tensor_t &A, const ATensor_t &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< Matrix_t > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const AFloat alpha=1, const AFloat beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const AFloat alpha=1, const AFloat beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static void Rel",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:8604,Integrability,rout,routines,8604,"nse Layers); Low-level functions required for the forward propagation of activations through the network. . static void Backward (Tensor_t &activationGradientsBackward, Matrix_t &weightGradients, Matrix_t &biasGradients, const Tensor_t &df, const Tensor_t &activationGradients, const Matrix_t &weights, const Tensor_t &activationBackward);  Perform the complete backward propagation step. ;  ; static void ScaleAdd (Matrix_t &A, const Matrix_t &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (Matrix_t &B, const Matrix_t &A);  ; template<typename AMatrix_t > ; static void CopyDiffArch (Matrix_t &B, const AMatrix_t &A);  ; static void ScaleAdd (Tensor_t &A, const Tensor_t &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (Tensor_t &A, const Tensor_t &B);  ; template<typename ATensor_t > ; static void CopyDiffArch (Tensor_t &A, const ATensor_t &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< Matrix_t > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const AFloat alpha=1, const AFloat beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const AFloat alpha=1, const AFloat beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static void Rel",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:10333,Integrability,rout,routing,10333,"id Relu (Tensor_t &B);  ; static void ReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Sigmoid (Tensor_t &B);  ; static void SigmoidDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Tanh (Tensor_t &B);  ; static void TanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (Tensor_t &B);  ; static void SymmetricReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SoftSign (Tensor_t &B);  ; static void SoftSignDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Gauss (Tensor_t &B);  ; static void GaussDerivative (Tensor_t &B, const Tensor_t &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void CrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t SoftmaxCrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Softmax transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void SoftmaxCrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:12502,Integrability,interface,interface,12502,"ts (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; Output Functions; Output functions transform the activations output of the output layer in the network to a valid prediction YHat for the desired usage of the network, e.g.; the identity function for regression or the sigmoid transformation for two-class classification. . static void Sigmoid (Matrix_t &YHat, const Matrix_t &);  ; static void Softmax (Matrix_t &YHat, const Matrix_t &);  ; Regularization; For each regularization type two functions are required, one named <Type>Regularization that evaluates the corresponding regularization functional for a given weight matrix and the Add<Type>RegularizationGradients, that adds the regularization component in the gradients to the provided matrix. . static Scalar_t L1Regularization (const Matrix_t &W);  ; static void AddL1RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; static Scalar_t L2Regularization (const Matrix_t &W);  ; static void AddL2RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; Initialization; For each initialization method, one function in the low-level interface is provided.; The naming scheme is ; Initialize<Type>; for a given initialization method Type. . static void InitializeGauss (Matrix_t &A);  ; static void InitializeUniform (Matrix_t &A);  ; static void InitializeIdentity (Matrix_t &A);  ; static void InitializeZero (Matrix_t &A);  ; static void InitializeZero (Tensor_t &A);  ; static void InitializeGlorotNormal (Matrix_t &A);  Truncated normal initialization (Glorot, called also Xavier normal) The values are sample with a normal distribution with stddev = sqrt(2/N_input + N_output) and values larger than 2 * stddev are discarded See Glorot & Bengio, AISTATS 2010 - http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf. ;  ; static void InitializeGlorotUniform (Matrix_t &A);  Sample from a uniform distribution in range [ -lim,+lim] where",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:21340,Integrability,interface,interface,21340,"trix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &);  ; static Matrix_t & GRULayerBackward (Matrix_t &state_gradients_backward, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, const Matrix_t &, Matrix_t &, bool);  Backward pass for GRU Network. ;  ; Additional Arithmetic Functions; Additional arithmetic on CUDA matrices used to implement the low-level interface. . static void Multiply (Matrix_t &C, const Matrix_t &A, const Matrix_t &B);  Standard multiplication of two matrices A and B with the result being written into C. ;  ; static void TransposeMultiply (Matrix_t &output, const Matrix_t &input, const Matrix_t &Weights, Scalar_t alpha=1.0, Scalar_t beta=0.);  Matrix multiplication of two matrices A and B^T (transposed) with the result being written into C. ;  ; static void Hadamard (Tensor_t &A, const Tensor_t &B);  In-place Hadamard (element-wise) product of matrices A and B with the result being written into A. ;  ; static void Hadamard (Matrix_t &A, const Matrix_t &B);  ; static void SumColumns (Matrix_t &B, const Matrix_t &A, Scalar_t alpha=1.0, Scalar_t beta=0.);  Sum columns of (m x n) matrix A and write the results into the first m elements in A. ;  ; static Scalar_t Sum (const Matrix_t &A);  Compute the sum of all elements in A. ;  ; static bool AlmostEquals (const Matrix_t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:8215,Modifiability,extend,extended,8215," AddRowWise (Matrix_t &output, const Matrix_t &biases);  Add the vectors biases row-wise to the matrix output. ;  ; static void AddRowWise (Tensor_t &output, const Matrix_t &biases);  ; Backward Propagation (Dense Layers); Low-level functions required for the forward propagation of activations through the network. . static void Backward (Tensor_t &activationGradientsBackward, Matrix_t &weightGradients, Matrix_t &biasGradients, const Tensor_t &df, const Tensor_t &activationGradients, const Matrix_t &weights, const Tensor_t &activationBackward);  Perform the complete backward propagation step. ;  ; static void ScaleAdd (Matrix_t &A, const Matrix_t &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (Matrix_t &B, const Matrix_t &A);  ; template<typename AMatrix_t > ; static void CopyDiffArch (Matrix_t &B, const AMatrix_t &A);  ; static void ScaleAdd (Tensor_t &A, const Tensor_t &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (Tensor_t &A, const Tensor_t &B);  ; template<typename ATensor_t > ; static void CopyDiffArch (Tensor_t &A, const ATensor_t &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< Matrix_t > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const AFloat alpha=1, const AFloat beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const AFl",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:14316,Modifiability,variab,variable,14316,"/v9/glorot10a/glorot10a.pdf. ;  ; static void InitializeGlorotUniform (Matrix_t &A);  Sample from a uniform distribution in range [ -lim,+lim] where lim = sqrt(6/N_in+N_out). ;  ; static TRandom & GetRandomGenerator ();  ; static void SetRandomSeed (size_t seed);  ; Dropout; static void DropoutForward (Tensor_t &A, TDescriptors *descriptors, TWorkspace *workspace, Scalar_t p);  Apply dropout with activation probability p to the given tensor A and scale the result by reciprocal of p. ;  ; static void DropoutForward (Matrix_t &A, Scalar_t p);  ; static void DropoutBackward (Tensor_t &, TDescriptors *, TWorkspace *);  ; Batch Normalization Layer Propagation; static void BatchNormLayerForwardTraining (int axis, const Tensor_t &x, Tensor_t &y, Matrix_t &gamma, Matrix_t &beta, Matrix_t &mean, Matrix_t &, Matrix_t &iVariance, Matrix_t &runningMeans, Matrix_t &runningVars, Scalar_t nTrainedBatches, Scalar_t momentum, Scalar_t epsilon, const TensorDescriptor_t &bnParDescriptor);  The input from each batch are normalized during training to have zero mean and unit variance and they are then scaled by two parameter, different for each input variable: ;  ; static void BatchNormLayerForwardInference (int axis, const Tensor_t &x, Matrix_t &gamma, Matrix_t &beta, Tensor_t &y, const Matrix_t &runningMeans, const Matrix_t &runningVars, Scalar_t epsilon, const TensorDescriptor_t &);  During inference the inputs are not normalized using the batch mean but the previously computed at running mean and variance. ;  ; static void BatchNormLayerBackward (int axis, const Tensor_t &x, const Tensor_t &dy, Tensor_t &dx, Matrix_t &gamma, Matrix_t &dgamma, Matrix_t &dbeta, const Matrix_t &mean, const Matrix_t &variance, const Matrix_t &iVariance, Scalar_t epsilon, const TensorDescriptor_t &);  ; Forward Propagation in Convolutional Layer; static size_t calculateDimension (size_t imgDim, size_t fltDim, size_t padding, size_t stride);  Calculate how many neurons ""fit"" in the output layer, given the ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:34967,Modifiability,variab,variable,34967,"rwardInference ; (; int ; axis, . const Tensor_t & ; x, . Matrix_t & ; gamma, . Matrix_t & ; beta, . Tensor_t & ; y, . const Matrix_t & ; runningMeans, . const Matrix_t & ; runningVars, . Scalar_t ; epsilon, . const TensorDescriptor_t & ;  . ). static . During inference the inputs are not normalized using the batch mean but the previously computed at running mean and variance. ; Definition at line 743 of file Propagation.cu. ◆ BatchNormLayerForwardTraining(). template<typename AFloat > . void TMVA::DNN::TCuda< AFloat >::BatchNormLayerForwardTraining ; (; int ; axis, . const Tensor_t & ; x, . Tensor_t & ; y, . Matrix_t & ; gamma, . Matrix_t & ; beta, . Matrix_t & ; mean, . Matrix_t & ; , . Matrix_t & ; iVariance, . Matrix_t & ; runningMeans, . Matrix_t & ; runningVars, . Scalar_t ; nTrainedBatches, . Scalar_t ; momentum, . Scalar_t ; epsilon, . const TensorDescriptor_t & ; bnParDescriptor . ). static . The input from each batch are normalized during training to have zero mean and unit variance and they are then scaled by two parameter, different for each input variable: . a scale factor \gamma gamma; an offset \beta beta . Definition at line 729 of file Propagation.cu. ◆ CalculateConvActivationGradients(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::CalculateConvActivationGradients ; (; Tensor_t & ; activationGradientsBackward, . const Tensor_t & ; df, . const Matrix_t & ; weights, . size_t ; batchSize, . size_t ; inputHeight, . size_t ; inputWidth, . size_t ; depth, . size_t ; height, . size_t ; width, . size_t ; filterDepth, . size_t ; filterHeight, . size_t ; filterWidth . ). static . Utility function for calculating the activation gradients of the layer before the convolutional layer. ; Definition at line 324 of file Propagation.cu. ◆ CalculateConvBiasGradients(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::CalculateConvBiasGradients ; (; Matrix_t & ; biasGradients, . const Tensor_t & ; df, . size_t ; batchSize, ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:67611,Modifiability,extend,extended,67611,"matrix A. ; Definition at line 207 of file Propagation.cu. ◆ ScaleAdd() [1/4]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::ScaleAdd ; (; Matrix_t & ; A, . const Matrix_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Adds a the elements in matrix B scaled by c to the elements in the matrix A. ; This is required for the weight update in the gradient descent step. . ◆ ScaleAdd() [2/4]. void TMVA::DNN::TCuda< double >::ScaleAdd ; (; TCudaMatrix< double > & ; B, . const TCudaMatrix< double > & ; A, . double ; alpha . ). Definition at line 330 of file Arithmetic.cu. ◆ ScaleAdd() [3/4]. void TMVA::DNN::TCuda< float >::ScaleAdd ; (; TCudaMatrix< float > & ; B, . const TCudaMatrix< float > & ; A, . float ; alpha . ). Definition at line 317 of file Arithmetic.cu. ◆ ScaleAdd() [4/4]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::ScaleAdd ; (; Tensor_t & ; A, . const Tensor_t & ; B, . Scalar_t ; beta = 1.0 . ). static . Above functions extended to vectors. . ◆ SetRandomSeed(). template<typename AFloat > . void TMVA::DNN::TCuda< AFloat >::SetRandomSeed ; (; size_t ; seed). static . Definition at line 31 of file Initialization.cu. ◆ Sigmoid() [1/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::Sigmoid ; (; Matrix_t & ; YHat, . const Matrix_t & ;  . ). static . ◆ Sigmoid() [2/2]. template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::Sigmoid ; (; Tensor_t & ; B). static . ◆ SigmoidDerivative(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::SigmoidDerivative ; (; Tensor_t & ; B, . const Tensor_t & ; A . ). static . Definition at line 111 of file ActivationFunctions.cu. ◆ Softmax(). template<typename AReal = Float_t> . void TMVA::DNN::TCuda< AFloat >::Softmax ; (; Matrix_t & ; YHat, . const Matrix_t & ;  . ). static . Definition at line 42 of file OutputFunctions.cu. ◆ SoftmaxCrossEntropy(). template<typename AReal = Float_t> . AFloat TMVA::DNN::TCu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:51187,Performance,tune,tuned,51187,", . size_t ; fltWidth, . size_t ; strideRows, . size_t ; strideCols, . size_t ; zeroPaddingHeight, . size_t ; zeroPaddingWidth . ). static . Transform the matrix B in local view format, suitable for convolution, and store it in matrix A. ; A helper for image operations that rearranges image regions into column vectors.; Parameters. [out]AThe output matrix. Each row corresponds to a receptive field. ; [in]BThe input matrix. Each row corresponds to a row in the image view. ; [in]imgHeightThe heigh of the input. ; [in]imgWidthThe output of the input. ; [in]fltHeightHeight of the kernel. ; [in]fltWidthWidth of the kernel. ; [in]strideRowsstride size in the horizontal dimension. ; [in]strideColsstride size in the vertical dimension. ; [in]zeroPaddingHeightThe padding in the horizontal dimension. ; [in]zeroPaddingWidthThe padding in the vertical dimension. This transformation allows us to express a 2D convolution as a matrix multiplication. We can therefore harness the finely tuned GEMM implementation of cuBLAS to achieve maximum performance. This function can greatly speed-up propagation in TConvLayer. ; Definition at line 183 of file Propagation.cu. ◆ Im2colFast(). template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::Im2colFast ; (; Matrix_t & ; A, . const Matrix_t & ; B, . const std::vector< int > & ; V . ). static . ◆ Im2colIndices(). template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::Im2colIndices ; (; std::vector< int > & ; V, . const Matrix_t & ; B, . size_t ; nLocalViews, . size_t ; imgHeight, . size_t ; imgWidth, . size_t ; fltHeight, . size_t ; fltWidth, . size_t ; strideRows, . size_t ; strideCols, . size_t ; zeroPaddingHeight, . size_t ; zeroPaddingWidth . ). static . ◆ InitializeActivationDescriptor(). template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::InitializeActivationDescriptor ; (; ActivationDescriptor_t & ; , . EActivationFunction ; , . double ; = 0.0 . ). inlinestatic . Definitio",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:51242,Performance,perform,performance,51242,", . size_t ; fltWidth, . size_t ; strideRows, . size_t ; strideCols, . size_t ; zeroPaddingHeight, . size_t ; zeroPaddingWidth . ). static . Transform the matrix B in local view format, suitable for convolution, and store it in matrix A. ; A helper for image operations that rearranges image regions into column vectors.; Parameters. [out]AThe output matrix. Each row corresponds to a receptive field. ; [in]BThe input matrix. Each row corresponds to a row in the image view. ; [in]imgHeightThe heigh of the input. ; [in]imgWidthThe output of the input. ; [in]fltHeightHeight of the kernel. ; [in]fltWidthWidth of the kernel. ; [in]strideRowsstride size in the horizontal dimension. ; [in]strideColsstride size in the vertical dimension. ; [in]zeroPaddingHeightThe padding in the horizontal dimension. ; [in]zeroPaddingWidthThe padding in the vertical dimension. This transformation allows us to express a 2D convolution as a matrix multiplication. We can therefore harness the finely tuned GEMM implementation of cuBLAS to achieve maximum performance. This function can greatly speed-up propagation in TConvLayer. ; Definition at line 183 of file Propagation.cu. ◆ Im2colFast(). template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::Im2colFast ; (; Matrix_t & ; A, . const Matrix_t & ; B, . const std::vector< int > & ; V . ). static . ◆ Im2colIndices(). template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::Im2colIndices ; (; std::vector< int > & ; V, . const Matrix_t & ; B, . size_t ; nLocalViews, . size_t ; imgHeight, . size_t ; imgWidth, . size_t ; fltHeight, . size_t ; fltWidth, . size_t ; strideRows, . size_t ; strideCols, . size_t ; zeroPaddingHeight, . size_t ; zeroPaddingWidth . ). static . ◆ InitializeActivationDescriptor(). template<typename AReal = Float_t> . static void TMVA::DNN::TCuda< AReal >::InitializeActivationDescriptor ; (; ActivationDescriptor_t & ; , . EActivationFunction ; , . double ; = 0.0 . ). inlinestatic . Definitio",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:10247,Safety,predict,prediction,10247,"lt matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const AFloat alpha=1, const AFloat beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const AFloat alpha=1, const AFloat beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static void ReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Sigmoid (Tensor_t &B);  ; static void SigmoidDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Tanh (Tensor_t &B);  ; static void TanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (Tensor_t &B);  ; static void SymmetricReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SoftSign (Tensor_t &B);  ; static void SoftSignDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Gauss (Tensor_t &B);  ; static void GaussDerivative (Tensor_t &B, const Tensor_t &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:10295,Safety,predict,prediction,10295,"lt matrix. . static void ActivationFunctionForward (Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const double coef=0.0, const AFloat alpha=1, const AFloat beta=0);  ; static void ActivationFunctionBackward (Tensor_t &dX, const Tensor_t &Y, const Tensor_t &dY, const Tensor_t &X, EActivationFunction activFunct, const ActivationDescriptor_t activationDescr, const AFloat alpha=1, const AFloat beta=0);  Computes the gradient of the activation function. ;  ; static void IdentityDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Relu (Tensor_t &B);  ; static void ReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Sigmoid (Tensor_t &B);  ; static void SigmoidDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Tanh (Tensor_t &B);  ; static void TanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (Tensor_t &B);  ; static void SymmetricReluDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SoftSign (Tensor_t &B);  ; static void SoftSignDerivative (Tensor_t &B, const Tensor_t &A);  ; static void Gauss (Tensor_t &B);  ; static void GaussDerivative (Tensor_t &B, const Tensor_t &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html:11520,Safety,predict,prediction,11520,"ng of the backpropagation algorithm. . static Scalar_t MeanSquaredError (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static void MeanSquaredErrorGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t CrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void CrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; static Scalar_t SoftmaxCrossEntropy (const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  Softmax transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void SoftmaxCrossEntropyGradients (Matrix_t &dY, const Matrix_t &Y, const Matrix_t &output, const Matrix_t &weights);  ; Output Functions; Output functions transform the activations output of the output layer in the network to a valid prediction YHat for the desired usage of the network, e.g.; the identity function for regression or the sigmoid transformation for two-class classification. . static void Sigmoid (Matrix_t &YHat, const Matrix_t &);  ; static void Softmax (Matrix_t &YHat, const Matrix_t &);  ; Regularization; For each regularization type two functions are required, one named <Type>Regularization that evaluates the corresponding regularization functional for a given weight matrix and the Add<Type>RegularizationGradients, that adds the regularization component in the gradients to the provided matrix. . static Scalar_t L1Regularization (const Matrix_t &W);  ; static void AddL1RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; static Scalar_t L2Regularization (const Matrix_t &W);  ; static void AddL2RegularizationGradients (Matrix_t &A, const Matrix_t &W, Scalar_t weightDecay);  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCuda.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCuda.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceBuffer.html:667,Integrability,synchroniz,synchronization,667,". ROOT: TMVA::DNN::TCudaDeviceBuffer< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCudaDeviceBuffer< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaDeviceBuffer< AFloat >TCudaDeviceBuffer. ; Service class for on-device memory buffers. Uses std::shared_pointer with custom destructor to ensure consistent memory management and allow for easy copying/moving. A device buffer has an associated CUDA compute stream , which is used for implicit synchronization of data transfers.; Template Parameters. AFloatThe floating point type to be stored in the buffers. . Definition at line 100 of file CudaBuffers.h. Classes; struct  TDestructor;  . Public Member Functions;  TCudaDeviceBuffer ()=default;  ;  TCudaDeviceBuffer (AFloat *, size_t size, cudaStream_t stream);  ;  TCudaDeviceBuffer (const TCudaDeviceBuffer &)=default;  ;  TCudaDeviceBuffer (size_t size);  ;  TCudaDeviceBuffer (size_t size, cudaStream_t stream);  ;  TCudaDeviceBuffer (TCudaDeviceBuffer &&)=default;  ; void CopyFrom (const TCudaHostBuffer< AFloat > &) const;  ; void CopyTo (const TCudaHostBuffer< AFloat > &) const;  ; AFloat * data () const;  ; cudaStream_t GetComputeStream () const;  ; size_t GetSize () const;  ; TCudaDeviceBuffer GetSubBuffer (size_t offset, size_t size);  Return sub-buffer of the current buffer. ;  ; size_t GetUseCount () const;  ;  operator AFloat * () const;  Convert to raw device data pointer. ;  ; TCudaDeviceBuffer & operator= (const TCudaDeviceBuffer &)=default;  ; TCudaDeviceBuffer & operator= (TCudaDeviceBuffer &&)=default;  ; void SetComputeStream (cudaStream_t stream);  . Private Attributes; cudaStream_t fComputeStream;  cudaStream for data transfer ;  ; struct TMVA::DNN::TCudaDeviceBuffer::TDestructor fDestructor;  ; std::shared_ptr< AFloat * > fDevicePointer;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaDeviceBuffer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceBuffer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html:624,Integrability,synchroniz,synchronization,624,. ROOT: TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaDeviceReference< AFloat >TCudaDeviceReference. ; Helper class emulating lvalue references for AFloat values that are physically on the device. Allows for example to assign to matrix elements. Note that device access through CudaDeviceReferences enforces synchronization with all streams and thus qualifies as performance killer. Only used for testing. ; Definition at line 58 of file CudaMatrix.h. Public Member Functions;  TCudaDeviceReference (AFloat *devicePointer);  ;  operator AFloat ();  ; void operator+= (AFloat value);  ; void operator-= (AFloat value);  ; void operator= (AFloat value);  ; void operator= (const TCudaDeviceReference &other);  . Private Attributes; AFloat * fDevicePointer;  . #include <TMVA/DNN/Architectures/Cuda/CudaMatrix.h>; Constructor & Destructor Documentation. ◆ TCudaDeviceReference(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::TCudaDeviceReference ; (; AFloat * ; devicePointer). Definition at line 209 of file CudaMatrix.h. Member Function Documentation. ◆ operator AFloat(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::operator AFloat. Definition at line 217 of file CudaMatrix.h. ◆ operator+=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator+= ; (; AFloat ; value). Definition at line 244 of file CudaMatrix.h. ◆ operator-=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator-= ; (; AFloat ; value). Definition at line 256 of file CudaMatrix.h. ◆ operator=() [1/2]. template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator= ; ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html:679,Performance,perform,performance,679,. ROOT: TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaDeviceReference< AFloat >TCudaDeviceReference. ; Helper class emulating lvalue references for AFloat values that are physically on the device. Allows for example to assign to matrix elements. Note that device access through CudaDeviceReferences enforces synchronization with all streams and thus qualifies as performance killer. Only used for testing. ; Definition at line 58 of file CudaMatrix.h. Public Member Functions;  TCudaDeviceReference (AFloat *devicePointer);  ;  operator AFloat ();  ; void operator+= (AFloat value);  ; void operator-= (AFloat value);  ; void operator= (AFloat value);  ; void operator= (const TCudaDeviceReference &other);  . Private Attributes; AFloat * fDevicePointer;  . #include <TMVA/DNN/Architectures/Cuda/CudaMatrix.h>; Constructor & Destructor Documentation. ◆ TCudaDeviceReference(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::TCudaDeviceReference ; (; AFloat * ; devicePointer). Definition at line 209 of file CudaMatrix.h. Member Function Documentation. ◆ operator AFloat(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::operator AFloat. Definition at line 217 of file CudaMatrix.h. ◆ operator+=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator+= ; (; AFloat ; value). Definition at line 244 of file CudaMatrix.h. ◆ operator-=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator-= ; (; AFloat ; value). Definition at line 256 of file CudaMatrix.h. ◆ operator=() [1/2]. template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator= ; ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html:579,Security,access,access,579,. ROOT: TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaDeviceReference< AFloat >TCudaDeviceReference. ; Helper class emulating lvalue references for AFloat values that are physically on the device. Allows for example to assign to matrix elements. Note that device access through CudaDeviceReferences enforces synchronization with all streams and thus qualifies as performance killer. Only used for testing. ; Definition at line 58 of file CudaMatrix.h. Public Member Functions;  TCudaDeviceReference (AFloat *devicePointer);  ;  operator AFloat ();  ; void operator+= (AFloat value);  ; void operator-= (AFloat value);  ; void operator= (AFloat value);  ; void operator= (const TCudaDeviceReference &other);  . Private Attributes; AFloat * fDevicePointer;  . #include <TMVA/DNN/Architectures/Cuda/CudaMatrix.h>; Constructor & Destructor Documentation. ◆ TCudaDeviceReference(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::TCudaDeviceReference ; (; AFloat * ; devicePointer). Definition at line 209 of file CudaMatrix.h. Member Function Documentation. ◆ operator AFloat(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::operator AFloat. Definition at line 217 of file CudaMatrix.h. ◆ operator+=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator+= ; (; AFloat ; value). Definition at line 244 of file CudaMatrix.h. ◆ operator-=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator-= ; (; AFloat ; value). Definition at line 256 of file CudaMatrix.h. ◆ operator=() [1/2]. template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator= ; ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html:713,Testability,test,testing,713,. ROOT: TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCudaDeviceReference< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaDeviceReference< AFloat >TCudaDeviceReference. ; Helper class emulating lvalue references for AFloat values that are physically on the device. Allows for example to assign to matrix elements. Note that device access through CudaDeviceReferences enforces synchronization with all streams and thus qualifies as performance killer. Only used for testing. ; Definition at line 58 of file CudaMatrix.h. Public Member Functions;  TCudaDeviceReference (AFloat *devicePointer);  ;  operator AFloat ();  ; void operator+= (AFloat value);  ; void operator-= (AFloat value);  ; void operator= (AFloat value);  ; void operator= (const TCudaDeviceReference &other);  . Private Attributes; AFloat * fDevicePointer;  . #include <TMVA/DNN/Architectures/Cuda/CudaMatrix.h>; Constructor & Destructor Documentation. ◆ TCudaDeviceReference(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::TCudaDeviceReference ; (; AFloat * ; devicePointer). Definition at line 209 of file CudaMatrix.h. Member Function Documentation. ◆ operator AFloat(). template<typename AFloat > . TMVA::DNN::TCudaDeviceReference< AFloat >::operator AFloat. Definition at line 217 of file CudaMatrix.h. ◆ operator+=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator+= ; (; AFloat ; value). Definition at line 244 of file CudaMatrix.h. ◆ operator-=(). template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator-= ; (; AFloat ; value). Definition at line 256 of file CudaMatrix.h. ◆ operator=() [1/2]. template<typename AFloat > . void TMVA::DNN::TCudaDeviceReference< AFloat >::operator= ; ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaDeviceReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaHostBuffer.html:733,Performance,perform,performed,733,". ROOT: TMVA::DNN::TCudaHostBuffer< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TCudaHostBuffer< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaHostBuffer< AFloat >TCudaHostBuffer. ; Wrapper class for pinned memory buffers on the host. Uses std::shared_pointer with custom destructor to ensure consistent memory management and allow for easy copying/moving of the buffers. Copying is asynchronous and will set the cudaStream of the device buffer so that subsequent computations on the device buffer can be performed on the same stream.; Template Parameters. AFloatThe floating point type to be stored in the buffers. . Definition at line 42 of file CudaBuffers.h. Classes; struct  TDestructor;  . Public Member Functions;  TCudaHostBuffer ()=default;  ;  TCudaHostBuffer (AFloat *);  ;  TCudaHostBuffer (const TCudaHostBuffer &)=default;  ;  TCudaHostBuffer (size_t size);  ;  TCudaHostBuffer (TCudaHostBuffer &&)=default;  ; AFloat * data () const;  ; size_t GetSize () const;  ; TCudaHostBuffer GetSubBuffer (size_t offset, size_t size);  Return sub-buffer of the current buffer. ;  ;  operator AFloat * () const;  ; TCudaHostBuffer & operator= (const TCudaHostBuffer &)=default;  ; TCudaHostBuffer & operator= (TCudaHostBuffer &&)=default;  ; AFloat & operator[] (size_t index);  ; AFloat operator[] (size_t index) const;  ; void SetConstVal (const AFloat constVal);  Sets the entire buffer to a constant value. ;  . Private Attributes; cudaStream_t fComputeStream;  cudaStream for data transfer ;  ; struct TMVA::DNN::TCudaHostBuffer::TDestructor fDestructor;  ; std::shared_ptr< AFloat * > fHostPointer;  Pointer to the buffer data. ;  ; size_t fOffset;  Offset for sub-buffers. ;  ; size_t fSize;  ; friend TCudaDeviceBuffer< AFloat >;  . #include <TMVA/DNN/Architectu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaHostBuffer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaHostBuffer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:845,Energy Efficiency,allocate,allocated,845,". ROOT: TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Static Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaMatrix< AFloat >TCudaMatrix Class. ; The TCudaMatrix class represents matrices on a CUDA device. The elements of the matrix are stored in a TCudaDeviceBuffer object which takes care of the allocation and freeing of the device memory. TCudaMatrices are lightweight object, that means on assignment and copy creation only a shallow copy is performed and no new element buffer allocated. To perform a deep copy use the static Copy method of the TCuda architecture class.; The TCudaDeviceBuffer has an associated cuda stream, on which the data is transferred to the device. This stream can be accessed through the GetComputeStream member function and used to synchronize computations.; The TCudaMatrix class also holds static references to CUDA resources. Those are the cublas handle, a buffer of curand states for the generation of random numbers as well as a vector containing ones, which is used for summing column matrices using matrix-vector multiplication. The class also has a static buffer for returning results from the device. ; Definition at line 102 of file CudaMatrix.h. Public Member Functions;  TCudaMatrix ();  ;  TCudaMatrix (const TCudaMatrix &)=default;  ;  TCudaMatrix (const TMatrixT< AFloat > &);  ;  TCudaMatrix (size_t i, size_t j);  ;  TCudaMatrix (TCudaDeviceBuffer< AFloat > buffer, size_t m, size_t n);  ;  TCudaMatrix (TCudaMatrix &&)=default;  ;  ~TCudaMatrix ()=default;  ; cudaStream_t GetComputeStream () const;  ; const cublasHandle_t & GetCublasHandle () const;  ; AFloat * GetDataPointer ();  ; con",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:3450,Energy Efficiency,allocate,allocated,3450," operator= (const TCudaMatrix &)=default;  ; TCudaMatrix & operator= (TCudaMatrix &&)=default;  ; void Print () const;  ; void SetComputeStream (cudaStream_t stream);  ; void Synchronize (const TCudaMatrix &) const;  Blocking synchronization with the associated compute stream, if it's not the default stream. ;  ; void Zero ();  . Static Public Member Functions; static curandState_t * GetCurandStatesPointer ();  ; static AFloat GetDeviceReturn ();  Transfer the value in the device return buffer to the host. ;  ; static AFloat * GetDeviceReturnPointer ();  Return device pointer to the device return buffer. ;  ; static size_t GetNDim ();  ; static AFloat * GetOnes ();  ; static void ResetDeviceReturn (AFloat value=0.0);  Set the return buffer on the device to the specified value. ;  . Static Public Attributes; static Bool_t gInitializeCurand = kFALSE;  . Private Member Functions; void InitializeCuda ();  Initializes all shared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ;  ; void InitializeCurandStates ();  . Private Attributes; TCudaDeviceBuffer< AFloat > fElementBuffer;  ; size_t fNCols;  ; size_t fNRows;  . Static Private Attributes; static cublasHandle_t fCublasHandle = nullptr;  ; static curandState_t * fCurandStates = nullptr;  ; static AFloat * fDeviceReturn = nullptr;  Buffer for kernel return values. ;  ; static size_t fInstances = 0;  Current number of matrix instances. ;  ; static size_t fNCurandStates = 0;  ; static size_t fNOnes = 0;  Current length of the one vector. ;  ; static AFloat * fOnes = nullptr;  Vector used for summations of columns. ;  . #include <TMVA/DNN/Architectures/Cuda/CudaMatrix.h>; Constructor & Destructor Documentation. ◆ TCudaMatrix() [1/6]. template<typename AFloat > . TMVA::DNN::TCudaMatrix< AFloat >::TCudaMatrix. Definition at line 57 of file CudaMatrix.cu. ◆ TCudaMatrix() [2/6]",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:8117,Energy Efficiency,allocate,allocated,8117,":TCudaMatrix< AFloat >::GetNcols ; (; ); const. inline . Definition at line 160 of file CudaMatrix.h. ◆ GetNDim(). template<typename AFloat > . static size_t TMVA::DNN::TCudaMatrix< AFloat >::GetNDim ; (; ). inlinestatic . Definition at line 158 of file CudaMatrix.h. ◆ GetNoElements(). template<typename AFloat > . size_t TMVA::DNN::TCudaMatrix< AFloat >::GetNoElements ; (; ); const. inline . Definition at line 161 of file CudaMatrix.h. ◆ GetNrows(). template<typename AFloat > . size_t TMVA::DNN::TCudaMatrix< AFloat >::GetNrows ; (; ); const. inline . Definition at line 159 of file CudaMatrix.h. ◆ GetOnes(). template<typename AFloat > . static AFloat * TMVA::DNN::TCudaMatrix< AFloat >::GetOnes ; (; ). inlinestatic . Definition at line 125 of file CudaMatrix.h. ◆ InitializeCuda(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::InitializeCuda. inlineprivate . Initializes all shared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ; Definition at line 103 of file CudaMatrix.cu. ◆ InitializeCurandStates(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::InitializeCurandStates. private . Definition at line 140 of file CudaMatrix.cu. ◆ operator TMatrixT(). template<typename AFloat > . TMVA::DNN::TCudaMatrix< AFloat >::operator TMatrixT. Convert cuda matrix to Root TMatrix. ; Performs synchronous data transfer. ; Definition at line 150 of file CudaMatrix.cu. ◆ operator()(). template<typename AFloat > . TCudaDeviceReference< AFloat > TMVA::DNN::TCudaMatrix< AFloat >::operator() ; (; size_t ; i, . size_t ; j . ); const. Access to elements of device matrices provided through TCudaDeviceReference class. ; Note that access is synchronous end enforces device synchronization on all streams. Only used for testing. ; Definition at line 310 of file CudaMatrix.h. ◆ operator=() [1/2]. template<type",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:1126,Integrability,synchroniz,synchronize,1126,"e Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Static Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaMatrix< AFloat >TCudaMatrix Class. ; The TCudaMatrix class represents matrices on a CUDA device. The elements of the matrix are stored in a TCudaDeviceBuffer object which takes care of the allocation and freeing of the device memory. TCudaMatrices are lightweight object, that means on assignment and copy creation only a shallow copy is performed and no new element buffer allocated. To perform a deep copy use the static Copy method of the TCuda architecture class.; The TCudaDeviceBuffer has an associated cuda stream, on which the data is transferred to the device. This stream can be accessed through the GetComputeStream member function and used to synchronize computations.; The TCudaMatrix class also holds static references to CUDA resources. Those are the cublas handle, a buffer of curand states for the generation of random numbers as well as a vector containing ones, which is used for summing column matrices using matrix-vector multiplication. The class also has a static buffer for returning results from the device. ; Definition at line 102 of file CudaMatrix.h. Public Member Functions;  TCudaMatrix ();  ;  TCudaMatrix (const TCudaMatrix &)=default;  ;  TCudaMatrix (const TMatrixT< AFloat > &);  ;  TCudaMatrix (size_t i, size_t j);  ;  TCudaMatrix (TCudaDeviceBuffer< AFloat > buffer, size_t m, size_t n);  ;  TCudaMatrix (TCudaMatrix &&)=default;  ;  ~TCudaMatrix ()=default;  ; cudaStream_t GetComputeStream () const;  ; const cublasHandle_t & GetCublasHandle () const;  ; AFloat * GetDataPointer ();  ; const AFloat * GetDataPointer () const;  ; TCudaDeviceBuffer< AFloat > GetDeviceBuffer () const;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:2660,Integrability,synchroniz,synchronization,2660,"udaMatrix ();  ;  TCudaMatrix (const TCudaMatrix &)=default;  ;  TCudaMatrix (const TMatrixT< AFloat > &);  ;  TCudaMatrix (size_t i, size_t j);  ;  TCudaMatrix (TCudaDeviceBuffer< AFloat > buffer, size_t m, size_t n);  ;  TCudaMatrix (TCudaMatrix &&)=default;  ;  ~TCudaMatrix ()=default;  ; cudaStream_t GetComputeStream () const;  ; const cublasHandle_t & GetCublasHandle () const;  ; AFloat * GetDataPointer ();  ; const AFloat * GetDataPointer () const;  ; TCudaDeviceBuffer< AFloat > GetDeviceBuffer () const;  ; size_t GetNcols () const;  ; size_t GetNoElements () const;  ; size_t GetNrows () const;  ;  operator TMatrixT () const;  Convert cuda matrix to Root TMatrix. ;  ; TCudaDeviceReference< AFloat > operator() (size_t i, size_t j) const;  Access to elements of device matrices provided through TCudaDeviceReference class. ;  ; TCudaMatrix & operator= (const TCudaMatrix &)=default;  ; TCudaMatrix & operator= (TCudaMatrix &&)=default;  ; void Print () const;  ; void SetComputeStream (cudaStream_t stream);  ; void Synchronize (const TCudaMatrix &) const;  Blocking synchronization with the associated compute stream, if it's not the default stream. ;  ; void Zero ();  . Static Public Member Functions; static curandState_t * GetCurandStatesPointer ();  ; static AFloat GetDeviceReturn ();  Transfer the value in the device return buffer to the host. ;  ; static AFloat * GetDeviceReturnPointer ();  Return device pointer to the device return buffer. ;  ; static size_t GetNDim ();  ; static AFloat * GetOnes ();  ; static void ResetDeviceReturn (AFloat value=0.0);  Set the return buffer on the device to the specified value. ;  . Static Public Attributes; static Bool_t gInitializeCurand = kFALSE;  . Private Member Functions; void InitializeCuda ();  Initializes all shared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ;  ; vo",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:8991,Integrability,synchroniz,synchronization,8991,"t >::InitializeCuda. inlineprivate . Initializes all shared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ; Definition at line 103 of file CudaMatrix.cu. ◆ InitializeCurandStates(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::InitializeCurandStates. private . Definition at line 140 of file CudaMatrix.cu. ◆ operator TMatrixT(). template<typename AFloat > . TMVA::DNN::TCudaMatrix< AFloat >::operator TMatrixT. Convert cuda matrix to Root TMatrix. ; Performs synchronous data transfer. ; Definition at line 150 of file CudaMatrix.cu. ◆ operator()(). template<typename AFloat > . TCudaDeviceReference< AFloat > TMVA::DNN::TCudaMatrix< AFloat >::operator() ; (; size_t ; i, . size_t ; j . ); const. Access to elements of device matrices provided through TCudaDeviceReference class. ; Note that access is synchronous end enforces device synchronization on all streams. Only used for testing. ; Definition at line 310 of file CudaMatrix.h. ◆ operator=() [1/2]. template<typename AFloat > . TCudaMatrix & TMVA::DNN::TCudaMatrix< AFloat >::operator= ; (; const TCudaMatrix< AFloat > & ; ). default . ◆ operator=() [2/2]. template<typename AFloat > . TCudaMatrix & TMVA::DNN::TCudaMatrix< AFloat >::operator= ; (; TCudaMatrix< AFloat > && ; ). default . ◆ Print(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::Print ; (; ); const. inline . Definition at line 174 of file CudaMatrix.h. ◆ ResetDeviceReturn(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::ResetDeviceReturn ; (; AFloat ; value = 0.0). inlinestatic . Set the return buffer on the device to the specified value. ; This is required for example for reductions in order to initialize the accumulator. ; Definition at line 293 of file CudaMatrix.h. ◆ SetComputeStream(). template<typename AFloat > . void TMVA::DNN::TCudaMatr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:10256,Integrability,synchroniz,synchronization,10256,"loat > . TCudaMatrix & TMVA::DNN::TCudaMatrix< AFloat >::operator= ; (; TCudaMatrix< AFloat > && ; ). default . ◆ Print(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::Print ; (; ); const. inline . Definition at line 174 of file CudaMatrix.h. ◆ ResetDeviceReturn(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::ResetDeviceReturn ; (; AFloat ; value = 0.0). inlinestatic . Set the return buffer on the device to the specified value. ; This is required for example for reductions in order to initialize the accumulator. ; Definition at line 293 of file CudaMatrix.h. ◆ SetComputeStream(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::SetComputeStream ; (; cudaStream_t ; stream). inline . Definition at line 275 of file CudaMatrix.h. ◆ Synchronize(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::Synchronize ; (; const TCudaMatrix< AFloat > & ; A); const. inline . Blocking synchronization with the associated compute stream, if it's not the default stream. ; Definition at line 282 of file CudaMatrix.h. ◆ Zero(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::Zero ; (; ). inline . Definition at line 179 of file CudaMatrix.h. Member Data Documentation. ◆ fCublasHandle. template<typename AFloat > . cublasHandle_t TMVA::DNN::TCudaMatrix< AFloat >::fCublasHandle = nullptr. staticprivate . Definition at line 109 of file CudaMatrix.h. ◆ fCurandStates. template<typename AFloat > . curandState_t * TMVA::DNN::TCudaMatrix< AFloat >::fCurandStates = nullptr. staticprivate . Definition at line 113 of file CudaMatrix.h. ◆ fDeviceReturn. template<typename AFloat > . AFloat * TMVA::DNN::TCudaMatrix< AFloat >::fDeviceReturn = nullptr. staticprivate . Buffer for kernel return values. ; Definition at line 110 of file CudaMatrix.h. ◆ fElementBuffer. template<typename AFloat > . TCudaDeviceBuffer<AFloat> TMVA::DNN::TCudaMatrix< AFloat >::fElementBuffer. private . Definition at line 119 of fi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:809,Performance,perform,performed,809,". ROOT: TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Static Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaMatrix< AFloat >TCudaMatrix Class. ; The TCudaMatrix class represents matrices on a CUDA device. The elements of the matrix are stored in a TCudaDeviceBuffer object which takes care of the allocation and freeing of the device memory. TCudaMatrices are lightweight object, that means on assignment and copy creation only a shallow copy is performed and no new element buffer allocated. To perform a deep copy use the static Copy method of the TCuda architecture class.; The TCudaDeviceBuffer has an associated cuda stream, on which the data is transferred to the device. This stream can be accessed through the GetComputeStream member function and used to synchronize computations.; The TCudaMatrix class also holds static references to CUDA resources. Those are the cublas handle, a buffer of curand states for the generation of random numbers as well as a vector containing ones, which is used for summing column matrices using matrix-vector multiplication. The class also has a static buffer for returning results from the device. ; Definition at line 102 of file CudaMatrix.h. Public Member Functions;  TCudaMatrix ();  ;  TCudaMatrix (const TCudaMatrix &)=default;  ;  TCudaMatrix (const TMatrixT< AFloat > &);  ;  TCudaMatrix (size_t i, size_t j);  ;  TCudaMatrix (TCudaDeviceBuffer< AFloat > buffer, size_t m, size_t n);  ;  TCudaMatrix (TCudaMatrix &&)=default;  ;  ~TCudaMatrix ()=default;  ; cudaStream_t GetComputeStream () const;  ; const cublasHandle_t & GetCublasHandle () const;  ; AFloat * GetDataPointer ();  ; con",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:859,Performance,perform,perform,859,". ROOT: TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Static Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaMatrix< AFloat >TCudaMatrix Class. ; The TCudaMatrix class represents matrices on a CUDA device. The elements of the matrix are stored in a TCudaDeviceBuffer object which takes care of the allocation and freeing of the device memory. TCudaMatrices are lightweight object, that means on assignment and copy creation only a shallow copy is performed and no new element buffer allocated. To perform a deep copy use the static Copy method of the TCuda architecture class.; The TCudaDeviceBuffer has an associated cuda stream, on which the data is transferred to the device. This stream can be accessed through the GetComputeStream member function and used to synchronize computations.; The TCudaMatrix class also holds static references to CUDA resources. Those are the cublas handle, a buffer of curand states for the generation of random numbers as well as a vector containing ones, which is used for summing column matrices using matrix-vector multiplication. The class also has a static buffer for returning results from the device. ; Definition at line 102 of file CudaMatrix.h. Public Member Functions;  TCudaMatrix ();  ;  TCudaMatrix (const TCudaMatrix &)=default;  ;  TCudaMatrix (const TMatrixT< AFloat > &);  ;  TCudaMatrix (size_t i, size_t j);  ;  TCudaMatrix (TCudaDeviceBuffer< AFloat > buffer, size_t m, size_t n);  ;  TCudaMatrix (TCudaMatrix &&)=default;  ;  ~TCudaMatrix ()=default;  ; cudaStream_t GetComputeStream () const;  ; const cublasHandle_t & GetCublasHandle () const;  ; AFloat * GetDataPointer ();  ; con",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:1060,Security,access,accessed,1060,"e Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Member Functions |; Static Public Member Functions |; Static Public Attributes |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCudaMatrix< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaMatrix< AFloat >TCudaMatrix Class. ; The TCudaMatrix class represents matrices on a CUDA device. The elements of the matrix are stored in a TCudaDeviceBuffer object which takes care of the allocation and freeing of the device memory. TCudaMatrices are lightweight object, that means on assignment and copy creation only a shallow copy is performed and no new element buffer allocated. To perform a deep copy use the static Copy method of the TCuda architecture class.; The TCudaDeviceBuffer has an associated cuda stream, on which the data is transferred to the device. This stream can be accessed through the GetComputeStream member function and used to synchronize computations.; The TCudaMatrix class also holds static references to CUDA resources. Those are the cublas handle, a buffer of curand states for the generation of random numbers as well as a vector containing ones, which is used for summing column matrices using matrix-vector multiplication. The class also has a static buffer for returning results from the device. ; Definition at line 102 of file CudaMatrix.h. Public Member Functions;  TCudaMatrix ();  ;  TCudaMatrix (const TCudaMatrix &)=default;  ;  TCudaMatrix (const TMatrixT< AFloat > &);  ;  TCudaMatrix (size_t i, size_t j);  ;  TCudaMatrix (TCudaDeviceBuffer< AFloat > buffer, size_t m, size_t n);  ;  TCudaMatrix (TCudaMatrix &&)=default;  ;  ~TCudaMatrix ()=default;  ; cudaStream_t GetComputeStream () const;  ; const cublasHandle_t & GetCublasHandle () const;  ; AFloat * GetDataPointer ();  ; const AFloat * GetDataPointer () const;  ; TCudaDeviceBuffer< AFloat > GetDeviceBuffer () const;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:8949,Security,access,access,8949,"t >::InitializeCuda. inlineprivate . Initializes all shared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ; Definition at line 103 of file CudaMatrix.cu. ◆ InitializeCurandStates(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::InitializeCurandStates. private . Definition at line 140 of file CudaMatrix.cu. ◆ operator TMatrixT(). template<typename AFloat > . TMVA::DNN::TCudaMatrix< AFloat >::operator TMatrixT. Convert cuda matrix to Root TMatrix. ; Performs synchronous data transfer. ; Definition at line 150 of file CudaMatrix.cu. ◆ operator()(). template<typename AFloat > . TCudaDeviceReference< AFloat > TMVA::DNN::TCudaMatrix< AFloat >::operator() ; (; size_t ; i, . size_t ; j . ); const. Access to elements of device matrices provided through TCudaDeviceReference class. ; Note that access is synchronous end enforces device synchronization on all streams. Only used for testing. ; Definition at line 310 of file CudaMatrix.h. ◆ operator=() [1/2]. template<typename AFloat > . TCudaMatrix & TMVA::DNN::TCudaMatrix< AFloat >::operator= ; (; const TCudaMatrix< AFloat > & ; ). default . ◆ operator=() [2/2]. template<typename AFloat > . TCudaMatrix & TMVA::DNN::TCudaMatrix< AFloat >::operator= ; (; TCudaMatrix< AFloat > && ; ). default . ◆ Print(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::Print ; (; ); const. inline . Definition at line 174 of file CudaMatrix.h. ◆ ResetDeviceReturn(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::ResetDeviceReturn ; (; AFloat ; value = 0.0). inlinestatic . Set the return buffer on the device to the specified value. ; This is required for example for reductions in order to initialize the accumulator. ; Definition at line 293 of file CudaMatrix.h. ◆ SetComputeStream(). template<typename AFloat > . void TMVA::DNN::TCudaMatr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html:9037,Testability,test,testing,9037,"hared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ; Definition at line 103 of file CudaMatrix.cu. ◆ InitializeCurandStates(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::InitializeCurandStates. private . Definition at line 140 of file CudaMatrix.cu. ◆ operator TMatrixT(). template<typename AFloat > . TMVA::DNN::TCudaMatrix< AFloat >::operator TMatrixT. Convert cuda matrix to Root TMatrix. ; Performs synchronous data transfer. ; Definition at line 150 of file CudaMatrix.cu. ◆ operator()(). template<typename AFloat > . TCudaDeviceReference< AFloat > TMVA::DNN::TCudaMatrix< AFloat >::operator() ; (; size_t ; i, . size_t ; j . ); const. Access to elements of device matrices provided through TCudaDeviceReference class. ; Note that access is synchronous end enforces device synchronization on all streams. Only used for testing. ; Definition at line 310 of file CudaMatrix.h. ◆ operator=() [1/2]. template<typename AFloat > . TCudaMatrix & TMVA::DNN::TCudaMatrix< AFloat >::operator= ; (; const TCudaMatrix< AFloat > & ; ). default . ◆ operator=() [2/2]. template<typename AFloat > . TCudaMatrix & TMVA::DNN::TCudaMatrix< AFloat >::operator= ; (; TCudaMatrix< AFloat > && ; ). default . ◆ Print(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::Print ; (; ); const. inline . Definition at line 174 of file CudaMatrix.h. ◆ ResetDeviceReturn(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::ResetDeviceReturn ; (; AFloat ; value = 0.0). inlinestatic . Set the return buffer on the device to the specified value. ; This is required for example for reductions in order to initialize the accumulator. ; Definition at line 293 of file CudaMatrix.h. ◆ SetComputeStream(). template<typename AFloat > . void TMVA::DNN::TCudaMatrix< AFloat >::SetComputeStream ; (; cudaStream_t ; stre",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaMatrix.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html:4324,Energy Efficiency,allocate,allocated,4324," TCudaDeviceReference< AFloat > operator() (size_t i, size_t j, size_t k, size_t l) const;  ; TCudaTensor & operator= (const TCudaTensor &)=default;  ; TCudaTensor & operator= (TCudaTensor &&)=default;  ; TCudaMatrix< AFloat > operator[] (size_t i) const;  ; void Print (const char *name=""Tensor"", bool truncate=false) const;  ; void PrintShape (const char *name=""Tensor"") const;  ; TCudaTensor< AFloat > Reshape (const Shape_t &newShape) const;  ; void ReshapeInPlace (const Shape_t &newShape);  ; void SetComputeStream (cudaStream_t stream);  ; void SetConstVal (const AFloat constVal);  ; void SetTensorDescriptor ();  ; void Zero ();  . Static Public Member Functions; static std::vector< std::size_t > ComputeStridesFromShape (const std::vector< std::size_t > &shape, bool rowmajorLayout);  This information is needed for the multi-dimensional indexing. ;  . Private Member Functions; void InitializeCuda ();  Initializes all shared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ;  ; void InitializeCurandStates ();  . Private Attributes; int fDevice;  Device associated with current tensor instance. ;  ; TCudaDeviceBuffer< AFloat > fElementBuffer;  ; MemoryLayout fMemoryLayout;  ; size_t fNDim;  Dimension of the tensor (first dimension is the batch size, second is the no. channels) ;  ; Shape_t fShape;  The shape vector (size of dimensions) needs to be ordered as no. ;  ; size_t fSize;  No. of elements. ;  ; int fStreamIndx;  Cuda stream associated with current instance. ;  ; Shape_t fStrides;  Strides between tensor dimensions (always assume dense, non overlapping tensor) ;  ; std::shared_ptr< TensorDescriptor > fTensorDescriptor;  . Static Private Attributes; static std::vector< int > fInstances;  For each GPU device keep the CUDA streams in which tensors are used. ;  . #include <TMVA/DNN/Architectures/Cuda/CudaTensor.h>; M",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html:13969,Energy Efficiency,allocate,allocated,13969,"::DNN::TCudaTensor< AFloat >::GetNrows ; (; ); const. inline . Definition at line 299 of file CudaTensor.h. ◆ GetShape(). template<typename AFloat > . const Shape_t & TMVA::DNN::TCudaTensor< AFloat >::GetShape ; (; ); const. inline . Definition at line 188 of file CudaTensor.h. ◆ GetSize(). template<typename AFloat > . size_t TMVA::DNN::TCudaTensor< AFloat >::GetSize ; (; ); const. inline . Definition at line 192 of file CudaTensor.h. ◆ GetStrides(). template<typename AFloat > . const Shape_t & TMVA::DNN::TCudaTensor< AFloat >::GetStrides ; (; ); const. inline . Definition at line 189 of file CudaTensor.h. ◆ GetWSize(). template<typename AFloat > . size_t TMVA::DNN::TCudaTensor< AFloat >::GetWSize ; (; ); const. inline . Definition at line 289 of file CudaTensor.h. ◆ InitializeCuda(). template<typename AFloat > . void TMVA::DNN::TCudaTensor< AFloat >::InitializeCuda. private . Initializes all shared devices resource and makes sure that a sufficient number of curand states are allocated on the device and initialized as well as that the one-vector for the summation over columns has the right size. ; Definition at line 366 of file CudaTensor.cu. ◆ InitializeCurandStates(). template<typename AFloat > . void TMVA::DNN::TCudaTensor< AFloat >::InitializeCurandStates. private . Definition at line 377 of file CudaTensor.cu. ◆ isEqual() [1/2]. template<typename AFloat > . bool TMVA::DNN::TCudaTensor< AFloat >::isEqual ; (; const AFloat * ; hostBufferOther, . size_t ; otherSize . ). inline . Definition at line 238 of file CudaTensor.h. ◆ isEqual() [2/2]. template<typename AFloat > . bool TMVA::DNN::TCudaTensor< AFloat >::isEqual ; (; TCudaTensor< AFloat > & ; other). inline . Definition at line 220 of file CudaTensor.h. ◆ operator TMatrixT(). template<typename AFloat > . TMVA::DNN::TCudaTensor< AFloat >::operator TMatrixT. Convert cuda matrix to Root TMatrix. ; Performs synchronous data transfer. ; Definition at line 175 of file CudaTensor.cu. ◆ operator()() [1/3]. template<typ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html:529,Modifiability,extend,extends,529,". ROOT: TMVA::DNN::TCudaTensor< AFloat > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Classes |; Public Types |; Public Member Functions |; Static Public Member Functions |; Private Member Functions |; Private Attributes |; Static Private Attributes |; List of all members ; TMVA::DNN::TCudaTensor< AFloat > Class Template Reference. ; template<typename AFloat>; class TMVA::DNN::TCudaTensor< AFloat >TCudaTensor Class. ; The TCudaTensor class extends the TCudaMatrix class for dimensions > 2. ; Definition at line 83 of file CudaTensor.h. Classes; struct  TensorDescriptor;  . Public Types; using MemoryLayout = TMVA::Experimental::MemoryLayout;  ; using Scalar_t = AFloat;  ; using Shape_t = std::vector< size_t >;  . Public Member Functions;  TCudaTensor ();  ;  TCudaTensor (const AFloat *data, const std::vector< size_t > &shape, MemoryLayout memlayout=MemoryLayout::ColumnMajor, int deviceIndx=0, int streamIndx=0);  ;  TCudaTensor (const std::vector< size_t > &shape, MemoryLayout memlayout=MemoryLayout::ColumnMajor, int deviceIndx=0, int streamIndx=0);  ;  TCudaTensor (const TCudaMatrix< AFloat > &m, size_t dim=2);  ;  TCudaTensor (const TCudaTensor &)=default;  ;  TCudaTensor (const TMatrixT< AFloat > &m, size_t dim=2);  ;  TCudaTensor (size_t bsize, size_t csize, size_t hsize, size_t wsize, MemoryLayout memlayout=MemoryLayout::ColumnMajor, int deviceIndx=0, int streamIndx=0);  ;  TCudaTensor (size_t bsize, size_t csize, size_t hwsize, MemoryLayout memlayout=MemoryLayout::ColumnMajor, int deviceIndx=0, int streamIndx=0);  ;  TCudaTensor (size_t n, size_t m, MemoryLayout memlayout=MemoryLayout::ColumnMajor, int deviceIndx=0, int streamIndx=0);  ;  TCudaTensor (TCudaDeviceBuffer< AFloat > buffer, const std::vector< size_t > &shape, MemoryLayout memlayout=MemoryLayout::ColumnMajor, int deviceIndx=0, int streamIndx=0);  ;  TCudaTensor (TCudaDeviceBuffer< AFloat > buffer, size_t n, size_t m);  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TCudaTensor.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDataLoader.html:19186,Integrability,rout,routine,19186,"ename Data_t , typename AArchitecture > . TBatch< AArchitecture > TMVA::DNN::TDataLoader< Data_t, AArchitecture >::GetBatch. Return the next batch from the training set. ; The TDataLoader object keeps an internal counter that cycles over the batches in the training set. ; Definition at line 228 of file DataLoader.h. ◆ operator=() [1/2]. template<typename Data_t , typename AArchitecture > . TDataLoader & TMVA::DNN::TDataLoader< Data_t, AArchitecture >::operator= ; (; const TDataLoader< Data_t, AArchitecture > & ; ). default . ◆ operator=() [2/2]. template<typename Data_t , typename AArchitecture > . TDataLoader & TMVA::DNN::TDataLoader< Data_t, AArchitecture >::operator= ; (; TDataLoader< Data_t, AArchitecture > && ; ). default . ◆ Shuffle(). template<typename Data_t , typename AArchitecture > . void TMVA::DNN::TDataLoader< Data_t, AArchitecture >::Shuffle. Shuffle the order of the samples in the batch. ; The shuffling is indirect, i.e. only the indices are shuffled. No input data is moved by this routine. ; Definition at line 269 of file DataLoader.h. Member Data Documentation. ◆ fBatchIndex. template<typename Data_t , typename AArchitecture > . size_t TMVA::DNN::TDataLoader< Data_t, AArchitecture >::fBatchIndex. private . Definition at line 143 of file DataLoader.h. ◆ fBatchSize. template<typename Data_t , typename AArchitecture > . size_t TMVA::DNN::TDataLoader< Data_t, AArchitecture >::fBatchSize. private . Definition at line 140 of file DataLoader.h. ◆ fData. template<typename Data_t , typename AArchitecture > . const Data_t& TMVA::DNN::TDataLoader< Data_t, AArchitecture >::fData. private . Definition at line 137 of file DataLoader.h. ◆ fDeviceBuffers. template<typename Data_t , typename AArchitecture > . std::vector<DeviceBuffer_t> TMVA::DNN::TDataLoader< Data_t, AArchitecture >::fDeviceBuffers. private . Definition at line 146 of file DataLoader.h. ◆ fHostBuffers. template<typename Data_t , typename AArchitecture > . std::vector<HostBuffer_t> TMVA::DNN::TDataL",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDataLoader.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html:5380,Integrability,rout,routine,5380,"pename AData , typename AReal > . TBatch< TReference< AReal > > TMVA::DNN::TDataLoader< AData, TReference< AReal > >::GetBatch. Return the next batch from the training set. ; The TDataLoader object keeps an internal counter that cycles over the batches in the training set. ; Definition at line 97 of file DataLoader.h. ◆ operator=() [1/2]. template<typename AData , typename AReal > . TDataLoader & TMVA::DNN::TDataLoader< AData, TReference< AReal > >::operator= ; (; const TDataLoader< AData, TReference< AReal > > & ; ). default . ◆ operator=() [2/2]. template<typename AData , typename AReal > . TDataLoader & TMVA::DNN::TDataLoader< AData, TReference< AReal > >::operator= ; (; TDataLoader< AData, TReference< AReal > > && ; ). default . ◆ Shuffle(). template<typename AData , typename AReal > . void TMVA::DNN::TDataLoader< AData, TReference< AReal > >::Shuffle. Shuffle the order of the samples in the batch. ; The shuffling is indirect, i.e. only the indices are shuffled. No input data is moved by this routine. ; Definition at line 115 of file DataLoader.h. Member Data Documentation. ◆ fBatchIndex. template<typename AData , typename AReal > . size_t TMVA::DNN::TDataLoader< AData, TReference< AReal > >::fBatchIndex. private . Definition at line 43 of file DataLoader.h. ◆ fBatchSize. template<typename AData , typename AReal > . size_t TMVA::DNN::TDataLoader< AData, TReference< AReal > >::fBatchSize. private . Definition at line 40 of file DataLoader.h. ◆ fData. template<typename AData , typename AReal > . const AData& TMVA::DNN::TDataLoader< AData, TReference< AReal > >::fData. private . Definition at line 37 of file DataLoader.h. ◆ fNInputFeatures. template<typename AData , typename AReal > . size_t TMVA::DNN::TDataLoader< AData, TReference< AReal > >::fNInputFeatures. private . Definition at line 41 of file DataLoader.h. ◆ fNOutputFeatures. template<typename AData , typename AReal > . size_t TMVA::DNN::TDataLoader< AData, TReference< AReal > >::fNOutputFeatures. private .",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:7669,Deployability,update,update,7669," ;  ; void Prediction (Matrix_t &predictions, EOutputFunction f) const;  Prediction based on activations stored in the last layer. ;  ; void Prediction (Matrix_t &predictions, Tensor_t &input, EOutputFunction f);  Prediction for the given inputs, based on what network learned. ;  ; void Print () const;  Print the Deep Net Info. ;  ; Scalar_t RegularizationTerm () const;  Function for computing the regularizaton term to be added to the loss function .  ; void ResetTraining ();  Function that reset some training flags after looping all the events but not the weights. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function that will update the weights and biases in the layers that contain weights and biases. ;  . Private Member Functions; size_t calculateDimension (int imgDim, int fltDim, int padding, int stride);  ; bool isInteger (Scalar_t x) const;  . Private Attributes; size_t fBatchDepth;  The depth of the batch used for training/testing. ;  ; size_t fBatchHeight;  The height of the batch used for training/testing. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; size_t fBatchWidth;  The width of the batch used for training/testing. ;  ; EInitialization fI;  The initialization method of the network. ;  ; size_t fInputDepth;  The depth of the input. ;  ; size_t fInputHeight;  The height of the input. ;  ; size_t fInputWidth;  The width of the input. ;  ; bool fIsTraining;  Is ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:29046,Deployability,update,update,29046,"ne 360 of file DeepNet.h. ◆ SetLossFunction(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetLossFunction ; (; ELossFunction ; J). inline . Definition at line 364 of file DeepNet.h. ◆ SetRegularization(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetRegularization ; (; ERegularization ; R). inline . Definition at line 366 of file DeepNet.h. ◆ SetWeightDecay(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetWeightDecay ; (; Scalar_t ; weightDecay). inline . Definition at line 367 of file DeepNet.h. ◆ Update(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Update ; (; Scalar_t ; learningRate). Function that will update the weights and biases in the layers that contain weights and biases. . Definition at line 1254 of file DeepNet.h. Member Data Documentation. ◆ fBatchDepth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchDepth. private . The depth of the batch used for training/testing. ; Definition at line 93 of file DeepNet.h. ◆ fBatchHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchHeight. private . The height of the batch used for training/testing. ; Definition at line 94 of file DeepNet.h. ◆ fBatchSize. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchSize. private . Batch size used for training and evaluation. ; Definition at line 88 of file DeepNet.h. ◆ fBatchWidth. template<typename Architecture_t , typename Layer",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:5079,Modifiability,layers,layers,5079," for adding Max Pooling layer in the Deep Neural Network, when the layer is already created. ;  ; TMaxPoolLayer< Architecture_t > * AddMaxPoolLayer (size_t frameHeight, size_t frameWidth, size_t strideRows, size_t strideCols, Scalar_t dropoutProbability=1.0);  Function for adding Pooling layer in the Deep Neural Network, with a given filter height and width, striding in rows and columns as well as the dropout probability. ;  ; TReshapeLayer< Architecture_t > * AddReshapeLayer (size_t depth, size_t height, size_t width, bool flattening);  Function for adding Reshape Layer in the Deep Neural Network, with a given height and width. ;  ; void AddReshapeLayer (TReshapeLayer< Architecture_t > *reshapeLayer);  Function for adding Reshape Layer in the Deep Neural Network, when the layer is already created. ;  ; void Backward (const Tensor_t &input, const Matrix_t &groundTruth, const Matrix_t &weights);  Function that executes the entire backward pass in the network. ;  ; void Clear ();  Remove all layers from the network. ;  ; void Forward (Tensor_t &input, bool applyDropout=false);  Function that executes the entire forward pass in the network. ;  ; size_t GetBatchDepth () const;  ; size_t GetBatchHeight () const;  ; size_t GetBatchSize () const;  Getters. ;  ; size_t GetBatchWidth () const;  ; size_t GetDepth () const;  ; EInitialization GetInitialization () const;  ; size_t GetInputDepth () const;  ; size_t GetInputHeight () const;  ; size_t GetInputWidth () const;  ; Layer_t * GetLayerAt (size_t i);  Get the layer in the vector of layers at position i. ;  ; const Layer_t * GetLayerAt (size_t i) const;  ; std::vector< Layer_t * > & GetLayers ();  ; const std::vector< Layer_t * > & GetLayers () const;  ; ELossFunction GetLossFunction () const;  ; size_t GetOutputWidth () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize ();  DAE functions. ;  ; bool IsTraining () const;  ; Scalar_t Loss (const Matrix_t &groundTrut",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:5627,Modifiability,layers,layers,5627,"ity. ;  ; TReshapeLayer< Architecture_t > * AddReshapeLayer (size_t depth, size_t height, size_t width, bool flattening);  Function for adding Reshape Layer in the Deep Neural Network, with a given height and width. ;  ; void AddReshapeLayer (TReshapeLayer< Architecture_t > *reshapeLayer);  Function for adding Reshape Layer in the Deep Neural Network, when the layer is already created. ;  ; void Backward (const Tensor_t &input, const Matrix_t &groundTruth, const Matrix_t &weights);  Function that executes the entire backward pass in the network. ;  ; void Clear ();  Remove all layers from the network. ;  ; void Forward (Tensor_t &input, bool applyDropout=false);  Function that executes the entire forward pass in the network. ;  ; size_t GetBatchDepth () const;  ; size_t GetBatchHeight () const;  ; size_t GetBatchSize () const;  Getters. ;  ; size_t GetBatchWidth () const;  ; size_t GetDepth () const;  ; EInitialization GetInitialization () const;  ; size_t GetInputDepth () const;  ; size_t GetInputHeight () const;  ; size_t GetInputWidth () const;  ; Layer_t * GetLayerAt (size_t i);  Get the layer in the vector of layers at position i. ;  ; const Layer_t * GetLayerAt (size_t i) const;  ; std::vector< Layer_t * > & GetLayers ();  ; const std::vector< Layer_t * > & GetLayers () const;  ; ELossFunction GetLossFunction () const;  ; size_t GetOutputWidth () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize ();  DAE functions. ;  ; bool IsTraining () const;  ; Scalar_t Loss (const Matrix_t &groundTruth, const Matrix_t &weights, bool includeRegularization=true) const;  Function for evaluating the loss, based on the activations stored in the last layer. ;  ; Scalar_t Loss (Tensor_t &input, const Matrix_t &groundTruth, const Matrix_t &weights, bool inTraining=false, bool includeRegularization=true);  Function for evaluating the loss, based on the propagation of the given input. ;  ; void Prediction (Matrix_t &predict",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:7706,Modifiability,layers,layers,7706," ;  ; void Prediction (Matrix_t &predictions, EOutputFunction f) const;  Prediction based on activations stored in the last layer. ;  ; void Prediction (Matrix_t &predictions, Tensor_t &input, EOutputFunction f);  Prediction for the given inputs, based on what network learned. ;  ; void Print () const;  Print the Deep Net Info. ;  ; Scalar_t RegularizationTerm () const;  Function for computing the regularizaton term to be added to the loss function .  ; void ResetTraining ();  Function that reset some training flags after looping all the events but not the weights. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function that will update the weights and biases in the layers that contain weights and biases. ;  . Private Member Functions; size_t calculateDimension (int imgDim, int fltDim, int padding, int stride);  ; bool isInteger (Scalar_t x) const;  . Private Attributes; size_t fBatchDepth;  The depth of the batch used for training/testing. ;  ; size_t fBatchHeight;  The height of the batch used for training/testing. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; size_t fBatchWidth;  The width of the batch used for training/testing. ;  ; EInitialization fI;  The initialization method of the network. ;  ; size_t fInputDepth;  The depth of the input. ;  ; size_t fInputHeight;  The height of the input. ;  ; size_t fInputWidth;  The width of the input. ;  ; bool fIsTraining;  Is ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:8581,Modifiability,layers,layers,8581,"WeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function that will update the weights and biases in the layers that contain weights and biases. ;  . Private Member Functions; size_t calculateDimension (int imgDim, int fltDim, int padding, int stride);  ; bool isInteger (Scalar_t x) const;  . Private Attributes; size_t fBatchDepth;  The depth of the batch used for training/testing. ;  ; size_t fBatchHeight;  The height of the batch used for training/testing. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; size_t fBatchWidth;  The width of the batch used for training/testing. ;  ; EInitialization fI;  The initialization method of the network. ;  ; size_t fInputDepth;  The depth of the input. ;  ; size_t fInputHeight;  The height of the input. ;  ; size_t fInputWidth;  The width of the input. ;  ; bool fIsTraining;  Is the network training? ;  ; ELossFunction fJ;  The loss function of the network. ;  ; std::vector< Layer_t * > fLayers;  The layers consisting the DeepNet. ;  ; ERegularization fR;  The regularization used for the network. ;  ; Scalar_t fWeightDecay;  The weight decay factor. ;  . #include <TMVA/DNN/DeepNet.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . using TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 77 of file DeepNet.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . using TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 78 of file DeepNet.h. ◆ Tensor_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . using TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Tensor_t = typename Architecture_t::Tensor_t. Definition at line 76 of file DeepNet.h. Constructor & Destructor Documentation. ◆ TDeepNet() [1",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:18267,Modifiability,layers,layers,18267," TReshapeLayer< Architecture_t > * ; reshapeLayer). Function for adding Reshape Layer in the Deep Neural Network, when the layer is already created. ; Definition at line 870 of file DeepNet.h. ◆ Backward(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Backward ; (; const Tensor_t & ; input, . const Matrix_t & ; groundTruth, . const Matrix_t & ; weights . ). Function that executes the entire backward pass in the network. ; Definition at line 1033 of file DeepNet.h. ◆ calculateDimension(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::calculateDimension ; (; int ; imgDim, . int ; fltDim, . int ; padding, . int ; stride . ). private . Definition at line 421 of file DeepNet.h. ◆ Clear(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Clear ; (; ). inline . Remove all layers from the network. ; Definition at line 334 of file DeepNet.h. ◆ Forward(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Forward ; (; Tensor_t & ; input, . bool ; applyDropout = false . ). Function that executes the entire forward pass in the network. ; Definition at line 896 of file DeepNet.h. ◆ GetBatchDepth(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetBatchDepth ; (; ); const. inline . Definition at line 342 of file DeepNet.h. ◆ GetBatchHeight(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetBatchHeight ; (; ); const. inline . Definition at line 343 of file DeepNet.h. ◆ GetBatchSize(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:21058,Modifiability,layers,layers,21058," . Definition at line 349 of file DeepNet.h. ◆ GetInputDepth(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetInputDepth ; (; ); const. inline . Definition at line 338 of file DeepNet.h. ◆ GetInputHeight(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetInputHeight ; (; ); const. inline . Definition at line 339 of file DeepNet.h. ◆ GetInputWidth(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetInputWidth ; (; ); const. inline . Definition at line 340 of file DeepNet.h. ◆ GetLayerAt() [1/2]. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . Layer_t * TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetLayerAt ; (; size_t ; i). inline . Get the layer in the vector of layers at position i. ; Definition at line 322 of file DeepNet.h. ◆ GetLayerAt() [2/2]. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . const Layer_t * TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetLayerAt ; (; size_t ; i); const. inline . Definition at line 323 of file DeepNet.h. ◆ GetLayers() [1/2]. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . std::vector< Layer_t * > & TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetLayers ; (; ). inline . Definition at line 330 of file DeepNet.h. ◆ GetLayers() [2/2]. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . const std::vector< Layer_t * > & TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::GetLayers ; (; ); const. inline . Definition at line 331 of file DeepNet.h. ◆ GetLossFunction(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . ELossFunction TMVA::DNN::TDeepNet< Architect",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:29083,Modifiability,layers,layers,29083,"ne 360 of file DeepNet.h. ◆ SetLossFunction(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetLossFunction ; (; ELossFunction ; J). inline . Definition at line 364 of file DeepNet.h. ◆ SetRegularization(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetRegularization ; (; ERegularization ; R). inline . Definition at line 366 of file DeepNet.h. ◆ SetWeightDecay(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetWeightDecay ; (; Scalar_t ; weightDecay). inline . Definition at line 367 of file DeepNet.h. ◆ Update(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Update ; (; Scalar_t ; learningRate). Function that will update the weights and biases in the layers that contain weights and biases. . Definition at line 1254 of file DeepNet.h. Member Data Documentation. ◆ fBatchDepth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchDepth. private . The depth of the batch used for training/testing. ; Definition at line 93 of file DeepNet.h. ◆ fBatchHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchHeight. private . The height of the batch used for training/testing. ; Definition at line 94 of file DeepNet.h. ◆ fBatchSize. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchSize. private . Batch size used for training and evaluation. ; Definition at line 88 of file DeepNet.h. ◆ fBatchWidth. template<typename Architecture_t , typename Layer",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:31968,Modifiability,layers,layers,31968,"th. private . The depth of the input. ; Definition at line 89 of file DeepNet.h. ◆ fInputHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fInputHeight. private . The height of the input. ; Definition at line 90 of file DeepNet.h. ◆ fInputWidth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fInputWidth. private . The width of the input. ; Definition at line 91 of file DeepNet.h. ◆ fIsTraining. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . bool TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fIsTraining. private . Is the network training? ; Definition at line 97 of file DeepNet.h. ◆ fJ. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . ELossFunction TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fJ. private . The loss function of the network. ; Definition at line 99 of file DeepNet.h. ◆ fLayers. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . std::vector<Layer_t *> TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fLayers. private . The layers consisting the DeepNet. ; Definition at line 86 of file DeepNet.h. ◆ fR. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . ERegularization TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fR. private . The regularization used for the network. ; Definition at line 101 of file DeepNet.h. ◆ fWeightDecay. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . Scalar_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fWeightDecay. private . The weight decay factor. ; Definition at line 102 of file DeepNet.h. tmva/tmva/inc/TMVA/DNN/DeepNet.h. TMVADNNTDeepNet. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:6489,Safety,predict,predictions,6489,"st;  ; size_t GetInputWidth () const;  ; Layer_t * GetLayerAt (size_t i);  Get the layer in the vector of layers at position i. ;  ; const Layer_t * GetLayerAt (size_t i) const;  ; std::vector< Layer_t * > & GetLayers ();  ; const std::vector< Layer_t * > & GetLayers () const;  ; ELossFunction GetLossFunction () const;  ; size_t GetOutputWidth () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize ();  DAE functions. ;  ; bool IsTraining () const;  ; Scalar_t Loss (const Matrix_t &groundTruth, const Matrix_t &weights, bool includeRegularization=true) const;  Function for evaluating the loss, based on the activations stored in the last layer. ;  ; Scalar_t Loss (Tensor_t &input, const Matrix_t &groundTruth, const Matrix_t &weights, bool inTraining=false, bool includeRegularization=true);  Function for evaluating the loss, based on the propagation of the given input. ;  ; void Prediction (Matrix_t &predictions, EOutputFunction f) const;  Prediction based on activations stored in the last layer. ;  ; void Prediction (Matrix_t &predictions, Tensor_t &input, EOutputFunction f);  Prediction for the given inputs, based on what network learned. ;  ; void Print () const;  Print the Deep Net Info. ;  ; Scalar_t RegularizationTerm () const;  Function for computing the regularizaton term to be added to the loss function .  ; void ResetTraining ();  Function that reset some training flags after looping all the events but not the weights. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; voi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:6619,Safety,predict,predictions,6619," Layer_t * GetLayerAt (size_t i) const;  ; std::vector< Layer_t * > & GetLayers ();  ; const std::vector< Layer_t * > & GetLayers () const;  ; ELossFunction GetLossFunction () const;  ; size_t GetOutputWidth () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize ();  DAE functions. ;  ; bool IsTraining () const;  ; Scalar_t Loss (const Matrix_t &groundTruth, const Matrix_t &weights, bool includeRegularization=true) const;  Function for evaluating the loss, based on the activations stored in the last layer. ;  ; Scalar_t Loss (Tensor_t &input, const Matrix_t &groundTruth, const Matrix_t &weights, bool inTraining=false, bool includeRegularization=true);  Function for evaluating the loss, based on the propagation of the given input. ;  ; void Prediction (Matrix_t &predictions, EOutputFunction f) const;  Prediction based on activations stored in the last layer. ;  ; void Prediction (Matrix_t &predictions, Tensor_t &input, EOutputFunction f);  Prediction for the given inputs, based on what network learned. ;  ; void Print () const;  Print the Deep Net Info. ;  ; Scalar_t RegularizationTerm () const;  Function for computing the regularizaton term to be added to the loss function .  ; void ResetTraining ();  Function that reset some training flags after looping all the events but not the weights. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:24578,Safety,predict,predictions,24578,"_t >::IsTraining ; (; ); const. inline . Definition at line 346 of file DeepNet.h. ◆ Loss() [1/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Loss ; (; const Matrix_t & ; groundTruth, . const Matrix_t & ; weights, . bool ; includeRegularization = true . ); const. Function for evaluating the loss, based on the activations stored in the last layer. ; Definition at line 1263 of file DeepNet.h. ◆ Loss() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Loss ; (; Tensor_t & ; input, . const Matrix_t & ; groundTruth, . const Matrix_t & ; weights, . bool ; inTraining = false, . bool ; includeRegularization = true . ). Function for evaluating the loss, based on the propagation of the given input. ; Definition at line 1279 of file DeepNet.h. ◆ Prediction() [1/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; predictions, . EOutputFunction ; f . ); const. Prediction based on activations stored in the last layer. ; Definition at line 1303 of file DeepNet.h. ◆ Prediction() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; predictions, . Tensor_t & ; input, . EOutputFunction ; f . ). Prediction for the given inputs, based on what network learned. ; Definition at line 1311 of file DeepNet.h. ◆ Print(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Print. Print the Deep Net Info. ; Definition at line 1321 of file DeepNet.h. ◆ RegularizationTerm(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::RegularizationTerm. Function for computing the regularizaton term to be added to the loss function . Definition at line 1289 of file DeepNet.h. ◆ ResetTrai",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:24888,Safety,predict,predictions,24888," = true . ); const. Function for evaluating the loss, based on the activations stored in the last layer. ; Definition at line 1263 of file DeepNet.h. ◆ Loss() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Loss ; (; Tensor_t & ; input, . const Matrix_t & ; groundTruth, . const Matrix_t & ; weights, . bool ; inTraining = false, . bool ; includeRegularization = true . ). Function for evaluating the loss, based on the propagation of the given input. ; Definition at line 1279 of file DeepNet.h. ◆ Prediction() [1/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; predictions, . EOutputFunction ; f . ); const. Prediction based on activations stored in the last layer. ; Definition at line 1303 of file DeepNet.h. ◆ Prediction() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; predictions, . Tensor_t & ; input, . EOutputFunction ; f . ). Prediction for the given inputs, based on what network learned. ; Definition at line 1311 of file DeepNet.h. ◆ Print(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Print. Print the Deep Net Info. ; Definition at line 1321 of file DeepNet.h. ◆ RegularizationTerm(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::RegularizationTerm. Function for computing the regularizaton term to be added to the loss function . Definition at line 1289 of file DeepNet.h. ◆ ResetTraining(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::ResetTraining. Function that reset some training flags after looping all the events but not the weights. ; Definition at line 886 of file DeepNet.h. ◆ SetBatchDepth(). template<typename Archite",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:7977,Testability,test,testing,7977,"unction that reset some training flags after looping all the events but not the weights. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function that will update the weights and biases in the layers that contain weights and biases. ;  . Private Member Functions; size_t calculateDimension (int imgDim, int fltDim, int padding, int stride);  ; bool isInteger (Scalar_t x) const;  . Private Attributes; size_t fBatchDepth;  The depth of the batch used for training/testing. ;  ; size_t fBatchHeight;  The height of the batch used for training/testing. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; size_t fBatchWidth;  The width of the batch used for training/testing. ;  ; EInitialization fI;  The initialization method of the network. ;  ; size_t fInputDepth;  The depth of the input. ;  ; size_t fInputHeight;  The height of the input. ;  ; size_t fInputWidth;  The width of the input. ;  ; bool fIsTraining;  Is the network training? ;  ; ELossFunction fJ;  The loss function of the network. ;  ; std::vector< Layer_t * > fLayers;  The layers consisting the DeepNet. ;  ; ERegularization fR;  The regularization used for the network. ;  ; Scalar_t fWeightDecay;  The weight decay factor. ;  . #include <TMVA/DNN/DeepNet.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . using TMVA::DNN::TDeepNet< Architecture_t,",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:8055,Testability,test,testing,8055,"ts. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function that will update the weights and biases in the layers that contain weights and biases. ;  . Private Member Functions; size_t calculateDimension (int imgDim, int fltDim, int padding, int stride);  ; bool isInteger (Scalar_t x) const;  . Private Attributes; size_t fBatchDepth;  The depth of the batch used for training/testing. ;  ; size_t fBatchHeight;  The height of the batch used for training/testing. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; size_t fBatchWidth;  The width of the batch used for training/testing. ;  ; EInitialization fI;  The initialization method of the network. ;  ; size_t fInputDepth;  The depth of the input. ;  ; size_t fInputHeight;  The height of the input. ;  ; size_t fInputWidth;  The width of the input. ;  ; bool fIsTraining;  Is the network training? ;  ; ELossFunction fJ;  The loss function of the network. ;  ; std::vector< Layer_t * > fLayers;  The layers consisting the DeepNet. ;  ; ERegularization fR;  The regularization used for the network. ;  ; Scalar_t fWeightDecay;  The weight decay factor. ;  . #include <TMVA/DNN/DeepNet.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . using TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 77 of f",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:8201,Testability,test,testing,8201," ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function that will update the weights and biases in the layers that contain weights and biases. ;  . Private Member Functions; size_t calculateDimension (int imgDim, int fltDim, int padding, int stride);  ; bool isInteger (Scalar_t x) const;  . Private Attributes; size_t fBatchDepth;  The depth of the batch used for training/testing. ;  ; size_t fBatchHeight;  The height of the batch used for training/testing. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; size_t fBatchWidth;  The width of the batch used for training/testing. ;  ; EInitialization fI;  The initialization method of the network. ;  ; size_t fInputDepth;  The depth of the input. ;  ; size_t fInputHeight;  The height of the input. ;  ; size_t fInputWidth;  The width of the input. ;  ; bool fIsTraining;  Is the network training? ;  ; ELossFunction fJ;  The loss function of the network. ;  ; std::vector< Layer_t * > fLayers;  The layers consisting the DeepNet. ;  ; ERegularization fR;  The regularization used for the network. ;  ; Scalar_t fWeightDecay;  The weight decay factor. ;  . #include <TMVA/DNN/DeepNet.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . using TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 77 of file DeepNet.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . using TMVA::DNN::TDeepNet< Archit",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:29416,Testability,test,testing,29416,"tecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetRegularization ; (; ERegularization ; R). inline . Definition at line 366 of file DeepNet.h. ◆ SetWeightDecay(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetWeightDecay ; (; Scalar_t ; weightDecay). inline . Definition at line 367 of file DeepNet.h. ◆ Update(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Update ; (; Scalar_t ; learningRate). Function that will update the weights and biases in the layers that contain weights and biases. . Definition at line 1254 of file DeepNet.h. Member Data Documentation. ◆ fBatchDepth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchDepth. private . The depth of the batch used for training/testing. ; Definition at line 93 of file DeepNet.h. ◆ fBatchHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchHeight. private . The height of the batch used for training/testing. ; Definition at line 94 of file DeepNet.h. ◆ fBatchSize. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchSize. private . Batch size used for training and evaluation. ; Definition at line 88 of file DeepNet.h. ◆ fBatchWidth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchWidth. private . The width of the batch used for training/testing. ; Definition at line 95 of file DeepNet.h. ◆ fI. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . EInitialization TMVA::D",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:29692,Testability,test,testing,29692,"e Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetWeightDecay ; (; Scalar_t ; weightDecay). inline . Definition at line 367 of file DeepNet.h. ◆ Update(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Update ; (; Scalar_t ; learningRate). Function that will update the weights and biases in the layers that contain weights and biases. . Definition at line 1254 of file DeepNet.h. Member Data Documentation. ◆ fBatchDepth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchDepth. private . The depth of the batch used for training/testing. ; Definition at line 93 of file DeepNet.h. ◆ fBatchHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchHeight. private . The height of the batch used for training/testing. ; Definition at line 94 of file DeepNet.h. ◆ fBatchSize. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchSize. private . Batch size used for training and evaluation. ; Definition at line 88 of file DeepNet.h. ◆ fBatchWidth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchWidth. private . The width of the batch used for training/testing. ; Definition at line 95 of file DeepNet.h. ◆ fI. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . EInitialization TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fI. private . The initialization method of the network. ; Definition at line 100 of file DeepNet.h. ◆ fInputDepth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Archi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:30231,Testability,test,testing,30231,"late<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchDepth. private . The depth of the batch used for training/testing. ; Definition at line 93 of file DeepNet.h. ◆ fBatchHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchHeight. private . The height of the batch used for training/testing. ; Definition at line 94 of file DeepNet.h. ◆ fBatchSize. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchSize. private . Batch size used for training and evaluation. ; Definition at line 88 of file DeepNet.h. ◆ fBatchWidth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchWidth. private . The width of the batch used for training/testing. ; Definition at line 95 of file DeepNet.h. ◆ fI. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . EInitialization TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fI. private . The initialization method of the network. ; Definition at line 100 of file DeepNet.h. ◆ fInputDepth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fInputDepth. private . The depth of the input. ; Definition at line 89 of file DeepNet.h. ◆ fInputHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fInputHeight. private . The height of the input. ; Definition at line 90 of file DeepNet.h. ◆ fInputWidth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fInputWidth. private",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:6725,Usability,learn,learned,6725," Layer_t * GetLayerAt (size_t i) const;  ; std::vector< Layer_t * > & GetLayers ();  ; const std::vector< Layer_t * > & GetLayers () const;  ; ELossFunction GetLossFunction () const;  ; size_t GetOutputWidth () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize ();  DAE functions. ;  ; bool IsTraining () const;  ; Scalar_t Loss (const Matrix_t &groundTruth, const Matrix_t &weights, bool includeRegularization=true) const;  Function for evaluating the loss, based on the activations stored in the last layer. ;  ; Scalar_t Loss (Tensor_t &input, const Matrix_t &groundTruth, const Matrix_t &weights, bool inTraining=false, bool includeRegularization=true);  Function for evaluating the loss, based on the propagation of the given input. ;  ; void Prediction (Matrix_t &predictions, EOutputFunction f) const;  Prediction based on activations stored in the last layer. ;  ; void Prediction (Matrix_t &predictions, Tensor_t &input, EOutputFunction f);  Prediction for the given inputs, based on what network learned. ;  ; void Print () const;  Print the Deep Net Info. ;  ; Scalar_t RegularizationTerm () const;  Function for computing the regularizaton term to be added to the loss function .  ; void ResetTraining ();  Function that reset some training flags after looping all the events but not the weights. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:7634,Usability,learn,learningRate,7634," ;  ; void Prediction (Matrix_t &predictions, EOutputFunction f) const;  Prediction based on activations stored in the last layer. ;  ; void Prediction (Matrix_t &predictions, Tensor_t &input, EOutputFunction f);  Prediction for the given inputs, based on what network learned. ;  ; void Print () const;  Print the Deep Net Info. ;  ; Scalar_t RegularizationTerm () const;  Function for computing the regularizaton term to be added to the loss function .  ; void ResetTraining ();  Function that reset some training flags after looping all the events but not the weights. ;  ; void SetBatchDepth (size_t batchDepth);  ; void SetBatchHeight (size_t batchHeight);  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetBatchWidth (size_t batchWidth);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInitialization (EInitialization I);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  ; void Update (Scalar_t learningRate);  Function that will update the weights and biases in the layers that contain weights and biases. ;  . Private Member Functions; size_t calculateDimension (int imgDim, int fltDim, int padding, int stride);  ; bool isInteger (Scalar_t x) const;  . Private Attributes; size_t fBatchDepth;  The depth of the batch used for training/testing. ;  ; size_t fBatchHeight;  The height of the batch used for training/testing. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; size_t fBatchWidth;  The width of the batch used for training/testing. ;  ; EInitialization fI;  The initialization method of the network. ;  ; size_t fInputDepth;  The depth of the input. ;  ; size_t fInputHeight;  The height of the input. ;  ; size_t fInputWidth;  The width of the input. ;  ; bool fIsTraining;  Is ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:25005,Usability,learn,learned,25005,"63 of file DeepNet.h. ◆ Loss() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Loss ; (; Tensor_t & ; input, . const Matrix_t & ; groundTruth, . const Matrix_t & ; weights, . bool ; inTraining = false, . bool ; includeRegularization = true . ). Function for evaluating the loss, based on the propagation of the given input. ; Definition at line 1279 of file DeepNet.h. ◆ Prediction() [1/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; predictions, . EOutputFunction ; f . ); const. Prediction based on activations stored in the last layer. ; Definition at line 1303 of file DeepNet.h. ◆ Prediction() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; predictions, . Tensor_t & ; input, . EOutputFunction ; f . ). Prediction for the given inputs, based on what network learned. ; Definition at line 1311 of file DeepNet.h. ◆ Print(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Print. Print the Deep Net Info. ; Definition at line 1321 of file DeepNet.h. ◆ RegularizationTerm(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::RegularizationTerm. Function for computing the regularizaton term to be added to the loss function . Definition at line 1289 of file DeepNet.h. ◆ ResetTraining(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::ResetTraining. Function that reset some training flags after looping all the events but not the weights. ; Definition at line 886 of file DeepNet.h. ◆ SetBatchDepth(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetBatchDept",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html:29012,Usability,learn,learningRate,29012," Architecture_t, Layer_t >::SetInputWidth ; (; size_t ; inputWidth). inline . Definition at line 360 of file DeepNet.h. ◆ SetLossFunction(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetLossFunction ; (; ELossFunction ; J). inline . Definition at line 364 of file DeepNet.h. ◆ SetRegularization(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetRegularization ; (; ERegularization ; R). inline . Definition at line 366 of file DeepNet.h. ◆ SetWeightDecay(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . void TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::SetWeightDecay ; (; Scalar_t ; weightDecay). inline . Definition at line 367 of file DeepNet.h. ◆ Update(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::Update ; (; Scalar_t ; learningRate). Function that will update the weights and biases in the layers that contain weights and biases. . Definition at line 1254 of file DeepNet.h. Member Data Documentation. ◆ fBatchDepth. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchDepth. private . The depth of the batch used for training/testing. ; Definition at line 93 of file DeepNet.h. ◆ fBatchHeight. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchHeight. private . The height of the batch used for training/testing. ; Definition at line 94 of file DeepNet.h. ◆ fBatchSize. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>> . size_t TMVA::DNN::TDeepNet< Architecture_t, Layer_t >::fBatchSize. private . Batch size used for training and evaluation. ; Definition a",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDeepNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDeepNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:725,Energy Efficiency,allocate,allocates,725,". ROOT: TMVA::DNN::TDenseLayer< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TDenseLayer< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDenseLayer< Architecture_t >Generic layer class. ; This generic layer class represents a dense layer of a neural network with a given width n and activation function f. The activation function of each layer is given by \(\mathbf{u} = \mathbf{W}\mathbf{x} + \boldsymbol{\theta}\).; In addition to the weight and bias matrices, each layer allocates memory for its activations and the corresponding input tensor before evaluation of the activation function as well as the gradients of the weights and biases.; The layer provides member functions for the forward propagation of activations through the given layer. ; Definition at line 59 of file DenseLayer.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Public Member Functions;  TDenseLayer (const TDenseLayer &);  Copy Constructor. ;  ;  TDenseLayer (size_t BatchSize, size_t InputWidth, size_t Width, EInitialization init, Scalar_t DropoutProbability, EActivationFunction f, ERegularization reg, Scalar_t weightDecay);  Constructor. ;  ;  TDenseLayer (TDenseLayer< Architecture_t > *layer);  Copy the dense layer provided as a pointer. ;  ;  ~TDenseLayer ();  Destructor. ;  ; virtual void AddWeightsXMLTo (void *parent);  Writes the information and the weights about the layer in an XML node. ;  ; void Backward (Tensor_t &gradients_backward, const Tensor_t &activations_backward);  Compute weight, bias and activation gradients. ;  ; void Forward (Tensor_t &input, bool applyDropout=false);  Compute activation of the layer fo",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:2638,Modifiability,inherit,inherited,2638,"oid *parent);  Writes the information and the weights about the layer in an XML node. ;  ; void Backward (Tensor_t &gradients_backward, const Tensor_t &activations_backward);  Compute weight, bias and activation gradients. ;  ; void Forward (Tensor_t &input, bool applyDropout=false);  Compute activation of the layer for the given input. ;  ; EActivationFunction GetActivationFunction () const;  ; Scalar_t GetDropoutProbability () const;  Getters. ;  ; Tensor_t & GetInputActivation ();  ; const Tensor_t & GetInputActivation () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Print () const;  std::vector<Matrix_t> &inp1, std::vector<Matrix_t> &inp2); ;  ; virtual void ReadWeightsFromXML (void *parent);  Read the information and the weights about the layer from XML node. ;  ; virtual void SetDropoutProbability (Scalar_t dropoutProbability);  Set dropout probabilities. ;  ;  Public Member Functions inherited from TMVA::DNN::VGeneralLayer< Architecture_t >;  VGeneralLayer (const VGeneralLayer &);  Copy Constructor. ;  ;  VGeneralLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t WeightsNSlices, size_t WeightsNRows, size_t WeightsNCols, size_t BiasesNSlices, size_t BiasesNRows, size_t BiasesNCols, size_t OutputNSlices, size_t OutputNRows, size_t OutputNCols, EInitialization Init);  Constructor. ;  ;  VGeneralLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t WeightsNSlices, std::vector< size_t > WeightsNRows, std::vector< size_t > WeightsNCols, size_t BiasesNSlices, std::vector< size_t > BiasesNRows, std::vector< size_t > BiasesNCols, size_t OutputNSlices, size_t OutputNRows, size_t OutputNCols, EInitialization Init);  General Constructor with different weights dimension. ;  ;  VGeneralLayer (VGeneralLayer< Architecture_t > *layer);  Copy the layer provided as a ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:8080,Modifiability,inherit,inherited,8080,");  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropoutProbability;  Probability that an input is active. ;  ; EActivationFunction fF;  Activation function of the layer. ;  ; Tensor_t fInputActivation;  output of GEMM and input to activation function ;  ; ERegularization fReg;  The regularization method. ;  ; Scalar_t fWeightDecay;  The weight decay. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; std::vector< Matrix_t > fBiases;  The biases associated to the layer. ;  ; std::vector< Matrix_t > fBiasGradients;  Gradients w.r.t. the bias values of the layer. ;  ; size_t fDepth;  The depth of the layer. ;  ; size_t fHeight;  The height of the layer. ;  ; EInitialization fInit;  The initialization method. ;  ; size_t fInputDepth;  The depth of the previous layer or input. ;  ; size_t fInputHeight;  The height of the previous layer or input. ;  ; size_t fInputWidth;  The width of the previous layer or input. ;  ; bool fIsTraining;  Flag indicating the mode. ;  ; Tensor_t fOutput;  Activations of this layer. ;  ; std::vector< Matrix_t > fWeightGradients;  Gradients w.r.t. the weights of the layer. ;  ; std::vector< Matrix_t > fWeights;  The weights associated to the layer. ;  ; size_t fWidth;  The ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:6571,Usability,learn,learningRate,6571,"dients ();  ; const std::vector< Matrix_t > & GetWeightGradients () const;  ; Matrix_t & GetWeightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *nam",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:6629,Usability,learn,learning,6629,"dients ();  ; const std::vector< Matrix_t > & GetWeightGradients () const;  ; Matrix_t & GetWeightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *nam",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:6729,Usability,learn,learningRate,6729,"size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropout",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:6793,Usability,learn,learning,6793,"size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropout",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:6900,Usability,learn,learningRate,6900,"ven initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropoutProbability;  Probability that an input is active. ;  ; EActivationFunction fF;  Activation function of the layer. ;  ; Tensor_t fInputActivation;  output of GEMM and input to a",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:6982,Usability,learn,learning,6982,"ven initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropoutProbability;  Probability that an input is active. ;  ; EActivationFunction fF;  Activation function of the layer. ;  ; Tensor_t fInputActivation;  output of GEMM and input to a",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:7093,Usability,learn,learningRate,7093,"fter a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropoutProbability;  Probability that an input is active. ;  ; EActivationFunction fF;  Activation function of the layer. ;  ; Tensor_t fInputActivation;  output of GEMM and input to activation function ;  ; ERegularization fReg;  The regularization method. ;  ; Scalar_t fWeightDecay;  The weight decay. ;  . Additional Inherited Members;  Protected Attributes inherited from",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:7177,Usability,learn,learning,7177,"fter a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropoutProbability;  Probability that an input is active. ;  ; EActivationFunction fF;  Activation function of the layer. ;  ; Tensor_t fInputActivation;  output of GEMM and input to activation function ;  ; ERegularization fReg;  The regularization method. ;  ; Scalar_t fWeightDecay;  The weight decay. ;  . Additional Inherited Members;  Protected Attributes inherited from",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:7280,Usability,learn,learningRate,7280,"::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropoutProbability;  Probability that an input is active. ;  ; EActivationFunction fF;  Activation function of the layer. ;  ; Tensor_t fInputActivation;  output of GEMM and input to activation function ;  ; ERegularization fReg;  The regularization method. ;  ; Scalar_t fWeightDecay;  The weight decay. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html:7345,Usability,learn,learning,7345,"::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; Architecture_t::ActivationDescriptor_t fActivationDesc;  ; Tensor_t fDerivatives;  activation function gradient ;  ; Scalar_t fDropoutProbability;  Probability that an input is active. ;  ; EActivationFunction fF;  Activation function of the layer. ;  ; Tensor_t fInputActivation;  output of GEMM and input to activation function ;  ; ERegularization fReg;  The regularization method. ;  ; Scalar_t fWeightDecay;  The weight decay. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDenseLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1081,Availability,error,error,1081,"ntDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1153,Availability,error,error,1153,"ntDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1305,Availability,error,error,1305,"ecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Arch",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1356,Availability,error,error,1356,"ecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Arch",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:3720,Availability,error,error,3720,"epNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; Scalar_t StepReducedWeightsLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/DLMinimizers.h>; Member Typedef Documentation. ◆ DeepNet_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::DeepNet_t = TDeepNet<Architecture_t>. Definition at line 67 of file DLMinimizers.h. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 69 of file DLMinimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 68 of file DLMinimizers.h. Constructor & Destructor Documentation. ◆ TDLGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TDLGradientDescent< Architecture_t >::TDLGradientDescent. Definition at line 163 of file DLMinimizers.h. ◆ TDLGr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6195,Availability,error,error,6195,late<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 147 of file DLMinimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 149 of file DLMinimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6267,Availability,error,error,6267,late<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 147 of file DLMinimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 149 of file DLMinimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6578,Availability,error,error,6578,. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Setters. ; Definition at line 153 of file DLMinimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 155 of file DLMinimizers.h. ◆ SetTestInterval(). t,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6629,Availability,error,error,6629,. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Setters. ; Definition at line 153 of file DLMinimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 155 of file DLMinimizers.h. ◆ SetTestInterval(). t,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:11668,Availability,error,error,11668,"rms of the first layer for compatibility with the previous implementation. ; Definition at line 193 of file DLMinimizers.h. ◆ StepReducedWeightsLoss(). template<typename Architecture_t > . auto TMVA::DNN::TDLGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. ; Definition at line 224 of file DLMinimizers.h. Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 72 of file DLMinimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the curr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:11937,Availability,error,error,11937," deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. ; Definition at line 224 of file DLMinimizers.h. Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 72 of file DLMinimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 73 of file DLMinimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 80 of file DLMini",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:13122,Availability,error,error,13122,vate . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 73 of file DLMinimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 80 of file DLMinimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 78 of file DLMinimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 79 of file DLMinimizers.h. tmva/tmva/inc/TMVA/DNN/DLMinimizers.h. TMVADNNTDLGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:2853,Integrability,synchroniz,synchronization,2853,"s);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses momentum. ;  ; void StepNesterov (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; void StepReducedWeights (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; Scalar_t StepReducedWeightsLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the mo",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:9354,Integrability,synchroniz,synchronization,9354," line 182 of file DLMinimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Step ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches . ). Perform multiple optimization steps simultaneously. ; Performs the backprop algorithm on the input batches given in batches on the neural networks given in nets. The forward and backward propagation steps are executed in an interleaving manner in order to exploit potential batch-level parallelism for asynchronous device calls. ; Definition at line 246 of file DLMinimizers.h. ◆ StepLoss(). template<typename Architecture_t > . auto TMVA::DNN::TDLGradientDescent< Architecture_t >::StepLoss ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Same as Step(...) but also evaluate the loss on the given training data. ; Note that this requires synchronization between host and device. ; Definition at line 212 of file DLMinimizers.h. ◆ StepMomentum(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::StepMomentum ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches, . Scalar_t ; momentum . ). Same as the Step(...) method for multiple batches but uses momentum. ; Definition at line 256 of file DLMinimizers.h. ◆ StepNesterov(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::StepNesterov ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches, . Scalar_t ; momentum . ). Same as the Step(...) method for multiple batches but uses Nesterov momentum. ; Definition at line 266 of file DLMinimizers.h. ◆ StepReducedWeights(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::StepReduc",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:10563,Integrability,synchroniz,synchronization,10563,"e_t >::StepMomentum ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches, . Scalar_t ; momentum . ). Same as the Step(...) method for multiple batches but uses momentum. ; Definition at line 256 of file DLMinimizers.h. ◆ StepNesterov(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::StepNesterov ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches, . Scalar_t ; momentum . ). Same as the Step(...) method for multiple batches but uses Nesterov momentum. ; Definition at line 266 of file DLMinimizers.h. ◆ StepReducedWeights(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::StepReducedWeights ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ; Trains the weights of each layer, but only the bias terms of the first layer for compatibility with the previous implementation. ; Definition at line 193 of file DLMinimizers.h. ◆ StepReducedWeightsLoss(). template<typename Architecture_t > . auto TMVA::DNN::TDLGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. ; Definition at line 224 of file DLMinimizers.h. Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 72 of file DLMinimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Archi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:11124,Integrability,synchroniz,synchronization,11124,"method for multiple batches but uses Nesterov momentum. ; Definition at line 266 of file DLMinimizers.h. ◆ StepReducedWeights(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::StepReducedWeights ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ; Trains the weights of each layer, but only the bias terms of the first layer for compatibility with the previous implementation. ; Definition at line 193 of file DLMinimizers.h. ◆ StepReducedWeightsLoss(). template<typename Architecture_t > . auto TMVA::DNN::TDLGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. ; Definition at line 224 of file DLMinimizers.h. Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 72 of file DLMinimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1792,Performance,optimiz,optimization,1792,"ent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses momentum. ;  ; void StepNesterov (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; void StepReducedWeights (DeepNet_t &de",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1970,Performance,optimiz,optimization,1970,"or () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses momentum. ;  ; void StepNesterov (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; void StepReducedWeights (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; Scalar_t StepReducedWeigh",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:3537,Performance,perform,performed,3537,"ure_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; void StepReducedWeights (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; Scalar_t StepReducedWeightsLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/DLMinimizers.h>; Member Typedef Documentation. ◆ DeepNet_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::DeepNet_t = TDeepNet<Architecture_t>. Definition at line 67 of file DLMinimizers.h. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 69 of file DLMinimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 68 of file DLMinimizers.h. Constructor & Destructor Documentation. ◆ TDLGradientDescent() [1/2]. template<",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:8051,Performance,optimiz,optimization,8051,"::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Setters. ; Definition at line 153 of file DLMinimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 155 of file DLMinimizers.h. ◆ SetTestInterval(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetTestInterval ; (; size_t ; interval). inline . Definition at line 154 of file DLMinimizers.h. ◆ Step() [1/2]. template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Step ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Perform a single optimization step on a given batch. ; Propagates the input matrix forward through the net, evaluates the loss and propagates the gradients backward through the net. The computed gradients are scaled by the learning rate \(\alpha\) and subtracted from the weights and bias values of each layer. ; Definition at line 182 of file DLMinimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Step ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches . ). Perform multiple optimization steps simultaneously. ; Performs the backprop algorithm on the input batches given in batches on the neural networks given in nets. The forward and backward propagation steps are executed in an interleaving manner in order to exploit potential batch-level parallelism for asynchronous device calls. ; Definition at line 246 of file DLMinimizers.h. ◆ StepLoss(). template<typename Architecture_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:8652,Performance,optimiz,optimization,8652,"DLGradientDescent< Architecture_t >::SetTestInterval ; (; size_t ; interval). inline . Definition at line 154 of file DLMinimizers.h. ◆ Step() [1/2]. template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Step ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Perform a single optimization step on a given batch. ; Propagates the input matrix forward through the net, evaluates the loss and propagates the gradients backward through the net. The computed gradients are scaled by the learning rate \(\alpha\) and subtracted from the weights and bias values of each layer. ; Definition at line 182 of file DLMinimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Step ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches . ). Perform multiple optimization steps simultaneously. ; Performs the backprop algorithm on the input batches given in batches on the neural networks given in nets. The forward and backward propagation steps are executed in an interleaving manner in order to exploit potential batch-level parallelism for asynchronous device calls. ; Definition at line 246 of file DLMinimizers.h. ◆ StepLoss(). template<typename Architecture_t > . auto TMVA::DNN::TDLGradientDescent< Architecture_t >::StepLoss ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Same as Step(...) but also evaluate the loss on the given training data. ; Note that this requires synchronization between host and device. ; Definition at line 212 of file DLMinimizers.h. ◆ StepMomentum(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::StepMomentum ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< A",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:12632,Performance,perform,performed,12632,vate . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 73 of file DLMinimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 80 of file DLMinimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 78 of file DLMinimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 79 of file DLMinimizers.h. tmva/tmva/inc/TMVA/DNN/DLMinimizers.h. TMVADNNTDLGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:786,Testability,test,testInterval,786,". ROOT: TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneous",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1076,Testability,test,test,1076,"ntDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1148,Testability,test,test,1148,"ntDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1240,Testability,test,testError,1240,"ecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Arch",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1300,Testability,test,test,1300,"ecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Arch",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:1351,Testability,test,test,1351,"ecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; Scalar_t StepLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; void StepMomentum (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Arch",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:3640,Testability,test,test,3640,"tiple batches but uses Nesterov momentum. ;  ; void StepReducedWeights (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; Scalar_t StepReducedWeightsLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/DLMinimizers.h>; Member Typedef Documentation. ◆ DeepNet_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::DeepNet_t = TDeepNet<Architecture_t>. Definition at line 67 of file DLMinimizers.h. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 69 of file DLMinimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 68 of file DLMinimizers.h. Constructor & Destructor Documentation. ◆ TDLGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TDLGradientDescent< Architecture_t >::T",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:3715,Testability,test,test,3715,"epNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; Scalar_t StepReducedWeightsLoss (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/DLMinimizers.h>; Member Typedef Documentation. ◆ DeepNet_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::DeepNet_t = TDeepNet<Architecture_t>. Definition at line 67 of file DLMinimizers.h. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 69 of file DLMinimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 68 of file DLMinimizers.h. Constructor & Destructor Documentation. ◆ TDLGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TDLGradientDescent< Architecture_t >::TDLGradientDescent. Definition at line 163 of file DLMinimizers.h. ◆ TDLGr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:4886,Testability,test,testInterval,4886,". template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::DeepNet_t = TDeepNet<Architecture_t>. Definition at line 67 of file DLMinimizers.h. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 69 of file DLMinimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 68 of file DLMinimizers.h. Constructor & Destructor Documentation. ◆ TDLGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TDLGradientDescent< Architecture_t >::TDLGradientDescent. Definition at line 163 of file DLMinimizers.h. ◆ TDLGradientDescent() [2/2]. template<typename Architecture_t > . TMVA::DNN::TDLGradientDescent< Architecture_t >::TDLGradientDescent ; (; Scalar_t ; learningRate, . size_t ; convergenceSteps, . size_t ; testInterval . ). Definition at line 172 of file DLMinimizers.h. Member Function Documentation. ◆ GetConvergenceCount(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceCount ; (; ); const. inline . Getters. ; Definition at line 146 of file DLMinimizers.h. ◆ GetConvergenceSteps(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 147 of file DLMinimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 149 of file DLMinimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6190,Testability,test,test,6190,late<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 147 of file DLMinimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 149 of file DLMinimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6262,Testability,test,test,6262,late<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 147 of file DLMinimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 149 of file DLMinimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6514,Testability,test,testError,6514,VA::DNN::TDLGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 149 of file DLMinimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Setters. ; Definition at line 153 of file DLMinimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Archit,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6573,Testability,test,test,6573,. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Setters. ; Definition at line 153 of file DLMinimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 155 of file DLMinimizers.h. ◆ SetTestInterval(). t,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:6624,Testability,test,test,6624,. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMinimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 148 of file DLMinimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged. Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 276 of file DLMinimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TDLGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 290 of file DLMinimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 89 of file DLMinimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 156 of file DLMinimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Setters. ; Definition at line 153 of file DLMinimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 155 of file DLMinimizers.h. ◆ SetTestInterval(). t,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:11663,Testability,test,test,11663,"rms of the first layer for compatibility with the previous implementation. ; Definition at line 193 of file DLMinimizers.h. ◆ StepReducedWeightsLoss(). template<typename Architecture_t > . auto TMVA::DNN::TDLGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. ; Definition at line 224 of file DLMinimizers.h. Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 72 of file DLMinimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the curr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:11932,Testability,test,test,11932," deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. ; Definition at line 224 of file DLMinimizers.h. Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 72 of file DLMinimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 73 of file DLMinimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 80 of file DLMini",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:12887,Testability,test,test,12887,vate . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 73 of file DLMinimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 80 of file DLMinimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 78 of file DLMinimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 79 of file DLMinimizers.h. tmva/tmva/inc/TMVA/DNN/DLMinimizers.h. TMVADNNTDLGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:13117,Testability,test,test,13117,vate . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 76 of file DLMinimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 74 of file DLMinimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 81 of file DLMinimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 82 of file DLMinimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 73 of file DLMinimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 80 of file DLMinimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 78 of file DLMinimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 79 of file DLMinimizers.h. tmva/tmva/inc/TMVA/DNN/DLMinimizers.h. TMVADNNTDLGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:740,Usability,learn,learningRate,740,". ROOT: TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TDLGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TDLGradientDescent< Architecture_t >; Definition at line 65 of file DLMinimizers.h. Public Types; using DeepNet_t = TDeepNet< Architecture_t >;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TDLGradientDescent ();  ;  TDLGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  Getters. ;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  Setters. ;  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; void Step (DeepNet_t &deepNet, std::vector< Matrix_t > &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; void Step (DeepNet_t &master, std::vector< DeepNet_t > &nets, std::vector< TTensorBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneous",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:4832,Usability,learn,learningRate,4832,"ng loss. ;  . #include <TMVA/DNN/DLMinimizers.h>; Member Typedef Documentation. ◆ DeepNet_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::DeepNet_t = TDeepNet<Architecture_t>. Definition at line 67 of file DLMinimizers.h. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 69 of file DLMinimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TDLGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 68 of file DLMinimizers.h. Constructor & Destructor Documentation. ◆ TDLGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TDLGradientDescent< Architecture_t >::TDLGradientDescent. Definition at line 163 of file DLMinimizers.h. ◆ TDLGradientDescent() [2/2]. template<typename Architecture_t > . TMVA::DNN::TDLGradientDescent< Architecture_t >::TDLGradientDescent ; (; Scalar_t ; learningRate, . size_t ; convergenceSteps, . size_t ; testInterval . ). Definition at line 172 of file DLMinimizers.h. Member Function Documentation. ◆ GetConvergenceCount(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceCount ; (; ); const. inline . Getters. ; Definition at line 146 of file DLMinimizers.h. ◆ GetConvergenceSteps(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 147 of file DLMinimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 149 of file DLMinimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TDLGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 150 of file DLMi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html:8257,Usability,learn,learning,8257,"enceSteps ; (; size_t ; steps). inline . Setters. ; Definition at line 153 of file DLMinimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 155 of file DLMinimizers.h. ◆ SetTestInterval(). template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::SetTestInterval ; (; size_t ; interval). inline . Definition at line 154 of file DLMinimizers.h. ◆ Step() [1/2]. template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Step ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Perform a single optimization step on a given batch. ; Propagates the input matrix forward through the net, evaluates the loss and propagates the gradients backward through the net. The computed gradients are scaled by the learning rate \(\alpha\) and subtracted from the weights and bias values of each layer. ; Definition at line 182 of file DLMinimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . void TMVA::DNN::TDLGradientDescent< Architecture_t >::Step ; (; DeepNet_t & ; master, . std::vector< DeepNet_t > & ; nets, . std::vector< TTensorBatch< Architecture_t > > & ; batches . ). Perform multiple optimization steps simultaneously. ; Performs the backprop algorithm on the input batches given in batches on the neural networks given in nets. The forward and backward propagation steps are executed in an interleaving manner in order to exploit potential batch-level parallelism for asynchronous device calls. ; Definition at line 246 of file DLMinimizers.h. ◆ StepLoss(). template<typename Architecture_t > . auto TMVA::DNN::TDLGradientDescent< Architecture_t >::StepLoss ; (; DeepNet_t & ; deepNet, . std::vector< Matrix_t > & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Same as Step(...) but als",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TDLGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1008,Availability,error,error,1008,". ROOT: TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1080,Availability,error,error,1080,". ROOT: TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1232,Availability,error,error,1232,"tributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &o",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1283,Availability,error,error,1283,"tributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &o",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:5018,Availability,error,error,5018,"e_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/Minimizers.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 59 of file Minimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 58 of file Minimizers.h. Constructor & Destructor Documentation. ◆ TGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent. Definition at line 175 of file Minimizers.h. ◆ TGradientDescent() [2/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent ; (; Scalar_t ; learningRate, . size_t ; convergenceSteps, . size_t ; testInterval . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7254,Availability,error,error,7254,s(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 160 of file Minimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 162 of file Minimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGrad,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7326,Availability,error,error,7326,s(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 160 of file Minimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 162 of file Minimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGrad,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7642,Availability,error,error,7642,GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Definition at line 165 of file Minimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 167 of file Minimizers.h. ◆ SetTestInterval(). template<typename Architecture,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7693,Availability,error,error,7693,GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Definition at line 165 of file Minimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 167 of file Minimizers.h. ◆ SetTestInterval(). template<typename Architecture,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:15032,Availability,error,error,15032," ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads . ); -> Scalar_t. Definition at line 259 of file Minimizers.h. ◆ TrainMomentum() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads = 1 . ). Same as Train(...) but uses the given momentum. . Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 62 of file Minimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training ses",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:15297,Availability,error,error,15297,"cent< Architecture_t >::TrainMomentum ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads = 1 . ). Same as Train(...) but uses the given momentum. . Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 62 of file Minimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 63 of file Minimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 70 of file Minimizers.h. ◆ fTestI",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:16462,Availability,error,error,16462,ecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 63 of file Minimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 70 of file Minimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 68 of file Minimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 69 of file Minimizers.h. tmva/tmva/inc/TMVA/DNN/Minimizers.h. TMVADNNTGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:2937,Integrability,synchroniz,synchronization,2937," Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Net_t > ; void StepMomentum (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses momentum. ;  ; template<typename Net_t > ; void StepNesterov (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; template<typename Net_t > ; void StepReducedWeights (Net_t &net, Matrix_t &input, const Matrix_t &output);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; template<typename Net_t > ; Scalar_t StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  ; template<typename Net_t > ; auto StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; auto Train (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t Train (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, size_t nThreads=1);  Train the given net using the given training input data (events), training output data (labels), test input data (events), test ou",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:10427,Integrability,synchroniz,synchronization,10427,"ine 374 of file Minimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Step ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). inline . Perform a single optimization step on a given batch. ; Propagates the input matrix forward through the net, evaluates the loss and propagates the gradients backward through the net. The computed gradients are scaled by the learning rate \(\alpha\) and subtracted from the weights and bias values of each layer. ; Definition at line 331 of file Minimizers.h. ◆ StepLoss() [1/2]. template<typename Architecture_t > . template<typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::StepLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Same as Step(...) but also evaluate the loss on the given training data. ; Note that this requires synchronization between host and device. . ◆ StepLoss() [2/2]. template<typename Architecture_t > . template<typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::StepLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline . Definition at line 352 of file Minimizers.h. ◆ StepMomentum(). template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::StepMomentum ; (; Net_t & ; master, . std::vector< Net_t > & ; nets, . std::vector< TBatch< Architecture_t > > & ; batches, . Scalar_t ; momentum . ). inline . Same as the Step(...) method for multiple batches but uses momentum. ; Definition at line 438 of file Minimizers.h. ◆ StepNesterov(). template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::StepNesterov ; (; Net_t & ; master, . std::vector< Net_t > & ; nets, . std::vector< TBatch< Architec",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:11927,Integrability,synchroniz,synchronization,11927,"tecture_t >::StepMomentum ; (; Net_t & ; master, . std::vector< Net_t > & ; nets, . std::vector< TBatch< Architecture_t > > & ; batches, . Scalar_t ; momentum . ). inline . Same as the Step(...) method for multiple batches but uses momentum. ; Definition at line 438 of file Minimizers.h. ◆ StepNesterov(). template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::StepNesterov ; (; Net_t & ; master, . std::vector< Net_t > & ; nets, . std::vector< TBatch< Architecture_t > > & ; batches, . Scalar_t ; momentum . ). inline . Same as the Step(...) method for multiple batches but uses Nesterov momentum. ; Definition at line 528 of file Minimizers.h. ◆ StepReducedWeights(). template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeights ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output . ). inline . Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ; Trains the weights of each layer, but only the bias terms of the first layer for compatibility with the previous implementation. ; Definition at line 617 of file Minimizers.h. ◆ StepReducedWeightsLoss() [1/2]. template<typename Architecture_t > . template<typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. . ◆ StepReducedWeightsLoss() [2/2]. template<typename Architecture_t > . template<typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline . Definition at line 642 of file Minimizers.h. ◆ Train() [1/2]. template<typen",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:12499,Integrability,synchroniz,synchronization,12499,"tep(...) method for multiple batches but uses Nesterov momentum. ; Definition at line 528 of file Minimizers.h. ◆ StepReducedWeights(). template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeights ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output . ). inline . Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ; Trains the weights of each layer, but only the bias terms of the first layer for compatibility with the previous implementation. ; Definition at line 617 of file Minimizers.h. ◆ StepReducedWeightsLoss() [1/2]. template<typename Architecture_t > . template<typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. . ◆ StepReducedWeightsLoss() [2/2]. template<typename Architecture_t > . template<typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline . Definition at line 642 of file Minimizers.h. ◆ Train() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads . ); -> Scalar_t. Definition at line 196 of file Minimizers.h. ◆ Train() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; Tes",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1727,Performance,optimiz,optimization,1727,"re_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Net_t > ; void StepMomentum (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses momentum. ;  ; template<typename Net_t > ; void StepNesterov (Net_t &master, ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1903,Performance,optimiz,optimization,1903,"onst;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Net_t > ; void StepMomentum (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses momentum. ;  ; template<typename Net_t > ; void StepNesterov (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; template<typename Net_t > ; void StepReducedWeights (Net_t &net, Matrix_t &input, con",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:4835,Performance,perform,performed,4835," input data (events), test output data (labels). ;  ; template<typename Data_t , typename Net_t > ; auto TrainMomentum (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/Minimizers.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 59 of file Minimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 58 of file Minimizers.h. Constructor & Destructor Documentation. ◆ TGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent. Definition at line 175 of file Minimizers.h. ◆ TGradientDescent() [2/2]. template<typename Architecture_t > ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:9104,Performance,optimiz,optimization,9104,"t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Definition at line 165 of file Minimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 167 of file Minimizers.h. ◆ SetTestInterval(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetTestInterval ; (; size_t ; interval). inline . Definition at line 166 of file Minimizers.h. ◆ Step() [1/2]. template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Step ; (; Net_t & ; master, . std::vector< Net_t > & ; nets, . std::vector< TBatch< Architecture_t > > & ; batches . ). inline . Perform multiple optimization steps simultaneously. ; Performs the backprop algorithm on the input batches given in batches on the neural networks given in nets. The forward and backward propagation steps are executed in an interleaving manner in order to exploit potential batch-level parallelism for asynchronous device calls. ; Definition at line 374 of file Minimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Step ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). inline . Perform a single optimization step on a given batch. ; Propagates the input matrix forward through the net, evaluates the loss and propagates the gradients backward through the net. The computed gradients are scaled by the learning rate \(\alpha\) and subtracted from the weights and bias values of each layer. ; Definition at line 331 of file Minimizers.h. ◆ StepLoss() [1/2]. template<typename A",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:9732,Performance,optimiz,optimization,9732,"erval ; (; size_t ; interval). inline . Definition at line 166 of file Minimizers.h. ◆ Step() [1/2]. template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Step ; (; Net_t & ; master, . std::vector< Net_t > & ; nets, . std::vector< TBatch< Architecture_t > > & ; batches . ). inline . Perform multiple optimization steps simultaneously. ; Performs the backprop algorithm on the input batches given in batches on the neural networks given in nets. The forward and backward propagation steps are executed in an interleaving manner in order to exploit potential batch-level parallelism for asynchronous device calls. ; Definition at line 374 of file Minimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Step ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). inline . Perform a single optimization step on a given batch. ; Propagates the input matrix forward through the net, evaluates the loss and propagates the gradients backward through the net. The computed gradients are scaled by the learning rate \(\alpha\) and subtracted from the weights and bias values of each layer. ; Definition at line 331 of file Minimizers.h. ◆ StepLoss() [1/2]. template<typename Architecture_t > . template<typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::StepLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Same as Step(...) but also evaluate the loss on the given training data. ; Note that this requires synchronization between host and device. . ◆ StepLoss() [2/2]. template<typename Architecture_t > . template<typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::StepLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:15980,Performance,perform,performed,15980,ecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 63 of file Minimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 70 of file Minimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 68 of file Minimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 69 of file Minimizers.h. tmva/tmva/inc/TMVA/DNN/Minimizers.h. TMVADNNTGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:725,Testability,test,testInterval,725,". ROOT: TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1003,Testability,test,test,1003,". ROOT: TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1075,Testability,test,test,1075,". ROOT: TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1167,Testability,test,testError,1167,"tributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &o",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1227,Testability,test,test,1227,"tributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &o",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:1278,Testability,test,test,1278,"tributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Same as Step(...) but also evaluate the loss on the given training data. ;  ; template<typename Net_t > ; auto StepLoss (Net_t &net, Matrix_t &input, const Matrix_t &o",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:3461,Testability,test,testData,3461,"ypename Net_t > ; void StepNesterov (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; template<typename Net_t > ; void StepReducedWeights (Net_t &net, Matrix_t &input, const Matrix_t &output);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; template<typename Net_t > ; Scalar_t StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  ; template<typename Net_t > ; auto StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; auto Train (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t Train (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, size_t nThreads=1);  Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). ;  ; template<typename Data_t , typename Net_t > ; auto TrainMomentum (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training ep",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:3831,Testability,test,test,3831,"ypename Net_t > ; void StepNesterov (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; template<typename Net_t > ; void StepReducedWeights (Net_t &net, Matrix_t &input, const Matrix_t &output);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; template<typename Net_t > ; Scalar_t StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  ; template<typename Net_t > ; auto StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; auto Train (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t Train (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, size_t nThreads=1);  Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). ;  ; template<typename Data_t , typename Net_t > ; auto TrainMomentum (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training ep",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:3857,Testability,test,test,3857,"ypename Net_t > ; void StepNesterov (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches, Scalar_t momentum);  Same as the Step(...) method for multiple batches but uses Nesterov momentum. ;  ; template<typename Net_t > ; void StepReducedWeights (Net_t &net, Matrix_t &input, const Matrix_t &output);  Does not evaluate the loss and therefore not trigger a possible synchronization with the device. ;  ; template<typename Net_t > ; Scalar_t StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Similar to StepReducedWeights(...) but also evaluates the loss. ;  ; template<typename Net_t > ; auto StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; auto Train (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t Train (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, size_t nThreads=1);  Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). ;  ; template<typename Data_t , typename Net_t > ; auto TrainMomentum (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training ep",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:4022,Testability,test,testData,4022,"ilar to StepReducedWeights(...) but also evaluates the loss. ;  ; template<typename Net_t > ; auto StepReducedWeightsLoss (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; auto Train (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t Train (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, size_t nThreads=1);  Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). ;  ; template<typename Data_t , typename Net_t > ; auto TrainMomentum (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <T",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:4938,Testability,test,test,4938,"t , typename Net_t > ; auto TrainMomentum (const Data_t &trainingData, size_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/Minimizers.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 59 of file Minimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 58 of file Minimizers.h. Constructor & Destructor Documentation. ◆ TGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent. Definition at line 175 of file Minimizers.h. ◆ TGradientDescent() [2/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent ; (; Scalar_",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:5013,Testability,test,test,5013,"e_t nTrainingSamples, const Data_t &testData, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads) -> Scalar_t;  ; template<typename Data_t , typename Net_t > ; Scalar_t TrainMomentum (const Data_t &TrainingDataIn, size_t nTrainingSamples, const Data_t &TestDataIn, size_t nTestSamples, Net_t &net, Scalar_t momentum, size_t nThreads=1);  Same as Train(...) but uses the given momentum. ;  . Private Attributes; size_t fBatchSize;  Batch size to use for the training. ;  ; size_t fConvergenceCount;  Current number of training epochs without. ;  ; size_t fConvergenceSteps;  Number of training epochs without considerable. ;  ; Scalar_t fLearningRate;  Learning rate \(\alpha\). ;  ; Scalar_t fMinimumError;  The minimum loss achieved on the training set during the current training session. ;  ; size_t fStepCount;  Number of steps performed in the current training session. ;  ; Scalar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/Minimizers.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 59 of file Minimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 58 of file Minimizers.h. Constructor & Destructor Documentation. ◆ TGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent. Definition at line 175 of file Minimizers.h. ◆ TGradientDescent() [2/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent ; (; Scalar_t ; learningRate, . size_t ; convergenceSteps, . size_t ; testInterval . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:5971,Testability,test,testInterval,5971,"l;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/Minimizers.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 59 of file Minimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 58 of file Minimizers.h. Constructor & Destructor Documentation. ◆ TGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent. Definition at line 175 of file Minimizers.h. ◆ TGradientDescent() [2/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent ; (; Scalar_t ; learningRate, . size_t ; convergenceSteps, . size_t ; testInterval . ). Definition at line 185 of file Minimizers.h. Member Function Documentation. ◆ GetConvergenceCount(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceCount ; (; ); const. inline . Definition at line 159 of file Minimizers.h. ◆ GetConvergenceSteps(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 160 of file Minimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 162 of file Minimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architectur",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7249,Testability,test,test,7249,s(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 160 of file Minimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 162 of file Minimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGrad,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7321,Testability,test,test,7321,s(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 160 of file Minimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 162 of file Minimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGrad,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7569,Testability,test,testError,7569,lar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 162 of file Minimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Definition at line 165 of file Minimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetLear,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7637,Testability,test,test,7637,GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Definition at line 165 of file Minimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 167 of file Minimizers.h. ◆ SetTestInterval(). template<typename Architecture,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:7688,Testability,test,test,7688,GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTrainingError ; (; ); const. inline . Definition at line 161 of file Minimizers.h. ◆ HasConverged() [1/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged. inline . Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ; Definition at line 667 of file Minimizers.h. ◆ HasConverged() [2/2]. template<typename Architecture_t > . bool TMVA::DNN::TGradientDescent< Architecture_t >::HasConverged ; (; Scalar_t ; testError). inline . Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ; Definition at line 681 of file Minimizers.h. ◆ Reset(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Reset ; (; ). inline . Reset minimizer object to default state. ; Definition at line 81 of file Minimizers.h. ◆ SetBatchSize(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetBatchSize ; (; Scalar_t ; rate). inline . Definition at line 168 of file Minimizers.h. ◆ SetConvergenceSteps(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetConvergenceSteps ; (; size_t ; steps). inline . Definition at line 165 of file Minimizers.h. ◆ SetLearningRate(). template<typename Architecture_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::SetLearningRate ; (; Scalar_t ; rate). inline . Definition at line 167 of file Minimizers.h. ◆ SetTestInterval(). template<typename Architecture,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:13122,Testability,test,testData,13122," file Minimizers.h. ◆ StepReducedWeightsLoss() [1/2]. template<typename Architecture_t > . template<typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Similar to StepReducedWeights(...) but also evaluates the loss. ; May trigger synchronization with the device. . ◆ StepReducedWeightsLoss() [2/2]. template<typename Architecture_t > . template<typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline . Definition at line 642 of file Minimizers.h. ◆ Train() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads . ); -> Scalar_t. Definition at line 196 of file Minimizers.h. ◆ Train() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads = 1 . ). Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). . ◆ TrainMomentum() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads . ); -> Scalar_t. Definiti",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:13685,Testability,test,test,13685,"ent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline . Definition at line 642 of file Minimizers.h. ◆ Train() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads . ); -> Scalar_t. Definition at line 196 of file Minimizers.h. ◆ Train() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads = 1 . ). Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). . ◆ TrainMomentum() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads . ); -> Scalar_t. Definition at line 259 of file Minimizers.h. ◆ TrainMomentum() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads = 1 . ). Same as Train(...) but uses the given momentum. . Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t T",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:13711,Testability,test,test,13711,"ent< Architecture_t >::StepReducedWeightsLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline . Definition at line 642 of file Minimizers.h. ◆ Train() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads . ); -> Scalar_t. Definition at line 196 of file Minimizers.h. ◆ Train() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads = 1 . ). Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). . ◆ TrainMomentum() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads . ); -> Scalar_t. Definition at line 259 of file Minimizers.h. ◆ TrainMomentum() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads = 1 . ). Same as Train(...) but uses the given momentum. . Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t T",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:13998,Testability,test,testData,13998,"::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads . ); -> Scalar_t. Definition at line 196 of file Minimizers.h. ◆ Train() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::Train ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . size_t ; nThreads = 1 . ). Train the given net using the given training input data (events), training output data (labels), test input data (events), test output data (labels). . ◆ TrainMomentum() [1/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; trainingData, . size_t ; nTrainingSamples, . const Data_t & ; testData, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads . ); -> Scalar_t. Definition at line 259 of file Minimizers.h. ◆ TrainMomentum() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads = 1 . ). Same as Train(...) but uses the given momentum. . Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 62 of file Minimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without.",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:15027,Testability,test,test,15027," ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads . ); -> Scalar_t. Definition at line 259 of file Minimizers.h. ◆ TrainMomentum() [2/2]. template<typename Architecture_t > . template<typename Data_t , typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::TrainMomentum ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads = 1 . ). Same as Train(...) but uses the given momentum. . Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 62 of file Minimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training ses",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:15292,Testability,test,test,15292,"cent< Architecture_t >::TrainMomentum ; (; const Data_t & ; TrainingDataIn, . size_t ; nTrainingSamples, . const Data_t & ; TestDataIn, . size_t ; nTestSamples, . Net_t & ; net, . Scalar_t ; momentum, . size_t ; nThreads = 1 . ). Same as Train(...) but uses the given momentum. . Member Data Documentation. ◆ fBatchSize. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fBatchSize. private . Batch size to use for the training. ; Definition at line 62 of file Minimizers.h. ◆ fConvergenceCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 63 of file Minimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 70 of file Minimizers.h. ◆ fTestI",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:16231,Testability,test,test,16231,ecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 63 of file Minimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 70 of file Minimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 68 of file Minimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 69 of file Minimizers.h. tmva/tmva/inc/TMVA/DNN/Minimizers.h. TMVADNNTGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:16457,Testability,test,test,16457,ecture_t >::fConvergenceCount. private . Current number of training epochs without. ; considerable decrease in the test error. ; Definition at line 66 of file Minimizers.h. ◆ fConvergenceSteps. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fConvergenceSteps. private . Number of training epochs without considerable. ; decrease in the test error for convergence. ; Definition at line 64 of file Minimizers.h. ◆ fLearningRate. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fLearningRate. private . Learning rate \(\alpha\). ; Definition at line 71 of file Minimizers.h. ◆ fMinimumError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fMinimumError. private . The minimum loss achieved on the training set during the current training session. ; Definition at line 72 of file Minimizers.h. ◆ fStepCount. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fStepCount. private . Number of steps performed in the current training session. ; Definition at line 63 of file Minimizers.h. ◆ fTestError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestError. private . Holds the most recently computed test loss. ; Definition at line 70 of file Minimizers.h. ◆ fTestInterval. template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::fTestInterval. private . Interval for the computation of the test error. ; Definition at line 68 of file Minimizers.h. ◆ fTrainingError. template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::fTrainingError. private . Holds the most recently computed training loss. ; Definition at line 69 of file Minimizers.h. tmva/tmva/inc/TMVA/DNN/Minimizers.h. TMVADNNTGradientDescent. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 (GVA Time) using Doxygen 1.9.8   ; . ,MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:679,Usability,learn,learningRate,679,". ROOT: TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TGradientDescent< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TGradientDescent< Architecture_t >; Definition at line 55 of file Minimizers.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TGradientDescent ();  ;  TGradientDescent (Scalar_t learningRate, size_t convergenceSteps, size_t testInterval);  ; size_t GetConvergenceCount () const;  ; size_t GetConvergenceSteps () const;  ; Scalar_t GetTestError () const;  ; size_t GetTestInterval () const;  ; Scalar_t GetTrainingError () const;  ; bool HasConverged ();  Increases the minimization step counter by the test error evaluation period and uses the current internal value of the test error to determine if the minimization has converged. ;  ; bool HasConverged (Scalar_t testError);  Increases the minimization step counter by the test error evaluation period and uses the provided test error value to determine if the minimization has converged. ;  ; void Reset ();  Reset minimizer object to default state. ;  ; void SetBatchSize (Scalar_t rate);  ; void SetConvergenceSteps (size_t steps);  ; void SetLearningRate (Scalar_t rate);  ; void SetTestInterval (size_t interval);  ; template<typename Net_t > ; void Step (Net_t &master, std::vector< Net_t > &nets, std::vector< TBatch< Architecture_t > > &batches);  Perform multiple optimization steps simultaneously. ;  ; template<typename Net_t > ; void Step (Net_t &net, Matrix_t &input, const Matrix_t &output, const Matrix_t &weights);  Perform a single optimization step on a given batch. ;  ; template<typename Net_t > ; Scalar_t StepLoss (Net_t &net",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:5917,Usability,learn,learningRate,5917,"calar_t fTestError;  Holds the most recently computed test loss. ;  ; size_t fTestInterval;  Interval for the computation of the test error. ;  ; Scalar_t fTrainingError;  Holds the most recently computed training loss. ;  . #include <TMVA/DNN/Minimizers.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 59 of file Minimizers.h. ◆ Scalar_t. template<typename Architecture_t > . using TMVA::DNN::TGradientDescent< Architecture_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 58 of file Minimizers.h. Constructor & Destructor Documentation. ◆ TGradientDescent() [1/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent. Definition at line 175 of file Minimizers.h. ◆ TGradientDescent() [2/2]. template<typename Architecture_t > . TMVA::DNN::TGradientDescent< Architecture_t >::TGradientDescent ; (; Scalar_t ; learningRate, . size_t ; convergenceSteps, . size_t ; testInterval . ). Definition at line 185 of file Minimizers.h. Member Function Documentation. ◆ GetConvergenceCount(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceCount ; (; ); const. inline . Definition at line 159 of file Minimizers.h. ◆ GetConvergenceSteps(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetConvergenceSteps ; (; ); const. inline . Definition at line 160 of file Minimizers.h. ◆ GetTestError(). template<typename Architecture_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestError ; (; ); const. inline . Definition at line 162 of file Minimizers.h. ◆ GetTestInterval(). template<typename Architecture_t > . size_t TMVA::DNN::TGradientDescent< Architecture_t >::GetTestInterval ; (; ); const. inline . Definition at line 163 of file Minimizers.h. ◆ GetTrainingError(",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html:9938,Usability,learn,learning,9938,"ep ; (; Net_t & ; master, . std::vector< Net_t > & ; nets, . std::vector< TBatch< Architecture_t > > & ; batches . ). inline . Perform multiple optimization steps simultaneously. ; Performs the backprop algorithm on the input batches given in batches on the neural networks given in nets. The forward and backward propagation steps are executed in an interleaving manner in order to exploit potential batch-level parallelism for asynchronous device calls. ; Definition at line 374 of file Minimizers.h. ◆ Step() [2/2]. template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::Step ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). inline . Perform a single optimization step on a given batch. ; Propagates the input matrix forward through the net, evaluates the loss and propagates the gradients backward through the net. The computed gradients are scaled by the learning rate \(\alpha\) and subtracted from the weights and bias values of each layer. ; Definition at line 331 of file Minimizers.h. ◆ StepLoss() [1/2]. template<typename Architecture_t > . template<typename Net_t > . Scalar_t TMVA::DNN::TGradientDescent< Architecture_t >::StepLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ). Same as Step(...) but also evaluate the loss on the given training data. ; Note that this requires synchronization between host and device. . ◆ StepLoss() [2/2]. template<typename Architecture_t > . template<typename Net_t > . auto TMVA::DNN::TGradientDescent< Architecture_t >::StepLoss ; (; Net_t & ; net, . Matrix_t & ; input, . const Matrix_t & ; output, . const Matrix_t & ; weights . ); -> Scalar_t. inline . Definition at line 352 of file Minimizers.h. ◆ StepMomentum(). template<typename Architecture_t > . template<typename Net_t > . void TMVA::DNN::TGradientDescent< Architecture_t >::StepMomentum ; (; Net_t & ; master, . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TGradientDescent.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TLayer.html:705,Energy Efficiency,allocate,allocates,705,". ROOT: TMVA::DNN::TLayer< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TLayer< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TLayer< Architecture_t >Generic layer class. ; This generic layer class represents a layer of a neural network with a given width n and activation function f. The activation function of each layer is given by \(\mathbf{u} =; \mathbf{W}\mathbf{x} + \boldsymbol{\theta}\).; In addition to the weight and bias matrices, each layer allocates memory for its activations and the corresponding first partial fDerivatives of the activation function as well as the gradients of the fWeights and fBiases.; The layer provides member functions for the forward propagation of activations through the given layer. ; Definition at line 52 of file Layer.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Public Member Functions;  TLayer (const TLayer &);  ;  TLayer (size_t BatchSize, size_t InputWidth, size_t Width, EActivationFunction f, Scalar_t dropoutProbability);  ; void Backward (Matrix_t &gradients_backward, const Matrix_t &activations_backward, ERegularization r, Scalar_t weightDecay);  Compute weight, bias and activation gradients. ;  ; void Forward (Matrix_t &input, bool applyDropout=false);  Compute activation of the layer for the given input. ;  ; EActivationFunction GetActivationFunction () const;  ; Matrix_t & GetActivationGradients ();  ; const Matrix_t & GetActivationGradients () const;  ; size_t GetBatchSize () const;  ; Matrix_t & GetBiases ();  ; const Matrix_t & GetBiases () const;  ; Matrix_t & GetBiasGradients ();  ; const Matrix_t & GetBiasGradients () const;  ; size_t G",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:3995,Availability,error,error,3995,"InputWidth () const;  ; Layer_t & GetLayer (size_t i);  ; const Layer_t & GetLayer (size_t i) const;  ; ELossFunction GetLossFunction () const;  ; Scalar_t GetNFlops ();  ; Matrix_t & GetOutput ();  ; size_t GetOutputWidth () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize (EInitialization m);  Initialize the weights in the net with the initialization method. ;  ; void InitializeGradients ();  Initialize the gradients in the net to zero. ;  ; LayerIterator_t LayersBegin ();  Iterator to the first layer of the net. ;  ; LayerIterator_t LayersEnd ();  Iterator to the last layer of the net. ;  ; Scalar_t Loss (const Matrix_t &Y, const Matrix_t &weights, bool includeRegularization=true) const;  Evaluate the loss function of the net using the activations that are currently stored in the output layer. ;  ; Scalar_t Loss (Matrix_t &X, const Matrix_t &Y, const Matrix_t &weights, bool applyDropout=false, bool includeRegularization=true);  Propagate the input batch X through the net and evaluate the error function for the resulting activations of the output layer. ;  ; void Prediction (Matrix_t &Y_hat, EOutputFunction f) const;  Compute the neural network prediction obtained from applying the output function f to the activation of the last layer in the network. ;  ; void Prediction (Matrix_t &Y_hat, Matrix_t &X, EOutputFunction f);  Compute the neural network prediction obtained from forwarding the batch X through the neural network and applying the output function f to the activation of the last layer in the network. ;  ; void Print ();  ; void SetBatchSize (size_t batchSize);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  . Private Attributes; size_t fBatchSize;  Batch size for training and evaluation o",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:13606,Availability,error,error,13606,"t.h. ◆ LayersEnd(). template<typename Architecture_t , typename Layer_t = TLayer<Architecture_t>> . LayerIterator_t TMVA::DNN::TNet< Architecture_t, Layer_t >::LayersEnd ; (; ). inline . Iterator to the last layer of the net. ; Definition at line 98 of file Net.h. ◆ Loss() [1/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TNet< Architecture_t, Layer_t >::Loss ; (; const Matrix_t & ; Y, . const Matrix_t & ; weights, . bool ; includeRegularization = true . ); const. inline . Evaluate the loss function of the net using the activations that are currently stored in the output layer. ; Definition at line 305 of file Net.h. ◆ Loss() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TNet< Architecture_t, Layer_t >::Loss ; (; Matrix_t & ; X, . const Matrix_t & ; Y, . const Matrix_t & ; weights, . bool ; applyDropout = false, . bool ; includeRegularization = true . ). inline . Propagate the input batch X through the net and evaluate the error function for the resulting activations of the output layer. ; Definition at line 320 of file Net.h. ◆ Prediction() [1/2]. template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; Y_hat, . EOutputFunction ; f . ); const. inline . Compute the neural network prediction obtained from applying the output function f to the activation of the last layer in the network. ; Definition at line 339 of file Net.h. ◆ Prediction() [2/2]. template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; Y_hat, . Matrix_t & ; X, . EOutputFunction ; f . ). inline . Compute the neural network prediction obtained from forwarding the batch X through the neural network and applying the output function f to the activation of the last layer in the network. ; Definition at line 329 of file Net.h. ◆ Print(). template<typename Architecture_t , typename Layer_t > . vo",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:836,Energy Efficiency,allocate,allocates,836,". ROOT: TMVA::DNN::TNet< Architecture_t, Layer_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TNet< Architecture_t, Layer_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = TLayer<Architecture_t>>; class TMVA::DNN::TNet< Architecture_t, Layer_t >Generic neural network class. ; This generic neural network class represents a concrete neural network through a vector of layers and coordinates the forward and backward propagation through the net.; The net takes as input a batch from the training data given in matrix form, with each row corresponding to a certain training event.; On construction, the neural network allocates all the memory required for the training of the neural net and keeps it until its destruction.; The Architecture type argument simply holds the architecture-specific data types, which are just the matrix type Matrix_t and the used scalar type Scalar_t.; Template Parameters. ArchitectureThe Architecture type that holds the ; Layer_tThe type used for the layers. Can be either Layer<Architecture> or SharedWeightLayer<Architecture>. datatypes for a given architecture. . Definition at line 49 of file Net.h. Public Types; using LayerIterator_t = typename std::vector< Layer_t >::iterator;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TNet ();  ;  TNet (const TNet &other);  ; template<typename OtherArchitecture_t > ;  TNet (size_t batchSize, const TNet< OtherArchitecture_t > &);  ;  TNet (size_t batchSize, size_t inputWidth, ELossFunction fJ, ERegularization fR=ERegularization::kNone, Scalar_t fWeightDecay=0.0);  Construct a neural net for a given batch size with given output function * and regularization. ;  ; template<typename SharedLayer > ; void AddL",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:588,Modifiability,layers,layers,588,". ROOT: TMVA::DNN::TNet< Architecture_t, Layer_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TNet< Architecture_t, Layer_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = TLayer<Architecture_t>>; class TMVA::DNN::TNet< Architecture_t, Layer_t >Generic neural network class. ; This generic neural network class represents a concrete neural network through a vector of layers and coordinates the forward and backward propagation through the net.; The net takes as input a batch from the training data given in matrix form, with each row corresponding to a certain training event.; On construction, the neural network allocates all the memory required for the training of the neural net and keeps it until its destruction.; The Architecture type argument simply holds the architecture-specific data types, which are just the matrix type Matrix_t and the used scalar type Scalar_t.; Template Parameters. ArchitectureThe Architecture type that holds the ; Layer_tThe type used for the layers. Can be either Layer<Architecture> or SharedWeightLayer<Architecture>. datatypes for a given architecture. . Definition at line 49 of file Net.h. Public Types; using LayerIterator_t = typename std::vector< Layer_t >::iterator;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TNet ();  ;  TNet (const TNet &other);  ; template<typename OtherArchitecture_t > ;  TNet (size_t batchSize, const TNet< OtherArchitecture_t > &);  ;  TNet (size_t batchSize, size_t inputWidth, ELossFunction fJ, ERegularization fR=ERegularization::kNone, Scalar_t fWeightDecay=0.0);  Construct a neural net for a given batch size with given output function * and regularization. ;  ; template<typename SharedLayer > ; void AddL",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:1201,Modifiability,layers,layers,1201,"f all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TNet< Architecture_t, Layer_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = TLayer<Architecture_t>>; class TMVA::DNN::TNet< Architecture_t, Layer_t >Generic neural network class. ; This generic neural network class represents a concrete neural network through a vector of layers and coordinates the forward and backward propagation through the net.; The net takes as input a batch from the training data given in matrix form, with each row corresponding to a certain training event.; On construction, the neural network allocates all the memory required for the training of the neural net and keeps it until its destruction.; The Architecture type argument simply holds the architecture-specific data types, which are just the matrix type Matrix_t and the used scalar type Scalar_t.; Template Parameters. ArchitectureThe Architecture type that holds the ; Layer_tThe type used for the layers. Can be either Layer<Architecture> or SharedWeightLayer<Architecture>. datatypes for a given architecture. . Definition at line 49 of file Net.h. Public Types; using LayerIterator_t = typename std::vector< Layer_t >::iterator;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TNet ();  ;  TNet (const TNet &other);  ; template<typename OtherArchitecture_t > ;  TNet (size_t batchSize, const TNet< OtherArchitecture_t > &);  ;  TNet (size_t batchSize, size_t inputWidth, ELossFunction fJ, ERegularization fR=ERegularization::kNone, Scalar_t fWeightDecay=0.0);  Construct a neural net for a given batch size with given output function * and regularization. ;  ; template<typename SharedLayer > ; void AddLayer (SharedLayer &layer);  Add a layer which shares its weights with another TNet instance. ;  ; template<typename SharedLayer_t > ; void AddLayer (SharedLayer_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:2526,Modifiability,layers,layers,2526,"chitecture_t::Scalar_t;  . Public Member Functions;  TNet ();  ;  TNet (const TNet &other);  ; template<typename OtherArchitecture_t > ;  TNet (size_t batchSize, const TNet< OtherArchitecture_t > &);  ;  TNet (size_t batchSize, size_t inputWidth, ELossFunction fJ, ERegularization fR=ERegularization::kNone, Scalar_t fWeightDecay=0.0);  Construct a neural net for a given batch size with given output function * and regularization. ;  ; template<typename SharedLayer > ; void AddLayer (SharedLayer &layer);  Add a layer which shares its weights with another TNet instance. ;  ; template<typename SharedLayer_t > ; void AddLayer (SharedLayer_t &layer);  ; void AddLayer (size_t width, EActivationFunction f, Scalar_t dropoutProbability=1.0);  Add a layer of the given size to the neural net. ;  ; void Backward (const Matrix_t &X, const Matrix_t &Y, const Matrix_t &weights);  Compute the weight gradients in the net from the given training samples X and training labels Y. ;  ; void Clear ();  Remove all layers from the network. ;  ; TNet< Architecture_t, TSharedLayer< Architecture_t > > CreateClone (size_t batchSize);  Create a clone that uses the same weight and biases matrices but potentially a difference batch size. ;  ; void Forward (Matrix_t &X, bool applyDropout=false);  Forward a given input through the neural net. ;  ; size_t GetBatchSize () const;  ; size_t GetDepth () const;  ; size_t GetInputWidth () const;  ; Layer_t & GetLayer (size_t i);  ; const Layer_t & GetLayer (size_t i) const;  ; ELossFunction GetLossFunction () const;  ; Scalar_t GetNFlops ();  ; Matrix_t & GetOutput ();  ; size_t GetOutputWidth () const;  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize (EInitialization m);  Initialize the weights in the net with the initialization method. ;  ; void InitializeGradients ();  Initialize the gradients in the net to zero. ;  ; LayerIterator_t LayersBegin ();  Iterator to the first layer of the net. ;  ; LayerIt",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:8582,Modifiability,layers,layers,8582,"itecture_t>> . template<typename SharedLayer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::AddLayer ; (; SharedLayer_t & ; layer). inline . Definition at line 247 of file Net.h. ◆ AddLayer() [3/3]. template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::AddLayer ; (; size_t ; width, . EActivationFunction ; f, . Scalar_t ; dropoutProbability = 1.0 . ). Add a layer of the given size to the neural net. ; Definition at line 225 of file Net.h. ◆ Backward(). template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Backward ; (; const Matrix_t & ; X, . const Matrix_t & ; Y, . const Matrix_t & ; weights . ). inline . Compute the weight gradients in the net from the given training samples X and training labels Y. ; Definition at line 285 of file Net.h. ◆ Clear(). template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Clear. Remove all layers from the network. ; Definition at line 239 of file Net.h. ◆ CreateClone(). template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TNet< Architecture_t, Layer_t >::CreateClone ; (; size_t ; batchSize). Create a clone that uses the same weight and biases matrices but potentially a difference batch size. ; Definition at line 212 of file Net.h. ◆ Forward(). template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Forward ; (; Matrix_t & ; X, . bool ; applyDropout = false . ). inline . Forward a given input through the neural net. ; Computes all layer activations up to the output layer ; Definition at line 273 of file Net.h. ◆ GetBatchSize(). template<typename Architecture_t , typename Layer_t = TLayer<Architecture_t>> . size_t TMVA::DNN::TNet< Architecture_t, Layer_t >::GetBatchSize ; (; ); const. inline . Definition at line 138 of file Net.h. ◆ GetDepth(). template<typename Architecture_t , typename Layer_t = TLayer<Archite",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:4154,Safety,predict,prediction,4154,"  ; ERegularization GetRegularization () const;  ; Scalar_t GetWeightDecay () const;  ; void Initialize (EInitialization m);  Initialize the weights in the net with the initialization method. ;  ; void InitializeGradients ();  Initialize the gradients in the net to zero. ;  ; LayerIterator_t LayersBegin ();  Iterator to the first layer of the net. ;  ; LayerIterator_t LayersEnd ();  Iterator to the last layer of the net. ;  ; Scalar_t Loss (const Matrix_t &Y, const Matrix_t &weights, bool includeRegularization=true) const;  Evaluate the loss function of the net using the activations that are currently stored in the output layer. ;  ; Scalar_t Loss (Matrix_t &X, const Matrix_t &Y, const Matrix_t &weights, bool applyDropout=false, bool includeRegularization=true);  Propagate the input batch X through the net and evaluate the error function for the resulting activations of the output layer. ;  ; void Prediction (Matrix_t &Y_hat, EOutputFunction f) const;  Compute the neural network prediction obtained from applying the output function f to the activation of the last layer in the network. ;  ; void Prediction (Matrix_t &Y_hat, Matrix_t &X, EOutputFunction f);  Compute the neural network prediction obtained from forwarding the batch X through the neural network and applying the output function f to the activation of the last layer in the network. ;  ; void Print ();  ; void SetBatchSize (size_t batchSize);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  . Private Attributes; size_t fBatchSize;  Batch size for training and evaluation of the Network. ;  ; Matrix_t fDummy;  Empty matrix for last step in back propagation. ;  ; size_t fInputWidth;  Number of features in a single input event. ;  ; ELossFunction fJ;  The loss function of the network. ;  ; std::vector< L",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:4362,Safety,predict,prediction,4362,"alize the gradients in the net to zero. ;  ; LayerIterator_t LayersBegin ();  Iterator to the first layer of the net. ;  ; LayerIterator_t LayersEnd ();  Iterator to the last layer of the net. ;  ; Scalar_t Loss (const Matrix_t &Y, const Matrix_t &weights, bool includeRegularization=true) const;  Evaluate the loss function of the net using the activations that are currently stored in the output layer. ;  ; Scalar_t Loss (Matrix_t &X, const Matrix_t &Y, const Matrix_t &weights, bool applyDropout=false, bool includeRegularization=true);  Propagate the input batch X through the net and evaluate the error function for the resulting activations of the output layer. ;  ; void Prediction (Matrix_t &Y_hat, EOutputFunction f) const;  Compute the neural network prediction obtained from applying the output function f to the activation of the last layer in the network. ;  ; void Prediction (Matrix_t &Y_hat, Matrix_t &X, EOutputFunction f);  Compute the neural network prediction obtained from forwarding the batch X through the neural network and applying the output function f to the activation of the last layer in the network. ;  ; void Print ();  ; void SetBatchSize (size_t batchSize);  ; void SetDropoutProbabilities (const std::vector< Double_t > &probabilities);  ; void SetInputWidth (size_t inputWidth);  ; void SetLossFunction (ELossFunction J);  ; void SetRegularization (ERegularization R);  ; void SetWeightDecay (Scalar_t weightDecay);  . Private Attributes; size_t fBatchSize;  Batch size for training and evaluation of the Network. ;  ; Matrix_t fDummy;  Empty matrix for last step in back propagation. ;  ; size_t fInputWidth;  Number of features in a single input event. ;  ; ELossFunction fJ;  The loss function of the network. ;  ; std::vector< Layer_t > fLayers;  Layers in the network. ;  ; ERegularization fR;  The regularization used for the network. ;  ; Scalar_t fWeightDecay;  The weight decay factor. ;  . #include <TMVA/DNN/Net.h>; Member Typedef Documentation. ◆ Laye",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:13945,Safety,predict,prediction,13945,"yer_t >::Loss ; (; const Matrix_t & ; Y, . const Matrix_t & ; weights, . bool ; includeRegularization = true . ); const. inline . Evaluate the loss function of the net using the activations that are currently stored in the output layer. ; Definition at line 305 of file Net.h. ◆ Loss() [2/2]. template<typename Architecture_t , typename Layer_t > . auto TMVA::DNN::TNet< Architecture_t, Layer_t >::Loss ; (; Matrix_t & ; X, . const Matrix_t & ; Y, . const Matrix_t & ; weights, . bool ; applyDropout = false, . bool ; includeRegularization = true . ). inline . Propagate the input batch X through the net and evaluate the error function for the resulting activations of the output layer. ; Definition at line 320 of file Net.h. ◆ Prediction() [1/2]. template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; Y_hat, . EOutputFunction ; f . ); const. inline . Compute the neural network prediction obtained from applying the output function f to the activation of the last layer in the network. ; Definition at line 339 of file Net.h. ◆ Prediction() [2/2]. template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; Y_hat, . Matrix_t & ; X, . EOutputFunction ; f . ). inline . Compute the neural network prediction obtained from forwarding the batch X through the neural network and applying the output function f to the activation of the last layer in the network. ; Definition at line 329 of file Net.h. ◆ Print(). template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Print. Definition at line 392 of file Net.h. ◆ SetBatchSize(). template<typename Architecture_t , typename Layer_t = TLayer<Architecture_t>> . void TMVA::DNN::TNet< Architecture_t, Layer_t >::SetBatchSize ; (; size_t ; batchSize). inline . Definition at line 148 of file Net.h. ◆ SetDropoutProbabilities(). template<typename Archi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:14337,Safety,predict,prediction,14337,"; X, . const Matrix_t & ; Y, . const Matrix_t & ; weights, . bool ; applyDropout = false, . bool ; includeRegularization = true . ). inline . Propagate the input batch X through the net and evaluate the error function for the resulting activations of the output layer. ; Definition at line 320 of file Net.h. ◆ Prediction() [1/2]. template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; Y_hat, . EOutputFunction ; f . ); const. inline . Compute the neural network prediction obtained from applying the output function f to the activation of the last layer in the network. ; Definition at line 339 of file Net.h. ◆ Prediction() [2/2]. template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Prediction ; (; Matrix_t & ; Y_hat, . Matrix_t & ; X, . EOutputFunction ; f . ). inline . Compute the neural network prediction obtained from forwarding the batch X through the neural network and applying the output function f to the activation of the last layer in the network. ; Definition at line 329 of file Net.h. ◆ Print(). template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::Print. Definition at line 392 of file Net.h. ◆ SetBatchSize(). template<typename Architecture_t , typename Layer_t = TLayer<Architecture_t>> . void TMVA::DNN::TNet< Architecture_t, Layer_t >::SetBatchSize ; (; size_t ; batchSize). inline . Definition at line 148 of file Net.h. ◆ SetDropoutProbabilities(). template<typename Architecture_t , typename Layer_t > . void TMVA::DNN::TNet< Architecture_t, Layer_t >::SetDropoutProbabilities ; (; const std::vector< Double_t > & ; probabilities). Definition at line 378 of file Net.h. ◆ SetInputWidth(). template<typename Architecture_t , typename Layer_t = TLayer<Architecture_t>> . void TMVA::DNN::TNet< Architecture_t, Layer_t >::SetInputWidth ; (; size_t ; inputWidth). inline . Definition at line 149",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html:973,Usability,simpl,simply,973,"TNet< Architecture_t, Layer_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TNet< Architecture_t, Layer_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = TLayer<Architecture_t>>; class TMVA::DNN::TNet< Architecture_t, Layer_t >Generic neural network class. ; This generic neural network class represents a concrete neural network through a vector of layers and coordinates the forward and backward propagation through the net.; The net takes as input a batch from the training data given in matrix form, with each row corresponding to a certain training event.; On construction, the neural network allocates all the memory required for the training of the neural net and keeps it until its destruction.; The Architecture type argument simply holds the architecture-specific data types, which are just the matrix type Matrix_t and the used scalar type Scalar_t.; Template Parameters. ArchitectureThe Architecture type that holds the ; Layer_tThe type used for the layers. Can be either Layer<Architecture> or SharedWeightLayer<Architecture>. datatypes for a given architecture. . Definition at line 49 of file Net.h. Public Types; using LayerIterator_t = typename std::vector< Layer_t >::iterator;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TNet ();  ;  TNet (const TNet &other);  ; template<typename OtherArchitecture_t > ;  TNet (size_t batchSize, const TNet< OtherArchitecture_t > &);  ;  TNet (size_t batchSize, size_t inputWidth, ELossFunction fJ, ERegularization fR=ERegularization::kNone, Scalar_t fWeightDecay=0.0);  Construct a neural net for a given batch size with given output function * and regularization. ;  ; template<typename SharedLayer > ; void AddLayer (SharedLayer ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TNet.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TNet.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:18197,Deployability,update,updates,18197,"MatrixT< AReal > > &in);  Rearrage data according to time fill B x T x D out with T x B x D matrix in. ;  . Static Private Attributes; static TRandom * fgRandomGen = nullptr;  . #include <TMVA/DNN/Architectures/Reference.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename AReal > . using TMVA::DNN::TReference< AReal >::Matrix_t = TMatrixT<AReal>. Definition at line 58 of file Reference.h. ◆ Scalar_t. template<typename AReal > . using TMVA::DNN::TReference< AReal >::Scalar_t = AReal. Definition at line 57 of file Reference.h. ◆ Tensor_t. template<typename AReal > . using TMVA::DNN::TReference< AReal >::Tensor_t = TMatrixT<AReal>. Definition at line 59 of file Reference.h. Member Function Documentation. ◆ AdamUpdate(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AdamUpdate ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; M, . const TMatrixT< AReal > & ; V, . AReal ; alpha, . AReal ; eps . ). static . Update functions for ADAM optimizer. ; Adam updates. ; Definition at line 103 of file Arithmetic.hxx. ◆ AdamUpdateFirstMom(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AdamUpdateFirstMom ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; B, . AReal ; beta . ). static . Definition at line 117 of file Arithmetic.hxx. ◆ AdamUpdateSecondMom(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AdamUpdateSecondMom ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; B, . AReal ; beta . ). static . Definition at line 129 of file Arithmetic.hxx. ◆ AddBiases(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::AddBiases ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; biases . ). static . Definition at line 30 of file DenoisePropagation.hxx. ◆ AddConvBiases(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AddConvBiases ; (; TMatrixT< AReal > & ; output, . const TMatrixT< AReal > & ; biases . ). static . Add the biases in the Convolutional ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:41029,Deployability,update,update,41029,"AReal > . void TMVA::DNN::TReference< AReal >::RotateWeights ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; B, . size_t ; filterDepth, . size_t ; filterHeight, . size_t ; filterWidth, . size_t ; numFilters . ). static . Rotates the matrix B, which is representing a weights, and stores them in the matrix A. ; Definition at line 144 of file Propagation.hxx. ◆ ScaleAdd() [1/2]. template<typename AReal > . void TMVA::DNN::TReference< AReal >::ScaleAdd ; (; std::vector< TMatrixT< Scalar_t > > & ; A, . const std::vector< TMatrixT< Scalar_t > > & ; B, . Scalar_t ; beta = 1.0 . ). static . Above functions extended to vectors. ; Definition at line 92 of file Propagation.hxx. ◆ ScaleAdd() [2/2]. template<typename AReal > . void TMVA::DNN::TReference< AReal >::ScaleAdd ; (; TMatrixT< Scalar_t > & ; A, . const TMatrixT< Scalar_t > & ; B, . Scalar_t ; beta = 1.0 . ). static . Adds a the elements in matrix B scaled by c to the elements in the matrix A. ; This is required for the weight update in the gradient descent step. ; Definition at line 76 of file Propagation.hxx. ◆ SetRandomSeed(). template<typename Real_t > . void TMVA::DNN::TReference< Real_t >::SetRandomSeed ; (; size_t ; seed). static . Definition at line 29 of file Initialization.hxx. ◆ Sigmoid() [1/2]. template<typename AReal > . static void TMVA::DNN::TReference< AReal >::Sigmoid ; (; TMatrixT< AReal > & ; B). static . ◆ Sigmoid() [2/2]. template<typename AReal > . void TMVA::DNN::TReference< AReal >::Sigmoid ; (; TMatrixT< AReal > & ; YHat, . const TMatrixT< AReal > & ; A . ). static . Definition at line 23 of file OutputFunctions.hxx. ◆ SigmoidDerivative(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::SigmoidDerivative ; (; TMatrixT< AReal > & ; B, . const TMatrixT< AReal > & ; A . ). inlinestatic . Definition at line 92 of file ActivationFunctions.hxx. ◆ Softmax(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::Softmax ; (; TMatrixT< AReal > & ; YHat, . const TMa",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:502,Integrability,interface,interface,502,". ROOT: TMVA::DNN::TReference< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TReference< AReal > Class Template Reference. ; template<typename AReal>; class TMVA::DNN::TReference< AReal >The reference architecture class. ; Class template that contains the reference implementation of the low-level interface for the DNN implementation. The reference implementation uses the TMatrixT class template to represent matrices.; Template Parameters. ARealThe floating point type used to represent scalars. . Definition at line 52 of file Reference.h. Public Types; using Matrix_t = TMatrixT< AReal >;  ; using Scalar_t = AReal;  ; using Tensor_t = TMatrixT< AReal >;  . Static Public Member Functions; static void AdamUpdate (TMatrixT< AReal > &A, const TMatrixT< AReal > &M, const TMatrixT< AReal > &V, AReal alpha, AReal eps);  Update functions for ADAM optimizer. ;  ; static void AdamUpdateFirstMom (TMatrixT< AReal > &A, const TMatrixT< AReal > &B, AReal beta);  ; static void AdamUpdateSecondMom (TMatrixT< AReal > &A, const TMatrixT< AReal > &B, AReal beta);  ; static void AddBiases (TMatrixT< AReal > &A, const TMatrixT< AReal > &biases);  ; static void ConstAdd (TMatrixT< AReal > &A, AReal beta);  Add the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConstMult (TMatrixT< AReal > &A, AReal beta);  Multiply the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConvLayerForward (std::vector< TMatrixT< AReal > > &, std::vector< TMatrixT< AReal > > &, const std::vector< TMatrixT< AReal > > &, const TMatrixT< AReal > &, const TMatrixT< AReal > &, const DNN::CNN::TConvParams &, EActivationFunction, std::vector< TMatrixT< AReal > > &);  Forward propagation in the Convolutional layer. ;  ; static void Co",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:9075,Integrability,interface,interface,9075," Scalar_t > &weights_candidate, const TMatrixT< Scalar_t > &weights_reset_state, const TMatrixT< Scalar_t > &weights_update_state, const TMatrixT< Scalar_t > &weights_candidate_state, const TMatrixT< Scalar_t > &input, TMatrixT< Scalar_t > &input_gradient);  Backward pass for GRU Network. ;  ; static void ScaleAdd (TMatrixT< Scalar_t > &A, const TMatrixT< Scalar_t > &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (TMatrixT< Scalar_t > &A, const TMatrixT< Scalar_t > &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (TMatrixT< Scalar_t > &A, const AMatrix_t &B);  ; static void ScaleAdd (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< TMatrixT< Scalar_t > > &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< TMatrixT< Scalar_t > > &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void Identity (TMatrixT< AReal > &B);  ; static void IdentityDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Relu (TMatrixT< AReal > &B);  ; static void ReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Sigmoid (TMatrixT< AReal > &B);  ; static void SigmoidDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Tanh (TMatrixT< AReal > &B);  ; static void TanhDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:9098,Integrability,rout,routines,9098," Scalar_t > &weights_candidate, const TMatrixT< Scalar_t > &weights_reset_state, const TMatrixT< Scalar_t > &weights_update_state, const TMatrixT< Scalar_t > &weights_candidate_state, const TMatrixT< Scalar_t > &input, TMatrixT< Scalar_t > &input_gradient);  Backward pass for GRU Network. ;  ; static void ScaleAdd (TMatrixT< Scalar_t > &A, const TMatrixT< Scalar_t > &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (TMatrixT< Scalar_t > &A, const TMatrixT< Scalar_t > &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (TMatrixT< Scalar_t > &A, const AMatrix_t &B);  ; static void ScaleAdd (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< TMatrixT< Scalar_t > > &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< TMatrixT< Scalar_t > > &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void Identity (TMatrixT< AReal > &B);  ; static void IdentityDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Relu (TMatrixT< AReal > &B);  ; static void ReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Sigmoid (TMatrixT< AReal > &B);  ; static void SigmoidDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Tanh (TMatrixT< AReal > &B);  ; static void TanhDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:10565,Integrability,rout,routing,10565,"; static void SigmoidDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Tanh (TMatrixT< AReal > &B);  ; static void TanhDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (TMatrixT< AReal > &B);  ; static void SymmetricReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void SoftSign (TMatrixT< AReal > &B);  ; static void SoftSignDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Gauss (TMatrixT< AReal > &B);  ; static void GaussDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static AReal MeanSquaredError (const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; static void MeanSquaredErrorGradients (TMatrixT< AReal > &dY, const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; static AReal CrossEntropy (const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void CrossEntropyGradients (TMatrixT< AReal > &dY, const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; static AReal SoftmaxCrossEntropy (const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  Softmax transformation is implicitly applied, thus output should hold the linear activati",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:12993,Integrability,interface,interface,12993,"TMatrixT< AReal > &weights);  ; Output Functions; Output functions transform the activations output of the output layer in the network to a valid prediction YHat for the desired usage of the network, e.g.; the identity function for regression or the sigmoid transformation for two-class classification. . static void Sigmoid (TMatrixT< AReal > &YHat, const TMatrixT< AReal > &);  ; static void Softmax (TMatrixT< AReal > &YHat, const TMatrixT< AReal > &);  ; Regularization; For each regularization type, two functions are required, one named <Type>Regularization that evaluates the corresponding regularization functional for a given weight matrix and the Add<Type>RegularizationGradients, that adds the regularization component in the gradients to the provided matrix. . static AReal L1Regularization (const TMatrixT< AReal > &W);  ; static void AddL1RegularizationGradients (TMatrixT< AReal > &A, const TMatrixT< AReal > &W, AReal weightDecay);  ; static AReal L2Regularization (const TMatrixT< AReal > &W);  ; static void AddL2RegularizationGradients (TMatrixT< AReal > &A, const TMatrixT< AReal > &W, AReal weightDecay);  ; Initialization; For each initialization method, one function in the low-level interface is provided.; The naming scheme is Initialize<Type> for a given initialization method Type. . static void InitializeGauss (TMatrixT< AReal > &A);  ; static void InitializeUniform (TMatrixT< AReal > &A);  ; static void InitializeIdentity (TMatrixT< AReal > &A);  ; static void InitializeZero (TMatrixT< AReal > &A);  ; static void InitializeGlorotUniform (TMatrixT< AReal > &A);  Sample from a uniform distribution in range [ -lim,+lim] where lim = sqrt(6/N_in+N_out). ;  ; static void InitializeGlorotNormal (TMatrixT< AReal > &A);  Truncated normal initialization (Glorot, called also Xavier normal) The values are sample with a normal distribution with stddev = sqrt(2/N_input + N_output) and values larger than 2 * stddev are discarded See Glorot & Bengio, AISTATS 2010 - http://jm",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:8738,Modifiability,extend,extended,8738,"ar_t > &fReset, const TMatrixT< Scalar_t > &fUpdate, const TMatrixT< Scalar_t > &fCandidate, const TMatrixT< Scalar_t > &weights_reset, const TMatrixT< Scalar_t > &weights_update, const TMatrixT< Scalar_t > &weights_candidate, const TMatrixT< Scalar_t > &weights_reset_state, const TMatrixT< Scalar_t > &weights_update_state, const TMatrixT< Scalar_t > &weights_candidate_state, const TMatrixT< Scalar_t > &input, TMatrixT< Scalar_t > &input_gradient);  Backward pass for GRU Network. ;  ; static void ScaleAdd (TMatrixT< Scalar_t > &A, const TMatrixT< Scalar_t > &B, Scalar_t beta=1.0);  Adds a the elements in matrix B scaled by c to the elements in the matrix A. ;  ; static void Copy (TMatrixT< Scalar_t > &A, const TMatrixT< Scalar_t > &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (TMatrixT< Scalar_t > &A, const AMatrix_t &B);  ; static void ScaleAdd (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< TMatrixT< Scalar_t > > &B, Scalar_t beta=1.0);  Above functions extended to vectors. ;  ; static void Copy (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< TMatrixT< Scalar_t > > &B);  ; template<typename AMatrix_t > ; static void CopyDiffArch (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void Identity (TMatrixT< AReal > &B);  ; static void IdentityDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Relu (TMatrixT< AReal > &B);  ; static void ReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Sigmoid (TMatrixT< AReal > &B);  ; static void SigmoidDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Tanh (TMatrixT< AReal > ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:40647,Modifiability,extend,extended,40647,"linestatic . Definition at line 58 of file ActivationFunctions.hxx. ◆ Reshape(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::Reshape ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; B . ). static . Transform the matrix B to a matrix with different dimensions A. ; Definition at line 393 of file Propagation.hxx. ◆ RotateWeights(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::RotateWeights ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; B, . size_t ; filterDepth, . size_t ; filterHeight, . size_t ; filterWidth, . size_t ; numFilters . ). static . Rotates the matrix B, which is representing a weights, and stores them in the matrix A. ; Definition at line 144 of file Propagation.hxx. ◆ ScaleAdd() [1/2]. template<typename AReal > . void TMVA::DNN::TReference< AReal >::ScaleAdd ; (; std::vector< TMatrixT< Scalar_t > > & ; A, . const std::vector< TMatrixT< Scalar_t > > & ; B, . Scalar_t ; beta = 1.0 . ). static . Above functions extended to vectors. ; Definition at line 92 of file Propagation.hxx. ◆ ScaleAdd() [2/2]. template<typename AReal > . void TMVA::DNN::TReference< AReal >::ScaleAdd ; (; TMatrixT< Scalar_t > & ; A, . const TMatrixT< Scalar_t > & ; B, . Scalar_t ; beta = 1.0 . ). static . Adds a the elements in matrix B scaled by c to the elements in the matrix A. ; This is required for the weight update in the gradient descent step. ; Definition at line 76 of file Propagation.hxx. ◆ SetRandomSeed(). template<typename Real_t > . void TMVA::DNN::TReference< Real_t >::SetRandomSeed ; (; size_t ; seed). static . Definition at line 29 of file Initialization.hxx. ◆ Sigmoid() [1/2]. template<typename AReal > . static void TMVA::DNN::TReference< AReal >::Sigmoid ; (; TMatrixT< AReal > & ; B). static . ◆ Sigmoid() [2/2]. template<typename AReal > . void TMVA::DNN::TReference< AReal >::Sigmoid ; (; TMatrixT< AReal > & ; YHat, . const TMatrixT< AReal > & ; A . ). static . Definition at line 23 of file OutputFunction",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:1053,Performance,optimiz,optimizer,1053,". ROOT: TMVA::DNN::TReference< AReal > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Static Public Member Functions |; Static Private Attributes |; List of all members ; TMVA::DNN::TReference< AReal > Class Template Reference. ; template<typename AReal>; class TMVA::DNN::TReference< AReal >The reference architecture class. ; Class template that contains the reference implementation of the low-level interface for the DNN implementation. The reference implementation uses the TMatrixT class template to represent matrices.; Template Parameters. ARealThe floating point type used to represent scalars. . Definition at line 52 of file Reference.h. Public Types; using Matrix_t = TMatrixT< AReal >;  ; using Scalar_t = AReal;  ; using Tensor_t = TMatrixT< AReal >;  . Static Public Member Functions; static void AdamUpdate (TMatrixT< AReal > &A, const TMatrixT< AReal > &M, const TMatrixT< AReal > &V, AReal alpha, AReal eps);  Update functions for ADAM optimizer. ;  ; static void AdamUpdateFirstMom (TMatrixT< AReal > &A, const TMatrixT< AReal > &B, AReal beta);  ; static void AdamUpdateSecondMom (TMatrixT< AReal > &A, const TMatrixT< AReal > &B, AReal beta);  ; static void AddBiases (TMatrixT< AReal > &A, const TMatrixT< AReal > &biases);  ; static void ConstAdd (TMatrixT< AReal > &A, AReal beta);  Add the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConstMult (TMatrixT< AReal > &A, AReal beta);  Multiply the constant beta to all the elements of matrix A and write the result into A. ;  ; static void ConvLayerForward (std::vector< TMatrixT< AReal > > &, std::vector< TMatrixT< AReal > > &, const std::vector< TMatrixT< AReal > > &, const TMatrixT< AReal > &, const TMatrixT< AReal > &, const DNN::CNN::TConvParams &, EActivationFunction, std::vector< TMatrixT< AReal > > &);  Forward propagation in the Convolutional layer. ;  ; static void Co",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:18179,Performance,optimiz,optimizer,18179," &out, const std::vector< TMatrixT< AReal > > &in);  Rearrage data according to time fill B x T x D out with T x B x D matrix in. ;  . Static Private Attributes; static TRandom * fgRandomGen = nullptr;  . #include <TMVA/DNN/Architectures/Reference.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename AReal > . using TMVA::DNN::TReference< AReal >::Matrix_t = TMatrixT<AReal>. Definition at line 58 of file Reference.h. ◆ Scalar_t. template<typename AReal > . using TMVA::DNN::TReference< AReal >::Scalar_t = AReal. Definition at line 57 of file Reference.h. ◆ Tensor_t. template<typename AReal > . using TMVA::DNN::TReference< AReal >::Tensor_t = TMatrixT<AReal>. Definition at line 59 of file Reference.h. Member Function Documentation. ◆ AdamUpdate(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AdamUpdate ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; M, . const TMatrixT< AReal > & ; V, . AReal ; alpha, . AReal ; eps . ). static . Update functions for ADAM optimizer. ; Adam updates. ; Definition at line 103 of file Arithmetic.hxx. ◆ AdamUpdateFirstMom(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AdamUpdateFirstMom ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; B, . AReal ; beta . ). static . Definition at line 117 of file Arithmetic.hxx. ◆ AdamUpdateSecondMom(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AdamUpdateSecondMom ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; B, . AReal ; beta . ). static . Definition at line 129 of file Arithmetic.hxx. ◆ AddBiases(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::AddBiases ; (; TMatrixT< AReal > & ; A, . const TMatrixT< AReal > & ; biases . ). static . Definition at line 30 of file DenoisePropagation.hxx. ◆ AddConvBiases(). template<typename AReal > . void TMVA::DNN::TReference< AReal >::AddConvBiases ; (; TMatrixT< AReal > & ; output, . const TMatrixT< AReal > & ; biases . ). static . Add the bi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:10479,Safety,predict,prediction,10479,"fArch (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void Identity (TMatrixT< AReal > &B);  ; static void IdentityDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Relu (TMatrixT< AReal > &B);  ; static void ReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Sigmoid (TMatrixT< AReal > &B);  ; static void SigmoidDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Tanh (TMatrixT< AReal > &B);  ; static void TanhDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (TMatrixT< AReal > &B);  ; static void SymmetricReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void SoftSign (TMatrixT< AReal > &B);  ; static void SoftSignDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Gauss (TMatrixT< AReal > &B);  ; static void GaussDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static AReal MeanSquaredError (const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; static void MeanSquaredErrorGradients (TMatrixT< AReal > &dY, const TMatrixT< AReal > &Y, const TMa",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:10527,Safety,predict,prediction,10527,"fArch (std::vector< TMatrixT< Scalar_t > > &A, const std::vector< AMatrix_t > &B);  ; Activation Functions; For each activation function, the low-level interface contains two routines.; One that applies the activation function to a matrix and one that evaluate the derivatives of the activation function at the elements of a given matrix and writes the results into the result matrix. . static void Identity (TMatrixT< AReal > &B);  ; static void IdentityDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Relu (TMatrixT< AReal > &B);  ; static void ReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Sigmoid (TMatrixT< AReal > &B);  ; static void SigmoidDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Tanh (TMatrixT< AReal > &B);  ; static void TanhDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void FastTanh (Tensor_t &B);  ; static void FastTanhDerivative (Tensor_t &B, const Tensor_t &A);  ; static void SymmetricRelu (TMatrixT< AReal > &B);  ; static void SymmetricReluDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void SoftSign (TMatrixT< AReal > &B);  ; static void SoftSignDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; static void Gauss (TMatrixT< AReal > &B);  ; static void GaussDerivative (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  ; Loss Functions; Loss functions compute a scalar value given the output of the network for a given training input and the expected network prediction Y that quantifies the quality of the prediction.; For each function also a routing that computes the gradients (suffixed by Gradients) must be provided for the starting of the backpropagation algorithm. . static AReal MeanSquaredError (const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; static void MeanSquaredErrorGradients (TMatrixT< AReal > &dY, const TMatrixT< AReal > &Y, const TMa",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:11932,Safety,predict,prediction,11932,"s);  ; static void MeanSquaredErrorGradients (TMatrixT< AReal > &dY, const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; static AReal CrossEntropy (const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  Sigmoid transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void CrossEntropyGradients (TMatrixT< AReal > &dY, const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; static AReal SoftmaxCrossEntropy (const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  Softmax transformation is implicitly applied, thus output should hold the linear activations of the last layer in the net. ;  ; static void SoftmaxCrossEntropyGradients (TMatrixT< AReal > &dY, const TMatrixT< AReal > &Y, const TMatrixT< AReal > &output, const TMatrixT< AReal > &weights);  ; Output Functions; Output functions transform the activations output of the output layer in the network to a valid prediction YHat for the desired usage of the network, e.g.; the identity function for regression or the sigmoid transformation for two-class classification. . static void Sigmoid (TMatrixT< AReal > &YHat, const TMatrixT< AReal > &);  ; static void Softmax (TMatrixT< AReal > &YHat, const TMatrixT< AReal > &);  ; Regularization; For each regularization type, two functions are required, one named <Type>Regularization that evaluates the corresponding regularization functional for a given weight matrix and the Add<Type>RegularizationGradients, that adds the regularization component in the gradients to the provided matrix. . static AReal L1Regularization (const TMatrixT< AReal > &W);  ; static void AddL1RegularizationGradients (TMatrixT< AReal > &A, const TMatrixT< AReal > &W, AReal weightDecay);  ; static AReal L2Regularization (const TMatrixT< AReal > &W);  ; static void AddL2R",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:3679,Usability,learn,learningRate,3679,"l each element of the matrix A and write the result into A. ;  ; static void ReconstructInput (TMatrixT< AReal > &compressedInput, TMatrixT< AReal > &reconstructedInput, TMatrixT< AReal > &fWeights);  ; static void SoftmaxAE (TMatrixT< AReal > &A);  ; static void SqrtElementWise (TMatrixT< AReal > &A);  Square root each element of the matrix A and write the result into A. ;  ; static void SquareElementWise (TMatrixT< AReal > &A);  Square each element of the matrix A and write the result into A. ;  ; static void SumColumns (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  Sum columns of (m x n) matrix A and write the results into the first m elements in A. ;  ; static void UpdateParams (TMatrixT< AReal > &x, TMatrixT< AReal > &tildeX, TMatrixT< AReal > &y, TMatrixT< AReal > &z, TMatrixT< AReal > &fVBiases, TMatrixT< AReal > &fHBiases, TMatrixT< AReal > &fWeights, TMatrixT< AReal > &VBiasError, TMatrixT< AReal > &HBiasError, AReal learningRate, size_t fBatchSize);  ; static void UpdateParamsLogReg (TMatrixT< AReal > &input, TMatrixT< AReal > &output, TMatrixT< AReal > &difference, TMatrixT< AReal > &p, TMatrixT< AReal > &fWeights, TMatrixT< AReal > &fBiases, AReal learningRate, size_t fBatchSize);  ; Forward Propagation; Low-level functions required for the forward propagation of activations through the network. . static void MultiplyTranspose (TMatrixT< Scalar_t > &output, const TMatrixT< Scalar_t > &input, const TMatrixT< Scalar_t > &weights);  Matrix-multiply input with the transpose of weights and write the results into output. ;  ; static void AddRowWise (TMatrixT< Scalar_t > &output, const TMatrixT< Scalar_t > &biases);  Add the vectors biases row-wise to the matrix output. ;  ; Backward Propagation; Low-level functions required for the forward propagation of activations through the network. . static void Backward (TMatrixT< Scalar_t > &activationGradientsBackward, TMatrixT< Scalar_t > &weightGradients, TMatrixT< Scalar_t > &biasGradients, TMatrixT< Scalar_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:3917,Usability,learn,learningRate,3917,"l each element of the matrix A and write the result into A. ;  ; static void ReconstructInput (TMatrixT< AReal > &compressedInput, TMatrixT< AReal > &reconstructedInput, TMatrixT< AReal > &fWeights);  ; static void SoftmaxAE (TMatrixT< AReal > &A);  ; static void SqrtElementWise (TMatrixT< AReal > &A);  Square root each element of the matrix A and write the result into A. ;  ; static void SquareElementWise (TMatrixT< AReal > &A);  Square each element of the matrix A and write the result into A. ;  ; static void SumColumns (TMatrixT< AReal > &B, const TMatrixT< AReal > &A);  Sum columns of (m x n) matrix A and write the results into the first m elements in A. ;  ; static void UpdateParams (TMatrixT< AReal > &x, TMatrixT< AReal > &tildeX, TMatrixT< AReal > &y, TMatrixT< AReal > &z, TMatrixT< AReal > &fVBiases, TMatrixT< AReal > &fHBiases, TMatrixT< AReal > &fWeights, TMatrixT< AReal > &VBiasError, TMatrixT< AReal > &HBiasError, AReal learningRate, size_t fBatchSize);  ; static void UpdateParamsLogReg (TMatrixT< AReal > &input, TMatrixT< AReal > &output, TMatrixT< AReal > &difference, TMatrixT< AReal > &p, TMatrixT< AReal > &fWeights, TMatrixT< AReal > &fBiases, AReal learningRate, size_t fBatchSize);  ; Forward Propagation; Low-level functions required for the forward propagation of activations through the network. . static void MultiplyTranspose (TMatrixT< Scalar_t > &output, const TMatrixT< Scalar_t > &input, const TMatrixT< Scalar_t > &weights);  Matrix-multiply input with the transpose of weights and write the results into output. ;  ; static void AddRowWise (TMatrixT< Scalar_t > &output, const TMatrixT< Scalar_t > &biases);  Add the vectors biases row-wise to the matrix output. ;  ; Backward Propagation; Low-level functions required for the forward propagation of activations through the network. . static void Backward (TMatrixT< Scalar_t > &activationGradientsBackward, TMatrixT< Scalar_t > &weightGradients, TMatrixT< Scalar_t > &biasGradients, TMatrixT< Scalar_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:45581,Usability,learn,learningRate,45581,"ence< Real_t >::SymmetricReluDerivative ; (; TMatrixT< AReal > & ; B, . const TMatrixT< AReal > & ; A . ). inlinestatic . Definition at line 157 of file ActivationFunctions.hxx. ◆ Tanh(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::Tanh ; (; TMatrixT< AReal > & ; B). inlinestatic . Definition at line 109 of file ActivationFunctions.hxx. ◆ TanhDerivative(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::TanhDerivative ; (; TMatrixT< AReal > & ; B, . const TMatrixT< AReal > & ; A . ). inlinestatic . Definition at line 125 of file ActivationFunctions.hxx. ◆ UpdateParams(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::UpdateParams ; (; TMatrixT< AReal > & ; x, . TMatrixT< AReal > & ; tildeX, . TMatrixT< AReal > & ; y, . TMatrixT< AReal > & ; z, . TMatrixT< AReal > & ; fVBiases, . TMatrixT< AReal > & ; fHBiases, . TMatrixT< AReal > & ; fWeights, . TMatrixT< AReal > & ; VBiasError, . TMatrixT< AReal > & ; HBiasError, . AReal ; learningRate, . size_t ; fBatchSize . ). static . Definition at line 48 of file DenoisePropagation.hxx. ◆ UpdateParamsLogReg(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::UpdateParamsLogReg ; (; TMatrixT< AReal > & ; input, . TMatrixT< AReal > & ; output, . TMatrixT< AReal > & ; difference, . TMatrixT< AReal > & ; p, . TMatrixT< AReal > & ; fWeights, . TMatrixT< AReal > & ; fBiases, . AReal ; learningRate, . size_t ; fBatchSize . ). static . Definition at line 191 of file DenoisePropagation.hxx. Member Data Documentation. ◆ fgRandomGen. template<typename Real_t > . TRandom * TMVA::DNN::TReference< Real_t >::fgRandomGen = nullptr. staticprivate . Definition at line 55 of file Reference.h. tmva/tmva/inc/TMVA/DNN/Architectures/Reference.h; tmva/tmva/src/DNN/Architectures/Reference/ActivationFunctions.hxx; tmva/tmva/src/DNN/Architectures/Reference/Arithmetic.hxx; tmva/tmva/src/DNN/Architectures/Reference/DenoisePropagation.hxx; tmva/tmva/src/DNN/Architectures/Reference/",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html:46000,Usability,learn,learningRate,46000,":DNN::TReference< Real_t >::TanhDerivative ; (; TMatrixT< AReal > & ; B, . const TMatrixT< AReal > & ; A . ). inlinestatic . Definition at line 125 of file ActivationFunctions.hxx. ◆ UpdateParams(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::UpdateParams ; (; TMatrixT< AReal > & ; x, . TMatrixT< AReal > & ; tildeX, . TMatrixT< AReal > & ; y, . TMatrixT< AReal > & ; z, . TMatrixT< AReal > & ; fVBiases, . TMatrixT< AReal > & ; fHBiases, . TMatrixT< AReal > & ; fWeights, . TMatrixT< AReal > & ; VBiasError, . TMatrixT< AReal > & ; HBiasError, . AReal ; learningRate, . size_t ; fBatchSize . ). static . Definition at line 48 of file DenoisePropagation.hxx. ◆ UpdateParamsLogReg(). template<typename AReal > . void TMVA::DNN::TReference< Real_t >::UpdateParamsLogReg ; (; TMatrixT< AReal > & ; input, . TMatrixT< AReal > & ; output, . TMatrixT< AReal > & ; difference, . TMatrixT< AReal > & ; p, . TMatrixT< AReal > & ; fWeights, . TMatrixT< AReal > & ; fBiases, . AReal ; learningRate, . size_t ; fBatchSize . ). static . Definition at line 191 of file DenoisePropagation.hxx. Member Data Documentation. ◆ fgRandomGen. template<typename Real_t > . TRandom * TMVA::DNN::TReference< Real_t >::fgRandomGen = nullptr. staticprivate . Definition at line 55 of file Reference.h. tmva/tmva/inc/TMVA/DNN/Architectures/Reference.h; tmva/tmva/src/DNN/Architectures/Reference/ActivationFunctions.hxx; tmva/tmva/src/DNN/Architectures/Reference/Arithmetic.hxx; tmva/tmva/src/DNN/Architectures/Reference/DenoisePropagation.hxx; tmva/tmva/src/DNN/Architectures/Reference/Initialization.hxx; tmva/tmva/src/DNN/Architectures/Reference/LossFunctions.hxx; tmva/tmva/src/DNN/Architectures/Reference/OutputFunctions.hxx; tmva/tmva/src/DNN/Architectures/Reference/Propagation.hxx; tmva/tmva/src/DNN/Architectures/Reference/RecurrentPropagation.hxx; tmva/tmva/src/DNN/Architectures/Reference/Regularization.hxx. TMVADNNTReference. ROOT master - Reference Guide Generated on Tue Nov 5 2024 09:45:46 ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReference.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReference.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:1345,Availability,error,error,1345," Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TReshapeLayer< Architecture_t >; Definition at line 41 of file ReshapeLayer.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Public Member Functions;  TReshapeLayer (const TReshapeLayer &);  Copy Constructor. ;  ;  TReshapeLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t OutputNSlices, size_t OutputNRows, size_t OutputNCols, bool Flattening);  Constructor. ;  ;  TReshapeLayer (TReshapeLayer< Architecture_t > *layer);  Copy the reshape layer provided as a pointer. ;  ;  ~TReshapeLayer ();  Destructor. ;  ; virtual void AddWeightsXMLTo (void *parent);  Writes the information and the weights about the layer in an XML node. ;  ; void Backward (Tensor_t &gradients_backward, const Tensor_t &activations_backward);  Backpropagates the error. ;  ; void Forward (Tensor_t &input, bool applyDropout=false);  The input must be in 3D tensor form with the different matrices corresponding to different events in the batch. ;  ; bool isFlattening () const;  TODO Add documentation Does this layer flatten? (necessary for DenseLayer) B x D1 x D2 --> 1 x B x (D1 * D2) ;  ; void Print () const;  Prints the info about the layer. ;  ; virtual void ReadWeightsFromXML (void *parent);  Read the information and the weights about the layer from XML node. ;  ;  Public Member Functions inherited from TMVA::DNN::VGeneralLayer< Architecture_t >;  VGeneralLayer (const VGeneralLayer &);  Copy Constructor. ;  ;  VGeneralLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t WeightsNSlices, size_t WeightsNRows, size_t WeightsNCols, size_t BiasesNSlices, size_t BiasesNRows, size_t BiasesNCols, size_t OutputNSlices, size",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:10623,Availability,error,error,10623," ◆ TReshapeLayer() [3/3]. template<typename Architecture_t > . TMVA::DNN::TReshapeLayer< Architecture_t >::TReshapeLayer ; (; const TReshapeLayer< Architecture_t > & ; layer). Copy Constructor. ; Definition at line 119 of file ReshapeLayer.h. ◆ ~TReshapeLayer(). template<typename Architecture_t > . TMVA::DNN::TReshapeLayer< Architecture_t >::~TReshapeLayer. Destructor. ; Definition at line 127 of file ReshapeLayer.h. Member Function Documentation. ◆ AddWeightsXMLTo(). template<typename Architecture_t > . auto TMVA::DNN::TReshapeLayer< Architecture_t >::AddWeightsXMLTo ; (; void * ; parent). virtual . Writes the information and the weights about the layer in an XML node. ; Implements TMVA::DNN::VGeneralLayer< Architecture_t >.; Definition at line 180 of file ReshapeLayer.h. ◆ Backward(). template<typename Architecture_t > . auto TMVA::DNN::TReshapeLayer< Architecture_t >::Backward ; (; Tensor_t & ; gradients_backward, . const Tensor_t & ; activations_backward . ). virtual . Backpropagates the error. ; Must only be called directly at the corresponding call to Forward(...). ; Implements TMVA::DNN::VGeneralLayer< Architecture_t >.; Definition at line 149 of file ReshapeLayer.h. ◆ Forward(). template<typename Architecture_t > . auto TMVA::DNN::TReshapeLayer< Architecture_t >::Forward ; (; Tensor_t & ; input, . bool ; applyDropout = false . ). virtual . The input must be in 3D tensor form with the different matrices corresponding to different events in the batch. ; It transforms the input matrices. ; Implements TMVA::DNN::VGeneralLayer< Architecture_t >.; Definition at line 134 of file ReshapeLayer.h. ◆ isFlattening(). template<typename Architecture_t > . bool TMVA::DNN::TReshapeLayer< Architecture_t >::isFlattening ; (; ); const. inline . TODO Add documentation Does this layer flatten? (necessary for DenseLayer) B x D1 x D2 --> 1 x B x (D1 * D2) ; Definition at line 86 of file ReshapeLayer.h. ◆ Print(). template<typename Architecture_t > . auto TMVA::DNN::TReshapeLayer< ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:1882,Modifiability,inherit,inherited,1882,"putNCols, bool Flattening);  Constructor. ;  ;  TReshapeLayer (TReshapeLayer< Architecture_t > *layer);  Copy the reshape layer provided as a pointer. ;  ;  ~TReshapeLayer ();  Destructor. ;  ; virtual void AddWeightsXMLTo (void *parent);  Writes the information and the weights about the layer in an XML node. ;  ; void Backward (Tensor_t &gradients_backward, const Tensor_t &activations_backward);  Backpropagates the error. ;  ; void Forward (Tensor_t &input, bool applyDropout=false);  The input must be in 3D tensor form with the different matrices corresponding to different events in the batch. ;  ; bool isFlattening () const;  TODO Add documentation Does this layer flatten? (necessary for DenseLayer) B x D1 x D2 --> 1 x B x (D1 * D2) ;  ; void Print () const;  Prints the info about the layer. ;  ; virtual void ReadWeightsFromXML (void *parent);  Read the information and the weights about the layer from XML node. ;  ;  Public Member Functions inherited from TMVA::DNN::VGeneralLayer< Architecture_t >;  VGeneralLayer (const VGeneralLayer &);  Copy Constructor. ;  ;  VGeneralLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t WeightsNSlices, size_t WeightsNRows, size_t WeightsNCols, size_t BiasesNSlices, size_t BiasesNRows, size_t BiasesNCols, size_t OutputNSlices, size_t OutputNRows, size_t OutputNCols, EInitialization Init);  Constructor. ;  ;  VGeneralLayer (size_t BatchSize, size_t InputDepth, size_t InputHeight, size_t InputWidth, size_t Depth, size_t Height, size_t Width, size_t WeightsNSlices, std::vector< size_t > WeightsNRows, std::vector< size_t > WeightsNCols, size_t BiasesNSlices, std::vector< size_t > BiasesNRows, std::vector< size_t > BiasesNCols, size_t OutputNSlices, size_t OutputNRows, size_t OutputNCols, EInitialization Init);  General Constructor with different weights dimension. ;  ;  VGeneralLayer (VGeneralLayer< Architecture_t > *layer);  Copy the layer provided as a ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:7028,Modifiability,inherit,inherited,7028,"arningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; std::vector< Matrix_t > fBiases;  The biases associated to the layer. ;  ; std::vector< Matrix_t > fBiasGradients;  Gradients w.r.t. the bias values of the layer. ;  ; size_t fDepth;  The depth of the layer. ;  ; size_t fHeight;  The height of the layer. ;  ; EInitialization fInit;  The initialization method. ;  ; size_t fInputDepth;  The depth of the previous layer or input. ;  ; size_t fInputHeight;  The height of the previous layer or input. ;  ; size_t fInputWidth;  The width of the previous layer or input. ;  ; bool fIsTraining;  Flag indicating the mode. ;  ; Tensor_t fOutput;  Activations of this layer. ;  ; std::vector< Matrix_t > fWeightGradients;  Gradients w.r.t. the weights of the layer. ;  ; std::vector< Matrix_t > fWeights;  The weights associated to the layer. ;  ; size_t fWidth;  The ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:5893,Usability,learn,learningRate,5893,"ightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:5951,Usability,learn,learning,5951,"ightGradientsAt (size_t i);  ; const Matrix_t & GetWeightGradientsAt (size_t i) const;  ; std::vector< Matrix_t > & GetWeights ();  ; const std::vector< Matrix_t > & GetWeights () const;  ; Matrix_t & GetWeightsAt (size_t i);  ; const Matrix_t & GetWeightsAt (size_t i) const;  ; size_t GetWidth () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6051,Usability,learn,learningRate,6051,"th () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6115,Usability,learn,learning,6115,"th () const;  ; virtual void Initialize ();  Initialize the weights and biases according to the given initialization method. ;  ; bool IsTraining () const;  ; void ReadMatrixXML (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6222,Usability,learn,learningRate,6222,"L (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6304,Usability,learn,learning,6304,"L (void *node, const char *name, Matrix_t &matrix);  ; virtual void ResetTraining ();  Reset some training flags after a loop on all batches Some layer (e.g. ;  ; void SetBatchSize (size_t batchSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6415,Usability,learn,learningRate,6415,"chSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; std::vector< Matrix_t > fBiases;  The biases associated to the layer. ;  ; std::vector< Matrix_t > fBiasGradients;  Gradients w.r.t. the bias values of the layer. ;  ; size_t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6499,Usability,learn,learning,6499,"chSize);  Setters. ;  ; void SetDepth (size_t depth);  ; virtual void SetDropoutProbability (Scalar_t);  Set Dropout probability. ;  ; virtual void SetExtraLayerParameters (const std::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; std::vector< Matrix_t > fBiases;  The biases associated to the layer. ;  ; std::vector< Matrix_t > fBiasGradients;  Gradients w.r.t. the bias values of the layer. ;  ; size_t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6602,Usability,learn,learningRate,6602,"::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; std::vector< Matrix_t > fBiases;  The biases associated to the layer. ;  ; std::vector< Matrix_t > fBiasGradients;  Gradients w.r.t. the bias values of the layer. ;  ; size_t fDepth;  The depth of the layer. ;  ; size_t fHeight;  The height of the layer. ;  ; EInitialization fInit;  The initialization method. ;  ; size_t fInputDepth;  The depth of the pr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html:6667,Usability,learn,learning,6667,"::vector< Matrix_t > &);  ; void SetHeight (size_t height);  ; void SetInputDepth (size_t inputDepth);  ; void SetInputHeight (size_t inputHeight);  ; void SetInputWidth (size_t inputWidth);  ; void SetIsTraining (bool isTraining);  ; void SetWidth (size_t width);  ; void Update (const Scalar_t learningRate);  Updates the weights and biases, given the learning rate. ;  ; void UpdateBiases (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the biases, given the gradients and the learning rate. ;  ; void UpdateBiasGradients (const std::vector< Matrix_t > &biasGradients, const Scalar_t learningRate);  Updates the bias gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeightGradients (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weight gradients, given some other weight gradients and learning rate. ;  ; void UpdateWeights (const std::vector< Matrix_t > &weightGradients, const Scalar_t learningRate);  Updates the weights, given the gradients and the learning rate,. ;  ; void WriteMatrixToXML (void *node, const char *name, const Matrix_t &matrix);  ; void WriteTensorToXML (void *node, const char *name, const std::vector< Matrix_t > &tensor);  helper functions for XML ;  . Private Attributes; bool fFlattening;  Whether the layer is doing flattening. ;  . Additional Inherited Members;  Protected Attributes inherited from TMVA::DNN::VGeneralLayer< Architecture_t >; Tensor_t fActivationGradients;  Gradients w.r.t. the activations of this layer. ;  ; size_t fBatchSize;  Batch size used for training and evaluation. ;  ; std::vector< Matrix_t > fBiases;  The biases associated to the layer. ;  ; std::vector< Matrix_t > fBiasGradients;  Gradients w.r.t. the bias values of the layer. ;  ; size_t fDepth;  The depth of the layer. ;  ; size_t fHeight;  The height of the layer. ;  ; EInitialization fInit;  The initialization method. ;  ; size_t fInputDepth;  The depth of the pr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TReshapeLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:3033,Deployability,update,updates,3033,"ed from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:3656,Deployability,update,updates,3656,"t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/RMSProp.h>. Inheritance diagram for TMVA::DNN::TRMSProp< Architectu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:11111,Deployability,update,updates,11111,"ual . Update the biases, given the current bias gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 196 of file RMSProp.h. ◆ UpdateWeights(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . auto TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::UpdateWeights ; (; size_t ; layerIndex, . std::vector< Matrix_t > & ; weights, . const std::vector< Matrix_t > & ; weightGradients . ). protectedvirtual . Update the weights, given the current weight gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 152 of file RMSProp.h. Member Data Documentation. ◆ fBiasUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fBiasUpdates. protected . The accumulation of the past Biases for performing updates. ; Definition at line 60 of file RMSProp.h. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 53 of file RMSProp.h. ◆ fMomentum. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fMomentum. protected . The momentum used for training. ; Definition at line 51 of file RMSProp.h. ◆ fPastSquaredBiasGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:13342,Deployability,update,updates,13342,"ypename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightGradients. protected . The sum of the square of the past weight gradients associated with the deep net. ; Definition at line 55 of file RMSProp.h. ◆ fRho. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fRho. protected . The Rho constant used by the optimizer. ; Definition at line 52 of file RMSProp.h. ◆ fWeightUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWeightUpdates. protected . The accumulation of the past Weights for performing updates. ; Definition at line 59 of file RMSProp.h. ◆ fWorkBiasTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor1. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 64 of file RMSProp.h. ◆ fWorkBiasTensor2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor2. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 68 of file RMSProp.h. ◆ fWorkWeightTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNe",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:893,Modifiability,inherit,inherited,893,". ROOT: TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >RMSProp Optimizer class. ; This class represents the RMSProp Optimizer with options for applying momentum. ; Definition at line 45 of file RMSProp.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TRMSProp (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t momentum=0.0, Scalar_t rho=0.9, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TRMSProp ()=default;  Destructor. ;  ; std::vector< std::vector< Matrix_t > > & GetBiasUpdates ();  ; std::vector< Matrix_t > & GetBiasUpdatesAt (size_t i);  ; Scalar_t GetEpsilon () const;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; Scalar_t GetRho () const;  ; std::vector< std::vector< Matrix_t > > & GetWeightUpdates ();  ; std::vector< Matrix_t > & GetWeightUpdatesAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimize",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:1966,Modifiability,inherit,inherited,1966,"chitecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TRMSProp (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t momentum=0.0, Scalar_t rho=0.9, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TRMSProp ()=default;  Destructor. ;  ; std::vector< std::vector< Matrix_t > > & GetBiasUpdates ();  ; std::vector< Matrix_t > & GetBiasUpdatesAt (size_t i);  ; Scalar_t GetEpsilon () const;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; Scalar_t GetRho () const;  ; std::vector< std::vector< Matrix_t > > & GetWeightUpdates ();  ; std::vector< Matrix_t > & GetWeightUpdatesAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, con",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:4242,Modifiability,inherit,inherited,4242," for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/RMSProp.h>. Inheritance diagram for TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matr",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:2498,Performance,optimiz,optimization,2498," ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; Scalar_t GetRho () const;  ; std::vector< std::vector< Matrix_t > > & GetWeightUpdates ();  ; std::vector< Matrix_t > & GetWeightUpdatesAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:3022,Performance,perform,performing,3022,"ed from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:3532,Performance,optimiz,optimizer,3532," ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:3645,Performance,perform,performing,3645,"t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/RMSProp.h>. Inheritance diagram for TMVA::DNN::TRMSProp< Architectu",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:11100,Performance,perform,performing,11100,"ual . Update the biases, given the current bias gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 196 of file RMSProp.h. ◆ UpdateWeights(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . auto TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::UpdateWeights ; (; size_t ; layerIndex, . std::vector< Matrix_t > & ; weights, . const std::vector< Matrix_t > & ; weightGradients . ). protectedvirtual . Update the weights, given the current weight gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 152 of file RMSProp.h. Member Data Documentation. ◆ fBiasUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fBiasUpdates. protected . The accumulation of the past Biases for performing updates. ; Definition at line 60 of file RMSProp.h. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 53 of file RMSProp.h. ◆ fMomentum. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fMomentum. protected . The momentum used for training. ; Definition at line 51 of file RMSProp.h. ◆ fPastSquaredBiasGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:12952,Performance,optimiz,optimizer,12952,"name DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasGradients. protected . The sum of the square of the past bias gradients associated with the deep net. ; Definition at line 57 of file RMSProp.h. ◆ fPastSquaredWeightGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightGradients. protected . The sum of the square of the past weight gradients associated with the deep net. ; Definition at line 55 of file RMSProp.h. ◆ fRho. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fRho. protected . The Rho constant used by the optimizer. ; Definition at line 52 of file RMSProp.h. ◆ fWeightUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWeightUpdates. protected . The accumulation of the past Weights for performing updates. ; Definition at line 59 of file RMSProp.h. ◆ fWorkBiasTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor1. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 64 of file RMSProp.h. ◆ fWorkBiasTensor2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architectur",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:13331,Performance,perform,performing,13331,"ypename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredWeightGradients. protected . The sum of the square of the past weight gradients associated with the deep net. ; Definition at line 55 of file RMSProp.h. ◆ fRho. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fRho. protected . The Rho constant used by the optimizer. ; Definition at line 52 of file RMSProp.h. ◆ fWeightUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWeightUpdates. protected . The accumulation of the past Weights for performing updates. ; Definition at line 59 of file RMSProp.h. ◆ fWorkBiasTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor1. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 64 of file RMSProp.h. ◆ fWorkBiasTensor2. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fWorkBiasTensor2. protected . working tensor used to keep a temporary copy of bias or bias gradients ; Definition at line 68 of file RMSProp.h. ◆ fWorkWeightTensor1. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNe",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:3094,Safety,avoid,avoid,3094,"eepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of the past weight gradients associated with the deep net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:11435,Safety,avoid,avoid,11435,"eights ; (; size_t ; layerIndex, . std::vector< Matrix_t > & ; weights, . const std::vector< Matrix_t > & ; weightGradients . ). protectedvirtual . Update the weights, given the current weight gradients. ; Implements TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >.; Definition at line 152 of file RMSProp.h. Member Data Documentation. ◆ fBiasUpdates. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fBiasUpdates. protected . The accumulation of the past Biases for performing updates. ; Definition at line 60 of file RMSProp.h. ◆ fEpsilon. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fEpsilon. protected . The Smoothing term used to avoid division by zero. ; Definition at line 53 of file RMSProp.h. ◆ fMomentum. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fMomentum. protected . The momentum used for training. ; Definition at line 51 of file RMSProp.h. ◆ fPastSquaredBiasGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector<std::vector<Matrix_t> > TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::fPastSquaredBiasGradients. protected . The sum of the square of the past bias gradients associated with the deep net. ; Definition at line 57 of file RMSProp.h. ◆ fPastSquaredWeightGradients. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::ve",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:1144,Usability,learn,learningRate,1144,"nce Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >RMSProp Optimizer class. ; This class represents the RMSProp Optimizer with options for applying momentum. ; Definition at line 45 of file RMSProp.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TRMSProp (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t momentum=0.0, Scalar_t rho=0.9, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TRMSProp ()=default;  Destructor. ;  ; std::vector< std::vector< Matrix_t > > & GetBiasUpdates ();  ; std::vector< Matrix_t > & GetBiasUpdatesAt (size_t i);  ; Scalar_t GetEpsilon () const;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; Scalar_t GetRho () const;  ; std::vector< std::vector< Matrix_t > > & GetWeightUpdates ();  ; std::vector< Matrix_t > & GetWeightUpdatesAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; v",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:2064,Usability,learn,learningRate,2064,"chitecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TRMSProp (DeepNet_t &deepNet, Scalar_t learningRate=0.001, Scalar_t momentum=0.0, Scalar_t rho=0.9, Scalar_t epsilon=1e-7);  Constructor. ;  ;  ~TRMSProp ()=default;  Destructor. ;  ; std::vector< std::vector< Matrix_t > > & GetBiasUpdates ();  ; std::vector< Matrix_t > & GetBiasUpdatesAt (size_t i);  ; Scalar_t GetEpsilon () const;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; Scalar_t GetRho () const;  ; std::vector< std::vector< Matrix_t > > & GetWeightUpdates ();  ; std::vector< Matrix_t > & GetWeightUpdatesAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, con",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:2432,Usability,learn,learningRate,2432,"lon () const;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredBiasGradients ();  ; std::vector< Matrix_t > & GetPastSquaredBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastSquaredWeightGradients ();  ; std::vector< Matrix_t > & GetPastSquaredWeightGradientsAt (size_t i);  ; Scalar_t GetRho () const;  ; std::vector< std::vector< Matrix_t > > & GetWeightUpdates ();  ; std::vector< Matrix_t > & GetWeightUpdatesAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; std::vector< std::vector< Matrix_t > > fBiasUpdates;  The accumulation of the past Biases for performing updates. ;  ; Scalar_t fEpsilon;  The Smoothing term used to avoid division by zero. ;  ; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredBiasGradients;  The sum of the square of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastSquaredWeightGradients;  The sum of the square of t",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:4479,Usability,learn,learning,4479,"net. ;  ; Scalar_t fRho;  The Rho constant used by the optimizer. ;  ; std::vector< std::vector< Matrix_t > > fWeightUpdates;  The accumulation of the past Weights for performing updates. ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor1;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkBiasTensor2;  working tensor used to keep a temporary copy of bias or bias gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor1;  working tensor used to keep a temporary copy of weights or weight gradients ;  ; std::vector< std::vector< Matrix_t > > fWorkWeightTensor2;  working tensor used to keep a temporary copy of weights or weight gradients ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/RMSProp.h>. Inheritance diagram for TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file RMSProp.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file RMSProp.h. Constructor & Destructor Documentation. ◆ TRMSProp(). template<typename Architecture_t , typename Layer_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html:5612,Usability,learn,learningRate,5612,"_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 47 of file RMSProp.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 48 of file RMSProp.h. Constructor & Destructor Documentation. ◆ TRMSProp(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::TRMSProp ; (; DeepNet_t & ; deepNet, . Scalar_t ; learningRate = 0.001, . Scalar_t ; momentum = 0.0, . Scalar_t ; rho = 0.9, . Scalar_t ; epsilon = 1e-7 . ). Constructor. ; Definition at line 107 of file RMSProp.h. ◆ ~TRMSProp(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::~TRMSProp ; (; ). default . Destructor. . Member Function Documentation. ◆ GetBiasUpdates(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector< std::vector< Matrix_t > > & TMVA::DNN::TRMSProp< Architecture_t, Layer_t, DeepNet_t >::GetBiasUpdates ; (; ). inline . Definition at line 98 of file RMSProp.h. ◆ GetBiasUpdatesAt(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector< Matrix_t > & TMVA::DNN::TRMSProp< ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TRMSProp.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TRMSProp.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:951,Modifiability,inherit,inherited,951,". ROOT: TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >Stochastic Batch Gradient Descent Optimizer class. ; This class represents the Stochastic Batch Gradient Descent Optimizer with options for applying momentum and nesterov momentum. ; Definition at line 46 of file SGD.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TSGD (Scalar_t learningRate, DeepNet_t &deepNet, Scalar_t momentum);  Constructor. ;  ;  ~TSGD ()=default;  Destructor. ;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastBiasGradients ();  ; std::vector< Matrix_t > & GetPastBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastWeightGradients ();  ; std::vector< Matrix_t > & GetPastWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:1629,Modifiability,inherit,inherited,1629,"TSGD< Architecture_t, Layer_t, DeepNet_t >Stochastic Batch Gradient Descent Optimizer class. ; This class represents the Stochastic Batch Gradient Descent Optimizer with options for applying momentum and nesterov momentum. ; Definition at line 46 of file SGD.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TSGD (Scalar_t learningRate, DeepNet_t &deepNet, Scalar_t momentum);  Constructor. ;  ;  ~TSGD ()=default;  Destructor. ;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastBiasGradients ();  ; std::vector< Matrix_t > & GetPastBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastWeightGradients ();  ; std::vector< Matrix_t > & GetPastWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:2935,Modifiability,inherit,inherited,2935,"t;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastBiasGradients;  The sum of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastWeightGradients;  The sum of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/SGD.h>. Inheritance diagram for TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 48 of file SGD.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architec",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:2161,Performance,optimiz,optimization,2161,"mber Functions;  TSGD (Scalar_t learningRate, DeepNet_t &deepNet, Scalar_t momentum);  Constructor. ;  ;  ~TSGD ()=default;  Destructor. ;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastBiasGradients ();  ; std::vector< Matrix_t > & GetPastBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastWeightGradients ();  ; std::vector< Matrix_t > & GetPastWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastBiasGradients;  The sum of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastWeightGradients;  The sum of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scal",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:1178,Usability,learn,learningRate,1178,"; Public Types |; Public Member Functions |; Protected Member Functions |; Protected Attributes |; List of all members ; TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t > Class Template Reference. ; template<typename Architecture_t, typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>>; class TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >Stochastic Batch Gradient Descent Optimizer class. ; This class represents the Stochastic Batch Gradient Descent Optimizer with options for applying momentum and nesterov momentum. ; Definition at line 46 of file SGD.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TSGD (Scalar_t learningRate, DeepNet_t &deepNet, Scalar_t momentum);  Constructor. ;  ;  ~TSGD ()=default;  Destructor. ;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastBiasGradients ();  ; std::vector< Matrix_t > & GetPastBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastWeightGradients ();  ; std::vector< Matrix_t > & GetPastWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected M",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:1727,Usability,learn,learningRate,1727,"TSGD< Architecture_t, Layer_t, DeepNet_t >Stochastic Batch Gradient Descent Optimizer class. ; This class represents the Stochastic Batch Gradient Descent Optimizer with options for applying momentum and nesterov momentum. ; Definition at line 46 of file SGD.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ;  Public Types inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TSGD (Scalar_t learningRate, DeepNet_t &deepNet, Scalar_t momentum);  Constructor. ;  ;  ~TSGD ()=default;  Destructor. ;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastBiasGradients ();  ; std::vector< Matrix_t > & GetPastBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastWeightGradients ();  ; std::vector< Matrix_t > & GetPastWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:2095,Usability,learn,learningRate,2095,"calar_t = typename Architecture_t::Scalar_t;  . Public Member Functions;  TSGD (Scalar_t learningRate, DeepNet_t &deepNet, Scalar_t momentum);  Constructor. ;  ;  ~TSGD ()=default;  Destructor. ;  ; Scalar_t GetMomentum () const;  Getters. ;  ; std::vector< std::vector< Matrix_t > > & GetPastBiasGradients ();  ; std::vector< Matrix_t > & GetPastBiasGradientsAt (size_t i);  ; std::vector< std::vector< Matrix_t > > & GetPastWeightGradients ();  ; std::vector< Matrix_t > & GetPastWeightGradientsAt (size_t i);  ;  Public Member Functions inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >;  VOptimizer (Scalar_t learningRate, DeepNet_t &deepNet);  Constructor. ;  ; virtual ~VOptimizer ()=default;  Virtual Destructor. ;  ; size_t GetGlobalStep () const;  ; Layer_t * GetLayerAt (size_t i);  ; std::vector< Layer_t * > & GetLayers ();  ; Scalar_t GetLearningRate () const;  Getters. ;  ; void IncrementGlobalStep ();  Increments the global step. ;  ; void SetLearningRate (size_t learningRate);  Setters. ;  ; void Step ();  Performs one step of optimization. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastBiasGradients;  The sum of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastWeightGradients;  The sum of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep; ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:3172,Usability,learn,learning,3172,"ion. ;  . Protected Member Functions; void UpdateBiases (size_t layerIndex, std::vector< Matrix_t > &biases, const std::vector< Matrix_t > &biasGradients);  Update the biases, given the current bias gradients. ;  ; void UpdateWeights (size_t layerIndex, std::vector< Matrix_t > &weights, const std::vector< Matrix_t > &weightGradients);  Update the weights, given the current weight gradients. ;  . Protected Attributes; Scalar_t fMomentum;  The momentum used for training. ;  ; std::vector< std::vector< Matrix_t > > fPastBiasGradients;  The sum of the past bias gradients associated with the deep net. ;  ; std::vector< std::vector< Matrix_t > > fPastWeightGradients;  The sum of the past weight gradients associated with the deep net. ;  ;  Protected Attributes inherited from TMVA::DNN::VOptimizer< Architecture_t, Layer_t, DeepNet_t >; DeepNet_t & fDeepNet;  The reference to the deep net. ;  ; size_t fGlobalStep;  The current global step count during training. ;  ; Scalar_t fLearningRate;  The learning rate used for training. ;  . #include <TMVA/DNN/SGD.h>. Inheritance diagram for TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 48 of file SGD.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 49 of file SGD.h. Constructor & Destructor Documentation. ◆ TSGD(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . TMV",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html:4244,Usability,learn,learningRate,4244,"clude <TMVA/DNN/SGD.h>. Inheritance diagram for TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >:. This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.; [legend]; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 48 of file SGD.h. ◆ Scalar_t. template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . using TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::Scalar_t = typename Architecture_t::Scalar_t. Definition at line 49 of file SGD.h. Constructor & Destructor Documentation. ◆ TSGD(). template<typename Architecture_t , typename Layer_t , typename DeepNet_t > . TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::TSGD ; (; Scalar_t ; learningRate, . DeepNet_t & ; deepNet, . Scalar_t ; momentum . ). Constructor. ; Definition at line 86 of file SGD.h. ◆ ~TSGD(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::~TSGD ; (; ). default . Destructor. . Member Function Documentation. ◆ GetMomentum(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . Scalar_t TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::GetMomentum ; (; ); const. inline . Getters. ; Definition at line 72 of file SGD.h. ◆ GetPastBiasGradients(). template<typename Architecture_t , typename Layer_t = VGeneralLayer<Architecture_t>, typename DeepNet_t = TDeepNet<Architecture_t, Layer_t>> . std::vector< std::vector< Matrix_t > > & TMVA::DNN::TSGD< Architecture_t, Layer_t, DeepNet_t >::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSGD.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSGD.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSharedLayer.html:460,Modifiability,layers,layers,460,". ROOT: TMVA::DNN::TSharedLayer< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TSharedLayer< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TSharedLayer< Architecture_t >Layer class width shared weight and bias layers. ; Like the Layer class only that weight matrices are shared between different instances of the net, which can be used to implement multithreading 'Hogwild' style. ; Definition at line 147 of file Layer.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Scalar_t = typename Architecture_t::Scalar_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Public Member Functions;  TSharedLayer (const TSharedLayer &layer);  ;  TSharedLayer (size_t fBatchSize, TLayer< Architecture_t > &layer);  ; void Backward (Matrix_t &gradients_backward, const Matrix_t &activations_backward, ERegularization r, Scalar_t weightDecay);  Compute weight, bias and activation gradients. ;  ; void Forward (Matrix_t &input, bool applyDropout=false);  Compute activation of the layer for the given input. ;  ; EActivationFunction GetActivationFunction () const;  ; Matrix_t & GetActivationGradients ();  ; const Matrix_t & GetActivationGradients () const;  ; size_t GetBatchSize () const;  ; Matrix_t & GetBiases ();  ; const Matrix_t & GetBiases () const;  ; Matrix_t & GetBiasGradients ();  ; const Matrix_t & GetBiasGradients () const;  ; size_t GetDropoutProbability () const;  ; size_t GetInputWidth () const;  ; Matrix_t & GetOutput ();  ; const Matrix_t & GetOutput () const;  ; Matrix_t & GetWeightGradients ();  ; const Matrix_t & GetWeightGradients () const;  ; Matrix_t & GetWeights () const;  ; size_t GetWidth () const;  ; void Print () const;  ; void SetDropoutProbability (Scalar_t p);  . Private Attributes;",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TSharedLayer.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TSharedLayer.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorBatch.html:582,Security,access,accessed,582,". ROOT: TMVA::DNN::TTensorBatch< Architecture_t > Class Template Reference. ; ROOT  ; . master. Reference Guide ; .  . Loading...; Searching...; No Matches. List of all members |; Public Types |; Public Member Functions |; Private Attributes |; List of all members ; TMVA::DNN::TTensorBatch< Architecture_t > Class Template Reference. ; template<typename Architecture_t>; class TMVA::DNN::TTensorBatch< Architecture_t >TTensorBatch. ; Class representing training batches consisting of a vector of matrices as input data and a matrix of output data. The input and output data can be accessed using the GetInput() and GetOutput() member functions.; Template Parameters. Architecture_tThe underlying architecture. . Definition at line 59 of file TensorDataLoader.h. Public Types; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Public Member Functions;  TTensorBatch (const TTensorBatch &)=default;  ;  TTensorBatch (Tensor_t &, Matrix_t &, Matrix_t &);  ;  TTensorBatch (TTensorBatch &&)=default;  ; Tensor_t & GetInput ();  Return the tensor representing the input data. ;  ; Matrix_t & GetOutput ();  Return the matrix representing the output data. ;  ; Matrix_t & GetWeights ();  Return the matrix holding the event weights. ;  ; TTensorBatch & operator= (const TTensorBatch &)=default;  ; TTensorBatch & operator= (TTensorBatch &&)=default;  . Private Attributes; Tensor_t fInputTensor;  The input tensor batch, one matrix one input. ;  ; Matrix_t fOutputMatrix;  The output matrix representing the ground truth. ;  ; Matrix_t fWeightMatrix;  The event/example weights. ;  . #include <TMVA/DNN/TensorDataLoader.h>; Member Typedef Documentation. ◆ Matrix_t. template<typename Architecture_t > . using TMVA::DNN::TTensorBatch< Architecture_t >::Matrix_t = typename Architecture_t::Matrix_t. Definition at line 61 of file TensorDataLoader.h. ◆ Tensor_t. template<typename Architecture_t > . using TMVA::DNN::TTensorBatch< Architecture_t >::",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorBatch.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorBatch.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html:25224,Integrability,rout,routine,25224,">::GetTensorBatch. Return the next batch from the training set. ; The TTensorDataLoader object keeps an internal counter that cycles over the batches in the training set. ; Definition at line 233 of file TensorDataLoader.h. ◆ operator=() [1/2]. template<typename Data_t , typename Architecture_t > . TTensorDataLoader & TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::operator= ; (; const TTensorDataLoader< Data_t, Architecture_t > & ; ). default . ◆ operator=() [2/2]. template<typename Data_t , typename Architecture_t > . TTensorDataLoader & TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::operator= ; (; TTensorDataLoader< Data_t, Architecture_t > && ; ). default . ◆ Shuffle(). template<typename Data_t , typename Architecture_t > . template<typename RNG > . void TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::Shuffle ; (; RNG & ; rng). Shuffle the order of the samples in the batch. ; The shuffling is indirect, i.e. only the indices are shuffled. No input data is moved by this routine. ; Definition at line 285 of file TensorDataLoader.h. Member Data Documentation. ◆ fBatchDepth. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fBatchDepth. private . The number of matrices in the tensor. ; Definition at line 146 of file TensorDataLoader.h. ◆ fBatchHeight. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fBatchHeight. private . The number od rows in each matrix. ; Definition at line 147 of file TensorDataLoader.h. ◆ fBatchIndex. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fBatchIndex. private . The index of the batch when there are multiple batches in parallel. ; Definition at line 150 of file TensorDataLoader.h. ◆ fBatchSize. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html:7569,Performance,load,loaded,7569,"ader & operator= (const TTensorDataLoader &)=default;  ; TTensorDataLoader & operator= (TTensorDataLoader &&)=default;  ; template<typename RNG > ; void Shuffle (RNG &rng);  Shuffle the order of the samples in the batch. ;  . Private Types; using BatchIterator_t = TTensorBatchIterator< Data_t, Architecture_t >;  ; using DeviceBuffer_t = typename Architecture_t::DeviceBuffer_t;  ; using HostBuffer_t = typename Architecture_t::HostBuffer_t;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Shape_t = typename Architecture_t::Tensor_t::Shape_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Private Attributes; size_t fBatchDepth;  The number of matrices in the tensor. ;  ; size_t fBatchHeight;  The number od rows in each matrix. ;  ; size_t fBatchIndex;  The index of the batch when there are multiple batches in parallel. ;  ; size_t fBatchSize;  The size of a batch. ;  ; size_t fBatchWidth;  The number of columns in each matrix. ;  ; const Data_t & fData;  The data that should be loaded in the batches. ;  ; std::vector< DeviceBuffer_t > fDeviceBuffers;  The device buffers used to keep the input, output and weight data. ;  ; std::vector< HostBuffer_t > fHostBuffers;  The host buffers used to load the input, output and weight data. ;  ; Shape_t fInputLayout;  The input data layout (does not include batch size) ;  ; size_t fNOutputFeatures;  The number of outputs from the classifier/regressor. ;  ; size_t fNSamples;  The total number of samples in the dataset. ;  ; size_t fNStreams;  Number of buffer pairs. ;  ; std::vector< size_t > fSampleIndices;  Ordering of the samples in the epoch. ;  . #include <TMVA/DNN/TensorDataLoader.h>; Member Typedef Documentation. ◆ BatchIterator_t. template<typename Data_t , typename Architecture_t > . using TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::BatchIterator_t = TTensorBatchIterator<Data_t, Architecture_t>. private . Definition at line 140 of file TensorDataLoader.h. ◆ DeviceBuffer_t. template<typen",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html:7784,Performance,load,load,7784," batch. ;  . Private Types; using BatchIterator_t = TTensorBatchIterator< Data_t, Architecture_t >;  ; using DeviceBuffer_t = typename Architecture_t::DeviceBuffer_t;  ; using HostBuffer_t = typename Architecture_t::HostBuffer_t;  ; using Matrix_t = typename Architecture_t::Matrix_t;  ; using Shape_t = typename Architecture_t::Tensor_t::Shape_t;  ; using Tensor_t = typename Architecture_t::Tensor_t;  . Private Attributes; size_t fBatchDepth;  The number of matrices in the tensor. ;  ; size_t fBatchHeight;  The number od rows in each matrix. ;  ; size_t fBatchIndex;  The index of the batch when there are multiple batches in parallel. ;  ; size_t fBatchSize;  The size of a batch. ;  ; size_t fBatchWidth;  The number of columns in each matrix. ;  ; const Data_t & fData;  The data that should be loaded in the batches. ;  ; std::vector< DeviceBuffer_t > fDeviceBuffers;  The device buffers used to keep the input, output and weight data. ;  ; std::vector< HostBuffer_t > fHostBuffers;  The host buffers used to load the input, output and weight data. ;  ; Shape_t fInputLayout;  The input data layout (does not include batch size) ;  ; size_t fNOutputFeatures;  The number of outputs from the classifier/regressor. ;  ; size_t fNSamples;  The total number of samples in the dataset. ;  ; size_t fNStreams;  Number of buffer pairs. ;  ; std::vector< size_t > fSampleIndices;  Ordering of the samples in the epoch. ;  . #include <TMVA/DNN/TensorDataLoader.h>; Member Typedef Documentation. ◆ BatchIterator_t. template<typename Data_t , typename Architecture_t > . using TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::BatchIterator_t = TTensorBatchIterator<Data_t, Architecture_t>. private . Definition at line 140 of file TensorDataLoader.h. ◆ DeviceBuffer_t. template<typename Data_t , typename Architecture_t > . using TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::DeviceBuffer_t = typename Architecture_t::DeviceBuffer_t. private . Definition at line 136 of file TensorDataL",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html:26733,Performance,load,loaded,26733," rows in each matrix. ; Definition at line 147 of file TensorDataLoader.h. ◆ fBatchIndex. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fBatchIndex. private . The index of the batch when there are multiple batches in parallel. ; Definition at line 150 of file TensorDataLoader.h. ◆ fBatchSize. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fBatchSize. private . The size of a batch. ; Definition at line 144 of file TensorDataLoader.h. ◆ fBatchWidth. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fBatchWidth. private . The number of columns in each matrix. ; Definition at line 148 of file TensorDataLoader.h. ◆ fData. template<typename Data_t , typename Architecture_t > . const Data_t& TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fData. private . The data that should be loaded in the batches. ; Definition at line 142 of file TensorDataLoader.h. ◆ fDeviceBuffers. template<typename Data_t , typename Architecture_t > . std::vector<DeviceBuffer_t> TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fDeviceBuffers. private . The device buffers used to keep the input, output and weight data. ; Definition at line 154 of file TensorDataLoader.h. ◆ fHostBuffers. template<typename Data_t , typename Architecture_t > . std::vector<HostBuffer_t> TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fHostBuffers. private . The host buffers used to load the input, output and weight data. ; Definition at line 155 of file TensorDataLoader.h. ◆ fInputLayout. template<typename Data_t , typename Architecture_t > . Shape_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fInputLayout. private . The input data layout (does not include batch size) ; Definition at line 145 of file TensorDataLoader.h. ◆ fNOutputFeatures. template<typename Data_t , typename Archi",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html:27314,Performance,load,load,27314,"idth. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fBatchWidth. private . The number of columns in each matrix. ; Definition at line 148 of file TensorDataLoader.h. ◆ fData. template<typename Data_t , typename Architecture_t > . const Data_t& TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fData. private . The data that should be loaded in the batches. ; Definition at line 142 of file TensorDataLoader.h. ◆ fDeviceBuffers. template<typename Data_t , typename Architecture_t > . std::vector<DeviceBuffer_t> TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fDeviceBuffers. private . The device buffers used to keep the input, output and weight data. ; Definition at line 154 of file TensorDataLoader.h. ◆ fHostBuffers. template<typename Data_t , typename Architecture_t > . std::vector<HostBuffer_t> TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fHostBuffers. private . The host buffers used to load the input, output and weight data. ; Definition at line 155 of file TensorDataLoader.h. ◆ fInputLayout. template<typename Data_t , typename Architecture_t > . Shape_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fInputLayout. private . The input data layout (does not include batch size) ; Definition at line 145 of file TensorDataLoader.h. ◆ fNOutputFeatures. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fNOutputFeatures. private . The number of outputs from the classifier/regressor. ; Definition at line 149 of file TensorDataLoader.h. ◆ fNSamples. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >::fNSamples. private . The total number of samples in the dataset. ; Definition at line 143 of file TensorDataLoader.h. ◆ fNStreams. template<typename Data_t , typename Architecture_t > . size_t TMVA::DNN::TTensorDataLoader< Data_t, Architecture_t >",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html:6656,Integrability,rout,routine,6656,"< AReal > >::GetTensorBatch. Return the next batch from the training set. ; The TTensorDataLoader object keeps an internal counter that cycles over the batches in the training set. ; Definition at line 136 of file TensorDataLoader.h. ◆ operator=() [1/2]. template<typename AData , typename AReal > . TTensorDataLoader & TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::operator= ; (; const TTensorDataLoader< AData, TReference< AReal > > & ; ). default . ◆ operator=() [2/2]. template<typename AData , typename AReal > . TTensorDataLoader & TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::operator= ; (; TTensorDataLoader< AData, TReference< AReal > > && ; ). default . ◆ Shuffle(). template<typename AData , typename AReal > . template<typename RNG > . void TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::Shuffle ; (; RNG & ; rng). Shuffle the order of the samples in the batch. ; The shuffling is indirect, i.e. only the indices are shuffled. No input data is moved by this routine. ; Definition at line 130 of file TensorDataLoader.h. Member Data Documentation. ◆ fBatchDepth. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fBatchDepth. private . The number of matrices in the tensor. ; Definition at line 55 of file TensorDataLoader.h. ◆ fBatchHeight. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fBatchHeight. private . The number od rows in each matrix. ; Definition at line 56 of file TensorDataLoader.h. ◆ fBatchIndex. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fBatchIndex. private . The index of the batch when there are multiple batches in parallel. ; Definition at line 59 of file TensorDataLoader.h. ◆ fBatchWidth. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fBatchWidth. private . ",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html:2146,Performance,load,loaded,2146,"erator);  Copy output matrix into the given host buffer. ;  ; void CopyTensorWeights (TMatrixT< AReal > &matrix, IndexIterator_t sampleIterator);  Copy weight matrix into the given host buffer. ;  ; BatchIterator_t end ();  ; TTensorBatch< TReference< AReal > > GetTensorBatch ();  Return the next batch from the training set. ;  ; TTensorDataLoader & operator= (const TTensorDataLoader &)=default;  ; TTensorDataLoader & operator= (TTensorDataLoader &&)=default;  ; template<typename RNG > ; void Shuffle (RNG &rng);  Shuffle the order of the samples in the batch. ;  . Private Types; using BatchIterator_t = TTensorBatchIterator< AData, TReference< AReal > >;  . Private Attributes; size_t fBatchDepth;  The number of matrices in the tensor. ;  ; size_t fBatchHeight;  The number od rows in each matrix. ;  ; size_t fBatchIndex;  The index of the batch when there are multiple batches in parallel. ;  ; size_t fBatchWidth;  The number of columns in each matrix. ;  ; const AData & fData;  The data that should be loaded in the batches. ;  ; std::vector< size_t > fInputShape;  Defines the batch depth, no. of channels and spatial dimensions of an input tensor. ;  ; size_t fNOutputFeatures;  The number of outputs from the classifier/regressor. ;  ; size_t fNSamples;  The total number of samples in the dataset. ;  ; std::vector< size_t > fSampleIndices;  Ordering of the samples in the epoch. ;  ; std::vector< TMatrixT< AReal > > inputTensor;  The 3D tensor used to keep the input data. ;  ; TMatrixT< AReal > outputMatrix;  The matrix used to keep the output. ;  ; TMatrixT< AReal > weightMatrix;  The matrix used to keep the batch weights. ;  . #include <TMVA/DNN/Architectures/Reference/TensorDataLoader.h>; Member Typedef Documentation. ◆ BatchIterator_t. template<typename AData , typename AReal > . using TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::BatchIterator_t = TTensorBatchIterator<AData, TReference<AReal> >. private . Definition at line 49 of file TensorDataLoader.",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html
https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html:7902,Performance,load,loaded,7902,"umber of matrices in the tensor. ; Definition at line 55 of file TensorDataLoader.h. ◆ fBatchHeight. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fBatchHeight. private . The number od rows in each matrix. ; Definition at line 56 of file TensorDataLoader.h. ◆ fBatchIndex. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fBatchIndex. private . The index of the batch when there are multiple batches in parallel. ; Definition at line 59 of file TensorDataLoader.h. ◆ fBatchWidth. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fBatchWidth. private . The number of columns in each matrix. ; Definition at line 57 of file TensorDataLoader.h. ◆ fData. template<typename AData , typename AReal > . const AData& TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fData. private . The data that should be loaded in the batches. ; Definition at line 51 of file TensorDataLoader.h. ◆ fInputShape. template<typename AData , typename AReal > . std::vector<size_t> TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fInputShape. private . Defines the batch depth, no. of channels and spatial dimensions of an input tensor. ; Definition at line 61 of file TensorDataLoader.h. ◆ fNOutputFeatures. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fNOutputFeatures. private . The number of outputs from the classifier/regressor. ; Definition at line 58 of file TensorDataLoader.h. ◆ fNSamples. template<typename AData , typename AReal > . size_t TMVA::DNN::TTensorDataLoader< AData, TReference< AReal > >::fNSamples. private . The total number of samples in the dataset. ; Definition at line 53 of file TensorDataLoader.h. ◆ fSampleIndices. template<typename AData , typename AReal > . std::vector<size_t> TMVA::DNN::TTensorDataLoader< AData",MatchSource.WIKI,doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html,root-project,root,v6-32-06,https://root.cern,https://root.cern/doc/master/classTMVA_1_1DNN_1_1TTensorDataLoader_3_01AData_00_01TReference_3_01AReal_01_4_01_4.html
