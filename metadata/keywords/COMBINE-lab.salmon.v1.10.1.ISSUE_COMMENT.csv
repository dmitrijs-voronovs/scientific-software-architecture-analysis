id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/COMBINE-lab/salmon/issues/2#issuecomment-103974922:729,Deployability,update,updated,729,"Hi @Melkaz,; These options have been added as of commit c207d0f28e5782f9a16747a72ac6f06c277fd4ed. There are some new options, all of which have reasonable defaults (the ones that were hard coded before). The relevant options here are: `--fldMean` which you can use to specify the expected mean length of the fragment distribution and `--fldSD` which you can use to specify the expected standard deviation of the fragment length distribution. These values are used to set the _prior_ on the fragment length distribution. This means that if you're using paired-end reads, the observations will overwhelm this prior quickly and we'll learn the empirical distribution. If you're using single-end data, then the prior won't really be updated and the values you specify above are what will be used in practice (e.g. to compute effective transcript lengths). Please let us know if you run into any trouble using this new feature. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/2#issuecomment-103974922
https://github.com/COMBINE-lab/salmon/issues/2#issuecomment-103974922:631,Usability,learn,learn,631,"Hi @Melkaz,; These options have been added as of commit c207d0f28e5782f9a16747a72ac6f06c277fd4ed. There are some new options, all of which have reasonable defaults (the ones that were hard coded before). The relevant options here are: `--fldMean` which you can use to specify the expected mean length of the fragment distribution and `--fldSD` which you can use to specify the expected standard deviation of the fragment length distribution. These values are used to set the _prior_ on the fragment length distribution. This means that if you're using paired-end reads, the observations will overwhelm this prior quickly and we'll learn the empirical distribution. If you're using single-end data, then the prior won't really be updated and the values you specify above are what will be used in practice (e.g. to compute effective transcript lengths). Please let us know if you run into any trouble using this new feature. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/2#issuecomment-103974922
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:570,Deployability,release,releases,570,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:200,Performance,perform,performance,200,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:304,Performance,perform,performance,304,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:360,Testability,test,testing,360,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:555,Testability,test,testing,555,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:662,Testability,test,tests,662,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:748,Testability,test,tests,748,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408:624,Usability,simpl,simple,624,"@vals, it shouldn't be a coverage issue, at least as compared to previous versions of Salmon. Hopefully we'll have a chance to look at this soon and see if we can figure out what might be causing the performance ""regression"" when `--useVBOpt` is enabled. As @dcjones suggests, we haven't really seen any performance degradation with the VB option in our other testing, so I suspect something characteristic of this dataset. @dcjones; it's great to see you drop by! I'm actually looking for a reasonable collection of datasets to do (automated) regression testing on new releases of salmon --- something to replace my fairly simple and manual existing regression tests. I'd greatly appreciate any suggestions or advice you may have about this! Such tests will become even more useful as we're experimenting with a few inference approaches and it would be great to have a reasonable spread of data to see the effects of different strategies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/6#issuecomment-112224408
https://github.com/COMBINE-lab/salmon/issues/8#issuecomment-237931240:237,Usability,learn,learned,237,"@mdshw5 — I certainly think that this information could be useful (and bias terms are taken into effect when computing the effective length, when bias modeling is enabled). The problem is that the position-specific start distribution is learned globally (well, conditioned on a few different length classes), rather than being transcript specific. So, it's not exactly clear how it would help too much in Shaun's case, since this is a particular transcript, where a splicing variation is causing a huge portion of the transcript to have no mapped reads. Unless this happens in many transcripts (globally), this particular transcript's contribution to the global position-specific start distribution will likely be rather small.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/8#issuecomment-237931240
https://github.com/COMBINE-lab/salmon/issues/8#issuecomment-237931240:369,Usability,clear,clear,369,"@mdshw5 — I certainly think that this information could be useful (and bias terms are taken into effect when computing the effective length, when bias modeling is enabled). The problem is that the position-specific start distribution is learned globally (well, conditioned on a few different length classes), rather than being transcript specific. So, it's not exactly clear how it would help too much in Shaun's case, since this is a particular transcript, where a splicing variation is causing a huge portion of the transcript to have no mapped reads. Unless this happens in many transcripts (globally), this particular transcript's contribution to the global position-specific start distribution will likely be rather small.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/8#issuecomment-237931240
https://github.com/COMBINE-lab/salmon/issues/10#issuecomment-193623757:147,Deployability,patch,patch,147,"Where does `extract-libdivsufsort.cmake` live? I don't find it in the `salmon` repository. Is it generated automatically by `cmake`? The following patch/hack using `unzip` works around the `cmake -E tar xfz` bug for me. It seems to only affect extracting the `libdivsufsort.zip`, perhaps because it's a `.zip`. If that is the case, and there's a `.tar.gz` distribution of `libdivsufsort`, then there may be a simple fix. ``` diff; --- libdivsufsort-prefix/src/libdivsufsort-stamp/extract-libdivsufsort.cmake.orig 2016-03-07 22:02:35.000000000 -0800; +++ libdivsufsort-prefix/src/libdivsufsort-stamp/extract-libdivsufsort.cmake 2016-03-07 22:06:49.000000000 -0800; @@ -23,7 +23,7 @@; # Extract it:; #; message(STATUS ""extracting... [tar xfz]""); -execute_process(COMMAND ${CMAKE_COMMAND} -E tar xfz ${filename}; +execute_process(COMMAND unzip ${filename}; WORKING_DIRECTORY ${ut_dir}; RESULT_VARIABLE rv). ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/10#issuecomment-193623757
https://github.com/COMBINE-lab/salmon/issues/10#issuecomment-193623757:701,Integrability,message,message,701,"Where does `extract-libdivsufsort.cmake` live? I don't find it in the `salmon` repository. Is it generated automatically by `cmake`? The following patch/hack using `unzip` works around the `cmake -E tar xfz` bug for me. It seems to only affect extracting the `libdivsufsort.zip`, perhaps because it's a `.zip`. If that is the case, and there's a `.tar.gz` distribution of `libdivsufsort`, then there may be a simple fix. ``` diff; --- libdivsufsort-prefix/src/libdivsufsort-stamp/extract-libdivsufsort.cmake.orig 2016-03-07 22:02:35.000000000 -0800; +++ libdivsufsort-prefix/src/libdivsufsort-stamp/extract-libdivsufsort.cmake 2016-03-07 22:06:49.000000000 -0800; @@ -23,7 +23,7 @@; # Extract it:; #; message(STATUS ""extracting... [tar xfz]""); -execute_process(COMMAND ${CMAKE_COMMAND} -E tar xfz ${filename}; +execute_process(COMMAND unzip ${filename}; WORKING_DIRECTORY ${ut_dir}; RESULT_VARIABLE rv). ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/10#issuecomment-193623757
https://github.com/COMBINE-lab/salmon/issues/10#issuecomment-193623757:409,Usability,simpl,simple,409,"Where does `extract-libdivsufsort.cmake` live? I don't find it in the `salmon` repository. Is it generated automatically by `cmake`? The following patch/hack using `unzip` works around the `cmake -E tar xfz` bug for me. It seems to only affect extracting the `libdivsufsort.zip`, perhaps because it's a `.zip`. If that is the case, and there's a `.tar.gz` distribution of `libdivsufsort`, then there may be a simple fix. ``` diff; --- libdivsufsort-prefix/src/libdivsufsort-stamp/extract-libdivsufsort.cmake.orig 2016-03-07 22:02:35.000000000 -0800; +++ libdivsufsort-prefix/src/libdivsufsort-stamp/extract-libdivsufsort.cmake 2016-03-07 22:06:49.000000000 -0800; @@ -23,7 +23,7 @@; # Extract it:; #; message(STATUS ""extracting... [tar xfz]""); -execute_process(COMMAND ${CMAKE_COMMAND} -E tar xfz ${filename}; +execute_process(COMMAND unzip ${filename}; WORKING_DIRECTORY ${ut_dir}; RESULT_VARIABLE rv). ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/10#issuecomment-193623757
https://github.com/COMBINE-lab/salmon/issues/15#issuecomment-144471362:978,Usability,simpl,simple,978,"Hi @nicolasstransky --- thanks for reporting this. Now the question is, how should this be handled? I see at least 2 obvious possibilities :; 1. Assume that the transcript name should be split at the first whitespace character **or** `|`. Currently,; it is only split at the first whitespace.; 2. If a gtf is provided for gene-level quantification, ensure that some non-trivial number of genes (e.g.; more than half?) have at least 1 transcript in the index corresponding to them. If not, then complain. Of course, there are also potentially other, better solutions; so I'm open to suggestions. The problem with 1 is that _de-novo_ assemblers may have transcript names that are not unique up to the first `|`, so that the whole name needs to be taken into account. The problem with 2 is that it alerts the user of this potential issue, but doesn't resolve it. In the latter case, the user could provide the transcript-to-gene mapping using the provided transcript names in the ""simple"" format — i.e. > a simple tab-delimited format where each line contains the name of a transcript and the gene to which it belongs separated by a tab. which is also accepted by the `--geneMap` option. I sort of lean toward 2, but, as I said, am happy to consider other suggestions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/15#issuecomment-144471362
https://github.com/COMBINE-lab/salmon/issues/15#issuecomment-144471362:1004,Usability,simpl,simple,1004,"Hi @nicolasstransky --- thanks for reporting this. Now the question is, how should this be handled? I see at least 2 obvious possibilities :; 1. Assume that the transcript name should be split at the first whitespace character **or** `|`. Currently,; it is only split at the first whitespace.; 2. If a gtf is provided for gene-level quantification, ensure that some non-trivial number of genes (e.g.; more than half?) have at least 1 transcript in the index corresponding to them. If not, then complain. Of course, there are also potentially other, better solutions; so I'm open to suggestions. The problem with 1 is that _de-novo_ assemblers may have transcript names that are not unique up to the first `|`, so that the whole name needs to be taken into account. The problem with 2 is that it alerts the user of this potential issue, but doesn't resolve it. In the latter case, the user could provide the transcript-to-gene mapping using the provided transcript names in the ""simple"" format — i.e. > a simple tab-delimited format where each line contains the name of a transcript and the gene to which it belongs separated by a tab. which is also accepted by the `--geneMap` option. I sort of lean toward 2, but, as I said, am happy to consider other suggestions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/15#issuecomment-144471362
https://github.com/COMBINE-lab/salmon/issues/15#issuecomment-144478110:235,Usability,usab,usable,235,"@mdshw5, the best option I've found so far is actually [rsem-prepare-reference](http://deweylab.biostat.wisc.edu/rsem/rsem-prepare-reference.html). It's a bit slower than gtf-to-fasta, but, so far, seems to do a better job producing a usable transcriptome in the general case.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/15#issuecomment-144478110
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:141,Deployability,install,installed,141,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:564,Deployability,install,installed,564,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:728,Deployability,install,install,728,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:1134,Deployability,install,install,1134,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:1228,Deployability,install,installation,1228,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:25,Integrability,depend,dependencies,25,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:334,Integrability,depend,dependency,334,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:252,Security,expose,expose,252,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957:1386,Usability,learn,learn,1386,"Hi guys,. Which existing dependencies would you like to be able to use? There are some of these libraries that cannot be replaced by already installed variants. Specifically,; - BWA --- since the version that is pulled in and used actually requires we expose certain functionality for our lightweight alignment procedure (though this dependency may go away all together if we deprecate lightweight alignment in favor of quasi-mapping).; - Jellyfish --- here, we require the ability to use jellyfish as a library. Specifically, we rely on some headers that are not installed with the standard package. Perhaps here there could be some synergy with Guillaume on making all of the things Salmon uses part of the standard Jellyfish install, but, at least currently, this isn't the case. The CMake build system already looks for existing versions of the following before fetching them:; - Boost; - tbb; - jemalloc. So, the the remaining guys are `libgff` (which is just some small libraryification of a gff parser that I put together a while ago, I don't know that it's in any package manager --- is it? It doesn't even have an associated install script) and `staden IO lib`. For Staden, I'd be happy to have it look for an existing installation, but there is no FindStaden.cmake that I know of, and I don't really know how to write FindX.cmake files appropriately. However, I'd be happy to learn and / or accept pull requests.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/19#issuecomment-193559957
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:525,Availability,reliab,reliable,525,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:143,Energy Efficiency,efficient,efficient,143,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:432,Performance,perform,performance,432,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:548,Performance,concurren,concurrency-enabled,548,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:108,Security,validat,validation,108,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:482,Testability,test,tested,482,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:540,Testability,test,tested,540,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801:390,Usability,simpl,simple,390,"@ctb — One thing that would be required for this (apart from some engineering of the command-line parsing / validation code) is a trustworthy, efficient, _multithreaded_ `FAST(A/Q)` parser for interleaved format reads. Right now, Salmon (& Sailfish, &RapMap, & most of the other HTS-centric methods we're developing) use the Jellyfish 2 read parser. I've made this choice since it's fairly simple to use, yet provides nice parallel performance and, most importantly, is fairly well-tested and trust-worthy. Can you suggest a reliable, well-tested, concurrency-enabled library for parsing reads in interleaved format?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152827801
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:1094,Availability,error,error,1094,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:101,Performance,perform,performance,101,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:214,Performance,multi-thread,multi-threaded,214,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:274,Performance,concurren,concurrent,274,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:285,Performance,queue,queue,285,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:562,Performance,perform,performance,562,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:881,Performance,perform,performance,881,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225:912,Usability,simpl,simply,912,"On Sun, Nov 01, 2015 at 06:15:19AM -0800, Rob Patro wrote:. > I, too, would like to see the relative performance of the two libraries. The only challenge is in making the comparison apples-to-apples (i.e. enabling multi-threaded parsing in seqtk with minimal overhead ??? a concurrent queue is cheap, but not free). . Other points worth considering:; - there's a runtime overhead to constantly changing sequencing formats. Some; programs want split, others want interleaved. We've settled on interleaved; because it enables streaming, which is a major win (2-4x performance); and; also because having one file is better than having 2 or 4.; - the management overhead to keeping track of many files is less for experts,; but is pretty significant for beginners. Enabling multiple input formats ++. So I think it'd be great to have the basic functionality, identify where; there are performance problems, and then simply note them for future ;). I would like to enable -1 and -2 in khmer scripts, but for our usual use cases; (multiple sequencing files being normalized and/or partitioned and/or error; trimmed) the command line syntax is too confusing ATM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-152829225
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168516778:141,Usability,simpl,simple,141,"Yea; so I think the only way to do this currently (without me modifying the jellyfish parser) is to just use 2 fifos. I could put together a simple bash or python script together for this if there is interest (until we support interleaved format natively, which I'll add to the feature list).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168516778
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168563402:37,Usability,clear,clear,37,"Actually, @mdshw5 --- it's not quite clear to me why the parser isn't doing the right thing in this case. If you take a look at how the paired-end sequence parser is actually populating the internal buffer (e.g. [here](https://github.com/COMBINE-lab/salmon/blob/master/include/PairSequenceParser.hpp#L182)), it is reading one entry from stream1 and then one entry from stream2. I'm guessing there may be some issue with having two different handles open to the same fifo? However, that doesn't seem like it should be a problem. Given the way the code is actually reading from the different streams, it's not clear to me why it's not currently working as expected. I'll try and take a deeper look.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168563402
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168563402:608,Usability,clear,clear,608,"Actually, @mdshw5 --- it's not quite clear to me why the parser isn't doing the right thing in this case. If you take a look at how the paired-end sequence parser is actually populating the internal buffer (e.g. [here](https://github.com/COMBINE-lab/salmon/blob/master/include/PairSequenceParser.hpp#L182)), it is reading one entry from stream1 and then one entry from stream2. I'm guessing there may be some issue with having two different handles open to the same fifo? However, that doesn't seem like it should be a problem. Given the way the code is actually reading from the different streams, it's not clear to me why it's not currently working as expected. I'll try and take a deeper look.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168563402
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168566647:18,Testability,test,test,18,"I actually didn't test it :). I'll confirm the current behavior tomorrow. Thanks for following up on this!. > On Jan 3, 2016, at 8:37 PM, Rob Patro notifications@github.com wrote:; > ; > Actually, @mdshw5 --- it's not quite clear to me why the parser isn't doing the right thing in this case. If you take a look at how the paired-end sequence parser is actually populating the internal buffer (e.g. here), it is reading one entry from stream1 and then one entry from stream2. I'm guessing there may be some issue with having two different handles open to the same fifo? However, that doesn't seem like it should be a problem. Given the way the code is actually reading from the different streams, it's not clear to me why it's not currently working as expected. I'll try and take a deeper look.; > ; > —; > Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168566647
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168566647:224,Usability,clear,clear,224,"I actually didn't test it :). I'll confirm the current behavior tomorrow. Thanks for following up on this!. > On Jan 3, 2016, at 8:37 PM, Rob Patro notifications@github.com wrote:; > ; > Actually, @mdshw5 --- it's not quite clear to me why the parser isn't doing the right thing in this case. If you take a look at how the paired-end sequence parser is actually populating the internal buffer (e.g. here), it is reading one entry from stream1 and then one entry from stream2. I'm guessing there may be some issue with having two different handles open to the same fifo? However, that doesn't seem like it should be a problem. Given the way the code is actually reading from the different streams, it's not clear to me why it's not currently working as expected. I'll try and take a deeper look.; > ; > —; > Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168566647
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168566647:706,Usability,clear,clear,706,"I actually didn't test it :). I'll confirm the current behavior tomorrow. Thanks for following up on this!. > On Jan 3, 2016, at 8:37 PM, Rob Patro notifications@github.com wrote:; > ; > Actually, @mdshw5 --- it's not quite clear to me why the parser isn't doing the right thing in this case. If you take a look at how the paired-end sequence parser is actually populating the internal buffer (e.g. here), it is reading one entry from stream1 and then one entry from stream2. I'm guessing there may be some issue with having two different handles open to the same fifo? However, that doesn't seem like it should be a problem. Given the way the code is actually reading from the different streams, it's not clear to me why it's not currently working as expected. I'll try and take a deeper look.; > ; > —; > Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168566647
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168582355:662,Usability,simpl,simply,662,"[Here's](https://gist.github.com/rob-p/963848ba35fb49fefaa3) a sketch of a shell script-based solution that might work. It relies on [this](https://gist.github.com/nathanhaigh/3521724) shell script to do the de-interleaving (but it can use whichever tool we might decide is best for the job). You'd run it with the interleaved file like so:. ```; ./runner.sh salmon quant -i index -l IU --interleaved interleaved.fq -o interleaved_quant; ```. Basically, the script checks to see if the `--interleaved` parameter is present. If so, it handles making the fifos and constructing the proper salmon command with them. Otherwise, if there is no --interleaved file, it simply runs the command as given.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168582355
https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168727465:289,Usability,learn,learned,289,"Yup; I think there are some potential places for improvement (e.g. the interleaved splitting code could be incorporated directly into this script, and the parsing could be improved to handle multiple interleaved files directly), but it seems to work pretty well. Also, when making this, I learned about the `trap` command, which should do what we want in terms of ensuring that any created fifos are cleaned up.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/29#issuecomment-168727465
https://github.com/COMBINE-lab/salmon/issues/32#issuecomment-166890654:476,Usability,simpl,simply,476,"Hi @HamletShaoE --- what version of Salmon are you using? If you are using the latest version, in addition to the `quant.sf` file, there is a file in the output directory called `stats.tsv`. The format of this file is a list of key-value pairs. The first key-value pair lists the total number of observed fragments in the input (and can be ignored for your purposes). Each subsequent line lists a transcript id followed by that transcript's computed effective length. You can simply take these values and join them (in the database / data frame sense) with the main quantification results. I should note that the next version of Salmon (v0.6.0), which should be out shortly, in addition to including a number of improvements and new features, will make these effective length values easier to get at --- they will appear in the main `quant.sf` file (versions compiled from the develop branch will already do this). For the time being, however, the `stats.tsv` file is the place to get this info. Finally, I'd mention that, though I don't know your use case, I'd be cautious of using FPKM any place that TPM might be used instead. Within a sample, they are proportional (thus the equation you provide), but between samples, FPKM values have un-necessary variation based on the average lengths of the expressed transcripts in the samples; an arbitrary variation for which TPM corrects.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/32#issuecomment-166890654
https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553:1016,Energy Efficiency,efficient,efficiently,1016,"Hi @roryk,. Salmon doesn't currently have the ability to output a pseudobam, but that is definitely possible (and not too difficult). We have a related feature planned; perhaps you could tell me if it suits your use case. However, first, I should mention that if you'd simply like a pseudobam for _all_ the mapping locations of the reads, you can use [RapMap](https://github.com/COMBINE-lab/RapMap). RapMap implements the quasi-mapping algorithm upon which Salmon and Sailfish are based (and RapMap is used as a library in the Salmon and Sailfish codebases). Given an index and set of reads, RapMap will report all of the multi-mapping locations that Salmon and Sailfish would consider during quantification. The other feature we have in the works is to have Salmon optionally output a `.bam` file (with actual alignments) post-quantification. It turns out that, given the quasi-mapping information and the quantification results, taking the extra step from quasi-mapping to an actual _alignment_ can be done fairly efficiently. In this mode, Salmon would make one more pass over the reads and, considering the estimated abundances, sample a single alignment for each multi-mapping read proportional to the relative abundance of the different multi-mapping targets (i.e. it would perform a sampling over the multi-mapping locations that would, in expectation, give the same abundances as the _soft_ assignments computed by the optimization algorithm). This feature will be very useful for [transrate](https://github.com/Blahah/transrate). However, given that your goal is to use outside information to perform the filtering yourself, this option may not be ideal for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553
https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553:1280,Performance,perform,perform,1280,"Hi @roryk,. Salmon doesn't currently have the ability to output a pseudobam, but that is definitely possible (and not too difficult). We have a related feature planned; perhaps you could tell me if it suits your use case. However, first, I should mention that if you'd simply like a pseudobam for _all_ the mapping locations of the reads, you can use [RapMap](https://github.com/COMBINE-lab/RapMap). RapMap implements the quasi-mapping algorithm upon which Salmon and Sailfish are based (and RapMap is used as a library in the Salmon and Sailfish codebases). Given an index and set of reads, RapMap will report all of the multi-mapping locations that Salmon and Sailfish would consider during quantification. The other feature we have in the works is to have Salmon optionally output a `.bam` file (with actual alignments) post-quantification. It turns out that, given the quasi-mapping information and the quantification results, taking the extra step from quasi-mapping to an actual _alignment_ can be done fairly efficiently. In this mode, Salmon would make one more pass over the reads and, considering the estimated abundances, sample a single alignment for each multi-mapping read proportional to the relative abundance of the different multi-mapping targets (i.e. it would perform a sampling over the multi-mapping locations that would, in expectation, give the same abundances as the _soft_ assignments computed by the optimization algorithm). This feature will be very useful for [transrate](https://github.com/Blahah/transrate). However, given that your goal is to use outside information to perform the filtering yourself, this option may not be ideal for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553
https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553:1427,Performance,optimiz,optimization,1427,"Hi @roryk,. Salmon doesn't currently have the ability to output a pseudobam, but that is definitely possible (and not too difficult). We have a related feature planned; perhaps you could tell me if it suits your use case. However, first, I should mention that if you'd simply like a pseudobam for _all_ the mapping locations of the reads, you can use [RapMap](https://github.com/COMBINE-lab/RapMap). RapMap implements the quasi-mapping algorithm upon which Salmon and Sailfish are based (and RapMap is used as a library in the Salmon and Sailfish codebases). Given an index and set of reads, RapMap will report all of the multi-mapping locations that Salmon and Sailfish would consider during quantification. The other feature we have in the works is to have Salmon optionally output a `.bam` file (with actual alignments) post-quantification. It turns out that, given the quasi-mapping information and the quantification results, taking the extra step from quasi-mapping to an actual _alignment_ can be done fairly efficiently. In this mode, Salmon would make one more pass over the reads and, considering the estimated abundances, sample a single alignment for each multi-mapping read proportional to the relative abundance of the different multi-mapping targets (i.e. it would perform a sampling over the multi-mapping locations that would, in expectation, give the same abundances as the _soft_ assignments computed by the optimization algorithm). This feature will be very useful for [transrate](https://github.com/Blahah/transrate). However, given that your goal is to use outside information to perform the filtering yourself, this option may not be ideal for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553
https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553:1602,Performance,perform,perform,1602,"Hi @roryk,. Salmon doesn't currently have the ability to output a pseudobam, but that is definitely possible (and not too difficult). We have a related feature planned; perhaps you could tell me if it suits your use case. However, first, I should mention that if you'd simply like a pseudobam for _all_ the mapping locations of the reads, you can use [RapMap](https://github.com/COMBINE-lab/RapMap). RapMap implements the quasi-mapping algorithm upon which Salmon and Sailfish are based (and RapMap is used as a library in the Salmon and Sailfish codebases). Given an index and set of reads, RapMap will report all of the multi-mapping locations that Salmon and Sailfish would consider during quantification. The other feature we have in the works is to have Salmon optionally output a `.bam` file (with actual alignments) post-quantification. It turns out that, given the quasi-mapping information and the quantification results, taking the extra step from quasi-mapping to an actual _alignment_ can be done fairly efficiently. In this mode, Salmon would make one more pass over the reads and, considering the estimated abundances, sample a single alignment for each multi-mapping read proportional to the relative abundance of the different multi-mapping targets (i.e. it would perform a sampling over the multi-mapping locations that would, in expectation, give the same abundances as the _soft_ assignments computed by the optimization algorithm). This feature will be very useful for [transrate](https://github.com/Blahah/transrate). However, given that your goal is to use outside information to perform the filtering yourself, this option may not be ideal for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553
https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553:269,Usability,simpl,simply,269,"Hi @roryk,. Salmon doesn't currently have the ability to output a pseudobam, but that is definitely possible (and not too difficult). We have a related feature planned; perhaps you could tell me if it suits your use case. However, first, I should mention that if you'd simply like a pseudobam for _all_ the mapping locations of the reads, you can use [RapMap](https://github.com/COMBINE-lab/RapMap). RapMap implements the quasi-mapping algorithm upon which Salmon and Sailfish are based (and RapMap is used as a library in the Salmon and Sailfish codebases). Given an index and set of reads, RapMap will report all of the multi-mapping locations that Salmon and Sailfish would consider during quantification. The other feature we have in the works is to have Salmon optionally output a `.bam` file (with actual alignments) post-quantification. It turns out that, given the quasi-mapping information and the quantification results, taking the extra step from quasi-mapping to an actual _alignment_ can be done fairly efficiently. In this mode, Salmon would make one more pass over the reads and, considering the estimated abundances, sample a single alignment for each multi-mapping read proportional to the relative abundance of the different multi-mapping targets (i.e. it would perform a sampling over the multi-mapping locations that would, in expectation, give the same abundances as the _soft_ assignments computed by the optimization algorithm). This feature will be very useful for [transrate](https://github.com/Blahah/transrate). However, given that your goal is to use outside information to perform the filtering yourself, this option may not be ideal for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/38#issuecomment-175092553
https://github.com/COMBINE-lab/salmon/issues/45#issuecomment-193960879:101,Usability,simpl,simply,101,"I'll have to check if the ignoring of the `CPPFLAGS` is a problem with bwa's build system, or if I'm simply neglecting to pass the environment along properly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/45#issuecomment-193960879
https://github.com/COMBINE-lab/salmon/issues/49#issuecomment-197975002:368,Usability,learn,learn,368,Done the required work. Sorry for bothering everyone. Downloaded the refGene.gtf file from UCSC for mm9 having transcript information and then used `gffread` to build the transcript.fa for the mm9. Finally ran salmon indexes and to my surprise it finished in matter of few minutes < 3'. Thanks for all the suggestions. This is something which I always like getting to learn something new every day. Closing the issue.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/49#issuecomment-197975002
https://github.com/COMBINE-lab/salmon/issues/50#issuecomment-197891864:127,Usability,simpl,simply,127,"Hi Warren,. I apologize for the short response --- I'm on my way to a meeting. I'm assuming that what was happening before was simply that the problem was diagnosed during VBEM as soon as it pops up, whereas, the NaNs are allowed to propagate during the standard EM. If I had to guess (and this is just a guess), I would blame edge effects in the FSPD for the appearance of `NaN`s (not sure why this goes away without bias correction though!). Also, were I to disable a feature, that (`--useFSPD`) would be the one I would choose to disable since it usually has a negligible effect. That being said, I would be **very** grateful if you were able to produce a dataset that exhibits this issue, as I'd like to fix this ASAP. My guess is that it's a rather small issue, but the problem is that NANs propagate quickly and without mercy!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/50#issuecomment-197891864
https://github.com/COMBINE-lab/salmon/issues/52#issuecomment-256045452:377,Deployability,pipeline,pipeline,377,"Hi @schelhorn,. Sorry for the uncharacteristically slow response on this. We're going full steam ahead for the RECOMB deadline, so I've been less responsive than usual. Anyway, I've invited you to the repository for the fusion project (it's currently private). Feel free to poke around, but it's probably not useful until we can send you a short writeup describing the current pipeline (since things are still very ""alpha""). Regarding calling fusions from the sam output of Salmon, one can't do this directly because there are, by default, no encompassing reads (i.e. individual reads split between transcripts) and, to improve abundance estimation, salmon is conservative with it's use of spanning reads. However, we can get at this information from quasi-mapping, so I can definitely consider adding some flags to provide this info (this is the type of thing we output in the fusion pipeline currently, and then we have to postprocess it).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/52#issuecomment-256045452
https://github.com/COMBINE-lab/salmon/issues/52#issuecomment-256045452:885,Deployability,pipeline,pipeline,885,"Hi @schelhorn,. Sorry for the uncharacteristically slow response on this. We're going full steam ahead for the RECOMB deadline, so I've been less responsive than usual. Anyway, I've invited you to the repository for the fusion project (it's currently private). Feel free to poke around, but it's probably not useful until we can send you a short writeup describing the current pipeline (since things are still very ""alpha""). Regarding calling fusions from the sam output of Salmon, one can't do this directly because there are, by default, no encompassing reads (i.e. individual reads split between transcripts) and, to improve abundance estimation, salmon is conservative with it's use of spanning reads. However, we can get at this information from quasi-mapping, so I can definitely consider adding some flags to provide this info (this is the type of thing we output in the fusion pipeline currently, and then we have to postprocess it).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/52#issuecomment-256045452
https://github.com/COMBINE-lab/salmon/issues/52#issuecomment-256045452:146,Usability,responsiv,responsive,146,"Hi @schelhorn,. Sorry for the uncharacteristically slow response on this. We're going full steam ahead for the RECOMB deadline, so I've been less responsive than usual. Anyway, I've invited you to the repository for the fusion project (it's currently private). Feel free to poke around, but it's probably not useful until we can send you a short writeup describing the current pipeline (since things are still very ""alpha""). Regarding calling fusions from the sam output of Salmon, one can't do this directly because there are, by default, no encompassing reads (i.e. individual reads split between transcripts) and, to improve abundance estimation, salmon is conservative with it's use of spanning reads. However, we can get at this information from quasi-mapping, so I can definitely consider adding some flags to provide this info (this is the type of thing we output in the fusion pipeline currently, and then we have to postprocess it).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/52#issuecomment-256045452
https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373:726,Deployability,release,release,726,"Hi @vals,. So there was a very subtle bug in `useFSPD` that would (in a very non-reproducible manner) trigger such a segfault. It was related to some very tricky locking behavior. However, the manner in which `useFSPD` corrected for position specific bias isn't actually compatible with our new sequence-specific and fragment-gc bias models. Thus, I've deprecated `useFSPD`. The replacement is the flag `posBias`. This models the same type of positional bias, but does so in a way that is compatible with our other bias models. It also doesn't rely on the tricky threading behavior, so it should be more stable. Unlike sequence-specific and fragment-gc bias, however, the `posBias` option is still _experimental_ in the 0.7.0 release. However, we have been testing it internally, and I'd be very grateful for your feedback if you have a chance to try it out. Assuming things look good, we can promote it from experimental in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373
https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373:934,Deployability,release,release,934,"Hi @vals,. So there was a very subtle bug in `useFSPD` that would (in a very non-reproducible manner) trigger such a segfault. It was related to some very tricky locking behavior. However, the manner in which `useFSPD` corrected for position specific bias isn't actually compatible with our new sequence-specific and fragment-gc bias models. Thus, I've deprecated `useFSPD`. The replacement is the flag `posBias`. This models the same type of positional bias, but does so in a way that is compatible with our other bias models. It also doesn't rely on the tricky threading behavior, so it should be more stable. Unlike sequence-specific and fragment-gc bias, however, the `posBias` option is still _experimental_ in the 0.7.0 release. However, we have been testing it internally, and I'd be very grateful for your feedback if you have a chance to try it out. Assuming things look good, we can promote it from experimental in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373
https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373:757,Testability,test,testing,757,"Hi @vals,. So there was a very subtle bug in `useFSPD` that would (in a very non-reproducible manner) trigger such a segfault. It was related to some very tricky locking behavior. However, the manner in which `useFSPD` corrected for position specific bias isn't actually compatible with our new sequence-specific and fragment-gc bias models. Thus, I've deprecated `useFSPD`. The replacement is the flag `posBias`. This models the same type of positional bias, but does so in a way that is compatible with our other bias models. It also doesn't rely on the tricky threading behavior, so it should be more stable. Unlike sequence-specific and fragment-gc bias, however, the `posBias` option is still _experimental_ in the 0.7.0 release. However, we have been testing it internally, and I'd be very grateful for your feedback if you have a chance to try it out. Assuming things look good, we can promote it from experimental in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373
https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373:814,Usability,feedback,feedback,814,"Hi @vals,. So there was a very subtle bug in `useFSPD` that would (in a very non-reproducible manner) trigger such a segfault. It was related to some very tricky locking behavior. However, the manner in which `useFSPD` corrected for position specific bias isn't actually compatible with our new sequence-specific and fragment-gc bias models. Thus, I've deprecated `useFSPD`. The replacement is the flag `posBias`. This models the same type of positional bias, but does so in a way that is compatible with our other bias models. It also doesn't rely on the tricky threading behavior, so it should be more stable. Unlike sequence-specific and fragment-gc bias, however, the `posBias` option is still _experimental_ in the 0.7.0 release. However, we have been testing it internally, and I'd be very grateful for your feedback if you have a chance to try it out. Assuming things look good, we can promote it from experimental in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/64#issuecomment-241234373
https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-238090033:1479,Energy Efficiency,allocate,allocated,1479,"-zero). Thus, if the only mapping for a read disagrees with the expected type, it will still be used. There is a way to modify this behavior, but since stranded library prep is imperfect, the default behavior is the most reasonable for most situations. The reason that you'll see consistency in most cases, regardless of the library type, is as follows. Imagine that I have a read that maps to transcript 1 in the forward orientation and transcript 2 in the reverse orientation. Further, imagine I have a stranded library, and I expect all reads to map in the reverse orientation. If the mapping to transcript 1 is ""spurious"", there are unlikely to be many othe reads mapping to that transcript in this manner, while we would expect other reads to map to transcript 2 in the prescribed manner. Since Salmon considers all of the reads in its probabilistic model when deciding how each read should be allocated, the fact that many reads map to transcript 2 will increase its abundance and, likewise, increase the probability that we assign this read to transcript 2 --- that is, the other mappings will help us make the right choice, regardless of the fact that we neglected to assign a stranded library type. That said, there are situations where the library type makes a difference. This is most often for a few transcripts that are very sequence similar (e.g. Paralogs that happen to be on opposite strands). In this case, most of the reads that map to one transcript will map to the other as well. In this case, the much larger conditional probability of agreeing with the prescribed library type will cause these reads to be allocated to the transcript to which they map in the expected orientation. However, the fraction of such transcripts is usually a small proportion of all expressed transcripts in an experiment, which is why, even if you do have a stranded library and some strand-specific expression, you'd expect the overall concordance to be very high between runs with different provide",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-238090033
https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-238090033:2208,Energy Efficiency,allocate,allocated,2208,"ed. There is a way to modify this behavior, but since stranded library prep is imperfect, the default behavior is the most reasonable for most situations. The reason that you'll see consistency in most cases, regardless of the library type, is as follows. Imagine that I have a read that maps to transcript 1 in the forward orientation and transcript 2 in the reverse orientation. Further, imagine I have a stranded library, and I expect all reads to map in the reverse orientation. If the mapping to transcript 1 is ""spurious"", there are unlikely to be many othe reads mapping to that transcript in this manner, while we would expect other reads to map to transcript 2 in the prescribed manner. Since Salmon considers all of the reads in its probabilistic model when deciding how each read should be allocated, the fact that many reads map to transcript 2 will increase its abundance and, likewise, increase the probability that we assign this read to transcript 2 --- that is, the other mappings will help us make the right choice, regardless of the fact that we neglected to assign a stranded library type. That said, there are situations where the library type makes a difference. This is most often for a few transcripts that are very sequence similar (e.g. Paralogs that happen to be on opposite strands). In this case, most of the reads that map to one transcript will map to the other as well. In this case, the much larger conditional probability of agreeing with the prescribed library type will cause these reads to be allocated to the transcript to which they map in the expected orientation. However, the fraction of such transcripts is usually a small proportion of all expressed transcripts in an experiment, which is why, even if you do have a stranded library and some strand-specific expression, you'd expect the overall concordance to be very high between runs with different provided library types. Let me know if this answers your question, and if you have any others. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-238090033
https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-238090033:258,Usability,simpl,simply,258,"Hi @gresteban ,. Thanks for the kind words. I'm working on improving the documentation even more for v0.7.0, which should land soon. Regarding your question, what you're seeing is expected behavior. That is, for the vast majority of transcripts, Salmon will simply do the ""right thing"" regardless of the library type. This is because the library type is used as a ""soft"" rather than a ""hard"" filter when determining where a read may originate from (i.e. Orientations other than the expected type have a probability orders of magnitude smaller than the expected type, but still non-zero). Thus, if the only mapping for a read disagrees with the expected type, it will still be used. There is a way to modify this behavior, but since stranded library prep is imperfect, the default behavior is the most reasonable for most situations. The reason that you'll see consistency in most cases, regardless of the library type, is as follows. Imagine that I have a read that maps to transcript 1 in the forward orientation and transcript 2 in the reverse orientation. Further, imagine I have a stranded library, and I expect all reads to map in the reverse orientation. If the mapping to transcript 1 is ""spurious"", there are unlikely to be many othe reads mapping to that transcript in this manner, while we would expect other reads to map to transcript 2 in the prescribed manner. Since Salmon considers all of the reads in its probabilistic model when deciding how each read should be allocated, the fact that many reads map to transcript 2 will increase its abundance and, likewise, increase the probability that we assign this read to transcript 2 --- that is, the other mappings will help us make the right choice, regardless of the fact that we neglected to assign a stranded library type. That said, there are situations where the library type makes a difference. This is most often for a few transcripts that are very sequence similar (e.g. Paralogs that happen to be on opposite strands). In this cas",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-238090033
https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-388863963:386,Deployability,release,release,386,"Hi Alex,. The appropriate way to _force_ salmon to use the library type as a hard constraint is to pass the option `--incompatPrior 0.0` on the command line. This will tell salmon that it should consider a fragment mapping different than the library type to be impossible (i.e. this mapping should simply be discarded). This will actually be the default behavior starting from the next release anyway, as the current behavior seems to confuse more people than not. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-388863963
https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-388863963:298,Usability,simpl,simply,298,"Hi Alex,. The appropriate way to _force_ salmon to use the library type as a hard constraint is to pass the option `--incompatPrior 0.0` on the command line. This will tell salmon that it should consider a fragment mapping different than the library type to be impossible (i.e. this mapping should simply be discarded). This will actually be the default behavior starting from the next release anyway, as the current behavior seems to confuse more people than not. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/67#issuecomment-388863963
https://github.com/COMBINE-lab/salmon/pull/70#issuecomment-239534893:202,Deployability,install,installing,202,"> Default RPATH settings; > By default if you don't change any RPATH related settings, CMake will link the executables and shared libraries with full RPATH to all used libraries in the build tree. When installing, it will clear the RPATH of these targets so they are installed with an empty RPATH. https://cmake.org/Wiki/CMake_RPATH_handling#Default_RPATH_settings. We want this default `RPATH` behaviour from `cmake`. I'm unsure why we're getting non-default behaviour.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/70#issuecomment-239534893
https://github.com/COMBINE-lab/salmon/pull/70#issuecomment-239534893:267,Deployability,install,installed,267,"> Default RPATH settings; > By default if you don't change any RPATH related settings, CMake will link the executables and shared libraries with full RPATH to all used libraries in the build tree. When installing, it will clear the RPATH of these targets so they are installed with an empty RPATH. https://cmake.org/Wiki/CMake_RPATH_handling#Default_RPATH_settings. We want this default `RPATH` behaviour from `cmake`. I'm unsure why we're getting non-default behaviour.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/70#issuecomment-239534893
https://github.com/COMBINE-lab/salmon/pull/70#issuecomment-239534893:222,Usability,clear,clear,222,"> Default RPATH settings; > By default if you don't change any RPATH related settings, CMake will link the executables and shared libraries with full RPATH to all used libraries in the build tree. When installing, it will clear the RPATH of these targets so they are installed with an empty RPATH. https://cmake.org/Wiki/CMake_RPATH_handling#Default_RPATH_settings. We want this default `RPATH` behaviour from `cmake`. I'm unsure why we're getting non-default behaviour.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/70#issuecomment-239534893
https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885:255,Deployability,pipeline,pipeline,255,"@roryk I don't think an R package is the right answer :) . My real motivation is to load into Degust: http://www.vicbioinformatics.com/degust/. It can be done with simple Unix cut/paste or with a python script too. But I don't want to depend on R for the pipeline, or even littler. @vals I'll take a look at your script, but still be better if part of Salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885
https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885:235,Integrability,depend,depend,235,"@roryk I don't think an R package is the right answer :) . My real motivation is to load into Degust: http://www.vicbioinformatics.com/degust/. It can be done with simple Unix cut/paste or with a python script too. But I don't want to depend on R for the pipeline, or even littler. @vals I'll take a look at your script, but still be better if part of Salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885
https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885:84,Performance,load,load,84,"@roryk I don't think an R package is the right answer :) . My real motivation is to load into Degust: http://www.vicbioinformatics.com/degust/. It can be done with simple Unix cut/paste or with a python script too. But I don't want to depend on R for the pipeline, or even littler. @vals I'll take a look at your script, but still be better if part of Salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885
https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885:164,Usability,simpl,simple,164,"@roryk I don't think an R package is the right answer :) . My real motivation is to load into Degust: http://www.vicbioinformatics.com/degust/. It can be done with simple Unix cut/paste or with a python script too. But I don't want to depend on R for the pipeline, or even littler. @vals I'll take a look at your script, but still be better if part of Salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/77#issuecomment-240556885
https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463:1556,Deployability,update,update,1556,"Thanks for the thorough suggestions. Actually, we fall into the easier case since Salmon does not support mixing single and paired-end reads in a single BAM file. When performing quantification on a single sample, the reads for that sample must follow a uniform library type. For paired-end reads, the BAM file can contain paired-end and single-end alignments (i.e. orphans), but the reads must all have been paired _in sequencing_. Mixing different library types in the BAM file makes it difficult to assess the compatibility of a fragment with the expected library type, especially if fragments from the different library types are expected to exist in a specific ratio in the input. Anyway, my main motivation for having the separate `AS` and `AP` types was to prevent the need to ""peek"" in the file, since, currently, there is not an easy way to peek the first read without opening the first file twice. However, I've decided that the benefit of having the same uniform (and simpler) interface of `A` always representing automatic library type detection is probably worth it, so I've pushed this implementation (commit 6116b2a). So, when the user provides the `A` library type, Salmon will peek into the first record in the BAM file to determine if the fragment was paired in sequencing or not, and will then set the single / paired-end status on that basis. The only corollary to this is that, in alignment-based mode, the `A` flag is not compatible with an input stream (i.e. the input must be a regular file). I will be sure to document this when I update the docs for the version bump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463
https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463:988,Integrability,interface,interface,988,"Thanks for the thorough suggestions. Actually, we fall into the easier case since Salmon does not support mixing single and paired-end reads in a single BAM file. When performing quantification on a single sample, the reads for that sample must follow a uniform library type. For paired-end reads, the BAM file can contain paired-end and single-end alignments (i.e. orphans), but the reads must all have been paired _in sequencing_. Mixing different library types in the BAM file makes it difficult to assess the compatibility of a fragment with the expected library type, especially if fragments from the different library types are expected to exist in a specific ratio in the input. Anyway, my main motivation for having the separate `AS` and `AP` types was to prevent the need to ""peek"" in the file, since, currently, there is not an easy way to peek the first read without opening the first file twice. However, I've decided that the benefit of having the same uniform (and simpler) interface of `A` always representing automatic library type detection is probably worth it, so I've pushed this implementation (commit 6116b2a). So, when the user provides the `A` library type, Salmon will peek into the first record in the BAM file to determine if the fragment was paired in sequencing or not, and will then set the single / paired-end status on that basis. The only corollary to this is that, in alignment-based mode, the `A` flag is not compatible with an input stream (i.e. the input must be a regular file). I will be sure to document this when I update the docs for the version bump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463
https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463:168,Performance,perform,performing,168,"Thanks for the thorough suggestions. Actually, we fall into the easier case since Salmon does not support mixing single and paired-end reads in a single BAM file. When performing quantification on a single sample, the reads for that sample must follow a uniform library type. For paired-end reads, the BAM file can contain paired-end and single-end alignments (i.e. orphans), but the reads must all have been paired _in sequencing_. Mixing different library types in the BAM file makes it difficult to assess the compatibility of a fragment with the expected library type, especially if fragments from the different library types are expected to exist in a specific ratio in the input. Anyway, my main motivation for having the separate `AS` and `AP` types was to prevent the need to ""peek"" in the file, since, currently, there is not an easy way to peek the first read without opening the first file twice. However, I've decided that the benefit of having the same uniform (and simpler) interface of `A` always representing automatic library type detection is probably worth it, so I've pushed this implementation (commit 6116b2a). So, when the user provides the `A` library type, Salmon will peek into the first record in the BAM file to determine if the fragment was paired in sequencing or not, and will then set the single / paired-end status on that basis. The only corollary to this is that, in alignment-based mode, the `A` flag is not compatible with an input stream (i.e. the input must be a regular file). I will be sure to document this when I update the docs for the version bump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463
https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463:1048,Safety,detect,detection,1048,"Thanks for the thorough suggestions. Actually, we fall into the easier case since Salmon does not support mixing single and paired-end reads in a single BAM file. When performing quantification on a single sample, the reads for that sample must follow a uniform library type. For paired-end reads, the BAM file can contain paired-end and single-end alignments (i.e. orphans), but the reads must all have been paired _in sequencing_. Mixing different library types in the BAM file makes it difficult to assess the compatibility of a fragment with the expected library type, especially if fragments from the different library types are expected to exist in a specific ratio in the input. Anyway, my main motivation for having the separate `AS` and `AP` types was to prevent the need to ""peek"" in the file, since, currently, there is not an easy way to peek the first read without opening the first file twice. However, I've decided that the benefit of having the same uniform (and simpler) interface of `A` always representing automatic library type detection is probably worth it, so I've pushed this implementation (commit 6116b2a). So, when the user provides the `A` library type, Salmon will peek into the first record in the BAM file to determine if the fragment was paired in sequencing or not, and will then set the single / paired-end status on that basis. The only corollary to this is that, in alignment-based mode, the `A` flag is not compatible with an input stream (i.e. the input must be a regular file). I will be sure to document this when I update the docs for the version bump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463
https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463:979,Usability,simpl,simpler,979,"Thanks for the thorough suggestions. Actually, we fall into the easier case since Salmon does not support mixing single and paired-end reads in a single BAM file. When performing quantification on a single sample, the reads for that sample must follow a uniform library type. For paired-end reads, the BAM file can contain paired-end and single-end alignments (i.e. orphans), but the reads must all have been paired _in sequencing_. Mixing different library types in the BAM file makes it difficult to assess the compatibility of a fragment with the expected library type, especially if fragments from the different library types are expected to exist in a specific ratio in the input. Anyway, my main motivation for having the separate `AS` and `AP` types was to prevent the need to ""peek"" in the file, since, currently, there is not an easy way to peek the first read without opening the first file twice. However, I've decided that the benefit of having the same uniform (and simpler) interface of `A` always representing automatic library type detection is probably worth it, so I've pushed this implementation (commit 6116b2a). So, when the user provides the `A` library type, Salmon will peek into the first record in the BAM file to determine if the fragment was paired in sequencing or not, and will then set the single / paired-end status on that basis. The only corollary to this is that, in alignment-based mode, the `A` flag is not compatible with an input stream (i.e. the input must be a regular file). I will be sure to document this when I update the docs for the version bump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/79#issuecomment-242399463
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243813359:23,Usability,learn,learn,23,"Side note: does Salmon learn the fragment length distribution from the data, using the user-provided values as a starting point? Is it even possible to do this for stranded single-end data? (I know it's possible for unstranded single-end.)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243813359
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424:893,Availability,down,down,893,"Hi Ryan,. The difficulty is, indeed, exactly as you specify. Given a single-end read, one does not know the length of the _fragment_ from which it originates. In this case the ""right"" thing to do (the best thing we can do) is to consider the read as starting / ending a fragment of every possible length allowed by the user-provided fragment length distribution (with the contribution of each possible fragment weighted by the probability of observing a fragment of that length). In order to make this computationally feasible, one would have to do some clever pre-computation and thing a bit more about how to efficiently update the observed GC model (right now, each mapping contributes a single weight to the model, but under the naive implementation in the single-end case, each mapping would contribute different weights to each bin of the observed GC-bias curve, which would slow things down considerably). Also, as you point out, the quality of the correction would depend somewhat on the user providing appropriate parameters for the fragment length distribution mean and standard deviation — but this seems reasonable in the single-end case. That being said, I'm sure there's a way to handle this efficiently, I'd just have to think about it a bit. Regarding your second question; Salmon learns the fragment length distribution in paired-end data, but not with single-end data. Single-end data can provide a little bit of information (e.g. there is in upper bound on fragment lengths that one can infer based on single-end reads based on how far they map from the end of the transcript), but not enough information to reliably infer a fragment length distribution. cc @mikelove in case he has any thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424:1627,Availability,reliab,reliably,1627,"Hi Ryan,. The difficulty is, indeed, exactly as you specify. Given a single-end read, one does not know the length of the _fragment_ from which it originates. In this case the ""right"" thing to do (the best thing we can do) is to consider the read as starting / ending a fragment of every possible length allowed by the user-provided fragment length distribution (with the contribution of each possible fragment weighted by the probability of observing a fragment of that length). In order to make this computationally feasible, one would have to do some clever pre-computation and thing a bit more about how to efficiently update the observed GC model (right now, each mapping contributes a single weight to the model, but under the naive implementation in the single-end case, each mapping would contribute different weights to each bin of the observed GC-bias curve, which would slow things down considerably). Also, as you point out, the quality of the correction would depend somewhat on the user providing appropriate parameters for the fragment length distribution mean and standard deviation — but this seems reasonable in the single-end case. That being said, I'm sure there's a way to handle this efficiently, I'd just have to think about it a bit. Regarding your second question; Salmon learns the fragment length distribution in paired-end data, but not with single-end data. Single-end data can provide a little bit of information (e.g. there is in upper bound on fragment lengths that one can infer based on single-end reads based on how far they map from the end of the transcript), but not enough information to reliably infer a fragment length distribution. cc @mikelove in case he has any thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424:623,Deployability,update,update,623,"Hi Ryan,. The difficulty is, indeed, exactly as you specify. Given a single-end read, one does not know the length of the _fragment_ from which it originates. In this case the ""right"" thing to do (the best thing we can do) is to consider the read as starting / ending a fragment of every possible length allowed by the user-provided fragment length distribution (with the contribution of each possible fragment weighted by the probability of observing a fragment of that length). In order to make this computationally feasible, one would have to do some clever pre-computation and thing a bit more about how to efficiently update the observed GC model (right now, each mapping contributes a single weight to the model, but under the naive implementation in the single-end case, each mapping would contribute different weights to each bin of the observed GC-bias curve, which would slow things down considerably). Also, as you point out, the quality of the correction would depend somewhat on the user providing appropriate parameters for the fragment length distribution mean and standard deviation — but this seems reasonable in the single-end case. That being said, I'm sure there's a way to handle this efficiently, I'd just have to think about it a bit. Regarding your second question; Salmon learns the fragment length distribution in paired-end data, but not with single-end data. Single-end data can provide a little bit of information (e.g. there is in upper bound on fragment lengths that one can infer based on single-end reads based on how far they map from the end of the transcript), but not enough information to reliably infer a fragment length distribution. cc @mikelove in case he has any thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424:611,Energy Efficiency,efficient,efficiently,611,"Hi Ryan,. The difficulty is, indeed, exactly as you specify. Given a single-end read, one does not know the length of the _fragment_ from which it originates. In this case the ""right"" thing to do (the best thing we can do) is to consider the read as starting / ending a fragment of every possible length allowed by the user-provided fragment length distribution (with the contribution of each possible fragment weighted by the probability of observing a fragment of that length). In order to make this computationally feasible, one would have to do some clever pre-computation and thing a bit more about how to efficiently update the observed GC model (right now, each mapping contributes a single weight to the model, but under the naive implementation in the single-end case, each mapping would contribute different weights to each bin of the observed GC-bias curve, which would slow things down considerably). Also, as you point out, the quality of the correction would depend somewhat on the user providing appropriate parameters for the fragment length distribution mean and standard deviation — but this seems reasonable in the single-end case. That being said, I'm sure there's a way to handle this efficiently, I'd just have to think about it a bit. Regarding your second question; Salmon learns the fragment length distribution in paired-end data, but not with single-end data. Single-end data can provide a little bit of information (e.g. there is in upper bound on fragment lengths that one can infer based on single-end reads based on how far they map from the end of the transcript), but not enough information to reliably infer a fragment length distribution. cc @mikelove in case he has any thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424:1206,Energy Efficiency,efficient,efficiently,1206,"Hi Ryan,. The difficulty is, indeed, exactly as you specify. Given a single-end read, one does not know the length of the _fragment_ from which it originates. In this case the ""right"" thing to do (the best thing we can do) is to consider the read as starting / ending a fragment of every possible length allowed by the user-provided fragment length distribution (with the contribution of each possible fragment weighted by the probability of observing a fragment of that length). In order to make this computationally feasible, one would have to do some clever pre-computation and thing a bit more about how to efficiently update the observed GC model (right now, each mapping contributes a single weight to the model, but under the naive implementation in the single-end case, each mapping would contribute different weights to each bin of the observed GC-bias curve, which would slow things down considerably). Also, as you point out, the quality of the correction would depend somewhat on the user providing appropriate parameters for the fragment length distribution mean and standard deviation — but this seems reasonable in the single-end case. That being said, I'm sure there's a way to handle this efficiently, I'd just have to think about it a bit. Regarding your second question; Salmon learns the fragment length distribution in paired-end data, but not with single-end data. Single-end data can provide a little bit of information (e.g. there is in upper bound on fragment lengths that one can infer based on single-end reads based on how far they map from the end of the transcript), but not enough information to reliably infer a fragment length distribution. cc @mikelove in case he has any thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424:973,Integrability,depend,depend,973,"Hi Ryan,. The difficulty is, indeed, exactly as you specify. Given a single-end read, one does not know the length of the _fragment_ from which it originates. In this case the ""right"" thing to do (the best thing we can do) is to consider the read as starting / ending a fragment of every possible length allowed by the user-provided fragment length distribution (with the contribution of each possible fragment weighted by the probability of observing a fragment of that length). In order to make this computationally feasible, one would have to do some clever pre-computation and thing a bit more about how to efficiently update the observed GC model (right now, each mapping contributes a single weight to the model, but under the naive implementation in the single-end case, each mapping would contribute different weights to each bin of the observed GC-bias curve, which would slow things down considerably). Also, as you point out, the quality of the correction would depend somewhat on the user providing appropriate parameters for the fragment length distribution mean and standard deviation — but this seems reasonable in the single-end case. That being said, I'm sure there's a way to handle this efficiently, I'd just have to think about it a bit. Regarding your second question; Salmon learns the fragment length distribution in paired-end data, but not with single-end data. Single-end data can provide a little bit of information (e.g. there is in upper bound on fragment lengths that one can infer based on single-end reads based on how far they map from the end of the transcript), but not enough information to reliably infer a fragment length distribution. cc @mikelove in case he has any thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424:1297,Usability,learn,learns,1297,"Hi Ryan,. The difficulty is, indeed, exactly as you specify. Given a single-end read, one does not know the length of the _fragment_ from which it originates. In this case the ""right"" thing to do (the best thing we can do) is to consider the read as starting / ending a fragment of every possible length allowed by the user-provided fragment length distribution (with the contribution of each possible fragment weighted by the probability of observing a fragment of that length). In order to make this computationally feasible, one would have to do some clever pre-computation and thing a bit more about how to efficiently update the observed GC model (right now, each mapping contributes a single weight to the model, but under the naive implementation in the single-end case, each mapping would contribute different weights to each bin of the observed GC-bias curve, which would slow things down considerably). Also, as you point out, the quality of the correction would depend somewhat on the user providing appropriate parameters for the fragment length distribution mean and standard deviation — but this seems reasonable in the single-end case. That being said, I'm sure there's a way to handle this efficiently, I'd just have to think about it a bit. Regarding your second question; Salmon learns the fragment length distribution in paired-end data, but not with single-end data. Single-end data can provide a little bit of information (e.g. there is in upper bound on fragment lengths that one can infer based on single-end reads based on how far they map from the end of the transcript), but not enough information to reliably infer a fragment length distribution. cc @mikelove in case he has any thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-243833424
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-245366321:114,Testability,test,test,114,"Alright, I'll have a go at the simple model. @mikelove, once I have it implemented we can figure out a reasonable test. Actually, enabling the feature was _way_ easier than I thought. The actual bias application code (via re-estimation of effective lengths) can remain the same. I now have code-paths to build GC bias models treating single-end reads as equal to the _conditional_ mean fragment length (given the transcript). Let me know what you think would be a good way to test it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-245366321
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-245366321:476,Testability,test,test,476,"Alright, I'll have a go at the simple model. @mikelove, once I have it implemented we can figure out a reasonable test. Actually, enabling the feature was _way_ easier than I thought. The actual bias application code (via re-estimation of effective lengths) can remain the same. I now have code-paths to build GC bias models treating single-end reads as equal to the _conditional_ mean fragment length (given the transcript). Let me know what you think would be a good way to test it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-245366321
https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-245366321:31,Usability,simpl,simple,31,"Alright, I'll have a go at the simple model. @mikelove, once I have it implemented we can figure out a reasonable test. Actually, enabling the feature was _way_ easier than I thought. The actual bias application code (via re-estimation of effective lengths) can remain the same. I now have code-paths to build GC bias models treating single-end reads as equal to the _conditional_ mean fragment length (given the transcript). Let me know what you think would be a good way to test it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/83#issuecomment-245366321
https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704:781,Deployability,release,releases,781,"Hi @satta,. Thanks for bringing this to my attention. I am of two minds on this proposal. On one hand, I agree that it is cleaner, in theory, to have a RapMap shared library to which Salmon could simply link. Currently, Salmon pulls in the relevant portions of the RapMap code to call what is essentially an ill-defined public API for mapping. On the other hand, I have two concerns about separating the code at this point, one is major the other minor. The major concern is that both Salmon and RapMap are still very much under active development, core code and even the interfaces are undergoing reasonably rapid changes (thus the versioning < 1.0). This allows me to easily add features that may potentially benefit Salmon to the RapMap codebase, and then to synchronize Salmon releases with particular commits (tags) in the RapMap codebase. The current build system makes it very easy to pull in the appropriately versioned RapMap code. On the other hand, I have very little experience in properly versioning shared libraries so I would have to understand that better and how this could be done without complicating the build process. My _minor_ concern is that I don't know what effect, if any, separating the code into a separate shared library might have on compiler optimizations. Right now, since the relevant RapMap code is compiled alongside Salmon and they are linked together into the same module, certain optimizations may be possible that would not be so when linking to a shared library. My educated guess is that the effect of such optimizations would be negligible, but it's something that may be worth some exploration first. Overall, I'm very open to this idea, but I think I need to do some homework on it before we can commit and undertake the change.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704
https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704:572,Integrability,interface,interfaces,572,"Hi @satta,. Thanks for bringing this to my attention. I am of two minds on this proposal. On one hand, I agree that it is cleaner, in theory, to have a RapMap shared library to which Salmon could simply link. Currently, Salmon pulls in the relevant portions of the RapMap code to call what is essentially an ill-defined public API for mapping. On the other hand, I have two concerns about separating the code at this point, one is major the other minor. The major concern is that both Salmon and RapMap are still very much under active development, core code and even the interfaces are undergoing reasonably rapid changes (thus the versioning < 1.0). This allows me to easily add features that may potentially benefit Salmon to the RapMap codebase, and then to synchronize Salmon releases with particular commits (tags) in the RapMap codebase. The current build system makes it very easy to pull in the appropriately versioned RapMap code. On the other hand, I have very little experience in properly versioning shared libraries so I would have to understand that better and how this could be done without complicating the build process. My _minor_ concern is that I don't know what effect, if any, separating the code into a separate shared library might have on compiler optimizations. Right now, since the relevant RapMap code is compiled alongside Salmon and they are linked together into the same module, certain optimizations may be possible that would not be so when linking to a shared library. My educated guess is that the effect of such optimizations would be negligible, but it's something that may be worth some exploration first. Overall, I'm very open to this idea, but I think I need to do some homework on it before we can commit and undertake the change.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704
https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704:762,Integrability,synchroniz,synchronize,762,"Hi @satta,. Thanks for bringing this to my attention. I am of two minds on this proposal. On one hand, I agree that it is cleaner, in theory, to have a RapMap shared library to which Salmon could simply link. Currently, Salmon pulls in the relevant portions of the RapMap code to call what is essentially an ill-defined public API for mapping. On the other hand, I have two concerns about separating the code at this point, one is major the other minor. The major concern is that both Salmon and RapMap are still very much under active development, core code and even the interfaces are undergoing reasonably rapid changes (thus the versioning < 1.0). This allows me to easily add features that may potentially benefit Salmon to the RapMap codebase, and then to synchronize Salmon releases with particular commits (tags) in the RapMap codebase. The current build system makes it very easy to pull in the appropriately versioned RapMap code. On the other hand, I have very little experience in properly versioning shared libraries so I would have to understand that better and how this could be done without complicating the build process. My _minor_ concern is that I don't know what effect, if any, separating the code into a separate shared library might have on compiler optimizations. Right now, since the relevant RapMap code is compiled alongside Salmon and they are linked together into the same module, certain optimizations may be possible that would not be so when linking to a shared library. My educated guess is that the effect of such optimizations would be negligible, but it's something that may be worth some exploration first. Overall, I'm very open to this idea, but I think I need to do some homework on it before we can commit and undertake the change.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704
https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704:1274,Performance,optimiz,optimizations,1274,"Hi @satta,. Thanks for bringing this to my attention. I am of two minds on this proposal. On one hand, I agree that it is cleaner, in theory, to have a RapMap shared library to which Salmon could simply link. Currently, Salmon pulls in the relevant portions of the RapMap code to call what is essentially an ill-defined public API for mapping. On the other hand, I have two concerns about separating the code at this point, one is major the other minor. The major concern is that both Salmon and RapMap are still very much under active development, core code and even the interfaces are undergoing reasonably rapid changes (thus the versioning < 1.0). This allows me to easily add features that may potentially benefit Salmon to the RapMap codebase, and then to synchronize Salmon releases with particular commits (tags) in the RapMap codebase. The current build system makes it very easy to pull in the appropriately versioned RapMap code. On the other hand, I have very little experience in properly versioning shared libraries so I would have to understand that better and how this could be done without complicating the build process. My _minor_ concern is that I don't know what effect, if any, separating the code into a separate shared library might have on compiler optimizations. Right now, since the relevant RapMap code is compiled alongside Salmon and they are linked together into the same module, certain optimizations may be possible that would not be so when linking to a shared library. My educated guess is that the effect of such optimizations would be negligible, but it's something that may be worth some exploration first. Overall, I'm very open to this idea, but I think I need to do some homework on it before we can commit and undertake the change.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704
https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704:1419,Performance,optimiz,optimizations,1419,"Hi @satta,. Thanks for bringing this to my attention. I am of two minds on this proposal. On one hand, I agree that it is cleaner, in theory, to have a RapMap shared library to which Salmon could simply link. Currently, Salmon pulls in the relevant portions of the RapMap code to call what is essentially an ill-defined public API for mapping. On the other hand, I have two concerns about separating the code at this point, one is major the other minor. The major concern is that both Salmon and RapMap are still very much under active development, core code and even the interfaces are undergoing reasonably rapid changes (thus the versioning < 1.0). This allows me to easily add features that may potentially benefit Salmon to the RapMap codebase, and then to synchronize Salmon releases with particular commits (tags) in the RapMap codebase. The current build system makes it very easy to pull in the appropriately versioned RapMap code. On the other hand, I have very little experience in properly versioning shared libraries so I would have to understand that better and how this could be done without complicating the build process. My _minor_ concern is that I don't know what effect, if any, separating the code into a separate shared library might have on compiler optimizations. Right now, since the relevant RapMap code is compiled alongside Salmon and they are linked together into the same module, certain optimizations may be possible that would not be so when linking to a shared library. My educated guess is that the effect of such optimizations would be negligible, but it's something that may be worth some exploration first. Overall, I'm very open to this idea, but I think I need to do some homework on it before we can commit and undertake the change.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704
https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704:1549,Performance,optimiz,optimizations,1549,"Hi @satta,. Thanks for bringing this to my attention. I am of two minds on this proposal. On one hand, I agree that it is cleaner, in theory, to have a RapMap shared library to which Salmon could simply link. Currently, Salmon pulls in the relevant portions of the RapMap code to call what is essentially an ill-defined public API for mapping. On the other hand, I have two concerns about separating the code at this point, one is major the other minor. The major concern is that both Salmon and RapMap are still very much under active development, core code and even the interfaces are undergoing reasonably rapid changes (thus the versioning < 1.0). This allows me to easily add features that may potentially benefit Salmon to the RapMap codebase, and then to synchronize Salmon releases with particular commits (tags) in the RapMap codebase. The current build system makes it very easy to pull in the appropriately versioned RapMap code. On the other hand, I have very little experience in properly versioning shared libraries so I would have to understand that better and how this could be done without complicating the build process. My _minor_ concern is that I don't know what effect, if any, separating the code into a separate shared library might have on compiler optimizations. Right now, since the relevant RapMap code is compiled alongside Salmon and they are linked together into the same module, certain optimizations may be possible that would not be so when linking to a shared library. My educated guess is that the effect of such optimizations would be negligible, but it's something that may be worth some exploration first. Overall, I'm very open to this idea, but I think I need to do some homework on it before we can commit and undertake the change.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704
https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704:196,Usability,simpl,simply,196,"Hi @satta,. Thanks for bringing this to my attention. I am of two minds on this proposal. On one hand, I agree that it is cleaner, in theory, to have a RapMap shared library to which Salmon could simply link. Currently, Salmon pulls in the relevant portions of the RapMap code to call what is essentially an ill-defined public API for mapping. On the other hand, I have two concerns about separating the code at this point, one is major the other minor. The major concern is that both Salmon and RapMap are still very much under active development, core code and even the interfaces are undergoing reasonably rapid changes (thus the versioning < 1.0). This allows me to easily add features that may potentially benefit Salmon to the RapMap codebase, and then to synchronize Salmon releases with particular commits (tags) in the RapMap codebase. The current build system makes it very easy to pull in the appropriately versioned RapMap code. On the other hand, I have very little experience in properly versioning shared libraries so I would have to understand that better and how this could be done without complicating the build process. My _minor_ concern is that I don't know what effect, if any, separating the code into a separate shared library might have on compiler optimizations. Right now, since the relevant RapMap code is compiled alongside Salmon and they are linked together into the same module, certain optimizations may be possible that would not be so when linking to a shared library. My educated guess is that the effect of such optimizations would be negligible, but it's something that may be worth some exploration first. Overall, I'm very open to this idea, but I think I need to do some homework on it before we can commit and undertake the change.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/87#issuecomment-246027704
https://github.com/COMBINE-lab/salmon/issues/93#issuecomment-394000666:498,Usability,feedback,feedback,498,"Hi @mdshw5,. @k3yavi was originally developing the barcode algorithm in a separate repo, but all of this work has been merged into the salmon repo now. The new `alevin` command runs the single-cell method, which handles barcode identification and correction, mapping and UMI deduplication, and which is described in this [bioRxiv preprint](https://www.biorxiv.org/content/early/2018/06/01/335000) that just landed. We're still actively developing and improving the method and very much welcome any feedback!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/93#issuecomment-394000666
https://github.com/COMBINE-lab/salmon/issues/94#issuecomment-250350189:169,Availability,reliab,reliable,169,"Hi @vasisht,. Actually, the settings of these flags aren't incorrect according to the [SAM spec](https://samtools.github.io/hts-specs/SAMv1.pdf):. > Bit 0x4 is the only reliable place to tell whether the read is unmapped. If 0x4 is set, no assumptions can be made about RNAME, POS, CIGAR, MAPQ, and bits 0x2, 0x100, and 0x800. That is, if the unmapped flag is set, then there is not a specific ""correct"" setting for these other fields, since they should most likely be ignored anyway. That being said, concordant with some small changes in the [most-recent RapMap](https://github.com/COMBINE-lab/RapMap/releases/tag/v0.4.0), the CIGAR string will be set to `*` for unmapped reads in future versions of Salmon. We may consider setting other fields to `*` for unmapped reads to simplify the output, but, as the SAM spec suggests, these fields offer quite a bit of freedom in terms of ""legal"" values if the unmapped flag is set anyway.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/94#issuecomment-250350189
https://github.com/COMBINE-lab/salmon/issues/94#issuecomment-250350189:603,Deployability,release,releases,603,"Hi @vasisht,. Actually, the settings of these flags aren't incorrect according to the [SAM spec](https://samtools.github.io/hts-specs/SAMv1.pdf):. > Bit 0x4 is the only reliable place to tell whether the read is unmapped. If 0x4 is set, no assumptions can be made about RNAME, POS, CIGAR, MAPQ, and bits 0x2, 0x100, and 0x800. That is, if the unmapped flag is set, then there is not a specific ""correct"" setting for these other fields, since they should most likely be ignored anyway. That being said, concordant with some small changes in the [most-recent RapMap](https://github.com/COMBINE-lab/RapMap/releases/tag/v0.4.0), the CIGAR string will be set to `*` for unmapped reads in future versions of Salmon. We may consider setting other fields to `*` for unmapped reads to simplify the output, but, as the SAM spec suggests, these fields offer quite a bit of freedom in terms of ""legal"" values if the unmapped flag is set anyway.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/94#issuecomment-250350189
https://github.com/COMBINE-lab/salmon/issues/94#issuecomment-250350189:776,Usability,simpl,simplify,776,"Hi @vasisht,. Actually, the settings of these flags aren't incorrect according to the [SAM spec](https://samtools.github.io/hts-specs/SAMv1.pdf):. > Bit 0x4 is the only reliable place to tell whether the read is unmapped. If 0x4 is set, no assumptions can be made about RNAME, POS, CIGAR, MAPQ, and bits 0x2, 0x100, and 0x800. That is, if the unmapped flag is set, then there is not a specific ""correct"" setting for these other fields, since they should most likely be ignored anyway. That being said, concordant with some small changes in the [most-recent RapMap](https://github.com/COMBINE-lab/RapMap/releases/tag/v0.4.0), the CIGAR string will be set to `*` for unmapped reads in future versions of Salmon. We may consider setting other fields to `*` for unmapped reads to simplify the output, but, as the SAM spec suggests, these fields offer quite a bit of freedom in terms of ""legal"" values if the unmapped flag is set anyway.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/94#issuecomment-250350189
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:1329,Availability,down,downstream,1329,"ne phase and an offline phase. The online phase has access to _fragment-level_ information that is then summarized away during the offline phase (like the specific locations of each read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the other hand, if you are in need of something that produces identical output between runs (but that still also lets you assess uncertainty), then you can give [piscem](https://github.com/COMBINE-lab/piscem) => [piscem-infer](https://github.com/COMBINE-lab/piscem-infer) a try. That tool already work",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:1821,Availability,down,downstream,1821,"h read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the other hand, if you are in need of something that produces identical output between runs (but that still also lets you assess uncertainty), then you can give [piscem](https://github.com/COMBINE-lab/piscem) => [piscem-infer](https://github.com/COMBINE-lab/piscem-infer) a try. That tool already works well, but it is in active development and we'd certainly be happy to help build features that you and others might find useful, and would be happy to chat about that if you like.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:846,Deployability,update,updates,846,"Hi @nh13,. This is not something for which we currently have support or something that we currently plan. I'd be open to it, but I'm honestly not sure how to cleanly do it in the current architecture, and doing so would certainly incur a performance hit. Salmon runs 2 phases of inference; and online phase and an offline phase. The online phase has access to _fragment-level_ information that is then summarized away during the offline phase (like the specific locations of each read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:707,Integrability,depend,depend,707,"Hi @nh13,. This is not something for which we currently have support or something that we currently plan. I'd be open to it, but I'm honestly not sure how to cleanly do it in the current architecture, and doing so would certainly incur a performance hit. Salmon runs 2 phases of inference; and online phase and an offline phase. The online phase has access to _fragment-level_ information that is then summarized away during the offline phase (like the specific locations of each read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:238,Performance,perform,performance,238,"Hi @nh13,. This is not something for which we currently have support or something that we currently plan. I'd be open to it, but I'm honestly not sure how to cleanly do it in the current architecture, and doing so would certainly incur a performance hit. Salmon runs 2 phases of inference; and online phase and an offline phase. The online phase has access to _fragment-level_ information that is then summarized away during the offline phase (like the specific locations of each read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:350,Security,access,access,350,"Hi @nh13,. This is not something for which we currently have support or something that we currently plan. I'd be open to it, but I'm honestly not sure how to cleanly do it in the current architecture, and doing so would certainly incur a performance hit. Salmon runs 2 phases of inference; and online phase and an offline phase. The online phase has access to _fragment-level_ information that is then summarized away during the offline phase (like the specific locations of each read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:984,Testability,test,tested,984,"Hi @nh13,. This is not something for which we currently have support or something that we currently plan. I'd be open to it, but I'm honestly not sure how to cleanly do it in the current architecture, and doing so would certainly incur a performance hit. Salmon runs 2 phases of inference; and online phase and an offline phase. The online phase has access to _fragment-level_ information that is then summarized away during the offline phase (like the specific locations of each read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:1936,Testability,test,testing,1936,"h read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the other hand, if you are in need of something that produces identical output between runs (but that still also lets you assess uncertainty), then you can give [piscem](https://github.com/COMBINE-lab/piscem) => [piscem-infer](https://github.com/COMBINE-lab/piscem-infer) a try. That tool already works well, but it is in active development and we'd certainly be happy to help build features that you and others might find useful, and would be happy to chat about that if you like.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538:670,Usability,learn,learned,670,"Hi @nh13,. This is not something for which we currently have support or something that we currently plan. I'd be open to it, but I'm honestly not sure how to cleanly do it in the current architecture, and doing so would certainly incur a performance hit. Salmon runs 2 phases of inference; and online phase and an offline phase. The online phase has access to _fragment-level_ information that is then summarized away during the offline phase (like the specific locations of each read, the length of each observed fragment, etc.). That information goes away when the reads are summarized into range-factorized equivalence classes. Moreover, some of the model parameters learned during the online phase will depend (in their details) on the order in which observations are made. Ostensibly, observing the same data in the same order **and issuing updates to shared model parameters from worker threads in the same order** should result in identical values, however this has never been tested and was never a design goal. The reason for this is that differences between runs are within the bounds of the inherent inferential uncertainty of the estimated parameters anyway. That is, if one is relying on a specific value at a level of precision such that a different run of salmon would produce a value different enough to change a downstream analysis, then one is imparting more precision on the estimates than they can provide. Other methods that produce identical results between runs for these values may produce the same output, but the accuracy of the output at that level shouldn't be trusted in this case. The uncertainty of the parameter estimates can be evaluated based on the Gibb samples (or bootstrap replicates) that salmon computes. Of course, the small differences between runs rarely lead to differences in downstream analysis (almost certainly at the gene level and also at the transcript level if you use a differential testing method that is aware of inferential uncertainty). On the ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/102#issuecomment-2159300538
https://github.com/COMBINE-lab/salmon/issues/107#issuecomment-263408444:176,Energy Efficiency,allocate,allocate,176,"Hi @xinbindai,. What you are seeing in both Salmon and RSEM is the expected behavior. This is because, to a large extent, the entire purpose of these tools is to appropriately allocate mulit-mapping reads. In your case, it is likely the case that one of the two very similar transcripts could account for all of the reads, while the other could not. For example, image I have a simple scenario where I have two transcripts:. ```; ACACACTGTGTGTG; ACACACGGTGTGTG; ```. Now, imagine I observe the ""reads"":. ```; ACAC; ACAC; CACA; CACA; ACTG; CTGT; GTGT; TGTG; TGTG; ```. The majority of these reads could have come from either transcript (and are equally likely to have come from both). However, the fact that we observe `ACGT` and `CTGT` is rather strong evidence that we could explain all of the reads via the first transcript while positing 0 (or close to 0) abundance for the second. On a much larger scale, this is what Salmon and RSEM are doing --- they are finding the most likely abundances of the transcripts given the observed data (the reads). When there is unique evidence of one of the two variants, and no unique evidence of the other, the maximum likelihood estimate for the variant with no unique evidence is very small. I'm not sure how many reads you are mapping, but you likely got a somewhat different estimate from eXpress since it tends to regularize it's abundance estimates a bit more strongly than Salmon or RSEM. That being said, this is the intended behavior of these tools, they are meant to probabilistically allocate multi-mapping fragments to similar transcripts in a manner that maximizes a global likelihood, so I don't think that what you are seeing is un-expected. In fact, it is consistent with the probabilistic model that underlies all three tools.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/107#issuecomment-263408444
https://github.com/COMBINE-lab/salmon/issues/107#issuecomment-263408444:1535,Energy Efficiency,allocate,allocate,1535,"Hi @xinbindai,. What you are seeing in both Salmon and RSEM is the expected behavior. This is because, to a large extent, the entire purpose of these tools is to appropriately allocate mulit-mapping reads. In your case, it is likely the case that one of the two very similar transcripts could account for all of the reads, while the other could not. For example, image I have a simple scenario where I have two transcripts:. ```; ACACACTGTGTGTG; ACACACGGTGTGTG; ```. Now, imagine I observe the ""reads"":. ```; ACAC; ACAC; CACA; CACA; ACTG; CTGT; GTGT; TGTG; TGTG; ```. The majority of these reads could have come from either transcript (and are equally likely to have come from both). However, the fact that we observe `ACGT` and `CTGT` is rather strong evidence that we could explain all of the reads via the first transcript while positing 0 (or close to 0) abundance for the second. On a much larger scale, this is what Salmon and RSEM are doing --- they are finding the most likely abundances of the transcripts given the observed data (the reads). When there is unique evidence of one of the two variants, and no unique evidence of the other, the maximum likelihood estimate for the variant with no unique evidence is very small. I'm not sure how many reads you are mapping, but you likely got a somewhat different estimate from eXpress since it tends to regularize it's abundance estimates a bit more strongly than Salmon or RSEM. That being said, this is the intended behavior of these tools, they are meant to probabilistically allocate multi-mapping fragments to similar transcripts in a manner that maximizes a global likelihood, so I don't think that what you are seeing is un-expected. In fact, it is consistent with the probabilistic model that underlies all three tools.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/107#issuecomment-263408444
https://github.com/COMBINE-lab/salmon/issues/107#issuecomment-263408444:378,Usability,simpl,simple,378,"Hi @xinbindai,. What you are seeing in both Salmon and RSEM is the expected behavior. This is because, to a large extent, the entire purpose of these tools is to appropriately allocate mulit-mapping reads. In your case, it is likely the case that one of the two very similar transcripts could account for all of the reads, while the other could not. For example, image I have a simple scenario where I have two transcripts:. ```; ACACACTGTGTGTG; ACACACGGTGTGTG; ```. Now, imagine I observe the ""reads"":. ```; ACAC; ACAC; CACA; CACA; ACTG; CTGT; GTGT; TGTG; TGTG; ```. The majority of these reads could have come from either transcript (and are equally likely to have come from both). However, the fact that we observe `ACGT` and `CTGT` is rather strong evidence that we could explain all of the reads via the first transcript while positing 0 (or close to 0) abundance for the second. On a much larger scale, this is what Salmon and RSEM are doing --- they are finding the most likely abundances of the transcripts given the observed data (the reads). When there is unique evidence of one of the two variants, and no unique evidence of the other, the maximum likelihood estimate for the variant with no unique evidence is very small. I'm not sure how many reads you are mapping, but you likely got a somewhat different estimate from eXpress since it tends to regularize it's abundance estimates a bit more strongly than Salmon or RSEM. That being said, this is the intended behavior of these tools, they are meant to probabilistically allocate multi-mapping fragments to similar transcripts in a manner that maximizes a global likelihood, so I don't think that what you are seeing is un-expected. In fact, it is consistent with the probabilistic model that underlies all three tools.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/107#issuecomment-263408444
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:753,Deployability,release,release,753,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:124,Energy Efficiency,allocate,allocated,124,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:1233,Testability,test,testing,1233,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:196,Usability,simpl,simplified,196,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:428,Usability,simpl,simply,428,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:625,Usability,simpl,simply,625,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:848,Usability,clear,clear,848,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889:1043,Usability,simpl,simply,1043,"Hi @jan-g1,. The length of a feature is used during inference to determine the likelihood that multimapping reads should be allocated to different targets. You're describing what is essentially a simplified model where P(f | t) (i.e., the probability of a fragment given a transcript) is independent of length(t). There's currently no option to disable length normalization completely in Salmon, and you can't ""de-normalize"" by simply multiplying by a factor because those weights are considered during each and every round of the EM (or VBEM) algorithm. However, supporting this should actually be very straight-forward. We simply assign a uniform and identical length to all transcripts for the purpose of inference. I can add such a flag in the next release, though it will initially have to be incompatible with bias correction (since it's not clear right now how the biases for which we account interact with this type of sequencing). Also, it would be possible to run salmon with `--dumpEq`, and then to have a little script / tool that simply re-runs the EM, but without different length factors, using the equivalence class file. I might be able to hack something like that together on short notice if you'd be interested in testing it out. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/108#issuecomment-264659889
https://github.com/COMBINE-lab/salmon/issues/109#issuecomment-267003338:63,Usability,simpl,simple,63,"I like the opt-in structure, I think in general keeping models simple by default is a good way of doing things. I know people just run programs without reading documentation and expect it to work perfectly. But I think somewhere, maybe even in the default quantification help, there should be a table or decision tree with information about how to choose options. E.g. > ""Did you do random hexamer priming? -> Use the --seqBias option.""",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/109#issuecomment-267003338
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204:415,Availability,error,errorminEQClassWeight,415,"So I got the data and am trying to repro the issue now (thanks!). Quick question. I noticed in your example command you have `--libType ISF`. However, you have single-end reads, so the appropriate library type would be `SF` (i.e., they can't be ""inward"" facing reads, b/c there is no mate for each read). When I run your command as is, but replace `ISF` as `SF`, my run completes successfully, and I don't get any `errorminEQClassWeight` output. Could you let me know if this makes any difference for you (also, sorry that, apparently, we're not outputting a useful error message when one passes in a paired-end library type with single-end data). edit: Actually, it's even stranger. I noticed that in your command the library type comes *after* the reads to which it refers, but in this case, Salmon will not apply that library type to those reads (which explains why you're not getting a warning message). The restriction that the `--libType` flag comes before the reads it describes is buried in [the docs](http://salmon.readthedocs.io/en/latest/salmon.html#using-salmon), but I definitely need to make that clearer. Anyway, the point is that, in this case, Salmon should apply the ""default"" single-end library type (i.e., `U`) to your reads. So, presumably, that was what was happening when you saw the strange behavior during Gibbs sampling (and is also what was happening when my Gibbs sampling run completed successfully).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204:566,Availability,error,error,566,"So I got the data and am trying to repro the issue now (thanks!). Quick question. I noticed in your example command you have `--libType ISF`. However, you have single-end reads, so the appropriate library type would be `SF` (i.e., they can't be ""inward"" facing reads, b/c there is no mate for each read). When I run your command as is, but replace `ISF` as `SF`, my run completes successfully, and I don't get any `errorminEQClassWeight` output. Could you let me know if this makes any difference for you (also, sorry that, apparently, we're not outputting a useful error message when one passes in a paired-end library type with single-end data). edit: Actually, it's even stranger. I noticed that in your command the library type comes *after* the reads to which it refers, but in this case, Salmon will not apply that library type to those reads (which explains why you're not getting a warning message). The restriction that the `--libType` flag comes before the reads it describes is buried in [the docs](http://salmon.readthedocs.io/en/latest/salmon.html#using-salmon), but I definitely need to make that clearer. Anyway, the point is that, in this case, Salmon should apply the ""default"" single-end library type (i.e., `U`) to your reads. So, presumably, that was what was happening when you saw the strange behavior during Gibbs sampling (and is also what was happening when my Gibbs sampling run completed successfully).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204:572,Integrability,message,message,572,"So I got the data and am trying to repro the issue now (thanks!). Quick question. I noticed in your example command you have `--libType ISF`. However, you have single-end reads, so the appropriate library type would be `SF` (i.e., they can't be ""inward"" facing reads, b/c there is no mate for each read). When I run your command as is, but replace `ISF` as `SF`, my run completes successfully, and I don't get any `errorminEQClassWeight` output. Could you let me know if this makes any difference for you (also, sorry that, apparently, we're not outputting a useful error message when one passes in a paired-end library type with single-end data). edit: Actually, it's even stranger. I noticed that in your command the library type comes *after* the reads to which it refers, but in this case, Salmon will not apply that library type to those reads (which explains why you're not getting a warning message). The restriction that the `--libType` flag comes before the reads it describes is buried in [the docs](http://salmon.readthedocs.io/en/latest/salmon.html#using-salmon), but I definitely need to make that clearer. Anyway, the point is that, in this case, Salmon should apply the ""default"" single-end library type (i.e., `U`) to your reads. So, presumably, that was what was happening when you saw the strange behavior during Gibbs sampling (and is also what was happening when my Gibbs sampling run completed successfully).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204:898,Integrability,message,message,898,"So I got the data and am trying to repro the issue now (thanks!). Quick question. I noticed in your example command you have `--libType ISF`. However, you have single-end reads, so the appropriate library type would be `SF` (i.e., they can't be ""inward"" facing reads, b/c there is no mate for each read). When I run your command as is, but replace `ISF` as `SF`, my run completes successfully, and I don't get any `errorminEQClassWeight` output. Could you let me know if this makes any difference for you (also, sorry that, apparently, we're not outputting a useful error message when one passes in a paired-end library type with single-end data). edit: Actually, it's even stranger. I noticed that in your command the library type comes *after* the reads to which it refers, but in this case, Salmon will not apply that library type to those reads (which explains why you're not getting a warning message). The restriction that the `--libType` flag comes before the reads it describes is buried in [the docs](http://salmon.readthedocs.io/en/latest/salmon.html#using-salmon), but I definitely need to make that clearer. Anyway, the point is that, in this case, Salmon should apply the ""default"" single-end library type (i.e., `U`) to your reads. So, presumably, that was what was happening when you saw the strange behavior during Gibbs sampling (and is also what was happening when my Gibbs sampling run completed successfully).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204:1111,Usability,clear,clearer,1111,"So I got the data and am trying to repro the issue now (thanks!). Quick question. I noticed in your example command you have `--libType ISF`. However, you have single-end reads, so the appropriate library type would be `SF` (i.e., they can't be ""inward"" facing reads, b/c there is no mate for each read). When I run your command as is, but replace `ISF` as `SF`, my run completes successfully, and I don't get any `errorminEQClassWeight` output. Could you let me know if this makes any difference for you (also, sorry that, apparently, we're not outputting a useful error message when one passes in a paired-end library type with single-end data). edit: Actually, it's even stranger. I noticed that in your command the library type comes *after* the reads to which it refers, but in this case, Salmon will not apply that library type to those reads (which explains why you're not getting a warning message). The restriction that the `--libType` flag comes before the reads it describes is buried in [the docs](http://salmon.readthedocs.io/en/latest/salmon.html#using-salmon), but I definitely need to make that clearer. Anyway, the point is that, in this case, Salmon should apply the ""default"" single-end library type (i.e., `U`) to your reads. So, presumably, that was what was happening when you saw the strange behavior during Gibbs sampling (and is also what was happening when my Gibbs sampling run completed successfully).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266927204
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266932730:80,Usability,simpl,simply,80,"Well, if they *were* paired end, they would be ISF. I assumed that Salmon would simply ignore the pairing information if you fed it single-end reads. (I think this is how some other tools work, maybe?) I'll retry with fixed library specifications and see if that fixes things. Edit: I just noticed your edit. I'll reply again in a minute.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266932730
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:3787,Integrability,protocol,protocol,3787, [info] Index contained 182608 targets. processed 19000000 fragments; hits: 65897209; hits per frag: 3.47349. [2016-12-13 22:40:22.572] [jointLog] [info] Computed 137534 rich equivalence classes for further processing; [2016-12-13 22:40:22.572] [jointLog] [info] Counted 16265961 total reads in the equivalence classes; [2016-12-13 22:40:22.618] [jointLog] [info] Mapping rate = 83.509%. [2016-12-13 22:40:22.618] [jointLog] [info] finished quantifyLibrary(); [2016-12-13 22:40:22.619] [jointLog] [info] Starting optimizer; [2016-12-13 22:40:22.904] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2016-12-13 22:40:22.911] [jointLog] [info] iteration = 0 | max rel diff. = 299.976; [2016-12-13 22:40:23.620] [jointLog] [info] iteration = 100 | max rel diff. = 0.121769; [2016-12-13 22:40:24.367] [jointLog] [info] iteration = 200 | max rel diff. = 0.103587; [2016-12-13 22:40:25.102] [jointLog] [info] iteration = 300 | max rel diff. = 0.144748; [2016-12-13 22:40:25.815] [jointLog] [info] iteration = 400 | max rel diff. = 0.231057; [2016-12-13 22:40:26.505] [jointLog] [info] iteration = 500 | max rel diff. = 0.0156154; [2016-12-13 22:40:27.020] [jointLog] [info] iteration = 570 | max rel diff. = 0.00955966; [2016-12-13 22:40:27.052] [jointLog] [info] Finished optimizer; [2016-12-13 22:40:27.052] [jointLog] [info] writing output. [2016-12-13 22:40:27.523] [jointLog] [info] Starting Gibbs Sampler 1 week; 100% [=====================================================] in 44s; [2016-12-13 22:41:12.189] [jointLog] [info] Finished Gibbs Sampler; [2016-12-13 22:41:12.190] [jointLog] [warning] NOTE: Read Lib [SRR2454059.fq.gz] :. Detected a *potential* strand bias > 1% in an unstranded protocol check the file: test_quant/lib_format_counts.json for details; ```. edit: One note is that I was using my build of the same commit number. I'm running the executable you compiled now (since I had to put the appropriate libraries in the `LD_LIBRARY_PATH` to get it to be happy).,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:1908,Performance,load,loading,1908, ] => { Salmon_index_hg38.analysisSet_knownGene }; ### [ unmatedReads ] => { SRR2454059.fq.gz }; ### [ libType ] => { ISF }; ### [ useVBOpt ] => { }; ### [ output ] => { test_quant }; ### [ numGibbsSamples ] => { 100 }; ### [ threads ] => { 16 }; Logs will be written to test_quant/logs; [2016-12-13 22:38:54.413] [jointLog] [info] parsing read library format; [2016-12-13 22:38:54.413] [jointLog] [info] There is 1 library.; [2016-12-13 22:38:56.240] [stderrLog] [info] Loading Suffix Array; [2016-12-13 22:38:56.240] [jointLog] [info] Loading Quasi index; [2016-12-13 22:38:56.240] [jointLog] [info] Loading 32-bit quasi index; [2016-12-13 22:39:01.268] [stderrLog] [info] Loading Transcript Info; [2016-12-13 22:39:02.630] [stderrLog] [info] Loading Rank-Select Bit Array; [2016-12-13 22:39:03.041] [stderrLog] [info] There were 182608 set bits in the bit array; [2016-12-13 22:39:03.159] [stderrLog] [info] Computing transcript lengths; [2016-12-13 22:39:03.160] [stderrLog] [info] Waiting to finish loading hash; [2016-12-13 22:39:07.653] [stderrLog] [info] Done loading index; [2016-12-13 22:39:07.653] [jointLog] [info] done; [2016-12-13 22:39:07.653] [jointLog] [info] Index contained 182608 targets. processed 19000000 fragments; hits: 65897209; hits per frag: 3.47349. [2016-12-13 22:40:22.572] [jointLog] [info] Computed 137534 rich equivalence classes for further processing; [2016-12-13 22:40:22.572] [jointLog] [info] Counted 16265961 total reads in the equivalence classes; [2016-12-13 22:40:22.618] [jointLog] [info] Mapping rate = 83.509%. [2016-12-13 22:40:22.618] [jointLog] [info] finished quantifyLibrary(); [2016-12-13 22:40:22.619] [jointLog] [info] Starting optimizer; [2016-12-13 22:40:22.904] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2016-12-13 22:40:22.911] [jointLog] [info] iteration = 0 | max rel diff. = 299.976; [2016-12-13 22:40:23.620] [jointLog] [info] iteration = 100 | max rel diff. = 0.121769; [2016-12-13 22:40:24.367] [jointLog] [,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:1972,Performance,load,loading,1972, => { SRR2454059.fq.gz }; ### [ libType ] => { ISF }; ### [ useVBOpt ] => { }; ### [ output ] => { test_quant }; ### [ numGibbsSamples ] => { 100 }; ### [ threads ] => { 16 }; Logs will be written to test_quant/logs; [2016-12-13 22:38:54.413] [jointLog] [info] parsing read library format; [2016-12-13 22:38:54.413] [jointLog] [info] There is 1 library.; [2016-12-13 22:38:56.240] [stderrLog] [info] Loading Suffix Array; [2016-12-13 22:38:56.240] [jointLog] [info] Loading Quasi index; [2016-12-13 22:38:56.240] [jointLog] [info] Loading 32-bit quasi index; [2016-12-13 22:39:01.268] [stderrLog] [info] Loading Transcript Info; [2016-12-13 22:39:02.630] [stderrLog] [info] Loading Rank-Select Bit Array; [2016-12-13 22:39:03.041] [stderrLog] [info] There were 182608 set bits in the bit array; [2016-12-13 22:39:03.159] [stderrLog] [info] Computing transcript lengths; [2016-12-13 22:39:03.160] [stderrLog] [info] Waiting to finish loading hash; [2016-12-13 22:39:07.653] [stderrLog] [info] Done loading index; [2016-12-13 22:39:07.653] [jointLog] [info] done; [2016-12-13 22:39:07.653] [jointLog] [info] Index contained 182608 targets. processed 19000000 fragments; hits: 65897209; hits per frag: 3.47349. [2016-12-13 22:40:22.572] [jointLog] [info] Computed 137534 rich equivalence classes for further processing; [2016-12-13 22:40:22.572] [jointLog] [info] Counted 16265961 total reads in the equivalence classes; [2016-12-13 22:40:22.618] [jointLog] [info] Mapping rate = 83.509%. [2016-12-13 22:40:22.618] [jointLog] [info] finished quantifyLibrary(); [2016-12-13 22:40:22.619] [jointLog] [info] Starting optimizer; [2016-12-13 22:40:22.904] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2016-12-13 22:40:22.911] [jointLog] [info] iteration = 0 | max rel diff. = 299.976; [2016-12-13 22:40:23.620] [jointLog] [info] iteration = 100 | max rel diff. = 0.121769; [2016-12-13 22:40:24.367] [jointLog] [info] iteration = 200 | max rel diff. = 0.103587; [2016-12-13 22:40:25.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:2586,Performance,optimiz,optimizer,2586, Transcript Info; [2016-12-13 22:39:02.630] [stderrLog] [info] Loading Rank-Select Bit Array; [2016-12-13 22:39:03.041] [stderrLog] [info] There were 182608 set bits in the bit array; [2016-12-13 22:39:03.159] [stderrLog] [info] Computing transcript lengths; [2016-12-13 22:39:03.160] [stderrLog] [info] Waiting to finish loading hash; [2016-12-13 22:39:07.653] [stderrLog] [info] Done loading index; [2016-12-13 22:39:07.653] [jointLog] [info] done; [2016-12-13 22:39:07.653] [jointLog] [info] Index contained 182608 targets. processed 19000000 fragments; hits: 65897209; hits per frag: 3.47349. [2016-12-13 22:40:22.572] [jointLog] [info] Computed 137534 rich equivalence classes for further processing; [2016-12-13 22:40:22.572] [jointLog] [info] Counted 16265961 total reads in the equivalence classes; [2016-12-13 22:40:22.618] [jointLog] [info] Mapping rate = 83.509%. [2016-12-13 22:40:22.618] [jointLog] [info] finished quantifyLibrary(); [2016-12-13 22:40:22.619] [jointLog] [info] Starting optimizer; [2016-12-13 22:40:22.904] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2016-12-13 22:40:22.911] [jointLog] [info] iteration = 0 | max rel diff. = 299.976; [2016-12-13 22:40:23.620] [jointLog] [info] iteration = 100 | max rel diff. = 0.121769; [2016-12-13 22:40:24.367] [jointLog] [info] iteration = 200 | max rel diff. = 0.103587; [2016-12-13 22:40:25.102] [jointLog] [info] iteration = 300 | max rel diff. = 0.144748; [2016-12-13 22:40:25.815] [jointLog] [info] iteration = 400 | max rel diff. = 0.231057; [2016-12-13 22:40:26.505] [jointLog] [info] iteration = 500 | max rel diff. = 0.0156154; [2016-12-13 22:40:27.020] [jointLog] [info] iteration = 570 | max rel diff. = 0.00955966; [2016-12-13 22:40:27.052] [jointLog] [info] Finished optimizer; [2016-12-13 22:40:27.052] [jointLog] [info] writing output. [2016-12-13 22:40:27.523] [jointLog] [info] Starting Gibbs Sampler 1 week; 100% [=====================================================] in 44s; [2016-12,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:3363,Performance,optimiz,optimizer,3363, [info] Index contained 182608 targets. processed 19000000 fragments; hits: 65897209; hits per frag: 3.47349. [2016-12-13 22:40:22.572] [jointLog] [info] Computed 137534 rich equivalence classes for further processing; [2016-12-13 22:40:22.572] [jointLog] [info] Counted 16265961 total reads in the equivalence classes; [2016-12-13 22:40:22.618] [jointLog] [info] Mapping rate = 83.509%. [2016-12-13 22:40:22.618] [jointLog] [info] finished quantifyLibrary(); [2016-12-13 22:40:22.619] [jointLog] [info] Starting optimizer; [2016-12-13 22:40:22.904] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2016-12-13 22:40:22.911] [jointLog] [info] iteration = 0 | max rel diff. = 299.976; [2016-12-13 22:40:23.620] [jointLog] [info] iteration = 100 | max rel diff. = 0.121769; [2016-12-13 22:40:24.367] [jointLog] [info] iteration = 200 | max rel diff. = 0.103587; [2016-12-13 22:40:25.102] [jointLog] [info] iteration = 300 | max rel diff. = 0.144748; [2016-12-13 22:40:25.815] [jointLog] [info] iteration = 400 | max rel diff. = 0.231057; [2016-12-13 22:40:26.505] [jointLog] [info] iteration = 500 | max rel diff. = 0.0156154; [2016-12-13 22:40:27.020] [jointLog] [info] iteration = 570 | max rel diff. = 0.00955966; [2016-12-13 22:40:27.052] [jointLog] [info] Finished optimizer; [2016-12-13 22:40:27.052] [jointLog] [info] writing output. [2016-12-13 22:40:27.523] [jointLog] [info] Starting Gibbs Sampler 1 week; 100% [=====================================================] in 44s; [2016-12-13 22:41:12.189] [jointLog] [info] Finished Gibbs Sampler; [2016-12-13 22:41:12.190] [jointLog] [warning] NOTE: Read Lib [SRR2454059.fq.gz] :. Detected a *potential* strand bias > 1% in an unstranded protocol check the file: test_quant/lib_format_counts.json for details; ```. edit: One note is that I was using my build of the same commit number. I'm running the executable you compiled now (since I had to put the appropriate libraries in the `LD_LIBRARY_PATH` to get it to be happy).,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:1916,Security,hash,hash,1916, ] => { Salmon_index_hg38.analysisSet_knownGene }; ### [ unmatedReads ] => { SRR2454059.fq.gz }; ### [ libType ] => { ISF }; ### [ useVBOpt ] => { }; ### [ output ] => { test_quant }; ### [ numGibbsSamples ] => { 100 }; ### [ threads ] => { 16 }; Logs will be written to test_quant/logs; [2016-12-13 22:38:54.413] [jointLog] [info] parsing read library format; [2016-12-13 22:38:54.413] [jointLog] [info] There is 1 library.; [2016-12-13 22:38:56.240] [stderrLog] [info] Loading Suffix Array; [2016-12-13 22:38:56.240] [jointLog] [info] Loading Quasi index; [2016-12-13 22:38:56.240] [jointLog] [info] Loading 32-bit quasi index; [2016-12-13 22:39:01.268] [stderrLog] [info] Loading Transcript Info; [2016-12-13 22:39:02.630] [stderrLog] [info] Loading Rank-Select Bit Array; [2016-12-13 22:39:03.041] [stderrLog] [info] There were 182608 set bits in the bit array; [2016-12-13 22:39:03.159] [stderrLog] [info] Computing transcript lengths; [2016-12-13 22:39:03.160] [stderrLog] [info] Waiting to finish loading hash; [2016-12-13 22:39:07.653] [stderrLog] [info] Done loading index; [2016-12-13 22:39:07.653] [jointLog] [info] done; [2016-12-13 22:39:07.653] [jointLog] [info] Index contained 182608 targets. processed 19000000 fragments; hits: 65897209; hits per frag: 3.47349. [2016-12-13 22:40:22.572] [jointLog] [info] Computed 137534 rich equivalence classes for further processing; [2016-12-13 22:40:22.572] [jointLog] [info] Counted 16265961 total reads in the equivalence classes; [2016-12-13 22:40:22.618] [jointLog] [info] Mapping rate = 83.509%. [2016-12-13 22:40:22.618] [jointLog] [info] finished quantifyLibrary(); [2016-12-13 22:40:22.619] [jointLog] [info] Starting optimizer; [2016-12-13 22:40:22.904] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2016-12-13 22:40:22.911] [jointLog] [info] iteration = 0 | max rel diff. = 299.976; [2016-12-13 22:40:23.620] [jointLog] [info] iteration = 100 | max rel diff. = 0.121769; [2016-12-13 22:40:24.367] [jointLog] [,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:393,Testability,log,log,393,"No, that shouldn't cause a problem. When I ran your command (including the `ISF` and placing the `--libType` after the set of reads), my run still completed successfully (and didn't produce any warnings during Gibbs sampling). Salmon's behavior when running in unstranded mode on stranded data is simply to map the reads in the orientation they match, and to report on the console (and in the log) that there was a mapping bias (i.e. that the data look stranded). Specifically, here is what I get when I run (a close approximation of) your command. ```; $salmon quant --index Salmon_index_hg38.analysisSet_knownGene --unmatedReads SRR2454059.fq.gz --libType ISF --useVBOpt --output test_quant --; numGibbsSamples 100 --threads 16; Version Info: This is the most recent **development version** of Salmon.; ### salmon (mapping-based) v0.7.3; ### [ program ] => salmon; ### [ command ] => quant; ### [ index ] => { Salmon_index_hg38.analysisSet_knownGene }; ### [ unmatedReads ] => { SRR2454059.fq.gz }; ### [ libType ] => { ISF }; ### [ useVBOpt ] => { }; ### [ output ] => { test_quant }; ### [ numGibbsSamples ] => { 100 }; ### [ threads ] => { 16 }; Logs will be written to test_quant/logs; [2016-12-13 22:38:54.413] [jointLog] [info] parsing read library format; [2016-12-13 22:38:54.413] [jointLog] [info] There is 1 library.; [2016-12-13 22:38:56.240] [stderrLog] [info] Loading Suffix Array; [2016-12-13 22:38:56.240] [jointLog] [info] Loading Quasi index; [2016-12-13 22:38:56.240] [jointLog] [info] Loading 32-bit quasi index; [2016-12-13 22:39:01.268] [stderrLog] [info] Loading Transcript Info; [2016-12-13 22:39:02.630] [stderrLog] [info] Loading Rank-Select Bit Array; [2016-12-13 22:39:03.041] [stderrLog] [info] There were 182608 set bits in the bit array; [2016-12-13 22:39:03.159] [stderrLog] [info] Computing transcript lengths; [2016-12-13 22:39:03.160] [stderrLog] [info] Waiting to finish loading hash; [2016-12-13 22:39:07.653] [stderrLog] [info] Done loading index; [2016-12-13 22",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:1186,Testability,log,logs,1186,"ype` after the set of reads), my run still completed successfully (and didn't produce any warnings during Gibbs sampling). Salmon's behavior when running in unstranded mode on stranded data is simply to map the reads in the orientation they match, and to report on the console (and in the log) that there was a mapping bias (i.e. that the data look stranded). Specifically, here is what I get when I run (a close approximation of) your command. ```; $salmon quant --index Salmon_index_hg38.analysisSet_knownGene --unmatedReads SRR2454059.fq.gz --libType ISF --useVBOpt --output test_quant --; numGibbsSamples 100 --threads 16; Version Info: This is the most recent **development version** of Salmon.; ### salmon (mapping-based) v0.7.3; ### [ program ] => salmon; ### [ command ] => quant; ### [ index ] => { Salmon_index_hg38.analysisSet_knownGene }; ### [ unmatedReads ] => { SRR2454059.fq.gz }; ### [ libType ] => { ISF }; ### [ useVBOpt ] => { }; ### [ output ] => { test_quant }; ### [ numGibbsSamples ] => { 100 }; ### [ threads ] => { 16 }; Logs will be written to test_quant/logs; [2016-12-13 22:38:54.413] [jointLog] [info] parsing read library format; [2016-12-13 22:38:54.413] [jointLog] [info] There is 1 library.; [2016-12-13 22:38:56.240] [stderrLog] [info] Loading Suffix Array; [2016-12-13 22:38:56.240] [jointLog] [info] Loading Quasi index; [2016-12-13 22:38:56.240] [jointLog] [info] Loading 32-bit quasi index; [2016-12-13 22:39:01.268] [stderrLog] [info] Loading Transcript Info; [2016-12-13 22:39:02.630] [stderrLog] [info] Loading Rank-Select Bit Array; [2016-12-13 22:39:03.041] [stderrLog] [info] There were 182608 set bits in the bit array; [2016-12-13 22:39:03.159] [stderrLog] [info] Computing transcript lengths; [2016-12-13 22:39:03.160] [stderrLog] [info] Waiting to finish loading hash; [2016-12-13 22:39:07.653] [stderrLog] [info] Done loading index; [2016-12-13 22:39:07.653] [jointLog] [info] done; [2016-12-13 22:39:07.653] [jointLog] [info] Index contained 182608 ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878:297,Usability,simpl,simply,297,"No, that shouldn't cause a problem. When I ran your command (including the `ISF` and placing the `--libType` after the set of reads), my run still completed successfully (and didn't produce any warnings during Gibbs sampling). Salmon's behavior when running in unstranded mode on stranded data is simply to map the reads in the orientation they match, and to report on the console (and in the log) that there was a mapping bias (i.e. that the data look stranded). Specifically, here is what I get when I run (a close approximation of) your command. ```; $salmon quant --index Salmon_index_hg38.analysisSet_knownGene --unmatedReads SRR2454059.fq.gz --libType ISF --useVBOpt --output test_quant --; numGibbsSamples 100 --threads 16; Version Info: This is the most recent **development version** of Salmon.; ### salmon (mapping-based) v0.7.3; ### [ program ] => salmon; ### [ command ] => quant; ### [ index ] => { Salmon_index_hg38.analysisSet_knownGene }; ### [ unmatedReads ] => { SRR2454059.fq.gz }; ### [ libType ] => { ISF }; ### [ useVBOpt ] => { }; ### [ output ] => { test_quant }; ### [ numGibbsSamples ] => { 100 }; ### [ threads ] => { 16 }; Logs will be written to test_quant/logs; [2016-12-13 22:38:54.413] [jointLog] [info] parsing read library format; [2016-12-13 22:38:54.413] [jointLog] [info] There is 1 library.; [2016-12-13 22:38:56.240] [stderrLog] [info] Loading Suffix Array; [2016-12-13 22:38:56.240] [jointLog] [info] Loading Quasi index; [2016-12-13 22:38:56.240] [jointLog] [info] Loading 32-bit quasi index; [2016-12-13 22:39:01.268] [stderrLog] [info] Loading Transcript Info; [2016-12-13 22:39:02.630] [stderrLog] [info] Loading Rank-Select Bit Array; [2016-12-13 22:39:03.041] [stderrLog] [info] There were 182608 set bits in the bit array; [2016-12-13 22:39:03.159] [stderrLog] [info] Computing transcript lengths; [2016-12-13 22:39:03.160] [stderrLog] [info] Waiting to finish loading hash; [2016-12-13 22:39:07.653] [stderrLog] [info] Done loading index; [2016-12-13 22",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-266934878
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-267428600:276,Availability,down,down,276,"Yup, the execution is *definitely* inside the Gibbs sampler at that point, since that's the code that sets up the progress bar etc. So, I'll focus my attention there until (if/when) we can get a specific offending function name. Thanks so much for all your help tracking this down so far; I really appreciate it!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-267428600
https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-267428600:114,Usability,progress bar,progress bar,114,"Yup, the execution is *definitely* inside the Gibbs sampler at that point, since that's the code that sets up the progress bar etc. So, I'll focus my attention there until (if/when) we can get a specific offending function name. Thanks so much for all your help tracking this down so far; I really appreciate it!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/111#issuecomment-267428600
https://github.com/COMBINE-lab/salmon/issues/120#issuecomment-279960488:206,Deployability,pipeline,pipeline,206,"Hi vals,; Cutoffs for Salmon as well as STAR+featurecounts/RSEM are all >0, no matter it is normalized value (RPKM, TPM) or rawcount. To my knowledge, there shouldn't be a hugh difference between different pipeline in terms of number of detected genes. Somehow, I think Salmon is over-sensitive to some extent. It's good to know that there will be small >0 expression on most genes. That makes the thing clear~. Best!; Gary",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/120#issuecomment-279960488
https://github.com/COMBINE-lab/salmon/issues/120#issuecomment-279960488:237,Safety,detect,detected,237,"Hi vals,; Cutoffs for Salmon as well as STAR+featurecounts/RSEM are all >0, no matter it is normalized value (RPKM, TPM) or rawcount. To my knowledge, there shouldn't be a hugh difference between different pipeline in terms of number of detected genes. Somehow, I think Salmon is over-sensitive to some extent. It's good to know that there will be small >0 expression on most genes. That makes the thing clear~. Best!; Gary",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/120#issuecomment-279960488
https://github.com/COMBINE-lab/salmon/issues/120#issuecomment-279960488:404,Usability,clear,clear,404,"Hi vals,; Cutoffs for Salmon as well as STAR+featurecounts/RSEM are all >0, no matter it is normalized value (RPKM, TPM) or rawcount. To my knowledge, there shouldn't be a hugh difference between different pipeline in terms of number of detected genes. Somehow, I think Salmon is over-sensitive to some extent. It's good to know that there will be small >0 expression on most genes. That makes the thing clear~. Best!; Gary",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/120#issuecomment-279960488
https://github.com/COMBINE-lab/salmon/issues/122#issuecomment-282048231:574,Security,access,accession,574,"@demis001 salmon currently makes exact matches between the fasta headers and transcript annotations in the GTF, so no - it doesn't work. Since gene level summarization is pretty simple you could just use something like https://github.com/daler/gffutils to read your GTF, drop the version numbers from the GTF entries, then drop the version numbers from the salmon quant.sf file, and join the two yourself. The summarization from tx->gene is just summing each gene's transcripts' TPM values. @rob-p: I do think this is a common enough issue that salmon could handle dropping accession.version numbers with an extra option.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/122#issuecomment-282048231
https://github.com/COMBINE-lab/salmon/issues/122#issuecomment-282048231:178,Usability,simpl,simple,178,"@demis001 salmon currently makes exact matches between the fasta headers and transcript annotations in the GTF, so no - it doesn't work. Since gene level summarization is pretty simple you could just use something like https://github.com/daler/gffutils to read your GTF, drop the version numbers from the GTF entries, then drop the version numbers from the salmon quant.sf file, and join the two yourself. The summarization from tx->gene is just summing each gene's transcripts' TPM values. @rob-p: I do think this is a common enough issue that salmon could handle dropping accession.version numbers with an extra option.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/122#issuecomment-282048231
https://github.com/COMBINE-lab/salmon/issues/122#issuecomment-283477075:359,Usability,simpl,simply,359,"Hi @demis001,. I see. There is only one behavior built in (i.e., report the transcript as it's own gene). You can easily filter the gene -level quantification file to get rid of transcripts like this though. The easiest way would be something like:. ```; > cat <(head -1 quant.genes.sf) <(grep ""ENSG*"" quant.genes.sf) > quant.genes.filtered.sf; ```; That is, simply filter the `quant.genes.sf` file for any line that matches `ENSG`, leaving the rest behind (and adding the header line). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/122#issuecomment-283477075
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-285686442:773,Deployability,update,updated,773,"Hi @ECuris,. Yes, the values you give are used to form a normal distribution, which is then truncated on the left at 0. So, the parameters you provide are the mean and standard deviation of the distribution *prior* to truncation. However, I'll note that the values are such that 0 is usually sufficiently far (in terms of standard deviations from the mean) that the mean and standard deviation of the fragment length distribution are very similar before and after the 0 truncation. Finally, I'll mention that, if you have paired-end reads, Salmon will *always* learn the empirical fragment length distribution (since the experiment, itself, is the best estimator of the true distribution), but the `--fldMean` and `--fldSD` parameters define the prior distribution that is updated with observed fragment lengths.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-285686442
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-285686442:561,Usability,learn,learn,561,"Hi @ECuris,. Yes, the values you give are used to form a normal distribution, which is then truncated on the left at 0. So, the parameters you provide are the mean and standard deviation of the distribution *prior* to truncation. However, I'll note that the values are such that 0 is usually sufficiently far (in terms of standard deviations from the mean) that the mean and standard deviation of the fragment length distribution are very similar before and after the 0 truncation. Finally, I'll mention that, if you have paired-end reads, Salmon will *always* learn the empirical fragment length distribution (since the experiment, itself, is the best estimator of the true distribution), but the `--fldMean` and `--fldSD` parameters define the prior distribution that is updated with observed fragment lengths.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-285686442
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243:884,Availability,robust,robust,884,"Hi @Tima-Ze,. Yes, salmon can be used to quantify these reads, but the results will depend (somewhat) on the `--fldMean` and `--fldSD` flags that are used. It's important to note that this is not a unique characteristic of salmon, and any transcript-level quantification tool using a probabilistic model (e.g. RSEM, eXpress, BitSeq, etc.) have the same requirement. That is, the fragment length distribution should be known so that _effective_ transcript lengths can be estimated, which have an effect on fragment assignment probabilities. If the wrong fragment length distribution is specified, then the _effective_ transcript lengths will be off and this can affect the assignment of some fragments. This is only a requirement with single-end reads, since with paired-end reads the fragment length distribution is learned from the data. Further, the inference procedure is somewhat robust to these choices (small changes in fld mean and sd don't generally lead to drastically different results). If you have access to the BioAnalyzer results for the sequencing run, those can give information about the fragment length distribution (even in a single end experiment). If not, you can proceed with the default values. Even if they don't exactly match the true distribution in the single-end sample, at least the same values will be applied in all samples and so, ideally, most results of misspecification will wash out in subsequent differential analysis. . Finally, it's worth noting that the same restriction holds in both alignment-based and mapping-based modes. This is because in neither mode do single-end fragments provide sufficient information to estimate the fragment length distribution from the data. We only know where one end of a fragment mapped and cannot infer where the other end would be. This is not an alignment versus mapping (versus selective-alignment) issue, but rather is fundamental to having only observed one side of the entire fragment generated during fragmentation and ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243:84,Integrability,depend,depend,84,"Hi @Tima-Ze,. Yes, salmon can be used to quantify these reads, but the results will depend (somewhat) on the `--fldMean` and `--fldSD` flags that are used. It's important to note that this is not a unique characteristic of salmon, and any transcript-level quantification tool using a probabilistic model (e.g. RSEM, eXpress, BitSeq, etc.) have the same requirement. That is, the fragment length distribution should be known so that _effective_ transcript lengths can be estimated, which have an effect on fragment assignment probabilities. If the wrong fragment length distribution is specified, then the _effective_ transcript lengths will be off and this can affect the assignment of some fragments. This is only a requirement with single-end reads, since with paired-end reads the fragment length distribution is learned from the data. Further, the inference procedure is somewhat robust to these choices (small changes in fld mean and sd don't generally lead to drastically different results). If you have access to the BioAnalyzer results for the sequencing run, those can give information about the fragment length distribution (even in a single end experiment). If not, you can proceed with the default values. Even if they don't exactly match the true distribution in the single-end sample, at least the same values will be applied in all samples and so, ideally, most results of misspecification will wash out in subsequent differential analysis. . Finally, it's worth noting that the same restriction holds in both alignment-based and mapping-based modes. This is because in neither mode do single-end fragments provide sufficient information to estimate the fragment length distribution from the data. We only know where one end of a fragment mapped and cannot infer where the other end would be. This is not an alignment versus mapping (versus selective-alignment) issue, but rather is fundamental to having only observed one side of the entire fragment generated during fragmentation and ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243:1010,Security,access,access,1010,"l depend (somewhat) on the `--fldMean` and `--fldSD` flags that are used. It's important to note that this is not a unique characteristic of salmon, and any transcript-level quantification tool using a probabilistic model (e.g. RSEM, eXpress, BitSeq, etc.) have the same requirement. That is, the fragment length distribution should be known so that _effective_ transcript lengths can be estimated, which have an effect on fragment assignment probabilities. If the wrong fragment length distribution is specified, then the _effective_ transcript lengths will be off and this can affect the assignment of some fragments. This is only a requirement with single-end reads, since with paired-end reads the fragment length distribution is learned from the data. Further, the inference procedure is somewhat robust to these choices (small changes in fld mean and sd don't generally lead to drastically different results). If you have access to the BioAnalyzer results for the sequencing run, those can give information about the fragment length distribution (even in a single end experiment). If not, you can proceed with the default values. Even if they don't exactly match the true distribution in the single-end sample, at least the same values will be applied in all samples and so, ideally, most results of misspecification will wash out in subsequent differential analysis. . Finally, it's worth noting that the same restriction holds in both alignment-based and mapping-based modes. This is because in neither mode do single-end fragments provide sufficient information to estimate the fragment length distribution from the data. We only know where one end of a fragment mapped and cannot infer where the other end would be. This is not an alignment versus mapping (versus selective-alignment) issue, but rather is fundamental to having only observed one side of the entire fragment generated during fragmentation and prior to sequencing. You cannot know the length of a fragment given only a read fr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243:816,Usability,learn,learned,816,"Hi @Tima-Ze,. Yes, salmon can be used to quantify these reads, but the results will depend (somewhat) on the `--fldMean` and `--fldSD` flags that are used. It's important to note that this is not a unique characteristic of salmon, and any transcript-level quantification tool using a probabilistic model (e.g. RSEM, eXpress, BitSeq, etc.) have the same requirement. That is, the fragment length distribution should be known so that _effective_ transcript lengths can be estimated, which have an effect on fragment assignment probabilities. If the wrong fragment length distribution is specified, then the _effective_ transcript lengths will be off and this can affect the assignment of some fragments. This is only a requirement with single-end reads, since with paired-end reads the fragment length distribution is learned from the data. Further, the inference procedure is somewhat robust to these choices (small changes in fld mean and sd don't generally lead to drastically different results). If you have access to the BioAnalyzer results for the sequencing run, those can give information about the fragment length distribution (even in a single end experiment). If not, you can proceed with the default values. Even if they don't exactly match the true distribution in the single-end sample, at least the same values will be applied in all samples and so, ideally, most results of misspecification will wash out in subsequent differential analysis. . Finally, it's worth noting that the same restriction holds in both alignment-based and mapping-based modes. This is because in neither mode do single-end fragments provide sufficient information to estimate the fragment length distribution from the data. We only know where one end of a fragment mapped and cannot infer where the other end would be. This is not an alignment versus mapping (versus selective-alignment) issue, but rather is fundamental to having only observed one side of the entire fragment generated during fragmentation and ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750920243
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750943952:1967,Availability,down,downstream,1967,"ng of these considerations. It’s good to think about how data processing choices may affect your results and you are being thoughtful here. I wouldn’t say that, generally, alignment-free tools are more accurate than alignment-based ones. For example, you might look at our recent paper on how [alignment and mapping methodology can influence abundance estimation even when holding the quantification approach fixed](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8), or [this paper on the corner cases of alignment-free methodology](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4869-5) (note the second paper pre-dates the first, and the new selective-alignment methodology in salmon should largely address the issues raised in that paper). However, the bigger and more meaningful distinction is between methods that attempt to properly quantify abundance (generally using a generative statistical model) — including methods like RSEM, BitSeq, salmon, etc., and those that try to simply count aligned reads — including methods like HTSeq and featureCounts. Generally, the former type of methods are more accurate than the latter at both the gene level and the former can also offer transcript-level estimates if desired (counting based methods generally cannot). Finally, to your question more directly, I don’t believe that model misspecification that may result due to not knowing the fragment length distribution will generally have enough of a deleterious effect on the probabilistic quantification methods to degrade their performance to the level of counting based methods. I would still argue to prefer probabilistic quantification (i.e. salmon) to read counting, even if you don’t know the fragment length distribution. As I mentioned above, it may change the maximum likelihood estimates a bit, but should do so across all samples, hopefully minimizing the downstream effects on differential analysis. Good luck with your analysis!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750943952
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750943952:1629,Performance,perform,performance,1629,"ng of these considerations. It’s good to think about how data processing choices may affect your results and you are being thoughtful here. I wouldn’t say that, generally, alignment-free tools are more accurate than alignment-based ones. For example, you might look at our recent paper on how [alignment and mapping methodology can influence abundance estimation even when holding the quantification approach fixed](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8), or [this paper on the corner cases of alignment-free methodology](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4869-5) (note the second paper pre-dates the first, and the new selective-alignment methodology in salmon should largely address the issues raised in that paper). However, the bigger and more meaningful distinction is between methods that attempt to properly quantify abundance (generally using a generative statistical model) — including methods like RSEM, BitSeq, salmon, etc., and those that try to simply count aligned reads — including methods like HTSeq and featureCounts. Generally, the former type of methods are more accurate than the latter at both the gene level and the former can also offer transcript-level estimates if desired (counting based methods generally cannot). Finally, to your question more directly, I don’t believe that model misspecification that may result due to not knowing the fragment length distribution will generally have enough of a deleterious effect on the probabilistic quantification methods to degrade their performance to the level of counting based methods. I would still argue to prefer probabilistic quantification (i.e. salmon) to read counting, even if you don’t know the fragment length distribution. As I mentioned above, it may change the maximum likelihood estimates a bit, but should do so across all samples, hopefully minimizing the downstream effects on differential analysis. Good luck with your analysis!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750943952
https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750943952:1081,Usability,simpl,simply,1081,"ng of these considerations. It’s good to think about how data processing choices may affect your results and you are being thoughtful here. I wouldn’t say that, generally, alignment-free tools are more accurate than alignment-based ones. For example, you might look at our recent paper on how [alignment and mapping methodology can influence abundance estimation even when holding the quantification approach fixed](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8), or [this paper on the corner cases of alignment-free methodology](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4869-5) (note the second paper pre-dates the first, and the new selective-alignment methodology in salmon should largely address the issues raised in that paper). However, the bigger and more meaningful distinction is between methods that attempt to properly quantify abundance (generally using a generative statistical model) — including methods like RSEM, BitSeq, salmon, etc., and those that try to simply count aligned reads — including methods like HTSeq and featureCounts. Generally, the former type of methods are more accurate than the latter at both the gene level and the former can also offer transcript-level estimates if desired (counting based methods generally cannot). Finally, to your question more directly, I don’t believe that model misspecification that may result due to not knowing the fragment length distribution will generally have enough of a deleterious effect on the probabilistic quantification methods to degrade their performance to the level of counting based methods. I would still argue to prefer probabilistic quantification (i.e. salmon) to read counting, even if you don’t know the fragment length distribution. As I mentioned above, it may change the maximum likelihood estimates a bit, but should do so across all samples, hopefully minimizing the downstream effects on differential analysis. Good luck with your analysis!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/127#issuecomment-750943952
https://github.com/COMBINE-lab/salmon/issues/131#issuecomment-293273710:86,Performance,perform,perform,86,"I see. TopHat actually aligns the reads to the genome (using Bowtie and a strategy to perform split-read mapping). The results of TopHat, then, are meant to be used with tools like Cufflinks, which expects reads to be mapped directly to the genome rather than to the transcriptome. Salmon, on the other hand, works like tools such as RSEM / eXpress, which expect alignments to the transcriptome directly. This can be accomplished by either mapping the reads directly to the transcript sequences (using e.g. Bowtie2 / BWA-MEM) or by mapping the reads to the genome using a tool such as STAR, and telling it to project the alignments onto genomic coordinates. However, I should mention that the easiest thing to do is to simply have Salmon build and index on your transcript set and then pass it the raw (compressed) FASTQ files directly. Since Salmon provides an accurate and lightweight alignment proxy, it can accurately assess transcript abundance estimates directly from the raw (unaligned) sequenced reads. If you have questions about using either of these modes, please take a look at [the documentation](https://salmon.readthedocs.io/en/latest/). I'd also be happy to answer any other questions you might have.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/131#issuecomment-293273710
https://github.com/COMBINE-lab/salmon/issues/131#issuecomment-293273710:719,Usability,simpl,simply,719,"I see. TopHat actually aligns the reads to the genome (using Bowtie and a strategy to perform split-read mapping). The results of TopHat, then, are meant to be used with tools like Cufflinks, which expects reads to be mapped directly to the genome rather than to the transcriptome. Salmon, on the other hand, works like tools such as RSEM / eXpress, which expect alignments to the transcriptome directly. This can be accomplished by either mapping the reads directly to the transcript sequences (using e.g. Bowtie2 / BWA-MEM) or by mapping the reads to the genome using a tool such as STAR, and telling it to project the alignments onto genomic coordinates. However, I should mention that the easiest thing to do is to simply have Salmon build and index on your transcript set and then pass it the raw (compressed) FASTQ files directly. Since Salmon provides an accurate and lightweight alignment proxy, it can accurately assess transcript abundance estimates directly from the raw (unaligned) sequenced reads. If you have questions about using either of these modes, please take a look at [the documentation](https://salmon.readthedocs.io/en/latest/). I'd also be happy to answer any other questions you might have.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/131#issuecomment-293273710
https://github.com/COMBINE-lab/salmon/issues/132#issuecomment-303813062:290,Availability,down,down,290,"Yeah, this is definitely not your issue. In fact, I just figured out that my explanation above was incomplete. **You don't need to investigate anything on your end**. I simply didn't flush the entire contents of a FASTA file to disk before calling `salmon index`. In the course of tracking down the issue I fixed some of my code indentation, bringing some of my code into a more global scope, where the `with` context handler I was using to hold the FASTA file open went out of scope, flushing my final writes to disk. Sigh...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/132#issuecomment-303813062
https://github.com/COMBINE-lab/salmon/issues/132#issuecomment-303813062:169,Usability,simpl,simply,169,"Yeah, this is definitely not your issue. In fact, I just figured out that my explanation above was incomplete. **You don't need to investigate anything on your end**. I simply didn't flush the entire contents of a FASTA file to disk before calling `salmon index`. In the course of tracking down the issue I fixed some of my code indentation, bringing some of my code into a more global scope, where the `with` context handler I was using to hold the FASTA file open went out of scope, flushing my final writes to disk. Sigh...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/132#issuecomment-303813062
https://github.com/COMBINE-lab/salmon/issues/137#issuecomment-406366598:306,Safety,detect,detection,306,"Yup, and the fact that this ended up as `MU` is strange, since the library type frequencies clearly suggest `IU` (since `ISF` and `ISR` counts seem to dominate). Could it be the result of having the FASTQ files generated by converting from BAM which some sort of bias in the beginning reads? The automatic detection uses the first 10,000 reads to decide --- if these are mapped in a biased way, that could be the cause.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/137#issuecomment-406366598
https://github.com/COMBINE-lab/salmon/issues/137#issuecomment-406366598:92,Usability,clear,clearly,92,"Yup, and the fact that this ended up as `MU` is strange, since the library type frequencies clearly suggest `IU` (since `ISF` and `ISR` counts seem to dominate). Could it be the result of having the FASTQ files generated by converting from BAM which some sort of bias in the beginning reads? The automatic detection uses the first 10,000 reads to decide --- if these are mapped in a biased way, that could be the cause.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/137#issuecomment-406366598
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314430634:100,Availability,error,error,100,ld is just the GNU linker.; I still think it's not able to find the zlib **library** file since the error at `-lz` where `-l` gives the namespace of the library.; If you are confident about the inclusion of the `Zlib` then can you try clearing the cmake cache (i.e. remove the file CMakeCache.txt) and build again?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314430634
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314430634:254,Performance,cache,cache,254,ld is just the GNU linker.; I still think it's not able to find the zlib **library** file since the error at `-lz` where `-l` gives the namespace of the library.; If you are confident about the inclusion of the `Zlib` then can you try clearing the cmake cache (i.e. remove the file CMakeCache.txt) and build again?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314430634
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314430634:235,Usability,clear,clearing,235,ld is just the GNU linker.; I still think it's not able to find the zlib **library** file since the error at `-lz` where `-l` gives the namespace of the library.; If you are confident about the inclusion of the `Zlib` then can you try clearing the cmake cache (i.e. remove the file CMakeCache.txt) and build again?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314430634
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:1205,Availability,down,download,1205,"the item.**; **WARNING: Target ""unitTests"" requests linking to directory ""/users/work/jake/bin/zlib-1.2.11/"". Targets may link only to libraries. CMake is dropping the item.**. So I actually went back a step and check my initial cmake command in the ../salmon-0.8.2/build/ directory. It also had the same issue and therefore wasn't building correctly. I started the install again from ../salmon-0.8.2/build/ using the following: . cmake -DBOOST_ROOT=/users/work/jake/bin/boost_1_64_0/ -DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h .. . It seemed to work nicely and I got all the build files to propagate into the ../salmon-0.8.2/build/ directory. From here I ran 'make' which did a whole bunch of things I hadn't seen it do yet, so assumably it was working as intended. This is until it got to the following stage:. Scanning dependencies of target libbwa; [ 48%] Creating directories for 'libbwa'; [ 49%] Performing download step for 'libbwa'; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 125 0 125 0 0 167 0 --:--:-- --:--:-- --:--:-- 167; 0 0 0 219k 0 0 123k 0 --:--:-- 0:00:01 --:--:-- 326k; bwa-master.tar.gz: OK; bwa-0.7.12.3/.gitignore; bwa-0.7.12.3/.travis.yml; bwa-0.7.12.3/COPYING; bwa-0.7.12.3/ChangeLog; bwa-0.7.12.3/Makefile; bwa-0.7.12.3/NEWS.md; bwa-0.7.12.3/QSufSort.c; bwa-0.7.12.3/QSufSort.h; bwa-0.7.12.3/README-alt.md; bwa-0.7.12.3/README.md; bwa-0.7.12.3/bamlite.c; bwa-0.7.12.3/bamlite.h; bwa-0.7.12.3/bntseq.c; bwa-0.7.12.3/bntseq.h; bwa-0.7.12.3/bwa.1; bwa-0.7.12.3/bwa.c; bwa-0.7.12.3/bwa.h; bwa-0.7.12.3/bwakit/; bwa-0.7.12.3/bwakit/README.md; bwa-0.7.12.3/bwakit/bwa-postalt.js; bwa-0.7.12.3/bwakit/run-HLA; bwa-0.7.12.3/bwakit/run-bwamem; bwa-0.7.12.3/bwakit/run-gen-ref; bwa-0.7.12.3/bwakit/typeHLA-selctg.js; bwa-0.7.12.3/bwakit/typeHLA.js; bwa-0.7.12.3/bwakit/typeHLA.sh; bwa-0.7.12.3/bwamem.c; bwa-0.7.12.3/bwamem.h; bwa-0.7.12.3/bwamem_extra.c; bwa-0.7.12.3/bwamem_pair.c; bwa-0.7.12.3/bwape.c; bwa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:3481,Availability,error,error,3481,"-0.7.12.3/bwamem.c; bwa-0.7.12.3/bwamem.h; bwa-0.7.12.3/bwamem_extra.c; bwa-0.7.12.3/bwamem_pair.c; bwa-0.7.12.3/bwape.c; bwa-0.7.12.3/bwase.c; bwa-0.7.12.3/bwase.h; bwa-0.7.12.3/bwaseqio.c; bwa-0.7.12.3/bwashm.c; bwa-0.7.12.3/bwt.c; bwa-0.7.12.3/bwt.h; bwa-0.7.12.3/bwt_gen.c; bwa-0.7.12.3/bwt_lite.c; bwa-0.7.12.3/bwt_lite.h; bwa-0.7.12.3/bwtaln.c; bwa-0.7.12.3/bwtaln.h; bwa-0.7.12.3/bwtgap.c; bwa-0.7.12.3/bwtgap.h; bwa-0.7.12.3/bwtindex.c; bwa-0.7.12.3/bwtsw2.h; bwa-0.7.12.3/bwtsw2_aux.c; bwa-0.7.12.3/bwtsw2_chain.c; bwa-0.7.12.3/bwtsw2_core.c; bwa-0.7.12.3/bwtsw2_main.c; bwa-0.7.12.3/bwtsw2_pair.c; bwa-0.7.12.3/example.c; bwa-0.7.12.3/fastmap.c; bwa-0.7.12.3/is.c; bwa-0.7.12.3/kbtree.h; bwa-0.7.12.3/khash.h; bwa-0.7.12.3/kopen.c; bwa-0.7.12.3/kseq.h; bwa-0.7.12.3/ksort.h; bwa-0.7.12.3/kstring.c; bwa-0.7.12.3/kstring.h; bwa-0.7.12.3/ksw.c; bwa-0.7.12.3/ksw.h; bwa-0.7.12.3/kthread.c; bwa-0.7.12.3/kvec.h; bwa-0.7.12.3/main.c; bwa-0.7.12.3/malloc_wrap.c; bwa-0.7.12.3/malloc_wrap.h; bwa-0.7.12.3/maxk.c; bwa-0.7.12.3/pemerge.c; bwa-0.7.12.3/qualfa2fq.pl; bwa-0.7.12.3/utils.c; bwa-0.7.12.3/utils.h; bwa-0.7.12.3/xa2multi.pl; [ 50%] No patch step for 'libbwa'; [ 50%] No update step for 'libbwa'; [ 51%] No configure step for 'libbwa'; [ 51%] Performing build step for 'libbwa'; /bin/ld: cannot find -lz; collect2: error: ld returned 1 exit status; make[3]: *** [bwa] Error 1; make[2]: *** [libbwa-prefix/src/libbwa-stamp/libbwa-build] Error 2; make[1]: *** [CMakeFiles/libbwa.dir/all] Error 2; make: *** [all] Error 2. So as you said I'd say its having issued finding the zlibs library. Similar to how I used 'DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h' to specify the zlib library for 'cmake', is there a way to do it for the 'make' command? I've tried using the following but haven't had success:; make -I /users/work/jake/bin/zlib-1.2.11/zlib.h; make --include-dir=/users/work/jake/bin/zlib-1.2.11/zlib.h. Sorry for the very basic questions.... I'm kind of learning as I go.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:646,Deployability,install,install,646,"Okay, so I've made some progress. After deleting the Cache.txt file I tried to build again at which point I noticed the following:; **WARNING: Target ""salmon"" requests linking to directory ""/users/work/jake/bin/zlib-1.2.11/"". Targets may link only to libraries. CMake is dropping the item.**; **WARNING: Target ""unitTests"" requests linking to directory ""/users/work/jake/bin/zlib-1.2.11/"". Targets may link only to libraries. CMake is dropping the item.**. So I actually went back a step and check my initial cmake command in the ../salmon-0.8.2/build/ directory. It also had the same issue and therefore wasn't building correctly. I started the install again from ../salmon-0.8.2/build/ using the following: . cmake -DBOOST_ROOT=/users/work/jake/bin/boost_1_64_0/ -DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h .. . It seemed to work nicely and I got all the build files to propagate into the ../salmon-0.8.2/build/ directory. From here I ran 'make' which did a whole bunch of things I hadn't seen it do yet, so assumably it was working as intended. This is until it got to the following stage:. Scanning dependencies of target libbwa; [ 48%] Creating directories for 'libbwa'; [ 49%] Performing download step for 'libbwa'; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 125 0 125 0 0 167 0 --:--:-- --:--:-- --:--:-- 167; 0 0 0 219k 0 0 123k 0 --:--:-- 0:00:01 --:--:-- 326k; bwa-master.tar.gz: OK; bwa-0.7.12.3/.gitignore; bwa-0.7.12.3/.travis.yml; bwa-0.7.12.3/COPYING; bwa-0.7.12.3/ChangeLog; bwa-0.7.12.3/Makefile; bwa-0.7.12.3/NEWS.md; bwa-0.7.12.3/QSufSort.c; bwa-0.7.12.3/QSufSort.h; bwa-0.7.12.3/README-alt.md; bwa-0.7.12.3/README.md; bwa-0.7.12.3/bamlite.c; bwa-0.7.12.3/bamlite.h; bwa-0.7.12.3/bntseq.c; bwa-0.7.12.3/bntseq.h; bwa-0.7.12.3/bwa.1; bwa-0.7.12.3/bwa.c; bwa-0.7.12.3/bwa.h; bwa-0.7.12.3/bwakit/; bwa-0.7.12.3/bwakit/README.md; bwa-0.7.12.3/bwakit/bwa-postalt.js; bwa-0.7.12.3/bwakit/run-HLA; bwa-0.7.12.3/bwak",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:3302,Deployability,patch,patch,3302,"-0.7.12.3/bwamem.c; bwa-0.7.12.3/bwamem.h; bwa-0.7.12.3/bwamem_extra.c; bwa-0.7.12.3/bwamem_pair.c; bwa-0.7.12.3/bwape.c; bwa-0.7.12.3/bwase.c; bwa-0.7.12.3/bwase.h; bwa-0.7.12.3/bwaseqio.c; bwa-0.7.12.3/bwashm.c; bwa-0.7.12.3/bwt.c; bwa-0.7.12.3/bwt.h; bwa-0.7.12.3/bwt_gen.c; bwa-0.7.12.3/bwt_lite.c; bwa-0.7.12.3/bwt_lite.h; bwa-0.7.12.3/bwtaln.c; bwa-0.7.12.3/bwtaln.h; bwa-0.7.12.3/bwtgap.c; bwa-0.7.12.3/bwtgap.h; bwa-0.7.12.3/bwtindex.c; bwa-0.7.12.3/bwtsw2.h; bwa-0.7.12.3/bwtsw2_aux.c; bwa-0.7.12.3/bwtsw2_chain.c; bwa-0.7.12.3/bwtsw2_core.c; bwa-0.7.12.3/bwtsw2_main.c; bwa-0.7.12.3/bwtsw2_pair.c; bwa-0.7.12.3/example.c; bwa-0.7.12.3/fastmap.c; bwa-0.7.12.3/is.c; bwa-0.7.12.3/kbtree.h; bwa-0.7.12.3/khash.h; bwa-0.7.12.3/kopen.c; bwa-0.7.12.3/kseq.h; bwa-0.7.12.3/ksort.h; bwa-0.7.12.3/kstring.c; bwa-0.7.12.3/kstring.h; bwa-0.7.12.3/ksw.c; bwa-0.7.12.3/ksw.h; bwa-0.7.12.3/kthread.c; bwa-0.7.12.3/kvec.h; bwa-0.7.12.3/main.c; bwa-0.7.12.3/malloc_wrap.c; bwa-0.7.12.3/malloc_wrap.h; bwa-0.7.12.3/maxk.c; bwa-0.7.12.3/pemerge.c; bwa-0.7.12.3/qualfa2fq.pl; bwa-0.7.12.3/utils.c; bwa-0.7.12.3/utils.h; bwa-0.7.12.3/xa2multi.pl; [ 50%] No patch step for 'libbwa'; [ 50%] No update step for 'libbwa'; [ 51%] No configure step for 'libbwa'; [ 51%] Performing build step for 'libbwa'; /bin/ld: cannot find -lz; collect2: error: ld returned 1 exit status; make[3]: *** [bwa] Error 1; make[2]: *** [libbwa-prefix/src/libbwa-stamp/libbwa-build] Error 2; make[1]: *** [CMakeFiles/libbwa.dir/all] Error 2; make: *** [all] Error 2. So as you said I'd say its having issued finding the zlibs library. Similar to how I used 'DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h' to specify the zlib library for 'cmake', is there a way to do it for the 'make' command? I've tried using the following but haven't had success:; make -I /users/work/jake/bin/zlib-1.2.11/zlib.h; make --include-dir=/users/work/jake/bin/zlib-1.2.11/zlib.h. Sorry for the very basic questions.... I'm kind of learning as I go.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:3337,Deployability,update,update,3337,"-0.7.12.3/bwamem.c; bwa-0.7.12.3/bwamem.h; bwa-0.7.12.3/bwamem_extra.c; bwa-0.7.12.3/bwamem_pair.c; bwa-0.7.12.3/bwape.c; bwa-0.7.12.3/bwase.c; bwa-0.7.12.3/bwase.h; bwa-0.7.12.3/bwaseqio.c; bwa-0.7.12.3/bwashm.c; bwa-0.7.12.3/bwt.c; bwa-0.7.12.3/bwt.h; bwa-0.7.12.3/bwt_gen.c; bwa-0.7.12.3/bwt_lite.c; bwa-0.7.12.3/bwt_lite.h; bwa-0.7.12.3/bwtaln.c; bwa-0.7.12.3/bwtaln.h; bwa-0.7.12.3/bwtgap.c; bwa-0.7.12.3/bwtgap.h; bwa-0.7.12.3/bwtindex.c; bwa-0.7.12.3/bwtsw2.h; bwa-0.7.12.3/bwtsw2_aux.c; bwa-0.7.12.3/bwtsw2_chain.c; bwa-0.7.12.3/bwtsw2_core.c; bwa-0.7.12.3/bwtsw2_main.c; bwa-0.7.12.3/bwtsw2_pair.c; bwa-0.7.12.3/example.c; bwa-0.7.12.3/fastmap.c; bwa-0.7.12.3/is.c; bwa-0.7.12.3/kbtree.h; bwa-0.7.12.3/khash.h; bwa-0.7.12.3/kopen.c; bwa-0.7.12.3/kseq.h; bwa-0.7.12.3/ksort.h; bwa-0.7.12.3/kstring.c; bwa-0.7.12.3/kstring.h; bwa-0.7.12.3/ksw.c; bwa-0.7.12.3/ksw.h; bwa-0.7.12.3/kthread.c; bwa-0.7.12.3/kvec.h; bwa-0.7.12.3/main.c; bwa-0.7.12.3/malloc_wrap.c; bwa-0.7.12.3/malloc_wrap.h; bwa-0.7.12.3/maxk.c; bwa-0.7.12.3/pemerge.c; bwa-0.7.12.3/qualfa2fq.pl; bwa-0.7.12.3/utils.c; bwa-0.7.12.3/utils.h; bwa-0.7.12.3/xa2multi.pl; [ 50%] No patch step for 'libbwa'; [ 50%] No update step for 'libbwa'; [ 51%] No configure step for 'libbwa'; [ 51%] Performing build step for 'libbwa'; /bin/ld: cannot find -lz; collect2: error: ld returned 1 exit status; make[3]: *** [bwa] Error 1; make[2]: *** [libbwa-prefix/src/libbwa-stamp/libbwa-build] Error 2; make[1]: *** [CMakeFiles/libbwa.dir/all] Error 2; make: *** [all] Error 2. So as you said I'd say its having issued finding the zlibs library. Similar to how I used 'DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h' to specify the zlib library for 'cmake', is there a way to do it for the 'make' command? I've tried using the following but haven't had success:; make -I /users/work/jake/bin/zlib-1.2.11/zlib.h; make --include-dir=/users/work/jake/bin/zlib-1.2.11/zlib.h. Sorry for the very basic questions.... I'm kind of learning as I go.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:1114,Integrability,depend,dependencies,1114,"the item.**; **WARNING: Target ""unitTests"" requests linking to directory ""/users/work/jake/bin/zlib-1.2.11/"". Targets may link only to libraries. CMake is dropping the item.**. So I actually went back a step and check my initial cmake command in the ../salmon-0.8.2/build/ directory. It also had the same issue and therefore wasn't building correctly. I started the install again from ../salmon-0.8.2/build/ using the following: . cmake -DBOOST_ROOT=/users/work/jake/bin/boost_1_64_0/ -DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h .. . It seemed to work nicely and I got all the build files to propagate into the ../salmon-0.8.2/build/ directory. From here I ran 'make' which did a whole bunch of things I hadn't seen it do yet, so assumably it was working as intended. This is until it got to the following stage:. Scanning dependencies of target libbwa; [ 48%] Creating directories for 'libbwa'; [ 49%] Performing download step for 'libbwa'; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 125 0 125 0 0 167 0 --:--:-- --:--:-- --:--:-- 167; 0 0 0 219k 0 0 123k 0 --:--:-- 0:00:01 --:--:-- 326k; bwa-master.tar.gz: OK; bwa-0.7.12.3/.gitignore; bwa-0.7.12.3/.travis.yml; bwa-0.7.12.3/COPYING; bwa-0.7.12.3/ChangeLog; bwa-0.7.12.3/Makefile; bwa-0.7.12.3/NEWS.md; bwa-0.7.12.3/QSufSort.c; bwa-0.7.12.3/QSufSort.h; bwa-0.7.12.3/README-alt.md; bwa-0.7.12.3/README.md; bwa-0.7.12.3/bamlite.c; bwa-0.7.12.3/bamlite.h; bwa-0.7.12.3/bntseq.c; bwa-0.7.12.3/bntseq.h; bwa-0.7.12.3/bwa.1; bwa-0.7.12.3/bwa.c; bwa-0.7.12.3/bwa.h; bwa-0.7.12.3/bwakit/; bwa-0.7.12.3/bwakit/README.md; bwa-0.7.12.3/bwakit/bwa-postalt.js; bwa-0.7.12.3/bwakit/run-HLA; bwa-0.7.12.3/bwakit/run-bwamem; bwa-0.7.12.3/bwakit/run-gen-ref; bwa-0.7.12.3/bwakit/typeHLA-selctg.js; bwa-0.7.12.3/bwakit/typeHLA.js; bwa-0.7.12.3/bwakit/typeHLA.sh; bwa-0.7.12.3/bwamem.c; bwa-0.7.12.3/bwamem.h; bwa-0.7.12.3/bwamem_extra.c; bwa-0.7.12.3/bwamem_pair.c; bwa-0.7.12.3/bwape.c; bwa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:3373,Modifiability,config,configure,3373,"-0.7.12.3/bwamem.c; bwa-0.7.12.3/bwamem.h; bwa-0.7.12.3/bwamem_extra.c; bwa-0.7.12.3/bwamem_pair.c; bwa-0.7.12.3/bwape.c; bwa-0.7.12.3/bwase.c; bwa-0.7.12.3/bwase.h; bwa-0.7.12.3/bwaseqio.c; bwa-0.7.12.3/bwashm.c; bwa-0.7.12.3/bwt.c; bwa-0.7.12.3/bwt.h; bwa-0.7.12.3/bwt_gen.c; bwa-0.7.12.3/bwt_lite.c; bwa-0.7.12.3/bwt_lite.h; bwa-0.7.12.3/bwtaln.c; bwa-0.7.12.3/bwtaln.h; bwa-0.7.12.3/bwtgap.c; bwa-0.7.12.3/bwtgap.h; bwa-0.7.12.3/bwtindex.c; bwa-0.7.12.3/bwtsw2.h; bwa-0.7.12.3/bwtsw2_aux.c; bwa-0.7.12.3/bwtsw2_chain.c; bwa-0.7.12.3/bwtsw2_core.c; bwa-0.7.12.3/bwtsw2_main.c; bwa-0.7.12.3/bwtsw2_pair.c; bwa-0.7.12.3/example.c; bwa-0.7.12.3/fastmap.c; bwa-0.7.12.3/is.c; bwa-0.7.12.3/kbtree.h; bwa-0.7.12.3/khash.h; bwa-0.7.12.3/kopen.c; bwa-0.7.12.3/kseq.h; bwa-0.7.12.3/ksort.h; bwa-0.7.12.3/kstring.c; bwa-0.7.12.3/kstring.h; bwa-0.7.12.3/ksw.c; bwa-0.7.12.3/ksw.h; bwa-0.7.12.3/kthread.c; bwa-0.7.12.3/kvec.h; bwa-0.7.12.3/main.c; bwa-0.7.12.3/malloc_wrap.c; bwa-0.7.12.3/malloc_wrap.h; bwa-0.7.12.3/maxk.c; bwa-0.7.12.3/pemerge.c; bwa-0.7.12.3/qualfa2fq.pl; bwa-0.7.12.3/utils.c; bwa-0.7.12.3/utils.h; bwa-0.7.12.3/xa2multi.pl; [ 50%] No patch step for 'libbwa'; [ 50%] No update step for 'libbwa'; [ 51%] No configure step for 'libbwa'; [ 51%] Performing build step for 'libbwa'; /bin/ld: cannot find -lz; collect2: error: ld returned 1 exit status; make[3]: *** [bwa] Error 1; make[2]: *** [libbwa-prefix/src/libbwa-stamp/libbwa-build] Error 2; make[1]: *** [CMakeFiles/libbwa.dir/all] Error 2; make: *** [all] Error 2. So as you said I'd say its having issued finding the zlibs library. Similar to how I used 'DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h' to specify the zlib library for 'cmake', is there a way to do it for the 'make' command? I've tried using the following but haven't had success:; make -I /users/work/jake/bin/zlib-1.2.11/zlib.h; make --include-dir=/users/work/jake/bin/zlib-1.2.11/zlib.h. Sorry for the very basic questions.... I'm kind of learning as I go.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873:4138,Usability,learn,learning,4138,"-0.7.12.3/bwamem.c; bwa-0.7.12.3/bwamem.h; bwa-0.7.12.3/bwamem_extra.c; bwa-0.7.12.3/bwamem_pair.c; bwa-0.7.12.3/bwape.c; bwa-0.7.12.3/bwase.c; bwa-0.7.12.3/bwase.h; bwa-0.7.12.3/bwaseqio.c; bwa-0.7.12.3/bwashm.c; bwa-0.7.12.3/bwt.c; bwa-0.7.12.3/bwt.h; bwa-0.7.12.3/bwt_gen.c; bwa-0.7.12.3/bwt_lite.c; bwa-0.7.12.3/bwt_lite.h; bwa-0.7.12.3/bwtaln.c; bwa-0.7.12.3/bwtaln.h; bwa-0.7.12.3/bwtgap.c; bwa-0.7.12.3/bwtgap.h; bwa-0.7.12.3/bwtindex.c; bwa-0.7.12.3/bwtsw2.h; bwa-0.7.12.3/bwtsw2_aux.c; bwa-0.7.12.3/bwtsw2_chain.c; bwa-0.7.12.3/bwtsw2_core.c; bwa-0.7.12.3/bwtsw2_main.c; bwa-0.7.12.3/bwtsw2_pair.c; bwa-0.7.12.3/example.c; bwa-0.7.12.3/fastmap.c; bwa-0.7.12.3/is.c; bwa-0.7.12.3/kbtree.h; bwa-0.7.12.3/khash.h; bwa-0.7.12.3/kopen.c; bwa-0.7.12.3/kseq.h; bwa-0.7.12.3/ksort.h; bwa-0.7.12.3/kstring.c; bwa-0.7.12.3/kstring.h; bwa-0.7.12.3/ksw.c; bwa-0.7.12.3/ksw.h; bwa-0.7.12.3/kthread.c; bwa-0.7.12.3/kvec.h; bwa-0.7.12.3/main.c; bwa-0.7.12.3/malloc_wrap.c; bwa-0.7.12.3/malloc_wrap.h; bwa-0.7.12.3/maxk.c; bwa-0.7.12.3/pemerge.c; bwa-0.7.12.3/qualfa2fq.pl; bwa-0.7.12.3/utils.c; bwa-0.7.12.3/utils.h; bwa-0.7.12.3/xa2multi.pl; [ 50%] No patch step for 'libbwa'; [ 50%] No update step for 'libbwa'; [ 51%] No configure step for 'libbwa'; [ 51%] Performing build step for 'libbwa'; /bin/ld: cannot find -lz; collect2: error: ld returned 1 exit status; make[3]: *** [bwa] Error 1; make[2]: *** [libbwa-prefix/src/libbwa-stamp/libbwa-build] Error 2; make[1]: *** [CMakeFiles/libbwa.dir/all] Error 2; make: *** [all] Error 2. So as you said I'd say its having issued finding the zlibs library. Similar to how I used 'DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/zlib.h' to specify the zlib library for 'cmake', is there a way to do it for the 'make' command? I've tried using the following but haven't had success:; make -I /users/work/jake/bin/zlib-1.2.11/zlib.h; make --include-dir=/users/work/jake/bin/zlib-1.2.11/zlib.h. Sorry for the very basic questions.... I'm kind of learning as I go.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314451873
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314454908:140,Deployability,install,installing,140,"Even I am not sure how to add the flags in the make command explicitly.; But, I'd suggest you can try couple of things:; Like I said before installing zlib to apt-get/brew would be the easiest.; If not can you try `-DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/` i.e. remove `zlib.h`.; As you can see I am learning on the go too 😜",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314454908
https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314454908:308,Usability,learn,learning,308,"Even I am not sure how to add the flags in the make command explicitly.; But, I'd suggest you can try couple of things:; Like I said before installing zlib to apt-get/brew would be the easiest.; If not can you try `-DZLIB_LIBRARY=/users/work/jake/bin/zlib-1.2.11/` i.e. remove `zlib.h`.; As you can see I am learning on the go too 😜",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/141#issuecomment-314454908
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630:204,Availability,error,error,204,"Hi @kvittingseerup,. Sorry for letting this sit for so long without responding. Currently, Salmon does not support mixed paired-end and single-end library types, so this is presumably what is causing the error (granted, the error message here could be considerably better). Practically, I'd be curious what the difference is between allowing this and simply running Salmon with the _non-quality-trimmed_ paired-end reads. Specifically, if Salmon is not able to map a pair concordantly, but it can map one of the ends of the read, then it will already do so. . However, in the case that there's a really compelling reason to want to quality trim the reads prior to quantification (and to include the reads such that the mate has been completely quality-trimmed away), we would be able to support this. It will require a bit of modification to allow different library types to be processed back to back and to contribute to the same quantification estimates. In this case, I imagine what we would want to do for the orphans is essentially what Salmon would do internally if it can't map the mate. That is, we would learn essentially all of the parameters and biases from the pairs that do map concordantly, and then just include the orphaned reads as indicating an entire fragment but of unknown length. Let me know if you have any thoughts about the above, and sorry again for the delay!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630:224,Availability,error,error,224,"Hi @kvittingseerup,. Sorry for letting this sit for so long without responding. Currently, Salmon does not support mixed paired-end and single-end library types, so this is presumably what is causing the error (granted, the error message here could be considerably better). Practically, I'd be curious what the difference is between allowing this and simply running Salmon with the _non-quality-trimmed_ paired-end reads. Specifically, if Salmon is not able to map a pair concordantly, but it can map one of the ends of the read, then it will already do so. . However, in the case that there's a really compelling reason to want to quality trim the reads prior to quantification (and to include the reads such that the mate has been completely quality-trimmed away), we would be able to support this. It will require a bit of modification to allow different library types to be processed back to back and to contribute to the same quantification estimates. In this case, I imagine what we would want to do for the orphans is essentially what Salmon would do internally if it can't map the mate. That is, we would learn essentially all of the parameters and biases from the pairs that do map concordantly, and then just include the orphaned reads as indicating an entire fragment but of unknown length. Let me know if you have any thoughts about the above, and sorry again for the delay!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630:230,Integrability,message,message,230,"Hi @kvittingseerup,. Sorry for letting this sit for so long without responding. Currently, Salmon does not support mixed paired-end and single-end library types, so this is presumably what is causing the error (granted, the error message here could be considerably better). Practically, I'd be curious what the difference is between allowing this and simply running Salmon with the _non-quality-trimmed_ paired-end reads. Specifically, if Salmon is not able to map a pair concordantly, but it can map one of the ends of the read, then it will already do so. . However, in the case that there's a really compelling reason to want to quality trim the reads prior to quantification (and to include the reads such that the mate has been completely quality-trimmed away), we would be able to support this. It will require a bit of modification to allow different library types to be processed back to back and to contribute to the same quantification estimates. In this case, I imagine what we would want to do for the orphans is essentially what Salmon would do internally if it can't map the mate. That is, we would learn essentially all of the parameters and biases from the pairs that do map concordantly, and then just include the orphaned reads as indicating an entire fragment but of unknown length. Let me know if you have any thoughts about the above, and sorry again for the delay!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630:351,Usability,simpl,simply,351,"Hi @kvittingseerup,. Sorry for letting this sit for so long without responding. Currently, Salmon does not support mixed paired-end and single-end library types, so this is presumably what is causing the error (granted, the error message here could be considerably better). Practically, I'd be curious what the difference is between allowing this and simply running Salmon with the _non-quality-trimmed_ paired-end reads. Specifically, if Salmon is not able to map a pair concordantly, but it can map one of the ends of the read, then it will already do so. . However, in the case that there's a really compelling reason to want to quality trim the reads prior to quantification (and to include the reads such that the mate has been completely quality-trimmed away), we would be able to support this. It will require a bit of modification to allow different library types to be processed back to back and to contribute to the same quantification estimates. In this case, I imagine what we would want to do for the orphans is essentially what Salmon would do internally if it can't map the mate. That is, we would learn essentially all of the parameters and biases from the pairs that do map concordantly, and then just include the orphaned reads as indicating an entire fragment but of unknown length. Let me know if you have any thoughts about the above, and sorry again for the delay!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630:1113,Usability,learn,learn,1113,"Hi @kvittingseerup,. Sorry for letting this sit for so long without responding. Currently, Salmon does not support mixed paired-end and single-end library types, so this is presumably what is causing the error (granted, the error message here could be considerably better). Practically, I'd be curious what the difference is between allowing this and simply running Salmon with the _non-quality-trimmed_ paired-end reads. Specifically, if Salmon is not able to map a pair concordantly, but it can map one of the ends of the read, then it will already do so. . However, in the case that there's a really compelling reason to want to quality trim the reads prior to quantification (and to include the reads such that the mate has been completely quality-trimmed away), we would be able to support this. It will require a bit of modification to allow different library types to be processed back to back and to contribute to the same quantification estimates. In this case, I imagine what we would want to do for the orphans is essentially what Salmon would do internally if it can't map the mate. That is, we would learn essentially all of the parameters and biases from the pairs that do map concordantly, and then just include the orphaned reads as indicating an entire fragment but of unknown length. Let me know if you have any thoughts about the above, and sorry again for the delay!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-353196630
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072:314,Availability,error,errors,314,That sounds like a very good way of doing it :-). I'm sorry I was not clear enough - my question was acutally meant for a single sequence - let me try again:; Lets say we have a read pair where one mate maps fine - but the other mate have a problem - half of it is an adapter (or low quality sequence with to many errors). How would Salmon currently handle this situation where the first half of a sequence (e.g. nt 1-50) could be quasi-mapped to a transcript but the second half (nt 51-100) did not match anywhere? Would the the second half cause the whole sequence to be discarded or would it be enough that the first half matched for it to be considered/counted?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072:268,Energy Efficiency,adapt,adapter,268,That sounds like a very good way of doing it :-). I'm sorry I was not clear enough - my question was acutally meant for a single sequence - let me try again:; Lets say we have a read pair where one mate maps fine - but the other mate have a problem - half of it is an adapter (or low quality sequence with to many errors). How would Salmon currently handle this situation where the first half of a sequence (e.g. nt 1-50) could be quasi-mapped to a transcript but the second half (nt 51-100) did not match anywhere? Would the the second half cause the whole sequence to be discarded or would it be enough that the first half matched for it to be considered/counted?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072:268,Integrability,adapter,adapter,268,That sounds like a very good way of doing it :-). I'm sorry I was not clear enough - my question was acutally meant for a single sequence - let me try again:; Lets say we have a read pair where one mate maps fine - but the other mate have a problem - half of it is an adapter (or low quality sequence with to many errors). How would Salmon currently handle this situation where the first half of a sequence (e.g. nt 1-50) could be quasi-mapped to a transcript but the second half (nt 51-100) did not match anywhere? Would the the second half cause the whole sequence to be discarded or would it be enough that the first half matched for it to be considered/counted?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072:268,Modifiability,adapt,adapter,268,That sounds like a very good way of doing it :-). I'm sorry I was not clear enough - my question was acutally meant for a single sequence - let me try again:; Lets say we have a read pair where one mate maps fine - but the other mate have a problem - half of it is an adapter (or low quality sequence with to many errors). How would Salmon currently handle this situation where the first half of a sequence (e.g. nt 1-50) could be quasi-mapped to a transcript but the second half (nt 51-100) did not match anywhere? Would the the second half cause the whole sequence to be discarded or would it be enough that the first half matched for it to be considered/counted?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072:70,Usability,clear,clear,70,That sounds like a very good way of doing it :-). I'm sorry I was not clear enough - my question was acutally meant for a single sequence - let me try again:; Lets say we have a read pair where one mate maps fine - but the other mate have a problem - half of it is an adapter (or low quality sequence with to many errors). How would Salmon currently handle this situation where the first half of a sequence (e.g. nt 1-50) could be quasi-mapped to a transcript but the second half (nt 51-100) did not match anywhere? Would the the second half cause the whole sequence to be discarded or would it be enough that the first half matched for it to be considered/counted?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-354955072
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997:705,Availability,reliab,reliably,705,"Hi @kvittingseerup,. No need to apologize, I think it was I who was not clear. What I am saying is that this is *already* the way that Salmon handles such a case. That is, if you have a paired-end read, and one of the reads maps but the other doesn't (due to e.g., adapter contamination or just very low quality), then Salmon will consider the remaining (mapping) end of the read as representative of an entire fragment, and will resolve the fragment origin accordingly during optimization. Generally, not having both ends of a paired-end read leads to increased ambiguity, but this isn't a particularly big problem if it only happens to a generally small fraction of the reads. Further, since you cannot reliably infer the implied fragment length on a transcript from only a single-end read, such mappings will not contribute to the bias model. Again, however, as long as this doesn't happen to the vast majority of fragments, it should have only a negligible effect on quantification and bias correction. Please let me know if this description makes sense. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997:265,Energy Efficiency,adapt,adapter,265,"Hi @kvittingseerup,. No need to apologize, I think it was I who was not clear. What I am saying is that this is *already* the way that Salmon handles such a case. That is, if you have a paired-end read, and one of the reads maps but the other doesn't (due to e.g., adapter contamination or just very low quality), then Salmon will consider the remaining (mapping) end of the read as representative of an entire fragment, and will resolve the fragment origin accordingly during optimization. Generally, not having both ends of a paired-end read leads to increased ambiguity, but this isn't a particularly big problem if it only happens to a generally small fraction of the reads. Further, since you cannot reliably infer the implied fragment length on a transcript from only a single-end read, such mappings will not contribute to the bias model. Again, however, as long as this doesn't happen to the vast majority of fragments, it should have only a negligible effect on quantification and bias correction. Please let me know if this description makes sense. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997:265,Integrability,adapter,adapter,265,"Hi @kvittingseerup,. No need to apologize, I think it was I who was not clear. What I am saying is that this is *already* the way that Salmon handles such a case. That is, if you have a paired-end read, and one of the reads maps but the other doesn't (due to e.g., adapter contamination or just very low quality), then Salmon will consider the remaining (mapping) end of the read as representative of an entire fragment, and will resolve the fragment origin accordingly during optimization. Generally, not having both ends of a paired-end read leads to increased ambiguity, but this isn't a particularly big problem if it only happens to a generally small fraction of the reads. Further, since you cannot reliably infer the implied fragment length on a transcript from only a single-end read, such mappings will not contribute to the bias model. Again, however, as long as this doesn't happen to the vast majority of fragments, it should have only a negligible effect on quantification and bias correction. Please let me know if this description makes sense. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997:265,Modifiability,adapt,adapter,265,"Hi @kvittingseerup,. No need to apologize, I think it was I who was not clear. What I am saying is that this is *already* the way that Salmon handles such a case. That is, if you have a paired-end read, and one of the reads maps but the other doesn't (due to e.g., adapter contamination or just very low quality), then Salmon will consider the remaining (mapping) end of the read as representative of an entire fragment, and will resolve the fragment origin accordingly during optimization. Generally, not having both ends of a paired-end read leads to increased ambiguity, but this isn't a particularly big problem if it only happens to a generally small fraction of the reads. Further, since you cannot reliably infer the implied fragment length on a transcript from only a single-end read, such mappings will not contribute to the bias model. Again, however, as long as this doesn't happen to the vast majority of fragments, it should have only a negligible effect on quantification and bias correction. Please let me know if this description makes sense. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997:477,Performance,optimiz,optimization,477,"Hi @kvittingseerup,. No need to apologize, I think it was I who was not clear. What I am saying is that this is *already* the way that Salmon handles such a case. That is, if you have a paired-end read, and one of the reads maps but the other doesn't (due to e.g., adapter contamination or just very low quality), then Salmon will consider the remaining (mapping) end of the read as representative of an entire fragment, and will resolve the fragment origin accordingly during optimization. Generally, not having both ends of a paired-end read leads to increased ambiguity, but this isn't a particularly big problem if it only happens to a generally small fraction of the reads. Further, since you cannot reliably infer the implied fragment length on a transcript from only a single-end read, such mappings will not contribute to the bias model. Again, however, as long as this doesn't happen to the vast majority of fragments, it should have only a negligible effect on quantification and bias correction. Please let me know if this description makes sense. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997
https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997:72,Usability,clear,clear,72,"Hi @kvittingseerup,. No need to apologize, I think it was I who was not clear. What I am saying is that this is *already* the way that Salmon handles such a case. That is, if you have a paired-end read, and one of the reads maps but the other doesn't (due to e.g., adapter contamination or just very low quality), then Salmon will consider the remaining (mapping) end of the read as representative of an entire fragment, and will resolve the fragment origin accordingly during optimization. Generally, not having both ends of a paired-end read leads to increased ambiguity, but this isn't a particularly big problem if it only happens to a generally small fraction of the reads. Further, since you cannot reliably infer the implied fragment length on a transcript from only a single-end read, such mappings will not contribute to the bias model. Again, however, as long as this doesn't happen to the vast majority of fragments, it should have only a negligible effect on quantification and bias correction. Please let me know if this description makes sense. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/150#issuecomment-355881997
https://github.com/COMBINE-lab/salmon/issues/152#issuecomment-346981211:6,Integrability,message,messages,6,"These messages have been removed in 0.9.0. Also, the read parser has had a considerable overhaul to avoid simply busy waiting in a situation like this where the processing is much slower than the disk. Let me know if this problem is resolved on your end.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/152#issuecomment-346981211
https://github.com/COMBINE-lab/salmon/issues/152#issuecomment-346981211:100,Safety,avoid,avoid,100,"These messages have been removed in 0.9.0. Also, the read parser has had a considerable overhaul to avoid simply busy waiting in a situation like this where the processing is much slower than the disk. Let me know if this problem is resolved on your end.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/152#issuecomment-346981211
https://github.com/COMBINE-lab/salmon/issues/152#issuecomment-346981211:106,Usability,simpl,simply,106,"These messages have been removed in 0.9.0. Also, the read parser has had a considerable overhaul to avoid simply busy waiting in a situation like this where the processing is much slower than the disk. Let me know if this problem is resolved on your end.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/152#issuecomment-346981211
https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141:607,Deployability,release,release,607,"Hi @teshomem,. If you want to proceed with transcript-level differential expression, _all transcripts are relevant_. That is, the relevant tools (e.g. DESeq2, limma-voom, Sleuth, etc.) will expect to be provided with _all_ quantified isoforms for each gene. They will then automatically apply their own filtering criteria to determine which transcripts to actually test for DE. . If you want to proceed with DE at the gene level (and hence want to aggregate the quantification information from the level of transcripts to genes), the easiest option is to use the [tximport](http://bioconductor.org/packages/release/bioc/html/tximport.html) package. It can import all of the quantifications from multiple runs of Salmon, aggregate them to the gene level, and produce a count matrix that can then be used with traditional count-based gene-level DE tools. I would recommend the pipeline Salmon => tximport => DESeq2 for gene-level DE analysis. Finally, the best place for questions like this, that don't have to do with a specific bug or feature request for the Salmon software, is the [Google user group](https://groups.google.com/forum/#!forum/sailfish-users). This way, other users will be more likely to provide you with feedback and help answer your questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141
https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141:875,Deployability,pipeline,pipeline,875,"Hi @teshomem,. If you want to proceed with transcript-level differential expression, _all transcripts are relevant_. That is, the relevant tools (e.g. DESeq2, limma-voom, Sleuth, etc.) will expect to be provided with _all_ quantified isoforms for each gene. They will then automatically apply their own filtering criteria to determine which transcripts to actually test for DE. . If you want to proceed with DE at the gene level (and hence want to aggregate the quantification information from the level of transcripts to genes), the easiest option is to use the [tximport](http://bioconductor.org/packages/release/bioc/html/tximport.html) package. It can import all of the quantifications from multiple runs of Salmon, aggregate them to the gene level, and produce a count matrix that can then be used with traditional count-based gene-level DE tools. I would recommend the pipeline Salmon => tximport => DESeq2 for gene-level DE analysis. Finally, the best place for questions like this, that don't have to do with a specific bug or feature request for the Salmon software, is the [Google user group](https://groups.google.com/forum/#!forum/sailfish-users). This way, other users will be more likely to provide you with feedback and help answer your questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141
https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141:365,Testability,test,test,365,"Hi @teshomem,. If you want to proceed with transcript-level differential expression, _all transcripts are relevant_. That is, the relevant tools (e.g. DESeq2, limma-voom, Sleuth, etc.) will expect to be provided with _all_ quantified isoforms for each gene. They will then automatically apply their own filtering criteria to determine which transcripts to actually test for DE. . If you want to proceed with DE at the gene level (and hence want to aggregate the quantification information from the level of transcripts to genes), the easiest option is to use the [tximport](http://bioconductor.org/packages/release/bioc/html/tximport.html) package. It can import all of the quantifications from multiple runs of Salmon, aggregate them to the gene level, and produce a count matrix that can then be used with traditional count-based gene-level DE tools. I would recommend the pipeline Salmon => tximport => DESeq2 for gene-level DE analysis. Finally, the best place for questions like this, that don't have to do with a specific bug or feature request for the Salmon software, is the [Google user group](https://groups.google.com/forum/#!forum/sailfish-users). This way, other users will be more likely to provide you with feedback and help answer your questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141
https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141:1222,Usability,feedback,feedback,1222,"Hi @teshomem,. If you want to proceed with transcript-level differential expression, _all transcripts are relevant_. That is, the relevant tools (e.g. DESeq2, limma-voom, Sleuth, etc.) will expect to be provided with _all_ quantified isoforms for each gene. They will then automatically apply their own filtering criteria to determine which transcripts to actually test for DE. . If you want to proceed with DE at the gene level (and hence want to aggregate the quantification information from the level of transcripts to genes), the easiest option is to use the [tximport](http://bioconductor.org/packages/release/bioc/html/tximport.html) package. It can import all of the quantifications from multiple runs of Salmon, aggregate them to the gene level, and produce a count matrix that can then be used with traditional count-based gene-level DE tools. I would recommend the pipeline Salmon => tximport => DESeq2 for gene-level DE analysis. Finally, the best place for questions like this, that don't have to do with a specific bug or feature request for the Salmon software, is the [Google user group](https://groups.google.com/forum/#!forum/sailfish-users). This way, other users will be more likely to provide you with feedback and help answer your questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/154#issuecomment-329974141
https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951:283,Safety,avoid,avoid,283,"Hi @tobi-te ,; Salmon is a two-phase algorithm i.e. online and offline. The first phase i.e. online-phase learns various parameters before starting the offline-phase (order doesn't matter). Like most online learning algorithm Salmon also expects the input to be randomized enough to avoid bias or in the case of Salmon *possibly* nudge the offline-phase towards a local-minima, which can vary according to the data (not always). I think in your case even though the learnt online parameters are biased (because of non-random order) the estimated abundances at the end are corrected by the offline-phase pretty well and you are observing the similar results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951
https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951:106,Usability,learn,learns,106,"Hi @tobi-te ,; Salmon is a two-phase algorithm i.e. online and offline. The first phase i.e. online-phase learns various parameters before starting the offline-phase (order doesn't matter). Like most online learning algorithm Salmon also expects the input to be randomized enough to avoid bias or in the case of Salmon *possibly* nudge the offline-phase towards a local-minima, which can vary according to the data (not always). I think in your case even though the learnt online parameters are biased (because of non-random order) the estimated abundances at the end are corrected by the offline-phase pretty well and you are observing the similar results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951
https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951:207,Usability,learn,learning,207,"Hi @tobi-te ,; Salmon is a two-phase algorithm i.e. online and offline. The first phase i.e. online-phase learns various parameters before starting the offline-phase (order doesn't matter). Like most online learning algorithm Salmon also expects the input to be randomized enough to avoid bias or in the case of Salmon *possibly* nudge the offline-phase towards a local-minima, which can vary according to the data (not always). I think in your case even though the learnt online parameters are biased (because of non-random order) the estimated abundances at the end are corrected by the offline-phase pretty well and you are observing the similar results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951
https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951:466,Usability,learn,learnt,466,"Hi @tobi-te ,; Salmon is a two-phase algorithm i.e. online and offline. The first phase i.e. online-phase learns various parameters before starting the offline-phase (order doesn't matter). Like most online learning algorithm Salmon also expects the input to be randomized enough to avoid bias or in the case of Salmon *possibly* nudge the offline-phase towards a local-minima, which can vary according to the data (not always). I think in your case even though the learnt online parameters are biased (because of non-random order) the estimated abundances at the end are corrected by the offline-phase pretty well and you are observing the similar results.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/155#issuecomment-331262951
https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498:560,Usability,simpl,simply,560,"Hi @atasub,. It's hard to say exactly if this mapping rate is much lower than expected or not. Many RNA-seq experiments do end up with a mapping rate of 65-70%. One thing that might contribute to a lower mapping rate would be short reads relative to the minimum required exact match length (default of 31). If your reads are relatively short (after trimming, which it looks like you are doing here) --- say ~50bp, then one might try lowering the k value with which the index is built. This will allow more sensitive mapping. However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate _to known features_. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, @vals has an [*excellent* series of blog posts](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination) on investigating and addressing low mapping rates (albeit in single-cell data) that you might find useful. Let me know what you find.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498
https://github.com/COMBINE-lab/salmon/issues/162#issuecomment-345761296:169,Deployability,install,installing,169,"@apeltzer ,. I tried to use the pre-compiled binary myself under alpine within Docker. Initially, it suffers from the same issue (lack of a compliant libc). However, by installing the glibc compatibility layer (which was simply a few commands as laid out [here](https://github.com/sgerrand/alpine-pkg-glibc)), I was able to get the pre-compiled binary to work without issue. Actually, this layer may also fix building from source, but I have not checked yet.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/162#issuecomment-345761296
https://github.com/COMBINE-lab/salmon/issues/162#issuecomment-345761296:221,Usability,simpl,simply,221,"@apeltzer ,. I tried to use the pre-compiled binary myself under alpine within Docker. Initially, it suffers from the same issue (lack of a compliant libc). However, by installing the glibc compatibility layer (which was simply a few commands as laid out [here](https://github.com/sgerrand/alpine-pkg-glibc)), I was able to get the pre-compiled binary to work without issue. Actually, this layer may also fix building from source, but I have not checked yet.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/162#issuecomment-345761296
https://github.com/COMBINE-lab/salmon/issues/164#issuecomment-338278307:363,Usability,simpl,simply,363,"The easiest way is probably to mount the external data / the relevant external directories as a [volume](https://docs.docker.com/engine/admin/volumes/volumes/#create-and-manage-volumes) when you run the docker command. The TLDR version of that page is [here](https://stackoverflow.com/questions/42625947/docker-input-output-outside-the-container). Basically, you simply map some location on your host system to some directory _inside_ the docker image, and then the executable in the container can read and write directly to that volume.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/164#issuecomment-338278307
https://github.com/COMBINE-lab/salmon/issues/169#issuecomment-341731522:172,Security,access,access,172,"Hi Kadir,. So, the short answer is that on a _single sample_ the `-g` flag and `tximport` do something very similar. However, the real benefit of `tximport` is that it has access to _all_ of the samples when doing transcript to gene-level aggregation. . So, while Salmon with the `-g` flag will estimate the average expressed gene length in each sample, `tximport` will also have knowledge of how the average gene length varies across all samples. Also, `tximport` provides a few different options for how, exactly, you wish to aggregate. Generally, the `-g` option is completely reasonable, but `tximport` is the same in the simple case and better in the general case.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/169#issuecomment-341731522
https://github.com/COMBINE-lab/salmon/issues/169#issuecomment-341731522:626,Usability,simpl,simple,626,"Hi Kadir,. So, the short answer is that on a _single sample_ the `-g` flag and `tximport` do something very similar. However, the real benefit of `tximport` is that it has access to _all_ of the samples when doing transcript to gene-level aggregation. . So, while Salmon with the `-g` flag will estimate the average expressed gene length in each sample, `tximport` will also have knowledge of how the average gene length varies across all samples. Also, `tximport` provides a few different options for how, exactly, you wish to aggregate. Generally, the `-g` option is completely reasonable, but `tximport` is the same in the simple case and better in the general case.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/169#issuecomment-341731522
https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360:680,Integrability,message,message,680,"Hi @wdecoster,. Thanks for reporting this. One restriction that needs to be better documented (actually, I have to make sure it is properly documented at all!) is that the library type should come _before_ the reads they describe. That is, you should consider passing `-l SF -r {}` rather than `-r {} -l SF`. The reason for this is that the `-r` and `-1,-2` parameters are repeatable so you could, conceivably, pass multiple reads of different library types. However, this is a feature that nobody uses and frankly doesn't make too much sense (so I'll consider removing it in the future to simplify library type parsing). For the time being, I'll also consider printing a warning message when a read file is encountered without an explicitly pre-defined library type (in that case, the behavior, as you saw, is to assume an unstranded library). Could you let me know if passing `-l` before `-r` resolves the issue for you. As to your other suggestion. The internal capitalization rules follow those for camel-case naming of variables (as opposed to separating words with`_`). However, I realize this is somewhat esoteric and even among those who are familiar with such conventions, an arbitrary preference. I'll look into aliasing this flag (and maybe others) to be usable with different names as well. I just have to check how to do this (and if it is possible) with boost's program options.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360
https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360:1024,Modifiability,variab,variables,1024,"Hi @wdecoster,. Thanks for reporting this. One restriction that needs to be better documented (actually, I have to make sure it is properly documented at all!) is that the library type should come _before_ the reads they describe. That is, you should consider passing `-l SF -r {}` rather than `-r {} -l SF`. The reason for this is that the `-r` and `-1,-2` parameters are repeatable so you could, conceivably, pass multiple reads of different library types. However, this is a feature that nobody uses and frankly doesn't make too much sense (so I'll consider removing it in the future to simplify library type parsing). For the time being, I'll also consider printing a warning message when a read file is encountered without an explicitly pre-defined library type (in that case, the behavior, as you saw, is to assume an unstranded library). Could you let me know if passing `-l` before `-r` resolves the issue for you. As to your other suggestion. The internal capitalization rules follow those for camel-case naming of variables (as opposed to separating words with`_`). However, I realize this is somewhat esoteric and even among those who are familiar with such conventions, an arbitrary preference. I'll look into aliasing this flag (and maybe others) to be usable with different names as well. I just have to check how to do this (and if it is possible) with boost's program options.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360
https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360:590,Usability,simpl,simplify,590,"Hi @wdecoster,. Thanks for reporting this. One restriction that needs to be better documented (actually, I have to make sure it is properly documented at all!) is that the library type should come _before_ the reads they describe. That is, you should consider passing `-l SF -r {}` rather than `-r {} -l SF`. The reason for this is that the `-r` and `-1,-2` parameters are repeatable so you could, conceivably, pass multiple reads of different library types. However, this is a feature that nobody uses and frankly doesn't make too much sense (so I'll consider removing it in the future to simplify library type parsing). For the time being, I'll also consider printing a warning message when a read file is encountered without an explicitly pre-defined library type (in that case, the behavior, as you saw, is to assume an unstranded library). Could you let me know if passing `-l` before `-r` resolves the issue for you. As to your other suggestion. The internal capitalization rules follow those for camel-case naming of variables (as opposed to separating words with`_`). However, I realize this is somewhat esoteric and even among those who are familiar with such conventions, an arbitrary preference. I'll look into aliasing this flag (and maybe others) to be usable with different names as well. I just have to check how to do this (and if it is possible) with boost's program options.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360
https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360:1266,Usability,usab,usable,1266,"Hi @wdecoster,. Thanks for reporting this. One restriction that needs to be better documented (actually, I have to make sure it is properly documented at all!) is that the library type should come _before_ the reads they describe. That is, you should consider passing `-l SF -r {}` rather than `-r {} -l SF`. The reason for this is that the `-r` and `-1,-2` parameters are repeatable so you could, conceivably, pass multiple reads of different library types. However, this is a feature that nobody uses and frankly doesn't make too much sense (so I'll consider removing it in the future to simplify library type parsing). For the time being, I'll also consider printing a warning message when a read file is encountered without an explicitly pre-defined library type (in that case, the behavior, as you saw, is to assume an unstranded library). Could you let me know if passing `-l` before `-r` resolves the issue for you. As to your other suggestion. The internal capitalization rules follow those for camel-case naming of variables (as opposed to separating words with`_`). However, I realize this is somewhat esoteric and even among those who are familiar with such conventions, an arbitrary preference. I'll look into aliasing this flag (and maybe others) to be usable with different names as well. I just have to check how to do this (and if it is possible) with boost's program options.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/177#issuecomment-347524360
https://github.com/COMBINE-lab/salmon/issues/179#issuecomment-542828122:17,Usability,simpl,simple,17,"I have couple of simple question on ""list of dir"" to supply part.; 1) DO i have to supply the complete path or just names of dir as a txt file?. I tried both and it does not work! But when i try one folder in the dir it works. salmon quantmerge --quants barcode01_quant -o all_barcodes_merged.txt; Version Info: This is the most recent version of salmon.; [2019-10-16 14:15:06.726] [mergeLog] [info] samples: [ barcode01_quant ]; [2019-10-16 14:15:06.726] [mergeLog] [info] sample names : [ barcode01_quant ]; [2019-10-16 14:15:06.726] [mergeLog] [info] output column : TPM; [2019-10-16 14:15:06.726] [mergeLog] [info] output file : all_barcodes_merged.txt; [2019-10-16 14:15:06.726] [mergeLog] [info] Parsing barcode01_quant/quant.sf. ###; When i try a list of all the folders. almon quantmerge --quants quant_dir_list.txt -o all_barcodes_merged.txt; Version Info: This is the most recent version of salmon.; [2019-10-16 14:15:54.698] [mergeLog] [info] samples: [ quant_dir_list.txt ]; [2019-10-16 14:15:54.698] [mergeLog] [info] sample names : [ quant_dir_list.txt ]; [2019-10-16 14:15:54.698] [mergeLog] [info] output column : TPM; [2019-10-16 14:15:54.698] [mergeLog] [info] output file : all_barcodes_merged.txt; [2019-10-16 14:15:54.698] [mergeLog] [critical] The sample directory quant_dir_list.txt either doesn't exist, or doesn't contain a quant.sf file. head quant_dir_list.txt; barcode01_quant; barcode02_quant; barcode03_quant; barcode04_quant; barcode05_quant; barcode06_quant; barcode07_quant; barcode08_quant. I have even tried with complete path to the dir and it fails. What am i doing wrong. Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/179#issuecomment-542828122
https://github.com/COMBINE-lab/salmon/issues/181#issuecomment-357167540:226,Availability,down,downloads,226,"Actually, I was only checking the package for building it on Debian, I don't use it personally. ; From that point of view of packaging the software for Debian it would be desirable to be able to build the software without any downloads during the build process. The reason for this is that building packages on the Debian build servers does not allow downloads for the very simple reason that this would bypass the QA checks for proper licensing of all files required to build some software (The same is true for Ubuntu and Linux distributions based on Debian). . Best, ; Gert",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/181#issuecomment-357167540
https://github.com/COMBINE-lab/salmon/issues/181#issuecomment-357167540:351,Availability,down,downloads,351,"Actually, I was only checking the package for building it on Debian, I don't use it personally. ; From that point of view of packaging the software for Debian it would be desirable to be able to build the software without any downloads during the build process. The reason for this is that building packages on the Debian build servers does not allow downloads for the very simple reason that this would bypass the QA checks for proper licensing of all files required to build some software (The same is true for Ubuntu and Linux distributions based on Debian). . Best, ; Gert",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/181#issuecomment-357167540
https://github.com/COMBINE-lab/salmon/issues/181#issuecomment-357167540:374,Usability,simpl,simple,374,"Actually, I was only checking the package for building it on Debian, I don't use it personally. ; From that point of view of packaging the software for Debian it would be desirable to be able to build the software without any downloads during the build process. The reason for this is that building packages on the Debian build servers does not allow downloads for the very simple reason that this would bypass the QA checks for proper licensing of all files required to build some software (The same is true for Ubuntu and Linux distributions based on Debian). . Best, ; Gert",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/181#issuecomment-357167540
https://github.com/COMBINE-lab/salmon/issues/183#issuecomment-359218185:1364,Usability,clear,clearly,1364,"Hi @nskbe,. I apologize for the slow reply. Things have been super-busy over the ""break"" trying to get things done before the semester starts here. Let me see if I can answer your question satisfactorily. In retrospect, I think that the raw `lib_format_counts.json` file can be quite confusing, and I should definitely provide a more thorough description of it in the documentation (or include a tool to generate a more digestible report from it). Basically, the report records the number of each type of _mapping_ observed. By _mapping_, I mean the particular correspondence between a read and a transcript. A given read can have many mappings, and so can contribute multiple times to the entries of the `lib_format_counts.json` file. The reason changing the library type doesn't change the results of this file is because this file records how fragments _mapped_ to the reference, not necessarily how they were _assigned_ during quantification. So, for example, a read map map in both the forward `SF` and reverse complement `SR` orientations. If you pass the library type `-l SR` to Salmon, then the reverse complement mapping will be strongly preferred for assignment of the read to the forward mapping. However, before the read was probabilistically assigned to a transcript, both types of mappings were observed. From the report you have provided, you quite clearly have a `SR` library type. One other thing that I should note. As I mention above, Salmon will assign a higher probability to the preferred library type, but not, by default, a probability of 1. If you want to force Salmon to only assign reads that arise from mappings in the prescribed orientation, you should pass the flag `--incompatPrior 0.0`. This will tell Salmon that it should assign a probability of 0 to any read that maps to a transcript in a way that disagrees with the provided library type. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/183#issuecomment-359218185
https://github.com/COMBINE-lab/salmon/issues/186#issuecomment-359218368:30,Usability,feedback,feedback,30,"Hi @DawnEve,. Thanks for your feedback, and I'm sorry this caused you such a headache. You're right that it would be a good idea to link to the most common transcriptomes for model organisms (e.g. human and mouse) directly in the documentation. I'll add this in the future. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/186#issuecomment-359218368
https://github.com/COMBINE-lab/salmon/issues/189#issuecomment-362155044:155,Performance,load,load,155,"Hi @Miserlou,. I'm not necessarily opposed to this. What exactly would the dry-run do? For example, would it simply check if the input files exist, try to load the index, etc.? This seems like it could be useful functionality, though, in my experience `--dry-run` commands usually aren't effectful (i.e. they usually don't create directories or output files). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/189#issuecomment-362155044
https://github.com/COMBINE-lab/salmon/issues/189#issuecomment-362155044:109,Usability,simpl,simply,109,"Hi @Miserlou,. I'm not necessarily opposed to this. What exactly would the dry-run do? For example, would it simply check if the input files exist, try to load the index, etc.? This seems like it could be useful functionality, though, in my experience `--dry-run` commands usually aren't effectful (i.e. they usually don't create directories or output files). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/189#issuecomment-362155044
https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963:261,Availability,avail,available,261,"Hi! I'm really sorry for taking so long to get back to you; things have been quite hectic this semester. The reason it's not being show is because it's been placed in a parameter group that is not made visible by default; the `--posBias` option itself is still available. It's definitely still experimental in that it has not been tested nearly as thoroughly as the other bias models. However, it is useable. Once we have performed more testing, it will migrate into the normal options and be better documented. If you gather any useful data while using this flag, we'd love some feedback!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963
https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963:422,Performance,perform,performed,422,"Hi! I'm really sorry for taking so long to get back to you; things have been quite hectic this semester. The reason it's not being show is because it's been placed in a parameter group that is not made visible by default; the `--posBias` option itself is still available. It's definitely still experimental in that it has not been tested nearly as thoroughly as the other bias models. However, it is useable. Once we have performed more testing, it will migrate into the normal options and be better documented. If you gather any useful data while using this flag, we'd love some feedback!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963
https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963:331,Testability,test,tested,331,"Hi! I'm really sorry for taking so long to get back to you; things have been quite hectic this semester. The reason it's not being show is because it's been placed in a parameter group that is not made visible by default; the `--posBias` option itself is still available. It's definitely still experimental in that it has not been tested nearly as thoroughly as the other bias models. However, it is useable. Once we have performed more testing, it will migrate into the normal options and be better documented. If you gather any useful data while using this flag, we'd love some feedback!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963
https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963:437,Testability,test,testing,437,"Hi! I'm really sorry for taking so long to get back to you; things have been quite hectic this semester. The reason it's not being show is because it's been placed in a parameter group that is not made visible by default; the `--posBias` option itself is still available. It's definitely still experimental in that it has not been tested nearly as thoroughly as the other bias models. However, it is useable. Once we have performed more testing, it will migrate into the normal options and be better documented. If you gather any useful data while using this flag, we'd love some feedback!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963
https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963:580,Usability,feedback,feedback,580,"Hi! I'm really sorry for taking so long to get back to you; things have been quite hectic this semester. The reason it's not being show is because it's been placed in a parameter group that is not made visible by default; the `--posBias` option itself is still available. It's definitely still experimental in that it has not been tested nearly as thoroughly as the other bias models. However, it is useable. Once we have performed more testing, it will migrate into the normal options and be better documented. If you gather any useful data while using this flag, we'd love some feedback!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/191#issuecomment-367448963
https://github.com/COMBINE-lab/salmon/issues/193#issuecomment-371818629:25,Usability,simpl,simple,25,This seems like a fairly simple job for an external tool as well. What did you have in mind? Just transcript --> genomic coordinates in a table or are you interested in visualizing the pseudobam output mapped to genomic coordinates?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/193#issuecomment-371818629
https://github.com/COMBINE-lab/salmon/issues/195#issuecomment-598522020:136,Safety,predict,predicted,136,"Hi @rob-p,. I am also seeking guidance on the use of Salmon for metatranscriptomes. Similar to this issue and issue #350 I am using the predicted genes from a metagenome assembly as the 'reference transcriptome' and would like to quantify gene expression from the corresponding metatranscriptome data. My metaT data is unstranded. What flags are most appropriate for this purpose? `--meta`? Something else? Are there any assumptions within Salmon that make it unsuitable for metagenomic/metatranscriptomic data (for example, the probability of observing a fragment when organisms are present at different abudances)?. Rachael",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/195#issuecomment-598522020
https://github.com/COMBINE-lab/salmon/issues/195#issuecomment-598522020:30,Usability,guid,guidance,30,"Hi @rob-p,. I am also seeking guidance on the use of Salmon for metatranscriptomes. Similar to this issue and issue #350 I am using the predicted genes from a metagenome assembly as the 'reference transcriptome' and would like to quantify gene expression from the corresponding metatranscriptome data. My metaT data is unstranded. What flags are most appropriate for this purpose? `--meta`? Something else? Are there any assumptions within Salmon that make it unsuitable for metagenomic/metatranscriptomic data (for example, the probability of observing a fragment when organisms are present at different abudances)?. Rachael",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/195#issuecomment-598522020
https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467214391:140,Usability,clear,clearly,140,"Thanks for the quick reply. Do you have a place you could share the contents of `outputs/hs.grch39.index`? It seems to me that the index is clearly not being fully constructed, but it would be ideal to compare this with an index that I know is correct and look for specific differences. Thanks,; Rob. P.s. any details about your specific system might also be useful to help debug.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467214391
https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836:136,Energy Efficiency,allocate,allocated,136,"A follow up on this (with a lot of help from @raungar; thanks!) led to the conclusion that the problem was that insufficient memory was allocated to the cluster job during indexing (indexing this transcriptome takes ~4.3G). Allocating more memory to the job resolves the issue. The strange thing is that the cluster manager seemed to kill the job rather than refuse to allocate the memory (which would have resulted in a `bad_alloc` exception that would have made the problem clear). So, if you're indexing with salmon on a cluster and see this behavior, be aware of the memory allocation and that the cluster software may surreptitiously kill the process rather than simply fail to allocate the memory!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836
https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836:369,Energy Efficiency,allocate,allocate,369,"A follow up on this (with a lot of help from @raungar; thanks!) led to the conclusion that the problem was that insufficient memory was allocated to the cluster job during indexing (indexing this transcriptome takes ~4.3G). Allocating more memory to the job resolves the issue. The strange thing is that the cluster manager seemed to kill the job rather than refuse to allocate the memory (which would have resulted in a `bad_alloc` exception that would have made the problem clear). So, if you're indexing with salmon on a cluster and see this behavior, be aware of the memory allocation and that the cluster software may surreptitiously kill the process rather than simply fail to allocate the memory!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836
https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836:683,Energy Efficiency,allocate,allocate,683,"A follow up on this (with a lot of help from @raungar; thanks!) led to the conclusion that the problem was that insufficient memory was allocated to the cluster job during indexing (indexing this transcriptome takes ~4.3G). Allocating more memory to the job resolves the issue. The strange thing is that the cluster manager seemed to kill the job rather than refuse to allocate the memory (which would have resulted in a `bad_alloc` exception that would have made the problem clear). So, if you're indexing with salmon on a cluster and see this behavior, be aware of the memory allocation and that the cluster software may surreptitiously kill the process rather than simply fail to allocate the memory!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836
https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836:476,Usability,clear,clear,476,"A follow up on this (with a lot of help from @raungar; thanks!) led to the conclusion that the problem was that insufficient memory was allocated to the cluster job during indexing (indexing this transcriptome takes ~4.3G). Allocating more memory to the job resolves the issue. The strange thing is that the cluster manager seemed to kill the job rather than refuse to allocate the memory (which would have resulted in a `bad_alloc` exception that would have made the problem clear). So, if you're indexing with salmon on a cluster and see this behavior, be aware of the memory allocation and that the cluster software may surreptitiously kill the process rather than simply fail to allocate the memory!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836
https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836:668,Usability,simpl,simply,668,"A follow up on this (with a lot of help from @raungar; thanks!) led to the conclusion that the problem was that insufficient memory was allocated to the cluster job during indexing (indexing this transcriptome takes ~4.3G). Allocating more memory to the job resolves the issue. The strange thing is that the cluster manager seemed to kill the job rather than refuse to allocate the memory (which would have resulted in a `bad_alloc` exception that would have made the problem clear). So, if you're indexing with salmon on a cluster and see this behavior, be aware of the memory allocation and that the cluster software may surreptitiously kill the process rather than simply fail to allocate the memory!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/197#issuecomment-467720836
https://github.com/COMBINE-lab/salmon/issues/198#issuecomment-366005924:42,Usability,simpl,simple,42,"Hi @amaer ,. Yes, exactly. The input is a simple TSV where the first column is transcript names and the second column is the gene names (note, that the keys in column 1 are unique, but that many transcripts can map to the same gene). This is the same type of input accepted by tximport to do the transcript => gene mapping. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/198#issuecomment-366005924
https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278:53,Availability,down,downstream,53,"Hi @Tima-Ze,. This should not cause any trouble with downstream analysis. The indexing procedure is simply informing you that these transcripts (about which you are being warned) are shorter than the seed length used for alignment. This means that it simply won't be possible for fragments to align to these transcripts, and so they will always have a 0 abundance in the resulting `quant.sf` files. This isn't a problem, as these transcripts are too short to be measured via RNA-seq anyway. The indexing messages just let you know this in advance. You can safely ignore these warnings for your downstream analysis.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278
https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278:594,Availability,down,downstream,594,"Hi @Tima-Ze,. This should not cause any trouble with downstream analysis. The indexing procedure is simply informing you that these transcripts (about which you are being warned) are shorter than the seed length used for alignment. This means that it simply won't be possible for fragments to align to these transcripts, and so they will always have a 0 abundance in the resulting `quant.sf` files. This isn't a problem, as these transcripts are too short to be measured via RNA-seq anyway. The indexing messages just let you know this in advance. You can safely ignore these warnings for your downstream analysis.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278
https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278:504,Integrability,message,messages,504,"Hi @Tima-Ze,. This should not cause any trouble with downstream analysis. The indexing procedure is simply informing you that these transcripts (about which you are being warned) are shorter than the seed length used for alignment. This means that it simply won't be possible for fragments to align to these transcripts, and so they will always have a 0 abundance in the resulting `quant.sf` files. This isn't a problem, as these transcripts are too short to be measured via RNA-seq anyway. The indexing messages just let you know this in advance. You can safely ignore these warnings for your downstream analysis.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278
https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278:556,Safety,safe,safely,556,"Hi @Tima-Ze,. This should not cause any trouble with downstream analysis. The indexing procedure is simply informing you that these transcripts (about which you are being warned) are shorter than the seed length used for alignment. This means that it simply won't be possible for fragments to align to these transcripts, and so they will always have a 0 abundance in the resulting `quant.sf` files. This isn't a problem, as these transcripts are too short to be measured via RNA-seq anyway. The indexing messages just let you know this in advance. You can safely ignore these warnings for your downstream analysis.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278
https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278:100,Usability,simpl,simply,100,"Hi @Tima-Ze,. This should not cause any trouble with downstream analysis. The indexing procedure is simply informing you that these transcripts (about which you are being warned) are shorter than the seed length used for alignment. This means that it simply won't be possible for fragments to align to these transcripts, and so they will always have a 0 abundance in the resulting `quant.sf` files. This isn't a problem, as these transcripts are too short to be measured via RNA-seq anyway. The indexing messages just let you know this in advance. You can safely ignore these warnings for your downstream analysis.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278
https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278:251,Usability,simpl,simply,251,"Hi @Tima-Ze,. This should not cause any trouble with downstream analysis. The indexing procedure is simply informing you that these transcripts (about which you are being warned) are shorter than the seed length used for alignment. This means that it simply won't be possible for fragments to align to these transcripts, and so they will always have a 0 abundance in the resulting `quant.sf` files. This isn't a problem, as these transcripts are too short to be measured via RNA-seq anyway. The indexing messages just let you know this in advance. You can safely ignore these warnings for your downstream analysis.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/214#issuecomment-751366278
https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394164828:388,Deployability,install,install,388,"Hi @francicco,. It looks like the problem is that the compiler being used is newer than the linker being used to link bzip2 here (see e.g. [this](https://stackoverflow.com/questions/46058050/unable-to-compile-unrecognized-relocation)). You should try and make sure the appropriate (newer) linker is present in your path before the older one. Another option, of course, would be to simply install salmon through Bioconda, which will take care of such issues for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394164828
https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394164828:381,Usability,simpl,simply,381,"Hi @francicco,. It looks like the problem is that the compiler being used is newer than the linker being used to link bzip2 here (see e.g. [this](https://stackoverflow.com/questions/46058050/unable-to-compile-unrecognized-relocation)). You should try and make sure the appropriate (newer) linker is present in your path before the older one. Another option, of course, would be to simply install salmon through Bioconda, which will take care of such issues for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394164828
https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394764686:196,Availability,error,error,196,"Hi @francicco ,. Thanks for sharing the data. I'm able to re-create the problem and am trying to debug it now. The reason you see this behavior in 0.10.1 but not in 0.8.1 is because in 0.10.1 the error model is enabled by default, while it is not in 0.8.1. If you run the newest salmon with `--noErrorModel` (the default in 0.8.1), your sample runs to completion. However, using the error model helps (which is why I made it the default), so I'm trying to debug what's happening there. I'll keep you posted. One more point worth mentioning. I noticed you are passing a coordinate sorted BAM file. For quantification with salmon, you really should not be passing a coordinate sorted BAM. This is because Salmon expects the alignments to appear in a random order, which is important for how the streaming stochastic variational algorithm learns the auxiliary parameters. Sorting by coordinates biases the models towards the isoforms that appear earliest in the BAM file.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394764686
https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394764686:383,Availability,error,error,383,"Hi @francicco ,. Thanks for sharing the data. I'm able to re-create the problem and am trying to debug it now. The reason you see this behavior in 0.10.1 but not in 0.8.1 is because in 0.10.1 the error model is enabled by default, while it is not in 0.8.1. If you run the newest salmon with `--noErrorModel` (the default in 0.8.1), your sample runs to completion. However, using the error model helps (which is why I made it the default), so I'm trying to debug what's happening there. I'll keep you posted. One more point worth mentioning. I noticed you are passing a coordinate sorted BAM file. For quantification with salmon, you really should not be passing a coordinate sorted BAM. This is because Salmon expects the alignments to appear in a random order, which is important for how the streaming stochastic variational algorithm learns the auxiliary parameters. Sorting by coordinates biases the models towards the isoforms that appear earliest in the BAM file.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394764686
https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394764686:836,Usability,learn,learns,836,"Hi @francicco ,. Thanks for sharing the data. I'm able to re-create the problem and am trying to debug it now. The reason you see this behavior in 0.10.1 but not in 0.8.1 is because in 0.10.1 the error model is enabled by default, while it is not in 0.8.1. If you run the newest salmon with `--noErrorModel` (the default in 0.8.1), your sample runs to completion. However, using the error model helps (which is why I made it the default), so I'm trying to debug what's happening there. I'll keep you posted. One more point worth mentioning. I noticed you are passing a coordinate sorted BAM file. For quantification with salmon, you really should not be passing a coordinate sorted BAM. This is because Salmon expects the alignments to appear in a random order, which is important for how the streaming stochastic variational algorithm learns the auxiliary parameters. Sorting by coordinates biases the models towards the isoforms that appear earliest in the BAM file.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/231#issuecomment-394764686
https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128:35,Deployability,install,installed,35,"Nope; nothing special. Once you've installed conda, you simply do:. ```; $ conda config --add channels conda-forge; $ conda config --add channels bioconda; $ conda create -n salmon salmon=0.10.1; ```. then it will give you instructions on how to activate the environment to run salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128
https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128:81,Modifiability,config,config,81,"Nope; nothing special. Once you've installed conda, you simply do:. ```; $ conda config --add channels conda-forge; $ conda config --add channels bioconda; $ conda create -n salmon salmon=0.10.1; ```. then it will give you instructions on how to activate the environment to run salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128
https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128:124,Modifiability,config,config,124,"Nope; nothing special. Once you've installed conda, you simply do:. ```; $ conda config --add channels conda-forge; $ conda config --add channels bioconda; $ conda create -n salmon salmon=0.10.1; ```. then it will give you instructions on how to activate the environment to run salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128
https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128:56,Usability,simpl,simply,56,"Nope; nothing special. Once you've installed conda, you simply do:. ```; $ conda config --add channels conda-forge; $ conda config --add channels bioconda; $ conda create -n salmon salmon=0.10.1; ```. then it will give you instructions on how to activate the environment to run salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/232#issuecomment-394755128
https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521:161,Availability,error,errors,161,"10x Genomics Chromium genomic DNA sequencing selects roughly one million barcodes at random with replacement from a possible pool of four million barcodes. Most errors likely result from DNA oligo synthesis rather than sequencing errors (intuition but unconfirmed). There's on other open source tool that does this task `ema preproc`: https://github.com/arshajii/ema#usage. The authors of `ema` have reported that correcting off-by-one errors is sufficient. Their tool corrects off-by-one errors by default, and can optionally correct off-by-two errors. Long Ranger Basic corrects off-by-two errors. The uncorrected barcode may be stored in the `RX:Z` tag. The corrected barcode is stored in the `BX:Z` tag. Thanks for considering this feature request!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521
https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521:230,Availability,error,errors,230,"10x Genomics Chromium genomic DNA sequencing selects roughly one million barcodes at random with replacement from a possible pool of four million barcodes. Most errors likely result from DNA oligo synthesis rather than sequencing errors (intuition but unconfirmed). There's on other open source tool that does this task `ema preproc`: https://github.com/arshajii/ema#usage. The authors of `ema` have reported that correcting off-by-one errors is sufficient. Their tool corrects off-by-one errors by default, and can optionally correct off-by-two errors. Long Ranger Basic corrects off-by-two errors. The uncorrected barcode may be stored in the `RX:Z` tag. The corrected barcode is stored in the `BX:Z` tag. Thanks for considering this feature request!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521
https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521:436,Availability,error,errors,436,"10x Genomics Chromium genomic DNA sequencing selects roughly one million barcodes at random with replacement from a possible pool of four million barcodes. Most errors likely result from DNA oligo synthesis rather than sequencing errors (intuition but unconfirmed). There's on other open source tool that does this task `ema preproc`: https://github.com/arshajii/ema#usage. The authors of `ema` have reported that correcting off-by-one errors is sufficient. Their tool corrects off-by-one errors by default, and can optionally correct off-by-two errors. Long Ranger Basic corrects off-by-two errors. The uncorrected barcode may be stored in the `RX:Z` tag. The corrected barcode is stored in the `BX:Z` tag. Thanks for considering this feature request!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521
https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521:489,Availability,error,errors,489,"10x Genomics Chromium genomic DNA sequencing selects roughly one million barcodes at random with replacement from a possible pool of four million barcodes. Most errors likely result from DNA oligo synthesis rather than sequencing errors (intuition but unconfirmed). There's on other open source tool that does this task `ema preproc`: https://github.com/arshajii/ema#usage. The authors of `ema` have reported that correcting off-by-one errors is sufficient. Their tool corrects off-by-one errors by default, and can optionally correct off-by-two errors. Long Ranger Basic corrects off-by-two errors. The uncorrected barcode may be stored in the `RX:Z` tag. The corrected barcode is stored in the `BX:Z` tag. Thanks for considering this feature request!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521
https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521:546,Availability,error,errors,546,"10x Genomics Chromium genomic DNA sequencing selects roughly one million barcodes at random with replacement from a possible pool of four million barcodes. Most errors likely result from DNA oligo synthesis rather than sequencing errors (intuition but unconfirmed). There's on other open source tool that does this task `ema preproc`: https://github.com/arshajii/ema#usage. The authors of `ema` have reported that correcting off-by-one errors is sufficient. Their tool corrects off-by-one errors by default, and can optionally correct off-by-two errors. Long Ranger Basic corrects off-by-two errors. The uncorrected barcode may be stored in the `RX:Z` tag. The corrected barcode is stored in the `BX:Z` tag. Thanks for considering this feature request!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521
https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521:592,Availability,error,errors,592,"10x Genomics Chromium genomic DNA sequencing selects roughly one million barcodes at random with replacement from a possible pool of four million barcodes. Most errors likely result from DNA oligo synthesis rather than sequencing errors (intuition but unconfirmed). There's on other open source tool that does this task `ema preproc`: https://github.com/arshajii/ema#usage. The authors of `ema` have reported that correcting off-by-one errors is sufficient. Their tool corrects off-by-one errors by default, and can optionally correct off-by-two errors. Long Ranger Basic corrects off-by-two errors. The uncorrected barcode may be stored in the `RX:Z` tag. The corrected barcode is stored in the `BX:Z` tag. Thanks for considering this feature request!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521
https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521:238,Usability,intuit,intuition,238,"10x Genomics Chromium genomic DNA sequencing selects roughly one million barcodes at random with replacement from a possible pool of four million barcodes. Most errors likely result from DNA oligo synthesis rather than sequencing errors (intuition but unconfirmed). There's on other open source tool that does this task `ema preproc`: https://github.com/arshajii/ema#usage. The authors of `ema` have reported that correcting off-by-one errors is sufficient. Their tool corrects off-by-one errors by default, and can optionally correct off-by-two errors. Long Ranger Basic corrects off-by-two errors. The uncorrected barcode may be stored in the `RX:Z` tag. The corrected barcode is stored in the `BX:Z` tag. Thanks for considering this feature request!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/233#issuecomment-395182521
https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-395843922:108,Usability,pause,pause,108,"Hi @GWW,. Thanks for the link. We'll look into this. In the meantime, I also noticed something that gave me pause. Specifically, in your output, you have:. ```; ### [ mates1 ] => { /dev/fd/63 }; ### [ mates2 ] => { /dev/fd/62 }; ```. Which makes me think that you are using pipe / process substitution to feed the reads to alevin. While this works fine with normal salmon, it's not currently possible with alevin. This is because alevin goes through the barcode file once by itself, and then goes through both the barcode and read files in unison to assign reads to cells using the initial barcode mapping. Thus, the pipe can't be reset to read from the beginning again, which is a problem. Are you, in fact, using process substitution here? Alevin can read directly from gzipped fastq files, so that's not necessary. Could you see if you encounter the issue if you remove the process substitution (if you're using it)?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-395843922
https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-395869990:294,Usability,simpl,simply,294,"@k3yavi,. The backtrace seems to suggest the issue is arising from [here](https://github.com/COMBINE-lab/salmon/blob/master/src/Alevin.cpp#L648), and functions called within. Any idea how something untoward could happen with the number of threads being spawned? Also, it looks like here we are simply spawning and joining threads manually --- so we can't blame that on tbb ;P.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-395869990
https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-396325784:57,Availability,down,down,57,"Great; thanks for reporting this and helping us track it down. Please let us know if you run into anything else, or if you have other general feedback / suggestions regarding alevin!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-396325784
https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-396325784:142,Usability,feedback,feedback,142,"Great; thanks for reporting this and helping us track it down. Please let us know if you run into anything else, or if you have other general feedback / suggestions regarding alevin!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/234#issuecomment-396325784
https://github.com/COMBINE-lab/salmon/issues/235#issuecomment-398454333:81,Usability,feedback,feedback,81,"Hi @EPunzi,. I'm glad you were able to get this to work. Thanks for the detailed feedback. This is, indeed, interesting to us. Though, it looks like it's a bug in the Boost build. Could you let me know what OS (& version), and compiler you're using? I can report this upstream to Boost. Thanks again!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/235#issuecomment-398454333
https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869:353,Performance,cache,cache,353,"Hi @mathog,. > Is it really the case that Salmon cannot use 1.57.0?. It may be able to. We set the minimum required version to the lowest boost version we use in any of our testing machines where we run regression tests. Currently, this is 1.59.0. If you change the relevant `CMakeLists.txt` line, you *really* need to make sure you clear out the CMake cache. You can do this by removing `CMakeCache.txt` in your build directory, as well as the directory `CMakeFiles`. However, it might be easiest just to remove and remake the entire `build` directory. You may also try passing `-DBoost_NO_SYSTEM_PATHS=Bool:ON` to your cmake command. Finally, note that the build system is probably looking for the static libraries --- you can elide that preference by modifying [this line](https://github.com/COMBINE-lab/salmon/blob/master/CMakeLists.txt#L222). Finally, since salmon uses C++11, it's important that whatever boost you link against exposes a C++11 compatible ABI. Unfortunately, `FindBoost.cmake` is the most finicky of the module finding packages I know about 😦. If you use `-DFETCH_BOOST=TRUE`, then CMake will fetch a recent boost and build the libraries it needs and link them statically. I realize you want to avoid this, so hopefully one of the ideas above will help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869
https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869:1217,Safety,avoid,avoid,1217,"Hi @mathog,. > Is it really the case that Salmon cannot use 1.57.0?. It may be able to. We set the minimum required version to the lowest boost version we use in any of our testing machines where we run regression tests. Currently, this is 1.59.0. If you change the relevant `CMakeLists.txt` line, you *really* need to make sure you clear out the CMake cache. You can do this by removing `CMakeCache.txt` in your build directory, as well as the directory `CMakeFiles`. However, it might be easiest just to remove and remake the entire `build` directory. You may also try passing `-DBoost_NO_SYSTEM_PATHS=Bool:ON` to your cmake command. Finally, note that the build system is probably looking for the static libraries --- you can elide that preference by modifying [this line](https://github.com/COMBINE-lab/salmon/blob/master/CMakeLists.txt#L222). Finally, since salmon uses C++11, it's important that whatever boost you link against exposes a C++11 compatible ABI. Unfortunately, `FindBoost.cmake` is the most finicky of the module finding packages I know about 😦. If you use `-DFETCH_BOOST=TRUE`, then CMake will fetch a recent boost and build the libraries it needs and link them statically. I realize you want to avoid this, so hopefully one of the ideas above will help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869
https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869:934,Security,expose,exposes,934,"Hi @mathog,. > Is it really the case that Salmon cannot use 1.57.0?. It may be able to. We set the minimum required version to the lowest boost version we use in any of our testing machines where we run regression tests. Currently, this is 1.59.0. If you change the relevant `CMakeLists.txt` line, you *really* need to make sure you clear out the CMake cache. You can do this by removing `CMakeCache.txt` in your build directory, as well as the directory `CMakeFiles`. However, it might be easiest just to remove and remake the entire `build` directory. You may also try passing `-DBoost_NO_SYSTEM_PATHS=Bool:ON` to your cmake command. Finally, note that the build system is probably looking for the static libraries --- you can elide that preference by modifying [this line](https://github.com/COMBINE-lab/salmon/blob/master/CMakeLists.txt#L222). Finally, since salmon uses C++11, it's important that whatever boost you link against exposes a C++11 compatible ABI. Unfortunately, `FindBoost.cmake` is the most finicky of the module finding packages I know about 😦. If you use `-DFETCH_BOOST=TRUE`, then CMake will fetch a recent boost and build the libraries it needs and link them statically. I realize you want to avoid this, so hopefully one of the ideas above will help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869
https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869:173,Testability,test,testing,173,"Hi @mathog,. > Is it really the case that Salmon cannot use 1.57.0?. It may be able to. We set the minimum required version to the lowest boost version we use in any of our testing machines where we run regression tests. Currently, this is 1.59.0. If you change the relevant `CMakeLists.txt` line, you *really* need to make sure you clear out the CMake cache. You can do this by removing `CMakeCache.txt` in your build directory, as well as the directory `CMakeFiles`. However, it might be easiest just to remove and remake the entire `build` directory. You may also try passing `-DBoost_NO_SYSTEM_PATHS=Bool:ON` to your cmake command. Finally, note that the build system is probably looking for the static libraries --- you can elide that preference by modifying [this line](https://github.com/COMBINE-lab/salmon/blob/master/CMakeLists.txt#L222). Finally, since salmon uses C++11, it's important that whatever boost you link against exposes a C++11 compatible ABI. Unfortunately, `FindBoost.cmake` is the most finicky of the module finding packages I know about 😦. If you use `-DFETCH_BOOST=TRUE`, then CMake will fetch a recent boost and build the libraries it needs and link them statically. I realize you want to avoid this, so hopefully one of the ideas above will help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869
https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869:214,Testability,test,tests,214,"Hi @mathog,. > Is it really the case that Salmon cannot use 1.57.0?. It may be able to. We set the minimum required version to the lowest boost version we use in any of our testing machines where we run regression tests. Currently, this is 1.59.0. If you change the relevant `CMakeLists.txt` line, you *really* need to make sure you clear out the CMake cache. You can do this by removing `CMakeCache.txt` in your build directory, as well as the directory `CMakeFiles`. However, it might be easiest just to remove and remake the entire `build` directory. You may also try passing `-DBoost_NO_SYSTEM_PATHS=Bool:ON` to your cmake command. Finally, note that the build system is probably looking for the static libraries --- you can elide that preference by modifying [this line](https://github.com/COMBINE-lab/salmon/blob/master/CMakeLists.txt#L222). Finally, since salmon uses C++11, it's important that whatever boost you link against exposes a C++11 compatible ABI. Unfortunately, `FindBoost.cmake` is the most finicky of the module finding packages I know about 😦. If you use `-DFETCH_BOOST=TRUE`, then CMake will fetch a recent boost and build the libraries it needs and link them statically. I realize you want to avoid this, so hopefully one of the ideas above will help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869
https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869:333,Usability,clear,clear,333,"Hi @mathog,. > Is it really the case that Salmon cannot use 1.57.0?. It may be able to. We set the minimum required version to the lowest boost version we use in any of our testing machines where we run regression tests. Currently, this is 1.59.0. If you change the relevant `CMakeLists.txt` line, you *really* need to make sure you clear out the CMake cache. You can do this by removing `CMakeCache.txt` in your build directory, as well as the directory `CMakeFiles`. However, it might be easiest just to remove and remake the entire `build` directory. You may also try passing `-DBoost_NO_SYSTEM_PATHS=Bool:ON` to your cmake command. Finally, note that the build system is probably looking for the static libraries --- you can elide that preference by modifying [this line](https://github.com/COMBINE-lab/salmon/blob/master/CMakeLists.txt#L222). Finally, since salmon uses C++11, it's important that whatever boost you link against exposes a C++11 compatible ABI. Unfortunately, `FindBoost.cmake` is the most finicky of the module finding packages I know about 😦. If you use `-DFETCH_BOOST=TRUE`, then CMake will fetch a recent boost and build the libraries it needs and link them statically. I realize you want to avoid this, so hopefully one of the ideas above will help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/236#issuecomment-396781869
https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678:136,Deployability,release,release,136,"Hi @Munfred , thanks for all the useful comments and glad to hear that you were able to run alevin successfully. ; I agree, in the next release we can work on adding the flag for ignoring the reads below a certain length.; Regarding the Transcript to gene Mapping file, all `bioawk` script does it to generate a `tsv` file with the first column as transcript name (as present in the reference) and the second column is the relevant gene id to the transcript. We have described the format [here](http://salmon.readthedocs.io/en/latest/alevin.html) but we can also update the tutorial to reflect the same more clearly. I agree with your last point too, we are working on writing a python parser, to parse the binary format and would update the tutorial soon with the relevant code. Thanks again for using our tool and let us know if you have any other feedback / suggestion regarding improving the alevin pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678
https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678:563,Deployability,update,update,563,"Hi @Munfred , thanks for all the useful comments and glad to hear that you were able to run alevin successfully. ; I agree, in the next release we can work on adding the flag for ignoring the reads below a certain length.; Regarding the Transcript to gene Mapping file, all `bioawk` script does it to generate a `tsv` file with the first column as transcript name (as present in the reference) and the second column is the relevant gene id to the transcript. We have described the format [here](http://salmon.readthedocs.io/en/latest/alevin.html) but we can also update the tutorial to reflect the same more clearly. I agree with your last point too, we are working on writing a python parser, to parse the binary format and would update the tutorial soon with the relevant code. Thanks again for using our tool and let us know if you have any other feedback / suggestion regarding improving the alevin pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678
https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678:731,Deployability,update,update,731,"Hi @Munfred , thanks for all the useful comments and glad to hear that you were able to run alevin successfully. ; I agree, in the next release we can work on adding the flag for ignoring the reads below a certain length.; Regarding the Transcript to gene Mapping file, all `bioawk` script does it to generate a `tsv` file with the first column as transcript name (as present in the reference) and the second column is the relevant gene id to the transcript. We have described the format [here](http://salmon.readthedocs.io/en/latest/alevin.html) but we can also update the tutorial to reflect the same more clearly. I agree with your last point too, we are working on writing a python parser, to parse the binary format and would update the tutorial soon with the relevant code. Thanks again for using our tool and let us know if you have any other feedback / suggestion regarding improving the alevin pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678
https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678:903,Deployability,pipeline,pipeline,903,"Hi @Munfred , thanks for all the useful comments and glad to hear that you were able to run alevin successfully. ; I agree, in the next release we can work on adding the flag for ignoring the reads below a certain length.; Regarding the Transcript to gene Mapping file, all `bioawk` script does it to generate a `tsv` file with the first column as transcript name (as present in the reference) and the second column is the relevant gene id to the transcript. We have described the format [here](http://salmon.readthedocs.io/en/latest/alevin.html) but we can also update the tutorial to reflect the same more clearly. I agree with your last point too, we are working on writing a python parser, to parse the binary format and would update the tutorial soon with the relevant code. Thanks again for using our tool and let us know if you have any other feedback / suggestion regarding improving the alevin pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678
https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678:608,Usability,clear,clearly,608,"Hi @Munfred , thanks for all the useful comments and glad to hear that you were able to run alevin successfully. ; I agree, in the next release we can work on adding the flag for ignoring the reads below a certain length.; Regarding the Transcript to gene Mapping file, all `bioawk` script does it to generate a `tsv` file with the first column as transcript name (as present in the reference) and the second column is the relevant gene id to the transcript. We have described the format [here](http://salmon.readthedocs.io/en/latest/alevin.html) but we can also update the tutorial to reflect the same more clearly. I agree with your last point too, we are working on writing a python parser, to parse the binary format and would update the tutorial soon with the relevant code. Thanks again for using our tool and let us know if you have any other feedback / suggestion regarding improving the alevin pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678
https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678:850,Usability,feedback,feedback,850,"Hi @Munfred , thanks for all the useful comments and glad to hear that you were able to run alevin successfully. ; I agree, in the next release we can work on adding the flag for ignoring the reads below a certain length.; Regarding the Transcript to gene Mapping file, all `bioawk` script does it to generate a `tsv` file with the first column as transcript name (as present in the reference) and the second column is the relevant gene id to the transcript. We have described the format [here](http://salmon.readthedocs.io/en/latest/alevin.html) but we can also update the tutorial to reflect the same more clearly. I agree with your last point too, we are working on writing a python parser, to parse the binary format and would update the tutorial soon with the relevant code. Thanks again for using our tool and let us know if you have any other feedback / suggestion regarding improving the alevin pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/237#issuecomment-400439678
https://github.com/COMBINE-lab/salmon/issues/240#issuecomment-399944991:185,Usability,simpl,simply,185,"Hi @mmokrejs ,. I have written [a script](https://github.com/COMBINE-lab/salmon/blob/master/scripts/runner.sh) for users with this need (i.e., running with interleaved FASTQ files). It simply splits the interleaved file into two streams, however, and so won't deal with unsynchronized streams where not all reads are properly paired. Because this solutions is sort of piecemeal, I've not added it to the official documentation. However, full support for interleaved FASTQ files is something I'd be interested in if we can peel off the time to write up proper support. P.S. Thanks for the suggestions on the documentation, I'll go ahead and make the suggested changes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/240#issuecomment-399944991
https://github.com/COMBINE-lab/salmon/issues/240#issuecomment-400057894:99,Usability,clear,clear,99,"Thank you, well I do have valid pairs and remaining singletons in different files, it just was not clear one cannot use a file with interleaved reads. Of course I dropped the file (as an intermediate from the disk as I hoped to keep only the combined, interleaved file with pairs). So I re-created it. BTW, I can recommend `reformat.sh` from https://sourceforge.net/projects/bbmap/ bundle, it is a nice tool to split/reorder/merge FASTQ files. I infer you suggest people to use only pairs and forget the singletons, right? Or, could combine the results of two executions somehow together ...? Maybe not worth the efforts? I have quite a few singleton reads in addition to valid pairs, though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/240#issuecomment-400057894
https://github.com/COMBINE-lab/salmon/issues/242#issuecomment-400056707:40,Usability,feedback,feedback,40,"Hi @mmokrejs,. Thanks again for all the feedback, we'll work on these. I'll mention that (2) is not quite simple as it seems. Specifically, if you write mappings (`--writeMappings, -z`), the implicit file is STDOUT. I believe this has been discussed with @tseemann in the past. This is intentional so that one can immediately pipe that output to e.g. samtools to convert the SAM format to a BAM format. I am open to cleaner solutions that (ideally) don't involve having to write the BAM files directly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/242#issuecomment-400056707
https://github.com/COMBINE-lab/salmon/issues/242#issuecomment-400056707:106,Usability,simpl,simple,106,"Hi @mmokrejs,. Thanks again for all the feedback, we'll work on these. I'll mention that (2) is not quite simple as it seems. Specifically, if you write mappings (`--writeMappings, -z`), the implicit file is STDOUT. I believe this has been discussed with @tseemann in the past. This is intentional so that one can immediately pipe that output to e.g. samtools to convert the SAM format to a BAM format. I am open to cleaner solutions that (ideally) don't involve having to write the BAM files directly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/242#issuecomment-400056707
https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402571397:105,Usability,clear,clear,105,"Thanks @Munfred for sharing the file. I'll take a look into this. re> whitelist; I should have been more clear about this. I didn't mean to say the full 737k CB, what I meant was using the actual whitelisted CB by cellranger, which are as you say was 300. Usually they are stored in a folder with name `filtered_gene_bc_matrices`, Although you might have to remove `-1` from their name since cellranger adds it to specify the sample index.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402571397
https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402627049:476,Usability,clear,clearly,476,"Choosing the top 500 barcodes from the `frequency.txt` list worked! Thanks for bringing my attention to that, I had not realized how do the cuttoff manually. You might want to expand a bit more about the `--dumpFeatures` flag in the documentation (https://salmon.readthedocs.io/en/latest/alevin.html) and possibly mention it in the tutorial (https://combine-lab.github.io/alevin-tutorial/2018/running-alevin/). I think using the `features.txt` to pick barcodes should be more clearly advertised, since it's not straightforward with cell ranger and many people will find it useful. . Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402627049
https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402759530:80,Deployability,update,update,80,"Glad to hear that it worked for you. Thanks for the suggestions, we will surely update the document soon and be more clear about the manual cutoff.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402759530
https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402759530:117,Usability,clear,clear,117,"Glad to hear that it worked for you. Thanks for the suggestions, we will surely update the document soon and be more clear about the manual cutoff.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/245#issuecomment-402759530
https://github.com/COMBINE-lab/salmon/issues/247#issuecomment-412352707:376,Usability,feedback,feedback,376,"@Hoohm , I believe the custom length options looks good from our end. Feel free to reopen the issue if you face any problem while using alevin in this mode. Re: More customizable options like a regex for extracting CB and UMI is still in development and has been raised in issue #233 and will keep that issue open until we have more generic extraction. . Thanks again for the feedback and useful comments.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/247#issuecomment-412352707
https://github.com/COMBINE-lab/salmon/issues/249#issuecomment-404220623:504,Usability,simpl,simply,504,"Hi @afkoeppel,. Thanks for reporting this. While I completely understand that this is not the desired behavior, it is the expected behavior. That is, when collapsing duplicates during indexing, the invariant that is maintained is that all sequence-identical transcripts will be ""collapsed"" and a single representative maintained. In practice (i.e. in implementation), the transcript that is maintained is the first one encountered. In the short-term, your suggested solution is the best. That is you can simply remove the non-canonical transcripts from the input fasta file. Alternatively, you could re-order the entries in the file so that the canonical transcript occurs first. In the longer term, I'd be happy to implement a de-duplication procedure that is more aware of the semantics of the transcript names. However, I'd want to figure out how to do this that is agnostic to where the transcripts are coming from (i.e. that doesn't work only for Gencode, human etc., and that does something sensible when the transcripts are e.g. _de novo_ assembled contigs). On that front, I'm completely open to suggestions. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/249#issuecomment-404220623
https://github.com/COMBINE-lab/salmon/issues/249#issuecomment-407831888:19,Usability,simpl,simple,19,"Use Gencode is the simple solution. Also Gencode combines coding and non-coding while Ensembl has these as two FASTA files. What I'm thinking is that, `tximeta` can solve this for Ensembl post-hoc by simply renaming the duplicate transcripts after looking up the name of the transcript from the standard chromosome. it won't disrupt the function of `tximeta` because we index the sequence of the txome which doesn't care about the names of the duplicate transcripts",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/249#issuecomment-407831888
https://github.com/COMBINE-lab/salmon/issues/249#issuecomment-407831888:200,Usability,simpl,simply,200,"Use Gencode is the simple solution. Also Gencode combines coding and non-coding while Ensembl has these as two FASTA files. What I'm thinking is that, `tximeta` can solve this for Ensembl post-hoc by simply renaming the duplicate transcripts after looking up the name of the transcript from the standard chromosome. it won't disrupt the function of `tximeta` because we index the sequence of the txome which doesn't care about the names of the duplicate transcripts",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/249#issuecomment-407831888
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:274,Availability,mainten,maintenance,274,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:353,Availability,avail,available,353,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:106,Deployability,install,install,106,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:493,Deployability,release,releases,493,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:151,Modifiability,variab,variables,151,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:701,Testability,log,log,701,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:858,Testability,log,logs,858,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:4,Usability,guid,guidohooiveld,4,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271:1114,Usability,learn,learns,1114,"Hi @guidohooiveld, . Regarding your questions:. (1) The motivation behind asking users to use Bioconda to install the binary is to limit the number of variables we may encounter when someone is reporting a bug --- i.e. if there are fewer distribution channels there is less maintenance overhead. Nonetheless, as you can see, I've had to make the binary available anyway, because it was the only way some people could easily get the program. Therefore, I think I'll start attaching binaries to releases again. (2) Yes, though this functionality is not part of Salmon itself. I *highly* recommend the [MultiQC](http://multiqc.info/) tool. MultiQC has a salmon module, which will parse all of the salmon log files in an experiment directory and produce a report. This report will contain the mapping percentages for all of the samples extracted from the salmon logs (and will color them nicely). It will also produce other QC information from the salmon runs. We are currently working on an improved multi-QC module, which will also provide summaries for things like GC / seq bias by analyzing the models that salmon learns, but this module isn't yet complete. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/252#issuecomment-405442271
https://github.com/COMBINE-lab/salmon/issues/253#issuecomment-412352411:73,Availability,avail,available,73,"Just a heads up, issue #266 has been added and the solution is currently available in the source build from the develop branch. We will include this to master with the next planned release of Salmon v0.11.3. Thanks again for the useful feedbacks and comments.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/253#issuecomment-412352411
https://github.com/COMBINE-lab/salmon/issues/253#issuecomment-412352411:181,Deployability,release,release,181,"Just a heads up, issue #266 has been added and the solution is currently available in the source build from the develop branch. We will include this to master with the next planned release of Salmon v0.11.3. Thanks again for the useful feedbacks and comments.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/253#issuecomment-412352411
https://github.com/COMBINE-lab/salmon/issues/253#issuecomment-412352411:236,Usability,feedback,feedbacks,236,"Just a heads up, issue #266 has been added and the solution is currently available in the source build from the develop branch. We will include this to master with the next planned release of Salmon v0.11.3. Thanks again for the useful feedbacks and comments.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/253#issuecomment-412352411
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390:112,Deployability,pipeline,pipeline,112,"Hi @pophipi ,; Thanks for reporting the issue.; It looks like a bug has creeped in while merging the `drop-seq` pipeline to alevin [here](https://github.com/COMBINE-lab/salmon/blob/ad4f74a4e3d7424bcd0ec0c1ec2af300dcbffc44/src/AlevinUtils.cpp#L47-L48).; If it's an urgent requirement you can swap the above two lines by: . ```; umi = read.substr(pt.barcodeLength, pt.umiLength);; return true;; ```. If it's too much trouble to compile from source, we will release a version w/ the hot-fix by today/tomorrow and would update you soon.; Thanks again !. P.S: I was curious how did the `chromium` pipeline went through, since the length requirement of 10x based pipeline is longer and it should break much earlier. The experiment you forwarded above seems to have 25 length bases for CB+UMI sequences. I wonder has any of the Drop-seq guideline changed? I was in the impression it was 12 base CB and 8 base UMI if not, then `--dropseq` flag would not be ideal thing to use since it will just use 20 bases out of 25 present in the fastq files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390:455,Deployability,release,release,455,"Hi @pophipi ,; Thanks for reporting the issue.; It looks like a bug has creeped in while merging the `drop-seq` pipeline to alevin [here](https://github.com/COMBINE-lab/salmon/blob/ad4f74a4e3d7424bcd0ec0c1ec2af300dcbffc44/src/AlevinUtils.cpp#L47-L48).; If it's an urgent requirement you can swap the above two lines by: . ```; umi = read.substr(pt.barcodeLength, pt.umiLength);; return true;; ```. If it's too much trouble to compile from source, we will release a version w/ the hot-fix by today/tomorrow and would update you soon.; Thanks again !. P.S: I was curious how did the `chromium` pipeline went through, since the length requirement of 10x based pipeline is longer and it should break much earlier. The experiment you forwarded above seems to have 25 length bases for CB+UMI sequences. I wonder has any of the Drop-seq guideline changed? I was in the impression it was 12 base CB and 8 base UMI if not, then `--dropseq` flag would not be ideal thing to use since it will just use 20 bases out of 25 present in the fastq files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390:516,Deployability,update,update,516,"Hi @pophipi ,; Thanks for reporting the issue.; It looks like a bug has creeped in while merging the `drop-seq` pipeline to alevin [here](https://github.com/COMBINE-lab/salmon/blob/ad4f74a4e3d7424bcd0ec0c1ec2af300dcbffc44/src/AlevinUtils.cpp#L47-L48).; If it's an urgent requirement you can swap the above two lines by: . ```; umi = read.substr(pt.barcodeLength, pt.umiLength);; return true;; ```. If it's too much trouble to compile from source, we will release a version w/ the hot-fix by today/tomorrow and would update you soon.; Thanks again !. P.S: I was curious how did the `chromium` pipeline went through, since the length requirement of 10x based pipeline is longer and it should break much earlier. The experiment you forwarded above seems to have 25 length bases for CB+UMI sequences. I wonder has any of the Drop-seq guideline changed? I was in the impression it was 12 base CB and 8 base UMI if not, then `--dropseq` flag would not be ideal thing to use since it will just use 20 bases out of 25 present in the fastq files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390:592,Deployability,pipeline,pipeline,592,"Hi @pophipi ,; Thanks for reporting the issue.; It looks like a bug has creeped in while merging the `drop-seq` pipeline to alevin [here](https://github.com/COMBINE-lab/salmon/blob/ad4f74a4e3d7424bcd0ec0c1ec2af300dcbffc44/src/AlevinUtils.cpp#L47-L48).; If it's an urgent requirement you can swap the above two lines by: . ```; umi = read.substr(pt.barcodeLength, pt.umiLength);; return true;; ```. If it's too much trouble to compile from source, we will release a version w/ the hot-fix by today/tomorrow and would update you soon.; Thanks again !. P.S: I was curious how did the `chromium` pipeline went through, since the length requirement of 10x based pipeline is longer and it should break much earlier. The experiment you forwarded above seems to have 25 length bases for CB+UMI sequences. I wonder has any of the Drop-seq guideline changed? I was in the impression it was 12 base CB and 8 base UMI if not, then `--dropseq` flag would not be ideal thing to use since it will just use 20 bases out of 25 present in the fastq files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390:657,Deployability,pipeline,pipeline,657,"Hi @pophipi ,; Thanks for reporting the issue.; It looks like a bug has creeped in while merging the `drop-seq` pipeline to alevin [here](https://github.com/COMBINE-lab/salmon/blob/ad4f74a4e3d7424bcd0ec0c1ec2af300dcbffc44/src/AlevinUtils.cpp#L47-L48).; If it's an urgent requirement you can swap the above two lines by: . ```; umi = read.substr(pt.barcodeLength, pt.umiLength);; return true;; ```. If it's too much trouble to compile from source, we will release a version w/ the hot-fix by today/tomorrow and would update you soon.; Thanks again !. P.S: I was curious how did the `chromium` pipeline went through, since the length requirement of 10x based pipeline is longer and it should break much earlier. The experiment you forwarded above seems to have 25 length bases for CB+UMI sequences. I wonder has any of the Drop-seq guideline changed? I was in the impression it was 12 base CB and 8 base UMI if not, then `--dropseq` flag would not be ideal thing to use since it will just use 20 bases out of 25 present in the fastq files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390:830,Usability,guid,guideline,830,"Hi @pophipi ,; Thanks for reporting the issue.; It looks like a bug has creeped in while merging the `drop-seq` pipeline to alevin [here](https://github.com/COMBINE-lab/salmon/blob/ad4f74a4e3d7424bcd0ec0c1ec2af300dcbffc44/src/AlevinUtils.cpp#L47-L48).; If it's an urgent requirement you can swap the above two lines by: . ```; umi = read.substr(pt.barcodeLength, pt.umiLength);; return true;; ```. If it's too much trouble to compile from source, we will release a version w/ the hot-fix by today/tomorrow and would update you soon.; Thanks again !. P.S: I was curious how did the `chromium` pipeline went through, since the length requirement of 10x based pipeline is longer and it should break much earlier. The experiment you forwarded above seems to have 25 length bases for CB+UMI sequences. I wonder has any of the Drop-seq guideline changed? I was in the impression it was 12 base CB and 8 base UMI if not, then `--dropseq` flag would not be ideal thing to use since it will just use 20 bases out of 25 present in the fastq files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408168390
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408191888:323,Testability,test,testing,323,"We might have to go through the paper and the dropseq guidelines to check what really changed.; You might wanna check https://github.com/COMBINE-lab/salmon/issues/247, we actually have a hidden option to do customized umi/CB length options, however this goes into a little more unexplored territory and requires a bit more testing. We'd appreciate your feedback if you happen to run this mode.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408191888
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408191888:54,Usability,guid,guidelines,54,"We might have to go through the paper and the dropseq guidelines to check what really changed.; You might wanna check https://github.com/COMBINE-lab/salmon/issues/247, we actually have a hidden option to do customized umi/CB length options, however this goes into a little more unexplored territory and requires a bit more testing. We'd appreciate your feedback if you happen to run this mode.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408191888
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408191888:353,Usability,feedback,feedback,353,"We might have to go through the paper and the dropseq guidelines to check what really changed.; You might wanna check https://github.com/COMBINE-lab/salmon/issues/247, we actually have a hidden option to do customized umi/CB length options, however this goes into a little more unexplored territory and requires a bit more testing. We'd appreciate your feedback if you happen to run this mode.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-408191888
https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-415102037:80,Usability,feedback,feedbacks,80,"Glad to hear that, let us know if you need any other help or have suggestions / feedbacks to improve Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/258#issuecomment-415102037
https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030:90,Availability,error,error,90,"Sorry if I wasn't clear. I did try with and without the --chromium; option, with the same error. I just have try the exact command you provided (including the --chromium; flag), with option in the same order. The command log is then:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }; ### [ chromium ] => { }; ### [ mates1 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R1_001.fastq.gz }; ### [ mates2 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R2_001.fastq.gz }; ### [ index ] => { /path/to/salmonIndex }; ### [ output ] => { alevin_output }; ### [ tgMap ] => { tx2gene.tsv }. Now it seems to work. I'll tell you if the whole alignment is; successfull when it will end. Note that when I use the --chromium flag earlier in the command, ie:. salmon --no-version-check --chromium alevin -p 10 -lISR -1 [...]. The log contains:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ ] => { alevin }; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030
https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030:439,Availability,down,downloads,439,"Sorry if I wasn't clear. I did try with and without the --chromium; option, with the same error. I just have try the exact command you provided (including the --chromium; flag), with option in the same order. The command log is then:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }; ### [ chromium ] => { }; ### [ mates1 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R1_001.fastq.gz }; ### [ mates2 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R2_001.fastq.gz }; ### [ index ] => { /path/to/salmonIndex }; ### [ output ] => { alevin_output }; ### [ tgMap ] => { tx2gene.tsv }. Now it seems to work. I'll tell you if the whole alignment is; successfull when it will end. Note that when I use the --chromium flag earlier in the command, ie:. salmon --no-version-check --chromium alevin -p 10 -lISR -1 [...]. The log contains:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ ] => { alevin }; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030
https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030:521,Availability,down,downloads,521,"Sorry if I wasn't clear. I did try with and without the --chromium; option, with the same error. I just have try the exact command you provided (including the --chromium; flag), with option in the same order. The command log is then:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }; ### [ chromium ] => { }; ### [ mates1 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R1_001.fastq.gz }; ### [ mates2 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R2_001.fastq.gz }; ### [ index ] => { /path/to/salmonIndex }; ### [ output ] => { alevin_output }; ### [ tgMap ] => { tx2gene.tsv }. Now it seems to work. I'll tell you if the whole alignment is; successfull when it will end. Note that when I use the --chromium flag earlier in the command, ie:. salmon --no-version-check --chromium alevin -p 10 -lISR -1 [...]. The log contains:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ ] => { alevin }; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030
https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030:221,Testability,log,log,221,"Sorry if I wasn't clear. I did try with and without the --chromium; option, with the same error. I just have try the exact command you provided (including the --chromium; flag), with option in the same order. The command log is then:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }; ### [ chromium ] => { }; ### [ mates1 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R1_001.fastq.gz }; ### [ mates2 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R2_001.fastq.gz }; ### [ index ] => { /path/to/salmonIndex }; ### [ output ] => { alevin_output }; ### [ tgMap ] => { tx2gene.tsv }. Now it seems to work. I'll tell you if the whole alignment is; successfull when it will end. Note that when I use the --chromium flag earlier in the command, ie:. salmon --no-version-check --chromium alevin -p 10 -lISR -1 [...]. The log contains:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ ] => { alevin }; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030
https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030:920,Testability,log,log,920,"Sorry if I wasn't clear. I did try with and without the --chromium; option, with the same error. I just have try the exact command you provided (including the --chromium; flag), with option in the same order. The command log is then:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }; ### [ chromium ] => { }; ### [ mates1 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R1_001.fastq.gz }; ### [ mates2 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R2_001.fastq.gz }; ### [ index ] => { /path/to/salmonIndex }; ### [ output ] => { alevin_output }; ### [ tgMap ] => { tx2gene.tsv }. Now it seems to work. I'll tell you if the whole alignment is; successfull when it will end. Note that when I use the --chromium flag earlier in the command, ie:. salmon --no-version-check --chromium alevin -p 10 -lISR -1 [...]. The log contains:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ ] => { alevin }; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030
https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030:18,Usability,clear,clear,18,"Sorry if I wasn't clear. I did try with and without the --chromium; option, with the same error. I just have try the exact command you provided (including the --chromium; flag), with option in the same order. The command log is then:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }; ### [ chromium ] => { }; ### [ mates1 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R1_001.fastq.gz }; ### [ mates2 ] => {; /path/to/downloads/10xPBMC/pbmc4k_S1_L001_R2_001.fastq.gz }; ### [ index ] => { /path/to/salmonIndex }; ### [ output ] => { alevin_output }; ### [ tgMap ] => { tx2gene.tsv }. Now it seems to work. I'll tell you if the whole alignment is; successfull when it will end. Note that when I use the --chromium flag earlier in the command, ie:. salmon --no-version-check --chromium alevin -p 10 -lISR -1 [...]. The log contains:; ### salmon (single-cell-based) v0.11.1; ### [ program ] => salmon; ### [ command ] => alevin; ### [ ] => { alevin }; ### [ threads ] => { 10 }; ### [ libType ] => { ISR }",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/264#issuecomment-410331030
https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344:1449,Availability,redundant,redundant,1449,"Sorry if I wasn't clear. Also, maybe I am trying to bluntly transpose a; metric that comes from alignment-based quantification. Yes, sequencing; saturation relies on UMI, using the transcript reads associated to the UMIs. I am not sure to understand the difference between resolving ambiguity; or collision at the transcript level, with the evaluation of sequencing; saturation in mind. To be more precise, I am not sure to see how it; could be a problem in this computation. But I am probably missing an; important point?. The idea of quasi-mapping as I understand is identifying the transcripts; from which the reads could have originated, generating a quantification.; For the sequencing saturation, we don't really need to know where the; read align on the transcript sequence, we just want to know that the; read comes from one single transcript, a unique UMI. So if I am right,; it is possible to summarize this quantification at the level of UMIs,; and have an idea of the duplication level of the transcripts that have; been tagged with UMIs. From what I understand, this is where alevin; perform the deduplication computation to have a correct idea of the; transcript amount when UMI are added, prior amplifications resulting; from the RT/PCR steps. So I was imagining it could be possible to take the gene quantifications; from (de)duplicated UMIs, gene quantifications from unique UMIs, using; them to have an idea of the amount/ratio of redundant information in the; sequencing data, producing a metric very similar to the seq sat from the; 10x definition.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344
https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344:1097,Performance,perform,perform,1097,"Sorry if I wasn't clear. Also, maybe I am trying to bluntly transpose a; metric that comes from alignment-based quantification. Yes, sequencing; saturation relies on UMI, using the transcript reads associated to the UMIs. I am not sure to understand the difference between resolving ambiguity; or collision at the transcript level, with the evaluation of sequencing; saturation in mind. To be more precise, I am not sure to see how it; could be a problem in this computation. But I am probably missing an; important point?. The idea of quasi-mapping as I understand is identifying the transcripts; from which the reads could have originated, generating a quantification.; For the sequencing saturation, we don't really need to know where the; read align on the transcript sequence, we just want to know that the; read comes from one single transcript, a unique UMI. So if I am right,; it is possible to summarize this quantification at the level of UMIs,; and have an idea of the duplication level of the transcripts that have; been tagged with UMIs. From what I understand, this is where alevin; perform the deduplication computation to have a correct idea of the; transcript amount when UMI are added, prior amplifications resulting; from the RT/PCR steps. So I was imagining it could be possible to take the gene quantifications; from (de)duplicated UMIs, gene quantifications from unique UMIs, using; them to have an idea of the amount/ratio of redundant information in the; sequencing data, producing a metric very similar to the seq sat from the; 10x definition.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344
https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344:1449,Safety,redund,redundant,1449,"Sorry if I wasn't clear. Also, maybe I am trying to bluntly transpose a; metric that comes from alignment-based quantification. Yes, sequencing; saturation relies on UMI, using the transcript reads associated to the UMIs. I am not sure to understand the difference between resolving ambiguity; or collision at the transcript level, with the evaluation of sequencing; saturation in mind. To be more precise, I am not sure to see how it; could be a problem in this computation. But I am probably missing an; important point?. The idea of quasi-mapping as I understand is identifying the transcripts; from which the reads could have originated, generating a quantification.; For the sequencing saturation, we don't really need to know where the; read align on the transcript sequence, we just want to know that the; read comes from one single transcript, a unique UMI. So if I am right,; it is possible to summarize this quantification at the level of UMIs,; and have an idea of the duplication level of the transcripts that have; been tagged with UMIs. From what I understand, this is where alevin; perform the deduplication computation to have a correct idea of the; transcript amount when UMI are added, prior amplifications resulting; from the RT/PCR steps. So I was imagining it could be possible to take the gene quantifications; from (de)duplicated UMIs, gene quantifications from unique UMIs, using; them to have an idea of the amount/ratio of redundant information in the; sequencing data, producing a metric very similar to the seq sat from the; 10x definition.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344
https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344:18,Usability,clear,clear,18,"Sorry if I wasn't clear. Also, maybe I am trying to bluntly transpose a; metric that comes from alignment-based quantification. Yes, sequencing; saturation relies on UMI, using the transcript reads associated to the UMIs. I am not sure to understand the difference between resolving ambiguity; or collision at the transcript level, with the evaluation of sequencing; saturation in mind. To be more precise, I am not sure to see how it; could be a problem in this computation. But I am probably missing an; important point?. The idea of quasi-mapping as I understand is identifying the transcripts; from which the reads could have originated, generating a quantification.; For the sequencing saturation, we don't really need to know where the; read align on the transcript sequence, we just want to know that the; read comes from one single transcript, a unique UMI. So if I am right,; it is possible to summarize this quantification at the level of UMIs,; and have an idea of the duplication level of the transcripts that have; been tagged with UMIs. From what I understand, this is where alevin; perform the deduplication computation to have a correct idea of the; transcript amount when UMI are added, prior amplifications resulting; from the RT/PCR steps. So I was imagining it could be possible to take the gene quantifications; from (de)duplicated UMIs, gene quantifications from unique UMIs, using; them to have an idea of the amount/ratio of redundant information in the; sequencing data, producing a metric very similar to the seq sat from the; 10x definition.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/267#issuecomment-414331344
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302:829,Availability,robust,robust,829,"Hi @PeteHaitch ,; Thanks for your interest in *Alevin*.; Although in current Alevin we have concentrated mainly on learning more about Droplet based 3'-tagged single cell protocols, especially 10x; we are very much interested in extending it towards other protocols like CEL-seq.; However, there are couple of challenges/difference which should be considered before incorporating it into the Alevin pipeline. Currently Alevin relies on the fact that the droplet based protocols use PCR amplification of the library and the UMI deduplication phase of Alevin assumes an exponential model, I am not sure how true is this with CEL-seq? Another issue is that CEL-seq is a Fluidigm based system while the current application for Alevin is for microfluidics based. In general we have observed that the 10x cell isolation step is pretty robust in reporting the Cellular Barcodes(CB) and although we have a probabilisitic model to handle the CB based uncertainty but the ambiguous case like that are very less frequent, (although not true for Drop-Seq). Having said that, we might have to do some analysis to actually figure out the right model for Barcode correction in Fluidigm based system. Also, please do let us know of your experience in using the solution proposed in #247 . Looking forward to hearing back from you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302:399,Deployability,pipeline,pipeline,399,"Hi @PeteHaitch ,; Thanks for your interest in *Alevin*.; Although in current Alevin we have concentrated mainly on learning more about Droplet based 3'-tagged single cell protocols, especially 10x; we are very much interested in extending it towards other protocols like CEL-seq.; However, there are couple of challenges/difference which should be considered before incorporating it into the Alevin pipeline. Currently Alevin relies on the fact that the droplet based protocols use PCR amplification of the library and the UMI deduplication phase of Alevin assumes an exponential model, I am not sure how true is this with CEL-seq? Another issue is that CEL-seq is a Fluidigm based system while the current application for Alevin is for microfluidics based. In general we have observed that the 10x cell isolation step is pretty robust in reporting the Cellular Barcodes(CB) and although we have a probabilisitic model to handle the CB based uncertainty but the ambiguous case like that are very less frequent, (although not true for Drop-Seq). Having said that, we might have to do some analysis to actually figure out the right model for Barcode correction in Fluidigm based system. Also, please do let us know of your experience in using the solution proposed in #247 . Looking forward to hearing back from you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302:171,Integrability,protocol,protocols,171,"Hi @PeteHaitch ,; Thanks for your interest in *Alevin*.; Although in current Alevin we have concentrated mainly on learning more about Droplet based 3'-tagged single cell protocols, especially 10x; we are very much interested in extending it towards other protocols like CEL-seq.; However, there are couple of challenges/difference which should be considered before incorporating it into the Alevin pipeline. Currently Alevin relies on the fact that the droplet based protocols use PCR amplification of the library and the UMI deduplication phase of Alevin assumes an exponential model, I am not sure how true is this with CEL-seq? Another issue is that CEL-seq is a Fluidigm based system while the current application for Alevin is for microfluidics based. In general we have observed that the 10x cell isolation step is pretty robust in reporting the Cellular Barcodes(CB) and although we have a probabilisitic model to handle the CB based uncertainty but the ambiguous case like that are very less frequent, (although not true for Drop-Seq). Having said that, we might have to do some analysis to actually figure out the right model for Barcode correction in Fluidigm based system. Also, please do let us know of your experience in using the solution proposed in #247 . Looking forward to hearing back from you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302:256,Integrability,protocol,protocols,256,"Hi @PeteHaitch ,; Thanks for your interest in *Alevin*.; Although in current Alevin we have concentrated mainly on learning more about Droplet based 3'-tagged single cell protocols, especially 10x; we are very much interested in extending it towards other protocols like CEL-seq.; However, there are couple of challenges/difference which should be considered before incorporating it into the Alevin pipeline. Currently Alevin relies on the fact that the droplet based protocols use PCR amplification of the library and the UMI deduplication phase of Alevin assumes an exponential model, I am not sure how true is this with CEL-seq? Another issue is that CEL-seq is a Fluidigm based system while the current application for Alevin is for microfluidics based. In general we have observed that the 10x cell isolation step is pretty robust in reporting the Cellular Barcodes(CB) and although we have a probabilisitic model to handle the CB based uncertainty but the ambiguous case like that are very less frequent, (although not true for Drop-Seq). Having said that, we might have to do some analysis to actually figure out the right model for Barcode correction in Fluidigm based system. Also, please do let us know of your experience in using the solution proposed in #247 . Looking forward to hearing back from you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302:468,Integrability,protocol,protocols,468,"Hi @PeteHaitch ,; Thanks for your interest in *Alevin*.; Although in current Alevin we have concentrated mainly on learning more about Droplet based 3'-tagged single cell protocols, especially 10x; we are very much interested in extending it towards other protocols like CEL-seq.; However, there are couple of challenges/difference which should be considered before incorporating it into the Alevin pipeline. Currently Alevin relies on the fact that the droplet based protocols use PCR amplification of the library and the UMI deduplication phase of Alevin assumes an exponential model, I am not sure how true is this with CEL-seq? Another issue is that CEL-seq is a Fluidigm based system while the current application for Alevin is for microfluidics based. In general we have observed that the 10x cell isolation step is pretty robust in reporting the Cellular Barcodes(CB) and although we have a probabilisitic model to handle the CB based uncertainty but the ambiguous case like that are very less frequent, (although not true for Drop-Seq). Having said that, we might have to do some analysis to actually figure out the right model for Barcode correction in Fluidigm based system. Also, please do let us know of your experience in using the solution proposed in #247 . Looking forward to hearing back from you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302:229,Modifiability,extend,extending,229,"Hi @PeteHaitch ,; Thanks for your interest in *Alevin*.; Although in current Alevin we have concentrated mainly on learning more about Droplet based 3'-tagged single cell protocols, especially 10x; we are very much interested in extending it towards other protocols like CEL-seq.; However, there are couple of challenges/difference which should be considered before incorporating it into the Alevin pipeline. Currently Alevin relies on the fact that the droplet based protocols use PCR amplification of the library and the UMI deduplication phase of Alevin assumes an exponential model, I am not sure how true is this with CEL-seq? Another issue is that CEL-seq is a Fluidigm based system while the current application for Alevin is for microfluidics based. In general we have observed that the 10x cell isolation step is pretty robust in reporting the Cellular Barcodes(CB) and although we have a probabilisitic model to handle the CB based uncertainty but the ambiguous case like that are very less frequent, (although not true for Drop-Seq). Having said that, we might have to do some analysis to actually figure out the right model for Barcode correction in Fluidigm based system. Also, please do let us know of your experience in using the solution proposed in #247 . Looking forward to hearing back from you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302:115,Usability,learn,learning,115,"Hi @PeteHaitch ,; Thanks for your interest in *Alevin*.; Although in current Alevin we have concentrated mainly on learning more about Droplet based 3'-tagged single cell protocols, especially 10x; we are very much interested in extending it towards other protocols like CEL-seq.; However, there are couple of challenges/difference which should be considered before incorporating it into the Alevin pipeline. Currently Alevin relies on the fact that the droplet based protocols use PCR amplification of the library and the UMI deduplication phase of Alevin assumes an exponential model, I am not sure how true is this with CEL-seq? Another issue is that CEL-seq is a Fluidigm based system while the current application for Alevin is for microfluidics based. In general we have observed that the 10x cell isolation step is pretty robust in reporting the Cellular Barcodes(CB) and although we have a probabilisitic model to handle the CB based uncertainty but the ambiguous case like that are very less frequent, (although not true for Drop-Seq). Having said that, we might have to do some analysis to actually figure out the right model for Barcode correction in Fluidigm based system. Also, please do let us know of your experience in using the solution proposed in #247 . Looking forward to hearing back from you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-414162302
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-418580112:151,Integrability,protocol,protocol,151,"Hi @PeteHaitch! I agree with @PeteHaitch here --- I think we should provide an easy way to specify custom cb & umi parameters paired with a particular protocol. For 10x v2, since it's a very standard commercial protocol, I think simply having a `--chromium` flag is probably OK. But we should make it easy for ppl to tweak their CB & UMI lengths.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-418580112
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-418580112:211,Integrability,protocol,protocol,211,"Hi @PeteHaitch! I agree with @PeteHaitch here --- I think we should provide an easy way to specify custom cb & umi parameters paired with a particular protocol. For 10x v2, since it's a very standard commercial protocol, I think simply having a `--chromium` flag is probably OK. But we should make it easy for ppl to tweak their CB & UMI lengths.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-418580112
https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-418580112:229,Usability,simpl,simply,229,"Hi @PeteHaitch! I agree with @PeteHaitch here --- I think we should provide an easy way to specify custom cb & umi parameters paired with a particular protocol. For 10x v2, since it's a very standard commercial protocol, I think simply having a `--chromium` flag is probably OK. But we should make it easy for ppl to tweak their CB & UMI lengths.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/269#issuecomment-418580112
https://github.com/COMBINE-lab/salmon/issues/271#issuecomment-707349051:118,Usability,simpl,simply,118,"That is interesting. The attempt in the double redirect was to include all alignment records from the second sam file simply concatenated to the first. Assuming the SAM files contain the same header, this should be OK (simply another way to treat them as a single input). However this warning suggests that there were references in the file passed to `-t` that did not have a corresponding entry in the SAM file. Yet, with the redirect, the first sam file should contain the full header. I don't have a clear understanding of why this would happen yet.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/271#issuecomment-707349051
https://github.com/COMBINE-lab/salmon/issues/271#issuecomment-707349051:219,Usability,simpl,simply,219,"That is interesting. The attempt in the double redirect was to include all alignment records from the second sam file simply concatenated to the first. Assuming the SAM files contain the same header, this should be OK (simply another way to treat them as a single input). However this warning suggests that there were references in the file passed to `-t` that did not have a corresponding entry in the SAM file. Yet, with the redirect, the first sam file should contain the full header. I don't have a clear understanding of why this would happen yet.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/271#issuecomment-707349051
https://github.com/COMBINE-lab/salmon/issues/271#issuecomment-707349051:503,Usability,clear,clear,503,"That is interesting. The attempt in the double redirect was to include all alignment records from the second sam file simply concatenated to the first. Assuming the SAM files contain the same header, this should be OK (simply another way to treat them as a single input). However this warning suggests that there were references in the file passed to `-t` that did not have a corresponding entry in the SAM file. Yet, with the redirect, the first sam file should contain the full header. I don't have a clear understanding of why this would happen yet.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/271#issuecomment-707349051
https://github.com/COMBINE-lab/salmon/pull/276#issuecomment-416348427:396,Usability,clear,clear,396,"Hi @junaruga --- so, I cherry picked your changes into develop (thanks again!) and did some more exploring. It looks like `jemalloc` is to blame for the unitTests hanging on the travis server. I modified `unitTests` to not link against `jemalloc` (but still linked salmon against it), and everything passes. I did see some potentially related issues on the `jemalloc` github issues, but it's not clear _exactly_ what is causing this behavior.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/276#issuecomment-416348427
https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713:98,Deployability,release,release,98,"HI @mfansler ,; Thanks for asking the very important question.; Alevin is primarily (till current release) designed to work with 3'-tagged end, droplet based sequencing where the primary assumption is that most of the reads would ideally be sequenced from the 3'-end of the molecule. Although, Salmon is a transcript quantification tool for *bulk* RNA-seq but we believe in singe-cell (3'-tagged) sequencing, generating quantification at transcript level is fundamentally hard problem to solve. Specifically, one of the reason is, a lot of transcripts share the terminal exon, and the features like length effect which are used in bulk RNA-seq to resolve ambiguity is not directly usable in single-cell for resolving the transcript ambiguity making the problem hard.; It's possible in the future that assays are designed to help incorporate more information e.g. sequencing from both 5' or 3' end sequencing or use SMART-seq2 which sequence the full molecule. In latter case people have been using Salmon as-is for generating the transcript level quantification. . _In summary_: We believe it's a trade-off based on your use case i.e. if you wan't to generate transcript level counts then most-likely single cell protocols which sequence from the full length of the molecules like smart-seq2 is better suited but if the motivation is to get higher number of cell coverage w/ decent gene-level molecule counts that's where 3'-end tagged end sequencing protocols shines most. _One Liner_; Alevin generates only gene-level counts for droplet based sequencing (til latest release v0.11.2).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713
https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713:1568,Deployability,release,release,1568,"HI @mfansler ,; Thanks for asking the very important question.; Alevin is primarily (till current release) designed to work with 3'-tagged end, droplet based sequencing where the primary assumption is that most of the reads would ideally be sequenced from the 3'-end of the molecule. Although, Salmon is a transcript quantification tool for *bulk* RNA-seq but we believe in singe-cell (3'-tagged) sequencing, generating quantification at transcript level is fundamentally hard problem to solve. Specifically, one of the reason is, a lot of transcripts share the terminal exon, and the features like length effect which are used in bulk RNA-seq to resolve ambiguity is not directly usable in single-cell for resolving the transcript ambiguity making the problem hard.; It's possible in the future that assays are designed to help incorporate more information e.g. sequencing from both 5' or 3' end sequencing or use SMART-seq2 which sequence the full molecule. In latter case people have been using Salmon as-is for generating the transcript level quantification. . _In summary_: We believe it's a trade-off based on your use case i.e. if you wan't to generate transcript level counts then most-likely single cell protocols which sequence from the full length of the molecules like smart-seq2 is better suited but if the motivation is to get higher number of cell coverage w/ decent gene-level molecule counts that's where 3'-end tagged end sequencing protocols shines most. _One Liner_; Alevin generates only gene-level counts for droplet based sequencing (til latest release v0.11.2).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713
https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713:1213,Integrability,protocol,protocols,1213,"HI @mfansler ,; Thanks for asking the very important question.; Alevin is primarily (till current release) designed to work with 3'-tagged end, droplet based sequencing where the primary assumption is that most of the reads would ideally be sequenced from the 3'-end of the molecule. Although, Salmon is a transcript quantification tool for *bulk* RNA-seq but we believe in singe-cell (3'-tagged) sequencing, generating quantification at transcript level is fundamentally hard problem to solve. Specifically, one of the reason is, a lot of transcripts share the terminal exon, and the features like length effect which are used in bulk RNA-seq to resolve ambiguity is not directly usable in single-cell for resolving the transcript ambiguity making the problem hard.; It's possible in the future that assays are designed to help incorporate more information e.g. sequencing from both 5' or 3' end sequencing or use SMART-seq2 which sequence the full molecule. In latter case people have been using Salmon as-is for generating the transcript level quantification. . _In summary_: We believe it's a trade-off based on your use case i.e. if you wan't to generate transcript level counts then most-likely single cell protocols which sequence from the full length of the molecules like smart-seq2 is better suited but if the motivation is to get higher number of cell coverage w/ decent gene-level molecule counts that's where 3'-end tagged end sequencing protocols shines most. _One Liner_; Alevin generates only gene-level counts for droplet based sequencing (til latest release v0.11.2).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713
https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713:1451,Integrability,protocol,protocols,1451,"HI @mfansler ,; Thanks for asking the very important question.; Alevin is primarily (till current release) designed to work with 3'-tagged end, droplet based sequencing where the primary assumption is that most of the reads would ideally be sequenced from the 3'-end of the molecule. Although, Salmon is a transcript quantification tool for *bulk* RNA-seq but we believe in singe-cell (3'-tagged) sequencing, generating quantification at transcript level is fundamentally hard problem to solve. Specifically, one of the reason is, a lot of transcripts share the terminal exon, and the features like length effect which are used in bulk RNA-seq to resolve ambiguity is not directly usable in single-cell for resolving the transcript ambiguity making the problem hard.; It's possible in the future that assays are designed to help incorporate more information e.g. sequencing from both 5' or 3' end sequencing or use SMART-seq2 which sequence the full molecule. In latter case people have been using Salmon as-is for generating the transcript level quantification. . _In summary_: We believe it's a trade-off based on your use case i.e. if you wan't to generate transcript level counts then most-likely single cell protocols which sequence from the full length of the molecules like smart-seq2 is better suited but if the motivation is to get higher number of cell coverage w/ decent gene-level molecule counts that's where 3'-end tagged end sequencing protocols shines most. _One Liner_; Alevin generates only gene-level counts for droplet based sequencing (til latest release v0.11.2).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713
https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713:681,Usability,usab,usable,681,"HI @mfansler ,; Thanks for asking the very important question.; Alevin is primarily (till current release) designed to work with 3'-tagged end, droplet based sequencing where the primary assumption is that most of the reads would ideally be sequenced from the 3'-end of the molecule. Although, Salmon is a transcript quantification tool for *bulk* RNA-seq but we believe in singe-cell (3'-tagged) sequencing, generating quantification at transcript level is fundamentally hard problem to solve. Specifically, one of the reason is, a lot of transcripts share the terminal exon, and the features like length effect which are used in bulk RNA-seq to resolve ambiguity is not directly usable in single-cell for resolving the transcript ambiguity making the problem hard.; It's possible in the future that assays are designed to help incorporate more information e.g. sequencing from both 5' or 3' end sequencing or use SMART-seq2 which sequence the full molecule. In latter case people have been using Salmon as-is for generating the transcript level quantification. . _In summary_: We believe it's a trade-off based on your use case i.e. if you wan't to generate transcript level counts then most-likely single cell protocols which sequence from the full length of the molecules like smart-seq2 is better suited but if the motivation is to get higher number of cell coverage w/ decent gene-level molecule counts that's where 3'-end tagged end sequencing protocols shines most. _One Liner_; Alevin generates only gene-level counts for droplet based sequencing (til latest release v0.11.2).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/287#issuecomment-420627713
https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510:90,Deployability,release,release,90,"While I definitely trust the jemalloc devs, I do know that such things are possible, as a release is simply associated with a tagged commit, which _can_ be changed via a forced update to the tags. I know because, in my early days using git + GitHub, I did such a foolish thing. So, while I'm sure that the jemalloc devs wouldn't change the file associated with a tag, and while there are safeguards (e.g. check that the file we get matches the SHA of what we expect), simply pulling from a fork is a convenient way to handle this ""generally"" (for packages not as production-quality as jemalloc, or where the developers might not have tagged a release corresponding to what we need). I completely understand that you don't want to link against a standard jemalloc if we compile some strange version with custom modifications. However, here, we simply want to use the vanilla jemalloc. In fact, when salmon is built under bioconda, this is exactly what we do (we link against the conda jemalloc >= 5.1.0).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510
https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510:177,Deployability,update,update,177,"While I definitely trust the jemalloc devs, I do know that such things are possible, as a release is simply associated with a tagged commit, which _can_ be changed via a forced update to the tags. I know because, in my early days using git + GitHub, I did such a foolish thing. So, while I'm sure that the jemalloc devs wouldn't change the file associated with a tag, and while there are safeguards (e.g. check that the file we get matches the SHA of what we expect), simply pulling from a fork is a convenient way to handle this ""generally"" (for packages not as production-quality as jemalloc, or where the developers might not have tagged a release corresponding to what we need). I completely understand that you don't want to link against a standard jemalloc if we compile some strange version with custom modifications. However, here, we simply want to use the vanilla jemalloc. In fact, when salmon is built under bioconda, this is exactly what we do (we link against the conda jemalloc >= 5.1.0).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510
https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510:643,Deployability,release,release,643,"While I definitely trust the jemalloc devs, I do know that such things are possible, as a release is simply associated with a tagged commit, which _can_ be changed via a forced update to the tags. I know because, in my early days using git + GitHub, I did such a foolish thing. So, while I'm sure that the jemalloc devs wouldn't change the file associated with a tag, and while there are safeguards (e.g. check that the file we get matches the SHA of what we expect), simply pulling from a fork is a convenient way to handle this ""generally"" (for packages not as production-quality as jemalloc, or where the developers might not have tagged a release corresponding to what we need). I completely understand that you don't want to link against a standard jemalloc if we compile some strange version with custom modifications. However, here, we simply want to use the vanilla jemalloc. In fact, when salmon is built under bioconda, this is exactly what we do (we link against the conda jemalloc >= 5.1.0).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510
https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510:388,Safety,safe,safeguards,388,"While I definitely trust the jemalloc devs, I do know that such things are possible, as a release is simply associated with a tagged commit, which _can_ be changed via a forced update to the tags. I know because, in my early days using git + GitHub, I did such a foolish thing. So, while I'm sure that the jemalloc devs wouldn't change the file associated with a tag, and while there are safeguards (e.g. check that the file we get matches the SHA of what we expect), simply pulling from a fork is a convenient way to handle this ""generally"" (for packages not as production-quality as jemalloc, or where the developers might not have tagged a release corresponding to what we need). I completely understand that you don't want to link against a standard jemalloc if we compile some strange version with custom modifications. However, here, we simply want to use the vanilla jemalloc. In fact, when salmon is built under bioconda, this is exactly what we do (we link against the conda jemalloc >= 5.1.0).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510
https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510:101,Usability,simpl,simply,101,"While I definitely trust the jemalloc devs, I do know that such things are possible, as a release is simply associated with a tagged commit, which _can_ be changed via a forced update to the tags. I know because, in my early days using git + GitHub, I did such a foolish thing. So, while I'm sure that the jemalloc devs wouldn't change the file associated with a tag, and while there are safeguards (e.g. check that the file we get matches the SHA of what we expect), simply pulling from a fork is a convenient way to handle this ""generally"" (for packages not as production-quality as jemalloc, or where the developers might not have tagged a release corresponding to what we need). I completely understand that you don't want to link against a standard jemalloc if we compile some strange version with custom modifications. However, here, we simply want to use the vanilla jemalloc. In fact, when salmon is built under bioconda, this is exactly what we do (we link against the conda jemalloc >= 5.1.0).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510
https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510:468,Usability,simpl,simply,468,"While I definitely trust the jemalloc devs, I do know that such things are possible, as a release is simply associated with a tagged commit, which _can_ be changed via a forced update to the tags. I know because, in my early days using git + GitHub, I did such a foolish thing. So, while I'm sure that the jemalloc devs wouldn't change the file associated with a tag, and while there are safeguards (e.g. check that the file we get matches the SHA of what we expect), simply pulling from a fork is a convenient way to handle this ""generally"" (for packages not as production-quality as jemalloc, or where the developers might not have tagged a release corresponding to what we need). I completely understand that you don't want to link against a standard jemalloc if we compile some strange version with custom modifications. However, here, we simply want to use the vanilla jemalloc. In fact, when salmon is built under bioconda, this is exactly what we do (we link against the conda jemalloc >= 5.1.0).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510
https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510:843,Usability,simpl,simply,843,"While I definitely trust the jemalloc devs, I do know that such things are possible, as a release is simply associated with a tagged commit, which _can_ be changed via a forced update to the tags. I know because, in my early days using git + GitHub, I did such a foolish thing. So, while I'm sure that the jemalloc devs wouldn't change the file associated with a tag, and while there are safeguards (e.g. check that the file we get matches the SHA of what we expect), simply pulling from a fork is a convenient way to handle this ""generally"" (for packages not as production-quality as jemalloc, or where the developers might not have tagged a release corresponding to what we need). I completely understand that you don't want to link against a standard jemalloc if we compile some strange version with custom modifications. However, here, we simply want to use the vanilla jemalloc. In fact, when salmon is built under bioconda, this is exactly what we do (we link against the conda jemalloc >= 5.1.0).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/288#issuecomment-420339510
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:69,Availability,error,error,69,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:257,Availability,error,error,257,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:394,Availability,error,error,394,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:585,Availability,error,error,585,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:679,Availability,error,errors,679,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:762,Deployability,update,updated,762,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:507,Testability,test,testing,507,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665:342,Usability,clear,clear,342,"Hi @bsipos,. This is caused primarily by salmon's desire to apply an error model (by default) to the CIGAR strings. For secondary alignments, as you note, minmap2 doesn't write the read string, and so when salmon is trying to score the alignments under the error model, it can't find the relevant characters in the read. In general, it's not clear to me if one would actually want to apply the error model (designed primarily for short reads) when quantifying long reads (this is something we are currently testing in the lab). For the time being, I'd probably recommend disabling the error model when quantifying alignments from long reads (`--noErrorModel`). In that case, the errors should hopefully go away. Please let me know, and we'll be sure to keep you updated on best practices for long reads as we figure things out.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/289#issuecomment-420295665
https://github.com/COMBINE-lab/salmon/issues/290#issuecomment-424523688:70,Usability,clear,clear,70,"Hi @TSL-RamKrishna,. Thanks for the report. However, it's not exactly clear to me how this is related specifically to salmon. Generally, if the libm you have is _newer_ than the one salmon was built with, you should be OK. So, one option would be to simply remove the `libm.so.6` from the salmon `lib` directory and see what happens.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/290#issuecomment-424523688
https://github.com/COMBINE-lab/salmon/issues/290#issuecomment-424523688:250,Usability,simpl,simply,250,"Hi @TSL-RamKrishna,. Thanks for the report. However, it's not exactly clear to me how this is related specifically to salmon. Generally, if the libm you have is _newer_ than the one salmon was built with, you should be OK. So, one option would be to simply remove the `libm.so.6` from the salmon `lib` directory and see what happens.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/290#issuecomment-424523688
https://github.com/COMBINE-lab/salmon/issues/295#issuecomment-421417111:459,Integrability,wrap,wrapper,459,"Hi @davidnboone and @mcfwoodruff,. So, I should have mentioned that if you want to use the pre-compiled binary I provide, you have to put the `lib` folder in your path. One way to accomplish this is to run salmon as follows:. ```; DYLD_FALLBACK_LIBRARY_PATH=<path_to_salmon_folder>/lib <path_to_salmon_folder>/bin/salmon ; ```; Where `<path_to_salmon_folder>` is simply the top-level directory where you decompressed salmon. You can, of course, make a little wrapper script to make launching it less ugly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/295#issuecomment-421417111
https://github.com/COMBINE-lab/salmon/issues/295#issuecomment-421417111:363,Usability,simpl,simply,363,"Hi @davidnboone and @mcfwoodruff,. So, I should have mentioned that if you want to use the pre-compiled binary I provide, you have to put the `lib` folder in your path. One way to accomplish this is to run salmon as follows:. ```; DYLD_FALLBACK_LIBRARY_PATH=<path_to_salmon_folder>/lib <path_to_salmon_folder>/bin/salmon ; ```; Where `<path_to_salmon_folder>` is simply the top-level directory where you decompressed salmon. You can, of course, make a little wrapper script to make launching it less ugly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/295#issuecomment-421417111
https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645:84,Deployability,update,updated,84,"Hi Rob, . Thanks for the quick reply. I'm looking into it and will try this with an updated install of GCC >= 5.2.; The system default gcc is 4.8.5 but I set it to use a different install using environment modules to load gcc-4.9.2 but some environment variables may not have been set correctly, hence why the build file switches to a lower-version GCC but it isn't clear why it looks for 4.8.2 despite that.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645
https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645:92,Deployability,install,install,92,"Hi Rob, . Thanks for the quick reply. I'm looking into it and will try this with an updated install of GCC >= 5.2.; The system default gcc is 4.8.5 but I set it to use a different install using environment modules to load gcc-4.9.2 but some environment variables may not have been set correctly, hence why the build file switches to a lower-version GCC but it isn't clear why it looks for 4.8.2 despite that.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645
https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645:180,Deployability,install,install,180,"Hi Rob, . Thanks for the quick reply. I'm looking into it and will try this with an updated install of GCC >= 5.2.; The system default gcc is 4.8.5 but I set it to use a different install using environment modules to load gcc-4.9.2 but some environment variables may not have been set correctly, hence why the build file switches to a lower-version GCC but it isn't clear why it looks for 4.8.2 despite that.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645
https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645:253,Modifiability,variab,variables,253,"Hi Rob, . Thanks for the quick reply. I'm looking into it and will try this with an updated install of GCC >= 5.2.; The system default gcc is 4.8.5 but I set it to use a different install using environment modules to load gcc-4.9.2 but some environment variables may not have been set correctly, hence why the build file switches to a lower-version GCC but it isn't clear why it looks for 4.8.2 despite that.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645
https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645:217,Performance,load,load,217,"Hi Rob, . Thanks for the quick reply. I'm looking into it and will try this with an updated install of GCC >= 5.2.; The system default gcc is 4.8.5 but I set it to use a different install using environment modules to load gcc-4.9.2 but some environment variables may not have been set correctly, hence why the build file switches to a lower-version GCC but it isn't clear why it looks for 4.8.2 despite that.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645
https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645:366,Usability,clear,clear,366,"Hi Rob, . Thanks for the quick reply. I'm looking into it and will try this with an updated install of GCC >= 5.2.; The system default gcc is 4.8.5 but I set it to use a different install using environment modules to load gcc-4.9.2 but some environment variables may not have been set correctly, hence why the build file switches to a lower-version GCC but it isn't clear why it looks for 4.8.2 despite that.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/296#issuecomment-422891645
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:1635,Availability,down,downstream,1635,"observe the coverage bias in them. I'd suggest can you please try subsampling randomly across the full `Fastq` if you haven't tried that already.; * `re: subsampling coefficient:` If you are looking for per-CB level mapping rate for your sample that would be very easy to calculate, although getting one number for the full sample might be little tricky since the mapping rate might have large variance across the sample, but it would be an interesting plot to generate, do let us know how it looks in your case.; If you run Alevin with `--dumpFeatures` flag, alevin will generate a file `featureDump.txt`, whose first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:1842,Availability,down,downstream,1842,"ry easy to calculate, although getting one number for the full sample might be little tricky since the mapping rate might have large variance across the sample, but it would be an interesting plot to generate, do let us know how it looks in your case.; If you run Alevin with `--dumpFeatures` flag, alevin will generate a file `featureDump.txt`, whose first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:1972,Availability,down,downstream,1972,"t to generate, do let us know how it looks in your case.; If you run Alevin with `--dumpFeatures` flag, alevin will generate a file `featureDump.txt`, whose first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to inc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:2449,Availability,avail,available,2449," first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to include that in Alevin tutorial page. However, if you get stuck with any of the above steps do let us know, we'd be more than happy to help in that front too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:2760,Availability,down,downstream,2760," first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to include that in Alevin tutorial page. However, if you get stuck with any of the above steps do let us know, we'd be more than happy to help in that front too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:1501,Deployability,pipeline,pipeline,1501,"observe the coverage bias in them. I'd suggest can you please try subsampling randomly across the full `Fastq` if you haven't tried that already.; * `re: subsampling coefficient:` If you are looking for per-CB level mapping rate for your sample that would be very easy to calculate, although getting one number for the full sample might be little tricky since the mapping rate might have large variance across the sample, but it would be an interesting plot to generate, do let us know how it looks in your case.; If you run Alevin with `--dumpFeatures` flag, alevin will generate a file `featureDump.txt`, whose first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:2195,Deployability,upgrade,upgrade,2195," first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to include that in Alevin tutorial page. However, if you get stuck with any of the above steps do let us know, we'd be more than happy to help in that front too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:2358,Deployability,release,release,2358," first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to include that in Alevin tutorial page. However, if you get stuck with any of the above steps do let us know, we'd be more than happy to help in that front too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:2421,Deployability,release,release,2421," first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to include that in Alevin tutorial page. However, if you get stuck with any of the above steps do let us know, we'd be more than happy to help in that front too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:2833,Deployability,pipeline,pipeline,2833," first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to include that in Alevin tutorial page. However, if you get stuck with any of the above steps do let us know, we'd be more than happy to help in that front too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:1947,Performance,perform,perform,1947,"t to generate, do let us know how it looks in your case.; If you run Alevin with `--dumpFeatures` flag, alevin will generate a file `featureDump.txt`, whose first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to inc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:2673,Performance,perform,perform,2673," first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in summary the following steps would be the gist of the process.; - Use Alevin w/o any modification to the `fastq` on both of your sample to generate the gene count matrices. (We have made a major upgrade to the Alevin. We'd recommend using [v0.12.0-alpha](https://github.com/COMBINE-lab/salmon/tree/v0.12.0-alpha) for now, we are planning to make an official release before the end of this week, currently you can use pre-release. Unfortunately, not available on conda yet).; - Import Alevin count matrices into R using [this](https://combine-lab.github.io/alevin-tutorial/2018/alevin-seurat/) tutorial .; - Use [this](https://satijalab.org/seurat/immune_alignment.html) to perform the batch correction. ; We do realize it's currently complicated to use things downstream of Alevin and are working constantly on improving the overall pipeline to make the analyses as smooth as possible. If you happen to write a tutorial of your own on doing the analyses, we'd be happy to include that in Alevin tutorial page. However, if you get stuck with any of the above steps do let us know, we'd be more than happy to help in that front too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468:360,Usability,simpl,simply,360,"Hi @juugii ,; Thanks a lot for starting an interesting discussion related to Alevin. It does sound really interesting. If I may, I'd like to ask a couple of questions before going deeper into your questions. ; * When you say you try subsampling the `Fastq`, did you sample randomly across the full `Fastq` or chose the top X reads. One can imagine that if you simply, sample just the top X reads and the `Fastq` has not been generated in random order then a certain subset of CB will get all the reads and you'd still observe the coverage bias in them. I'd suggest can you please try subsampling randomly across the full `Fastq` if you haven't tried that already.; * `re: subsampling coefficient:` If you are looking for per-CB level mapping rate for your sample that would be very easy to calculate, although getting one number for the full sample might be little tricky since the mapping rate might have large variance across the sample, but it would be an interesting plot to generate, do let us know how it looks in your case.; If you run Alevin with `--dumpFeatures` flag, alevin will generate a file `featureDump.txt`, whose first column will be the per CB level mapping rate i.e. `#mapped reads/#raw reads`. If you wan't absolute values for per-CB reads and mapped reads, it should be in the file `filtered_cb_frequency.txt` and `mappedUMI.txt` respectively.; * `re: cellranger subsampling:` Correct me if I am wrong, when you say cellranger subsampling, do you mean the `cellranger aggregate` pipeline? It's possible you are talking about some other step which I am not aware of but if it's `aggregate` then I think it happens downstream of all the quantification. Indeed coverage bias correction is an important part of the aggregation step but in general it's not the only one and that's why we recommend using the `Seurat` package downstream of the Alevin quantified matrices. We will be more than happy to write a tutorial on, ""how to perform batch correction downstream of Alevin"" but in ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433169468
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:1053,Availability,down,downstream,1053," subsampling the Fastq, did you sample randomly across the full Fastq or chose the top X reads. Yes, I did perform a random subsampling, ie. taking a read with a p probability while reading the fastq files, p being the subsampling coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the alevin quantifications. So I am looking for a proper way to apply a correction be",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:1184,Availability,down,downstream,1184," I did perform a random subsampling, ie. taking a read with a p probability while reading the fastq files, p being the subsampling coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the alevin quantifications. So I am looking for a proper way to apply a correction before/during/after the alevin quantification, in a way similar to what cellranger do with STAR. Alter",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:1258,Availability,down,downstream,1258,"coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the alevin quantifications. So I am looking for a proper way to apply a correction before/during/after the alevin quantification, in a way similar to what cellranger do with STAR. Alternatively, could a subsampling covariate be added to the probalistic quantification model of alevin (if I understand it well), in sor",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:685,Deployability,pipeline,pipeline,685,"Hi @k3yavi,; Many thanks for you prompt answer, once again. >When you say you try subsampling the Fastq, did you sample randomly across the full Fastq or chose the top X reads. Yes, I did perform a random subsampling, ie. taking a read with a p probability while reading the fastq files, p being the subsampling coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:865,Integrability,depend,depending,865,"Hi @k3yavi,; Many thanks for you prompt answer, once again. >When you say you try subsampling the Fastq, did you sample randomly across the full Fastq or chose the top X reads. Yes, I did perform a random subsampling, ie. taking a read with a p probability while reading the fastq files, p being the subsampling coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:1729,Integrability,message,message,1729,"expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the alevin quantifications. So I am looking for a proper way to apply a correction before/during/after the alevin quantification, in a way similar to what cellranger do with STAR. Alternatively, could a subsampling covariate be added to the probalistic quantification model of alevin (if I understand it well), in sort that such a discrepency bewteen samples would be corrected?. I did look at the mappedUMI file:. ![image](https://user-images.githubusercontent.com/34892073/47551835-85ef9380-d903-11e8-893f-2a684576437b.png). So an option you would recommend is to simply compute the subsampling coefficient for a median ratio bewteen samples? I am expecting quite uneven distributions/variance in the mappedUMI between samples (partly due to a huge difference in term of proliferation that occur with a fraction of cells",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:188,Performance,perform,perform,188,"Hi @k3yavi,; Many thanks for you prompt answer, once again. >When you say you try subsampling the Fastq, did you sample randomly across the full Fastq or chose the top X reads. Yes, I did perform a random subsampling, ie. taking a read with a p probability while reading the fastq files, p being the subsampling coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:477,Usability,simpl,simple,477,"Hi @k3yavi,; Many thanks for you prompt answer, once again. >When you say you try subsampling the Fastq, did you sample randomly across the full Fastq or chose the top X reads. Yes, I did perform a random subsampling, ie. taking a read with a p probability while reading the fastq files, p being the subsampling coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:715,Usability,clear,clearly,715,"Hi @k3yavi,; Many thanks for you prompt answer, once again. >When you say you try subsampling the Fastq, did you sample randomly across the full Fastq or chose the top X reads. Yes, I did perform a random subsampling, ie. taking a read with a p probability while reading the fastq files, p being the subsampling coefficient I did mention (pE[0;1]). An implementation of this approach as an option during the transcript quantification would be great. I can provide you with the simple python script I use for the subsampling, but I am not sure if it is the proper way to subsample during alevin quantification. >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:1611,Usability,simpl,simply,1611," >when you say cellranger subsampling, do you mean the cellranger aggregate pipeline?. Yes, sorry for not clearly stating it. I did use the cellranger aggregate function indeed, which by default subsample the expression matrices with high sequencing depth depending on amount of mapped reads, if I understand well. >Use Alevin w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the alevin quantifications. So I am looking for a proper way to apply a correction before/during/after the alevin quantification, in a way similar to what cellranger do with STAR. Alternatively, could a subsampling covariate be added to the probalistic quantification model of alevin (if I understand it well), in sort that such a discrepency bewteen samples would be corrected?. I did look at the mappedUMI file:. ![image](https://user-images.githubusercontent.com/34892073/47551835-85ef9380-d903-11e8-893f-2a684576437b.png). So an option you would recommend is to simply compute the subsampling coefficient for ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913:2562,Usability,simpl,simply,2562,"w/o any modification to the fastq on both of your sample to generate the gene count matrices. I already did that, in downstream analyses I have a batch effect issue related to the sequencing depth. >that's why we recommend using the Seurat package downstream of the Alevin quantified matrices. I have some experience with downstream analyses with Seurat, Pagoda, Scater, scanpy and a few other tools, and I am aware of batch correction methods like CCA or MNN. But that is not what I am looking for here. I did both CCA and MNN but I loose some important information in the resulting eigenspaces or corrected matrix. I believe the proper way to correct my batch effect is to simply fix the difference between my two libraries, ie. the sequencing depth in this case. As I explained in my first message, cellranger aggregate (subsampling based on the amount of mapped reads) works very well in my case, correct the effect without any loss or modification of important genes in our scientific question. Not CCA or MNN. I would like to be able to do the same from the alevin quantifications. So I am looking for a proper way to apply a correction before/during/after the alevin quantification, in a way similar to what cellranger do with STAR. Alternatively, could a subsampling covariate be added to the probalistic quantification model of alevin (if I understand it well), in sort that such a discrepency bewteen samples would be corrected?. I did look at the mappedUMI file:. ![image](https://user-images.githubusercontent.com/34892073/47551835-85ef9380-d903-11e8-893f-2a684576437b.png). So an option you would recommend is to simply compute the subsampling coefficient for a median ratio bewteen samples? I am expecting quite uneven distributions/variance in the mappedUMI between samples (partly due to a huge difference in term of proliferation that occur with a fraction of cells in only one sample). Still, cellranger aggregate correct it nicely. Thanks again for your prompt answer and comments.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433319913
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155:524,Availability,down,downstream,524,"Thanks @juugii for the detailed response and clarifying my doubts.; It surely is possible to lose some information when projecting the data on lower dimensional space since none of the methods, CCA and MNN, are lossless. I do see your point and would like to understand more about the depth normalization problem you are facing with different experiments. . I tried to search the algorithm used by cellranger to aggregate the counts and it looks like the following:; * As I was saying earlier aggregation step was happening downstream of count/quantification of the gene count matrix, at least in cellranger pipeline.; > When doing large studies involving multiple biological samples (or multiple libraries or replicates of the same sample), it is best to run cellranger count on each of the libraries individually, and then pool the results using cellranger aggr. * It looks like they have three different modes to normalize the libraries; > There are three normalization modes:; mapped: (default) Subsample reads from higher-depth libraries until they all have an equal number of confidently mapped reads per cell.; raw: Subsample reads from higher-depth libraries until they all have an equal number of total (i.e. raw, mapping-independent) reads per cell.; none: Do not normalize at all. * Although it's not clear, what do they mean by `equal number of confidently mapped reads per cell`, does it mean median reads per cell ? Like you tried to show in the above plot the distribution can be very uneven. But the part that troubles me more is once `count` information has been generated it has lost the read level information, since we have deduplicated them, then how do they use the read counts to normalize. Unless that is dumped too, not clear. Quoting your text from above:; > Alternatively, could a subsampling covariate be added to the probabilistic quantification model of alevin. I think we can definitely work on correcting the subsampling bias in the probabilistic model of Alevin but we",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155:608,Deployability,pipeline,pipeline,608,"Thanks @juugii for the detailed response and clarifying my doubts.; It surely is possible to lose some information when projecting the data on lower dimensional space since none of the methods, CCA and MNN, are lossless. I do see your point and would like to understand more about the depth normalization problem you are facing with different experiments. . I tried to search the algorithm used by cellranger to aggregate the counts and it looks like the following:; * As I was saying earlier aggregation step was happening downstream of count/quantification of the gene count matrix, at least in cellranger pipeline.; > When doing large studies involving multiple biological samples (or multiple libraries or replicates of the same sample), it is best to run cellranger count on each of the libraries individually, and then pool the results using cellranger aggr. * It looks like they have three different modes to normalize the libraries; > There are three normalization modes:; mapped: (default) Subsample reads from higher-depth libraries until they all have an equal number of confidently mapped reads per cell.; raw: Subsample reads from higher-depth libraries until they all have an equal number of total (i.e. raw, mapping-independent) reads per cell.; none: Do not normalize at all. * Although it's not clear, what do they mean by `equal number of confidently mapped reads per cell`, does it mean median reads per cell ? Like you tried to show in the above plot the distribution can be very uneven. But the part that troubles me more is once `count` information has been generated it has lost the read level information, since we have deduplicated them, then how do they use the read counts to normalize. Unless that is dumped too, not clear. Quoting your text from above:; > Alternatively, could a subsampling covariate be added to the probabilistic quantification model of alevin. I think we can definitely work on correcting the subsampling bias in the probabilistic model of Alevin but we",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155:1312,Usability,clear,clear,1312,"ellranger to aggregate the counts and it looks like the following:; * As I was saying earlier aggregation step was happening downstream of count/quantification of the gene count matrix, at least in cellranger pipeline.; > When doing large studies involving multiple biological samples (or multiple libraries or replicates of the same sample), it is best to run cellranger count on each of the libraries individually, and then pool the results using cellranger aggr. * It looks like they have three different modes to normalize the libraries; > There are three normalization modes:; mapped: (default) Subsample reads from higher-depth libraries until they all have an equal number of confidently mapped reads per cell.; raw: Subsample reads from higher-depth libraries until they all have an equal number of total (i.e. raw, mapping-independent) reads per cell.; none: Do not normalize at all. * Although it's not clear, what do they mean by `equal number of confidently mapped reads per cell`, does it mean median reads per cell ? Like you tried to show in the above plot the distribution can be very uneven. But the part that troubles me more is once `count` information has been generated it has lost the read level information, since we have deduplicated them, then how do they use the read counts to normalize. Unless that is dumped too, not clear. Quoting your text from above:; > Alternatively, could a subsampling covariate be added to the probabilistic quantification model of alevin. I think we can definitely work on correcting the subsampling bias in the probabilistic model of Alevin but we might need a little more understanding of what's going on with the cellranger and why your way of aggregation is not working as intended. Unfortunately, I think we have to dig deeper into the cellranger codebase to really understand that and if possible, might need some subset of the relevant data to replicate your issue and work on improving that. Also, I am wondering, what's your way of check",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155:1745,Usability,clear,clear,1745,"ples (or multiple libraries or replicates of the same sample), it is best to run cellranger count on each of the libraries individually, and then pool the results using cellranger aggr. * It looks like they have three different modes to normalize the libraries; > There are three normalization modes:; mapped: (default) Subsample reads from higher-depth libraries until they all have an equal number of confidently mapped reads per cell.; raw: Subsample reads from higher-depth libraries until they all have an equal number of total (i.e. raw, mapping-independent) reads per cell.; none: Do not normalize at all. * Although it's not clear, what do they mean by `equal number of confidently mapped reads per cell`, does it mean median reads per cell ? Like you tried to show in the above plot the distribution can be very uneven. But the part that troubles me more is once `count` information has been generated it has lost the read level information, since we have deduplicated them, then how do they use the read counts to normalize. Unless that is dumped too, not clear. Quoting your text from above:; > Alternatively, could a subsampling covariate be added to the probabilistic quantification model of alevin. I think we can definitely work on correcting the subsampling bias in the probabilistic model of Alevin but we might need a little more understanding of what's going on with the cellranger and why your way of aggregation is not working as intended. Unfortunately, I think we have to dig deeper into the cellranger codebase to really understand that and if possible, might need some subset of the relevant data to replicate your issue and work on improving that. Also, I am wondering, what's your way of checking the batch effect and comparing Alevin and Cellranger aggregation? Just a guess, the tSNE plots of the two samples are separating out very clearly, if possible if you can share the figures and the analysis (may be a notebook) to us, we can help and investigate more thoroughly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155
https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155:2541,Usability,clear,clearly,2541,"ples (or multiple libraries or replicates of the same sample), it is best to run cellranger count on each of the libraries individually, and then pool the results using cellranger aggr. * It looks like they have three different modes to normalize the libraries; > There are three normalization modes:; mapped: (default) Subsample reads from higher-depth libraries until they all have an equal number of confidently mapped reads per cell.; raw: Subsample reads from higher-depth libraries until they all have an equal number of total (i.e. raw, mapping-independent) reads per cell.; none: Do not normalize at all. * Although it's not clear, what do they mean by `equal number of confidently mapped reads per cell`, does it mean median reads per cell ? Like you tried to show in the above plot the distribution can be very uneven. But the part that troubles me more is once `count` information has been generated it has lost the read level information, since we have deduplicated them, then how do they use the read counts to normalize. Unless that is dumped too, not clear. Quoting your text from above:; > Alternatively, could a subsampling covariate be added to the probabilistic quantification model of alevin. I think we can definitely work on correcting the subsampling bias in the probabilistic model of Alevin but we might need a little more understanding of what's going on with the cellranger and why your way of aggregation is not working as intended. Unfortunately, I think we have to dig deeper into the cellranger codebase to really understand that and if possible, might need some subset of the relevant data to replicate your issue and work on improving that. Also, I am wondering, what's your way of checking the batch effect and comparing Alevin and Cellranger aggregation? Just a guess, the tSNE plots of the two samples are separating out very clearly, if possible if you can share the figures and the analysis (may be a notebook) to us, we can help and investigate more thoroughly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/305#issuecomment-433453155
https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193:667,Integrability,protocol,protocols,667,"Hi @tomsing1 ,; Apologies for the slow response, I was out of country for a while. Thanks for your kind words and starting a very interesting suggestion.; It’s fascinating to see, how methods being used in single-cell RNA-seq is coming full circle back to the bulk RNA-seq experiments. We have to do some more digging to say clearly about the caveats of using Alevin with the mentioned 3’ bulk RNA-seq experiments but given the understanding from the picture of the shared image we don’t see any obvious show stoppers; although below mentioned concerns should be kept in mind while using Alevin for bulk data deduplication:. Alevin solves the problem pretty well for protocols where fragmentation of the cDNA molecule happens post PCR amplification. There might be some concerns about over-deduplication of the UMI if fragmenation happens before amplification. Although in current form, Illumina sample index can be given as an external whitelist to Alevin but user should be aware that Alevin performs a sequence correction step before starting any optimizations.; Alevin is designed for droplets based protocols, where one end of Paired end read is just the CB/UMI (i.e. no read sequence) and therefore Alevin can’t optimally use the full paired end information of the bulk 3' protocol if its both end has read-sequence for example the ambiguous mapping resolution based on a previously/empirically known approximate fragment length. We would be more than happy to help/discuss, how does the results look in bulk 3’ tagged protocols or if you have particular suggestions about what improvements can be done in Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193
https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193:1104,Integrability,protocol,protocols,1104,"Hi @tomsing1 ,; Apologies for the slow response, I was out of country for a while. Thanks for your kind words and starting a very interesting suggestion.; It’s fascinating to see, how methods being used in single-cell RNA-seq is coming full circle back to the bulk RNA-seq experiments. We have to do some more digging to say clearly about the caveats of using Alevin with the mentioned 3’ bulk RNA-seq experiments but given the understanding from the picture of the shared image we don’t see any obvious show stoppers; although below mentioned concerns should be kept in mind while using Alevin for bulk data deduplication:. Alevin solves the problem pretty well for protocols where fragmentation of the cDNA molecule happens post PCR amplification. There might be some concerns about over-deduplication of the UMI if fragmenation happens before amplification. Although in current form, Illumina sample index can be given as an external whitelist to Alevin but user should be aware that Alevin performs a sequence correction step before starting any optimizations.; Alevin is designed for droplets based protocols, where one end of Paired end read is just the CB/UMI (i.e. no read sequence) and therefore Alevin can’t optimally use the full paired end information of the bulk 3' protocol if its both end has read-sequence for example the ambiguous mapping resolution based on a previously/empirically known approximate fragment length. We would be more than happy to help/discuss, how does the results look in bulk 3’ tagged protocols or if you have particular suggestions about what improvements can be done in Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193
https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193:1279,Integrability,protocol,protocol,1279,"Hi @tomsing1 ,; Apologies for the slow response, I was out of country for a while. Thanks for your kind words and starting a very interesting suggestion.; It’s fascinating to see, how methods being used in single-cell RNA-seq is coming full circle back to the bulk RNA-seq experiments. We have to do some more digging to say clearly about the caveats of using Alevin with the mentioned 3’ bulk RNA-seq experiments but given the understanding from the picture of the shared image we don’t see any obvious show stoppers; although below mentioned concerns should be kept in mind while using Alevin for bulk data deduplication:. Alevin solves the problem pretty well for protocols where fragmentation of the cDNA molecule happens post PCR amplification. There might be some concerns about over-deduplication of the UMI if fragmenation happens before amplification. Although in current form, Illumina sample index can be given as an external whitelist to Alevin but user should be aware that Alevin performs a sequence correction step before starting any optimizations.; Alevin is designed for droplets based protocols, where one end of Paired end read is just the CB/UMI (i.e. no read sequence) and therefore Alevin can’t optimally use the full paired end information of the bulk 3' protocol if its both end has read-sequence for example the ambiguous mapping resolution based on a previously/empirically known approximate fragment length. We would be more than happy to help/discuss, how does the results look in bulk 3’ tagged protocols or if you have particular suggestions about what improvements can be done in Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193
https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193:1525,Integrability,protocol,protocols,1525,"Hi @tomsing1 ,; Apologies for the slow response, I was out of country for a while. Thanks for your kind words and starting a very interesting suggestion.; It’s fascinating to see, how methods being used in single-cell RNA-seq is coming full circle back to the bulk RNA-seq experiments. We have to do some more digging to say clearly about the caveats of using Alevin with the mentioned 3’ bulk RNA-seq experiments but given the understanding from the picture of the shared image we don’t see any obvious show stoppers; although below mentioned concerns should be kept in mind while using Alevin for bulk data deduplication:. Alevin solves the problem pretty well for protocols where fragmentation of the cDNA molecule happens post PCR amplification. There might be some concerns about over-deduplication of the UMI if fragmenation happens before amplification. Although in current form, Illumina sample index can be given as an external whitelist to Alevin but user should be aware that Alevin performs a sequence correction step before starting any optimizations.; Alevin is designed for droplets based protocols, where one end of Paired end read is just the CB/UMI (i.e. no read sequence) and therefore Alevin can’t optimally use the full paired end information of the bulk 3' protocol if its both end has read-sequence for example the ambiguous mapping resolution based on a previously/empirically known approximate fragment length. We would be more than happy to help/discuss, how does the results look in bulk 3’ tagged protocols or if you have particular suggestions about what improvements can be done in Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193
https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193:994,Performance,perform,performs,994,"Hi @tomsing1 ,; Apologies for the slow response, I was out of country for a while. Thanks for your kind words and starting a very interesting suggestion.; It’s fascinating to see, how methods being used in single-cell RNA-seq is coming full circle back to the bulk RNA-seq experiments. We have to do some more digging to say clearly about the caveats of using Alevin with the mentioned 3’ bulk RNA-seq experiments but given the understanding from the picture of the shared image we don’t see any obvious show stoppers; although below mentioned concerns should be kept in mind while using Alevin for bulk data deduplication:. Alevin solves the problem pretty well for protocols where fragmentation of the cDNA molecule happens post PCR amplification. There might be some concerns about over-deduplication of the UMI if fragmenation happens before amplification. Although in current form, Illumina sample index can be given as an external whitelist to Alevin but user should be aware that Alevin performs a sequence correction step before starting any optimizations.; Alevin is designed for droplets based protocols, where one end of Paired end read is just the CB/UMI (i.e. no read sequence) and therefore Alevin can’t optimally use the full paired end information of the bulk 3' protocol if its both end has read-sequence for example the ambiguous mapping resolution based on a previously/empirically known approximate fragment length. We would be more than happy to help/discuss, how does the results look in bulk 3’ tagged protocols or if you have particular suggestions about what improvements can be done in Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193
https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193:1050,Performance,optimiz,optimizations,1050,"Hi @tomsing1 ,; Apologies for the slow response, I was out of country for a while. Thanks for your kind words and starting a very interesting suggestion.; It’s fascinating to see, how methods being used in single-cell RNA-seq is coming full circle back to the bulk RNA-seq experiments. We have to do some more digging to say clearly about the caveats of using Alevin with the mentioned 3’ bulk RNA-seq experiments but given the understanding from the picture of the shared image we don’t see any obvious show stoppers; although below mentioned concerns should be kept in mind while using Alevin for bulk data deduplication:. Alevin solves the problem pretty well for protocols where fragmentation of the cDNA molecule happens post PCR amplification. There might be some concerns about over-deduplication of the UMI if fragmenation happens before amplification. Although in current form, Illumina sample index can be given as an external whitelist to Alevin but user should be aware that Alevin performs a sequence correction step before starting any optimizations.; Alevin is designed for droplets based protocols, where one end of Paired end read is just the CB/UMI (i.e. no read sequence) and therefore Alevin can’t optimally use the full paired end information of the bulk 3' protocol if its both end has read-sequence for example the ambiguous mapping resolution based on a previously/empirically known approximate fragment length. We would be more than happy to help/discuss, how does the results look in bulk 3’ tagged protocols or if you have particular suggestions about what improvements can be done in Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193
https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193:325,Usability,clear,clearly,325,"Hi @tomsing1 ,; Apologies for the slow response, I was out of country for a while. Thanks for your kind words and starting a very interesting suggestion.; It’s fascinating to see, how methods being used in single-cell RNA-seq is coming full circle back to the bulk RNA-seq experiments. We have to do some more digging to say clearly about the caveats of using Alevin with the mentioned 3’ bulk RNA-seq experiments but given the understanding from the picture of the shared image we don’t see any obvious show stoppers; although below mentioned concerns should be kept in mind while using Alevin for bulk data deduplication:. Alevin solves the problem pretty well for protocols where fragmentation of the cDNA molecule happens post PCR amplification. There might be some concerns about over-deduplication of the UMI if fragmenation happens before amplification. Although in current form, Illumina sample index can be given as an external whitelist to Alevin but user should be aware that Alevin performs a sequence correction step before starting any optimizations.; Alevin is designed for droplets based protocols, where one end of Paired end read is just the CB/UMI (i.e. no read sequence) and therefore Alevin can’t optimally use the full paired end information of the bulk 3' protocol if its both end has read-sequence for example the ambiguous mapping resolution based on a previously/empirically known approximate fragment length. We would be more than happy to help/discuss, how does the results look in bulk 3’ tagged protocols or if you have particular suggestions about what improvements can be done in Alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/306#issuecomment-439530193
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:204,Availability,error,error,204,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:530,Deployability,configurat,configuration,530,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:624,Deployability,configurat,configuration,624,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:269,Modifiability,config,configure,269,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:530,Modifiability,config,configuration,530,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:555,Modifiability,config,config,555,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:624,Modifiability,config,configuration,624,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:653,Modifiability,config,config,653,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:1338,Performance,load,load,1338,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099:742,Usability,guid,guide,742,"Rob,. Brilliant - I forgot that I built the boost libraries from whatever version of gcc was on the standard distribution. I have included -DFETCH_BOOST=TRUE, do you know why I am receiving the following error regarding a missing when executing make?. [ 5%] Performing configure step for 'libboost'; Building Boost.Build engine with toolset gcc... tools/build/src/engine/bin.linuxx86_64/b2; Detecting Python version... 2.7; Detecting Python root... /usr; Unicode/ICU support for Boost.Regex?... not found.; Generating Boost.Build configuration in project-config.jam... Bootstrapping is done. To build, run:. ./b2. To adjust configuration, edit 'project-config.jam'.; Further information:. - Command line help:; ./b2 --help. - Getting started guide:; http://www.boost.org/more/getting_started/unix-variants.html. - Boost.Build documentation:; http://www.boost.org/build/doc/html/index.html. using gcc : : /opt/gcc-8.2.0/bin/g++ ); [ 6%] Performing build step for 'libboost'; opt.jam: No such file or directory; /opt/salmon/external/boost_1_66_0/tools/build/src/build/toolset.jam:43: in toolset.using; ERROR: rule ""opt.init"" unknown in module ""toolset"".; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:461: in process-explicit-toolset-requests; /opt/salmon/external/boost_1_66_0/tools/build/src/build-system.jam:527: in load; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/modules.jam:295: in import; /opt/salmon/external/boost_1_66_0/tools/build/src/kernel/bootstrap.jam:139: in boost-build; /opt/salmon/external/boost_1_66_0/boost-build.jam:17: in module scope; make[2]: *** [libboost-prefix/src/libboost-stamp/libboost-build] Error 1; make[1]: *** [CMakeFiles/libboost.dir/all] Error 2; make: *** [all] Error 2. Thanks for all your help!. Nate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/309#issuecomment-436834099
https://github.com/COMBINE-lab/salmon/issues/311#issuecomment-439527284:390,Integrability,protocol,protocols,390,"Hi @mojakab ,; Thanks for your interest in Alevin. Currently most of our research efforts have gone into developing Alevin for droplet based 3' tag sequencing like 10x chromium and DropSeq. Although similar but Cel-Seq2 relies on a different cell isolation step which can potentially create assay specific bias between the experiments. Basically Alevin is designed to work with single-cell protocols which follows the following criteria:; * Droplet based cell isolation.; * 3' tag sequencing.; * Fragmentation post Amplification. We have similar such request in https://github.com/COMBINE-lab/salmon/issues/269, where the user was able to use Alevin with Cel-Seq2 but currently we have not explored the full potential of Alevin with Cel-Seq2 and might require more careful consideration. If you happen to use Alevin on Cel-Seq2 data we'd appreciate your feedback based on your experience.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/311#issuecomment-439527284
https://github.com/COMBINE-lab/salmon/issues/311#issuecomment-439527284:854,Usability,feedback,feedback,854,"Hi @mojakab ,; Thanks for your interest in Alevin. Currently most of our research efforts have gone into developing Alevin for droplet based 3' tag sequencing like 10x chromium and DropSeq. Although similar but Cel-Seq2 relies on a different cell isolation step which can potentially create assay specific bias between the experiments. Basically Alevin is designed to work with single-cell protocols which follows the following criteria:; * Droplet based cell isolation.; * 3' tag sequencing.; * Fragmentation post Amplification. We have similar such request in https://github.com/COMBINE-lab/salmon/issues/269, where the user was able to use Alevin with Cel-Seq2 but currently we have not explored the full potential of Alevin with Cel-Seq2 and might require more careful consideration. If you happen to use Alevin on Cel-Seq2 data we'd appreciate your feedback based on your experience.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/311#issuecomment-439527284
https://github.com/COMBINE-lab/salmon/issues/321#issuecomment-442610783:222,Usability,simpl,simply,222,"Also - the problem does not happen for Danio rerio indexes, which were created on the same machine. We have also had successes with Homo sap. _short_, but not Homo sap. _long_. Is it possible that some of the indexes were simply corrupted during creation? Although you're able to map to the index I provided above. I have no idea.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/321#issuecomment-442610783
https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599:57,Deployability,upgrade,upgrades,57,"HI @jamesrhowe ,; A couple of things, we have made major upgrades into Alevin with the release `v0.12.0` and would be releasing soon and it would take care of the problems you are facing.; We are still working on improving the tutorials and guide for using Alevin but for 10xV3 we added a new flag into `0.12.0` since the UMI length has increased; with the command line flag `--chromiumV3`. You might have to swap `--chromium` with `--chromiumV3`. I'd let you know once we release the latest version otherwise if you can compile from source, compiling `develop` should do the job for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599
https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599:87,Deployability,release,release,87,"HI @jamesrhowe ,; A couple of things, we have made major upgrades into Alevin with the release `v0.12.0` and would be releasing soon and it would take care of the problems you are facing.; We are still working on improving the tutorials and guide for using Alevin but for 10xV3 we added a new flag into `0.12.0` since the UMI length has increased; with the command line flag `--chromiumV3`. You might have to swap `--chromium` with `--chromiumV3`. I'd let you know once we release the latest version otherwise if you can compile from source, compiling `develop` should do the job for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599
https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599:473,Deployability,release,release,473,"HI @jamesrhowe ,; A couple of things, we have made major upgrades into Alevin with the release `v0.12.0` and would be releasing soon and it would take care of the problems you are facing.; We are still working on improving the tutorials and guide for using Alevin but for 10xV3 we added a new flag into `0.12.0` since the UMI length has increased; with the command line flag `--chromiumV3`. You might have to swap `--chromium` with `--chromiumV3`. I'd let you know once we release the latest version otherwise if you can compile from source, compiling `develop` should do the job for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599
https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599:241,Usability,guid,guide,241,"HI @jamesrhowe ,; A couple of things, we have made major upgrades into Alevin with the release `v0.12.0` and would be releasing soon and it would take care of the problems you are facing.; We are still working on improving the tutorials and guide for using Alevin but for 10xV3 we added a new flag into `0.12.0` since the UMI length has increased; with the command line flag `--chromiumV3`. You might have to swap `--chromium` with `--chromiumV3`. I'd let you know once we release the latest version otherwise if you can compile from source, compiling `develop` should do the job for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/324#issuecomment-443218599
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:929,Availability,down,downstream,929,"1) So `--celseq2` isn't there, I had initially checked here, to make sure that there wasn't something wrong with my command.. ; ```; alevin; ==========; salmon-based processing of single-cell RNA-seq data. alevin options:. mapping input options:; -l [ --libType ] arg Format string describing the library ; type; -i [ --index ] arg salmon index; -r [ --unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:1385,Availability,down,downstream,1385,"--unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes. (Default: 100000); --tgMap arg transcript to gene map tsv file; ```; 2) `salmon alevin -lISR -1 cells_CTTGTA_L001_R1_001.fastq.gz -2 cells_CTTGTA_L001_R2_001.fastq.gz --celseq2 -i AlevinIndex_develop/ -p 8 -o alevin_output --tgMap gencode.primary_assembly.tsv`. **The tsv I created myself (with tximport), but I don't think that is the issue here...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:1702,Availability,down,downstream,1702,"--unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes. (Default: 100000); --tgMap arg transcript to gene map tsv file; ```; 2) `salmon alevin -lISR -1 cells_CTTGTA_L001_R1_001.fastq.gz -2 cells_CTTGTA_L001_R2_001.fastq.gz --celseq2 -i AlevinIndex_develop/ -p 8 -o alevin_output --tgMap gencode.primary_assembly.tsv`. **The tsv I created myself (with tximport), but I don't think that is the issue here...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:587,Deployability,pipeline,pipeline,587,"1) So `--celseq2` isn't there, I had initially checked here, to make sure that there wasn't something wrong with my command.. ; ```; alevin; ==========; salmon-based processing of single-cell RNA-seq data. alevin options:. mapping input options:; -l [ --libType ] arg Format string describing the library ; type; -i [ --index ] arg salmon index; -r [ --unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:687,Integrability,protocol,protocol,687,"1) So `--celseq2` isn't there, I had initially checked here, to make sure that there wasn't something wrong with my command.. ; ```; alevin; ==========; salmon-based processing of single-cell RNA-seq data. alevin options:. mapping input options:; -l [ --libType ] arg Format string describing the library ; type; -i [ --index ] arg salmon index; -r [ --unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:760,Integrability,protocol,protocol,760,"1) So `--celseq2` isn't there, I had initially checked here, to make sure that there wasn't something wrong with my command.. ; ```; alevin; ==========; salmon-based processing of single-cell RNA-seq data. alevin options:. mapping input options:; -l [ --libType ] arg Format string describing the library ; type; -i [ --index ] arg salmon index; -r [ --unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:828,Integrability,protocol,protocol,828,"1) So `--celseq2` isn't there, I had initially checked here, to make sure that there wasn't something wrong with my command.. ; ```; alevin; ==========; salmon-based processing of single-cell RNA-seq data. alevin options:. mapping input options:; -l [ --libType ] arg Format string describing the library ; type; -i [ --index ] arg salmon index; -r [ --unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:1602,Security,hash,hash,1602,"--unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes. (Default: 100000); --tgMap arg transcript to gene map tsv file; ```; 2) `salmon alevin -lISR -1 cells_CTTGTA_L001_R1_001.fastq.gz -2 cells_CTTGTA_L001_R2_001.fastq.gz --celseq2 -i AlevinIndex_develop/ -p 8 -o alevin_output --tgMap gencode.primary_assembly.tsv`. **The tsv I created myself (with tximport), but I don't think that is the issue here...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536:1865,Usability,learn,learning,1865,"--unmatedReads ] arg List of files containing unmated reads ; of (e.g. single-end reads); -1 [ --mates1 ] arg File containing the #1 mates; -2 [ --mates2 ] arg File containing the #2 mates. alevin-specific Options:; --noDedup Stops the pipeline after CB sequence ; correction and quasi-mapping reads.; --dropseq Use DropSeq Single Cell protocol for ; the library; --chromium Use 10x chromium v2 Single Cell ; protocol for the library.; --gemcode Use 10x gemcode v1 Single Cell protocol; for the library.; --whitelist arg File containing white-list barcodes; --noQuant Don't run downstream barcode-salmon ; model.; --naive Run Gene level naive deduplication; --noSoftMap Don't use soft-assignment for quant ; instead do hard-assignment.; --mrna arg path to a file containing mito-RNA ; gene, one per line; --rrna arg path to a file containing ribosomal ; RNA, one per line; --useCorrelation Use pair-wise pearson correlation with ; True barcodes as a feature for ; white-list creation.; --dumpfq Dump barcode modified fastq file for ; downstream analysis by using coin toss ; for multi-mapping.; --debug Enabling this mode mode will try to ; ignore segfaults based on no whitelist ; mapping or no whitelist deduplicated ; count; --dumpBfh dump the big hash with all the barcodes; and the UMI sequence.; --dumpFeatures Dump features for whitelist and ; downstream analysis.; --dumpCsvCounts Dump cell v transcripts count matrix in; csv format.; --lowRegionMinNumBarcodes arg (=200) Minimum Number of CB to use for ; learning Low confidence region ; (Default: 200).; --maxNumBarcodes arg (=100000) Maximum allowable limit to process the ; cell barcodes. (Default: 100000); --tgMap arg transcript to gene map tsv file; ```; 2) `salmon alevin -lISR -1 cells_CTTGTA_L001_R1_001.fastq.gz -2 cells_CTTGTA_L001_R2_001.fastq.gz --celseq2 -i AlevinIndex_develop/ -p 8 -o alevin_output --tgMap gencode.primary_assembly.tsv`. **The tsv I created myself (with tximport), but I don't think that is the issue here...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/325#issuecomment-443517536
https://github.com/COMBINE-lab/salmon/issues/328#issuecomment-447403587:120,Security,access,access,120,"I'm in touch with the people administrating the cluster as well and based on their guidance I ran alevin with exclusive access, however that did not help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/328#issuecomment-447403587
https://github.com/COMBINE-lab/salmon/issues/328#issuecomment-447403587:83,Usability,guid,guidance,83,"I'm in touch with the people administrating the cluster as well and based on their guidance I ran alevin with exclusive access, however that did not help.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/328#issuecomment-447403587
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:366,Availability,echo,echo,366,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:445,Availability,echo,echo,445,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:909,Availability,echo,echo,909,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:935,Availability,echo,echo,935,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:952,Availability,echo,echo,952,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:1076,Availability,echo,echo,1076,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:1626,Availability,echo,echo,1626,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:117,Usability,simpl,simple,117,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203:72,Availability,down,downstream,72,"I have already run them all (successfully) separately as pairs, but for downstream analysis I need them to be a single library, so I thought it would be simpler to run them as multiple input files..?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203:153,Usability,simpl,simpler,153,"I have already run them all (successfully) separately as pairs, but for downstream analysis I need them to be a single library, so I thought it would be simpler to run them as multiple input files..?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060:475,Integrability,depend,depends,475,"Hi @Anto007,. Sounds like an interesting experiment! A couple of questions: (1) are you quantifying the meta-transcriptome or the metagenomes? What I mean is, are your target sequences the specific genes from the microbes, or the entire microbial genomes? Is the sequencing data RNA-seq from sequencing the mixture of expressed gene transcripts, or DNA-seq of the microbes? This will have an effect on how you expect reads to be generated. The effect of `--minScoreFraction` depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, `0.9` is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the `--minScoreFraction` you want to set is the one such that ; x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that:. x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 . so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Finally, I'd typically avoid using `--mimicStrictBT2`, since those are pretty harsh parameters. Of course, you could try mapping both with and without that flag and see how it affects your mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060:1465,Safety,avoid,avoid,1465,"Hi @Anto007,. Sounds like an interesting experiment! A couple of questions: (1) are you quantifying the meta-transcriptome or the metagenomes? What I mean is, are your target sequences the specific genes from the microbes, or the entire microbial genomes? Is the sequencing data RNA-seq from sequencing the mixture of expressed gene transcripts, or DNA-seq of the microbes? This will have an effect on how you expect reads to be generated. The effect of `--minScoreFraction` depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, `0.9` is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the `--minScoreFraction` you want to set is the one such that ; x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that:. x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 . so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Finally, I'd typically avoid using `--mimicStrictBT2`, since those are pretty harsh parameters. Of course, you could try mapping both with and without that flag and see how it affects your mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060:1282,Usability,simpl,simple,1282,"Hi @Anto007,. Sounds like an interesting experiment! A couple of questions: (1) are you quantifying the meta-transcriptome or the metagenomes? What I mean is, are your target sequences the specific genes from the microbes, or the entire microbial genomes? Is the sequencing data RNA-seq from sequencing the mixture of expressed gene transcripts, or DNA-seq of the microbes? This will have an effect on how you expect reads to be generated. The effect of `--minScoreFraction` depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, `0.9` is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the `--minScoreFraction` you want to set is the one such that ; x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that:. x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 . so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Finally, I'd typically avoid using `--mimicStrictBT2`, since those are pretty harsh parameters. Of course, you could try mapping both with and without that flag and see how it affects your mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869:82,Integrability,depend,depends,82,"@rob-p can you elaborate on this a bit more: . > The effect of --minScoreFraction depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, 0.9 is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the --minScoreFraction you want to set is the one such that x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that: x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Would the two parameter sets mentioned above have the same effect assuming read length 100?. Also, it says Alevin has a default minScoreFraction of 0.87. Would it be safe to assume differentiating between isoforms with Alevin is a similar problem to differentiating between orthologous genes in metagenomics/transcriptomics?. Which parameters would be relevant to control for this?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869:1206,Safety,safe,safe,1206,"@rob-p can you elaborate on this a bit more: . > The effect of --minScoreFraction depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, 0.9 is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the --minScoreFraction you want to set is the one such that x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that: x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Would the two parameter sets mentioned above have the same effect assuming read length 100?. Also, it says Alevin has a default minScoreFraction of 0.87. Would it be safe to assume differentiating between isoforms with Alevin is a similar problem to differentiating between orthologous genes in metagenomics/transcriptomics?. Which parameters would be relevant to control for this?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869:880,Usability,simpl,simple,880,"@rob-p can you elaborate on this a bit more: . > The effect of --minScoreFraction depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, 0.9 is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the --minScoreFraction you want to set is the one such that x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that: x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Would the two parameter sets mentioned above have the same effect assuming read length 100?. Also, it says Alevin has a default minScoreFraction of 0.87. Would it be safe to assume differentiating between isoforms with Alevin is a similar problem to differentiating between orthologous genes in metagenomics/transcriptomics?. Which parameters would be relevant to control for this?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:135,Availability,down,download,135,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:214,Availability,error,error,214,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:93,Deployability,install,install,93,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:181,Deployability,release,releases,181,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:261,Deployability,install,installed,261,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:333,Integrability,depend,dependencies,333,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:128,Usability,simpl,simply,128,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:405,Performance,load,load,405,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:202,Security,hash,hash,202,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:731,Security,hash,hash,731,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:332,Usability,simpl,simply,332,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869:67,Testability,log,log,67,"Hi @asher1234,. Thanks. I'll try and grab the data now. The 0.12.0 log here is quite informative. It looks like the problem is that none of the reads are making through the likelihood filter, which explains why you see the output you do. I'll take a look and see if there is a clear reason why. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869:277,Usability,clear,clear,277,"Hi @asher1234,. Thanks. I'll try and grab the data now. The 0.12.0 log here is quite informative. It looks like the problem is that none of the reads are making through the likelihood filter, which explains why you see the output you do. I'll take a look and see if there is a clear reason why. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:82,Availability,echo,echo,82,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:897,Availability,error,error,897,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1109,Availability,error,error,1109,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1170,Availability,error,error,1170,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1579,Availability,failure,failure,1579,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:607,Deployability,pipeline,pipeline,607,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1182,Performance,load,loading,1182,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:58,Testability,log,log,58,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:476,Testability,test,test,476,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:546,Testability,test,test,546,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:565,Testability,test,test,565,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:588,Testability,test,test,588,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:662,Testability,test,tests,662,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1526,Testability,log,log,1526,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1497,Usability,resume,resume,1497,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473:197,Availability,avail,available,197,"I'm curious too if there is any specific feedback from the devs on whether using salmon on bacterial coding sequences is generally seen as appropriate or not? (sorry if this information is already available elsewhere, I've looked a bit and not seen it yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473
https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473:41,Usability,feedback,feedback,41,"I'm curious too if there is any specific feedback from the devs on whether using salmon on bacterial coding sequences is generally seen as appropriate or not? (sorry if this information is already available elsewhere, I've looked a bit and not seen it yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075:166,Availability,down,downstream,166,"Hi @Munfred ,. Apologies for the delayed response.; Thanks for your very important question. We are aware of the problem and are extensively working on improving the downstream processing of the alevin output. Unfortunately, in current form there is no other direct way of loading alevin output matrix. We are thinking of alternative options like using `loompy` but it's a work in progress. We will definitely inform here once we have a simpler working version.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075:273,Performance,load,loading,273,"Hi @Munfred ,. Apologies for the delayed response.; Thanks for your very important question. We are aware of the problem and are extensively working on improving the downstream processing of the alevin output. Unfortunately, in current form there is no other direct way of loading alevin output matrix. We are thinking of alternative options like using `loompy` but it's a work in progress. We will definitely inform here once we have a simpler working version.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075:437,Usability,simpl,simpler,437,"Hi @Munfred ,. Apologies for the delayed response.; Thanks for your very important question. We are aware of the problem and are extensively working on improving the downstream processing of the alevin output. Unfortunately, in current form there is no other direct way of loading alevin output matrix. We are thinking of alternative options like using `loompy` but it's a work in progress. We will definitely inform here once we have a simpler working version.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:812,Deployability,patch,patch,812,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:872,Deployability,release,release,872,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:458,Performance,optimiz,optimization,458,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:745,Performance,optimiz,optimization,745,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:350,Usability,clear,clearly,350,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148:96,Deployability,pipeline,pipelines,96,"Hi @k3yavi - thanks for the response. We have been looking at using Alevin to support our wider pipelines in the gene expression group at the EBI, as a generic way of quantifying droplet experiments. It has worked well for the 10X v2 studies I've tried, but I'm experiencing some trouble with the drop-seq studies I've tried thus far due to noisy barcodes. May just be bad data, but I'll post an issue or two when I'm clearer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148:418,Usability,clear,clearer,418,"Hi @k3yavi - thanks for the response. We have been looking at using Alevin to support our wider pipelines in the gene expression group at the EBI, as a generic way of quantifying droplet experiments. It has worked well for the 10X v2 studies I've tried, but I'm experiencing some trouble with the drop-seq studies I've tried thus far due to noisy barcodes. May just be bad data, but I'll post an issue or two when I'm clearer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490143007:465,Usability,clear,clear,465,"Heya, we had the same problem with unclear knee-plots like this. We make an alternative plot that looks like these. The first is on high quality data from Allon's K562 data from the original inDrop paper; a knee plot works well on this dataset. ![image](https://user-images.githubusercontent.com/414586/57312464-9e631680-70bb-11e9-961d-5bf2c3ede38a.png). and this is from blood in the Zebrafish, the data is of less good quality. The knee plot for this data wasn't clear enough to draw a reasonable cutoff but this alternative plot makes it easier to pick the cutoff:. ![image](https://user-images.githubusercontent.com/414586/57312538-c3578980-70bb-11e9-910c-84017a5dbcde.png). These plots are made like this:. ```; barcode_plot = function(bcs, sample) {; bcs_hist = hist(log10(bcs$count), plot=FALSE, n=50); fLog = bcs_hist$count; xLog = bcs_hist$mids; y = fLog * (10^xLog) / sum(fLog * (10^xLog)); print(qplot(xLog, y) + geom_point() + theme_bw() + ggtitle(sample)); return(data.frame(x=xLog, y=y, sample=sample)); }; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490143007
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234:162,Availability,recover,recover,162,"Thanks- Jonathan. Yikes, that bad quality one looks like particularly bad quality, I have an example that looks like that in my failed examples. Were you able to recover usable data from it?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234:162,Safety,recover,recover,162,"Thanks- Jonathan. Yikes, that bad quality one looks like particularly bad quality, I have an example that looks like that in my failed examples. Were you able to recover usable data from it?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234:170,Usability,usab,usable,170,"Thanks- Jonathan. Yikes, that bad quality one looks like particularly bad quality, I have an example that looks like that in my failed examples. Were you able to recover usable data from it?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214:307,Safety,detect,detection,307,"Okay, thanks @k3yavi. Just to be clear- you're saying I should derive the whitelist from the filtered_cb_frequency rather than the raw? This is a much smaller file in the case of the bad data above (more so than I'd expect from the cb correction, 984), so I was afraid it had already been subjected to knee detection. I also note that it's also not in fact sorted by default.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214:33,Usability,clear,clear,33,"Okay, thanks @k3yavi. Just to be clear- you're saying I should derive the whitelist from the filtered_cb_frequency rather than the raw? This is a much smaller file in the case of the bad data above (more so than I'd expect from the cb correction, 984), so I was afraid it had already been subjected to knee detection. I also note that it's also not in fact sorted by default.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462:413,Energy Efficiency,adapt,adaptive,413,"Hi @tamuanand,. Thanks for the suggestion. You're right, of course, and we should change the wording in that readme. The cause of the sequence similarity is not always known, and frankly, not important for our particular application. We adopted this term as shorthand given it's common use and also because the version of MashMap used to compute these sequence-similar regions was introduced in the paper [A fast adaptive algorithm for computing whole-genome homology maps](https://academic.oup.com/bioinformatics/article/34/17/i748/5093242). In the preprint itself, we're generally careful to simply refer to these as sequence-similar regions ;).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462:413,Modifiability,adapt,adaptive,413,"Hi @tamuanand,. Thanks for the suggestion. You're right, of course, and we should change the wording in that readme. The cause of the sequence similarity is not always known, and frankly, not important for our particular application. We adopted this term as shorthand given it's common use and also because the version of MashMap used to compute these sequence-similar regions was introduced in the paper [A fast adaptive algorithm for computing whole-genome homology maps](https://academic.oup.com/bioinformatics/article/34/17/i748/5093242). In the preprint itself, we're generally careful to simply refer to these as sequence-similar regions ;).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462:594,Usability,simpl,simply,594,"Hi @tamuanand,. Thanks for the suggestion. You're right, of course, and we should change the wording in that readme. The cause of the sequence similarity is not always known, and frankly, not important for our particular application. We adopted this term as shorthand given it's common use and also because the version of MashMap used to compute these sequence-similar regions was introduced in the paper [A fast adaptive algorithm for computing whole-genome homology maps](https://academic.oup.com/bioinformatics/article/34/17/i748/5093242). In the preprint itself, we're generally careful to simply refer to these as sequence-similar regions ;).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499544736:276,Usability,clear,clear,276,"@tamuanand,. Thank you for pointing out the relevant literature, and I definitely appreciate your clarity on this issue. Also, I completely agree with your suggested re-wordings in the manuscript, as they correct the mistaken terminology and make the overall intent even more clear. We will be sure to address this when we revise the pre-print. Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499544736
https://github.com/COMBINE-lab/salmon/issues/371#issuecomment-501432106:307,Usability,simpl,simply,307,"Hi,; Thank you for the detailed explanation. ; For the first question, ideally you should include the alternate alleles. But they should be part of the transcriptome, instead of the genome. We expect the size of the decoys to double if alt alleles are included in the genome. This is because the decoys are simply regions of high sequence identity between a transcript and the genome. Hence, with alleles as part of the genome, each transcript will map almost equally well to alleles.; If you're running salmon in the alignment mode, the input bam/sam file should include the CIGAR string as well. There is a flag, `--noErrorModel`, to ignore the CIGAR but that is not recommended, since salmon uses the information for scoring the alignments. ; I hope that answers your concerns.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/371#issuecomment-501432106
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672:422,Availability,down,downstream,422,"Thanks @Ryan-Zhu ,. A couple of thoughts.; To me the data seem a little noisy as a lot of CB/reads are thrown away due to ""knee"" thresholding.; Check https://github.com/COMBINE-lab/salmon/issues/362 if you wanna play with how to customize alevin for user-define whitelisting. Having said that this is how you can parse the data from alevin.; Alevin use 1277 CB after its knee thresholding + 638 low confidence Barcode for downstream whitelisting = total 1915 CBs.; If you check the warning in the log it says :. ```; [2019-06-12 15:07:08.152] [alevinLog] [warning] Skipped 313 barcodes due to No mapped read; ```; Basically it means out of 1915, 313 didn't had any read mapped to them, so alevin doesn't report them in the output matrix. Alevin reports 1915 - 313 = 1602 CBs both in `.mtx` and `quants_mat.gz` file. You can check the order of the CB in the `quants_mat_rows.txt` file, which has 1602 rows/CBs. If you don't provide alevin with external whitelist alevin tries to do post whitelisting of it's own. Basically out of the 1277 high confidence CBs alevin initially find out through knee it assigns 647 CBs as final whitelisted CB as found in the `whitelist.txt` file. If you wan't to subsample these CBs you have to extract the information from the `.mtx` or `quants_mat.gz` file. You can check a simple python parse of the `quants_mat.gz` file [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L187-L230).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672:497,Testability,log,log,497,"Thanks @Ryan-Zhu ,. A couple of thoughts.; To me the data seem a little noisy as a lot of CB/reads are thrown away due to ""knee"" thresholding.; Check https://github.com/COMBINE-lab/salmon/issues/362 if you wanna play with how to customize alevin for user-define whitelisting. Having said that this is how you can parse the data from alevin.; Alevin use 1277 CB after its knee thresholding + 638 low confidence Barcode for downstream whitelisting = total 1915 CBs.; If you check the warning in the log it says :. ```; [2019-06-12 15:07:08.152] [alevinLog] [warning] Skipped 313 barcodes due to No mapped read; ```; Basically it means out of 1915, 313 didn't had any read mapped to them, so alevin doesn't report them in the output matrix. Alevin reports 1915 - 313 = 1602 CBs both in `.mtx` and `quants_mat.gz` file. You can check the order of the CB in the `quants_mat_rows.txt` file, which has 1602 rows/CBs. If you don't provide alevin with external whitelist alevin tries to do post whitelisting of it's own. Basically out of the 1277 high confidence CBs alevin initially find out through knee it assigns 647 CBs as final whitelisted CB as found in the `whitelist.txt` file. If you wan't to subsample these CBs you have to extract the information from the `.mtx` or `quants_mat.gz` file. You can check a simple python parse of the `quants_mat.gz` file [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L187-L230).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672:1307,Usability,simpl,simple,1307,"Thanks @Ryan-Zhu ,. A couple of thoughts.; To me the data seem a little noisy as a lot of CB/reads are thrown away due to ""knee"" thresholding.; Check https://github.com/COMBINE-lab/salmon/issues/362 if you wanna play with how to customize alevin for user-define whitelisting. Having said that this is how you can parse the data from alevin.; Alevin use 1277 CB after its knee thresholding + 638 low confidence Barcode for downstream whitelisting = total 1915 CBs.; If you check the warning in the log it says :. ```; [2019-06-12 15:07:08.152] [alevinLog] [warning] Skipped 313 barcodes due to No mapped read; ```; Basically it means out of 1915, 313 didn't had any read mapped to them, so alevin doesn't report them in the output matrix. Alevin reports 1915 - 313 = 1602 CBs both in `.mtx` and `quants_mat.gz` file. You can check the order of the CB in the `quants_mat_rows.txt` file, which has 1602 rows/CBs. If you don't provide alevin with external whitelist alevin tries to do post whitelisting of it's own. Basically out of the 1277 high confidence CBs alevin initially find out through knee it assigns 647 CBs as final whitelisted CB as found in the `whitelist.txt` file. If you wan't to subsample these CBs you have to extract the information from the `.mtx` or `quants_mat.gz` file. You can check a simple python parse of the `quants_mat.gz` file [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L187-L230).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686:879,Deployability,update,update,879,"okies, I think I see the issue. So look at the following lines in the log:; ```; [2019-06-17 21:21:44.518] [alevinLog] [info] Total 824863; [2019-06-17 21:22:47.680] [alevinLog] [info] Total Unique barcodes found: 3474567; ```; What it means is alevin found total: `3,474,567` unique CB in the whole sample and keeps `824,863` CB for further processing which is ~23% of the CB. So all the `keepCBFraction` values above 0.23 would have no effect. If you wan't to generate the `whitelist.txt`, alevin has to have some low confidence CB to learn from, so I am guessing in your case any value from 0.15-0.20 should ideally work. Having said that, I am still exploring why even setting `freqThreshold` to 0, alevin not considers all `3M` CB for processing, I guess there is some kind of filter which is coming into the picture but I might need a bit more time to explore that. I will update here once I figure it out. Thanks again for raising the issue and investing your time in improving alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686:70,Testability,log,log,70,"okies, I think I see the issue. So look at the following lines in the log:; ```; [2019-06-17 21:21:44.518] [alevinLog] [info] Total 824863; [2019-06-17 21:22:47.680] [alevinLog] [info] Total Unique barcodes found: 3474567; ```; What it means is alevin found total: `3,474,567` unique CB in the whole sample and keeps `824,863` CB for further processing which is ~23% of the CB. So all the `keepCBFraction` values above 0.23 would have no effect. If you wan't to generate the `whitelist.txt`, alevin has to have some low confidence CB to learn from, so I am guessing in your case any value from 0.15-0.20 should ideally work. Having said that, I am still exploring why even setting `freqThreshold` to 0, alevin not considers all `3M` CB for processing, I guess there is some kind of filter which is coming into the picture but I might need a bit more time to explore that. I will update here once I figure it out. Thanks again for raising the issue and investing your time in improving alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686:537,Usability,learn,learn,537,"okies, I think I see the issue. So look at the following lines in the log:; ```; [2019-06-17 21:21:44.518] [alevinLog] [info] Total 824863; [2019-06-17 21:22:47.680] [alevinLog] [info] Total Unique barcodes found: 3474567; ```; What it means is alevin found total: `3,474,567` unique CB in the whole sample and keeps `824,863` CB for further processing which is ~23% of the CB. So all the `keepCBFraction` values above 0.23 would have no effect. If you wan't to generate the `whitelist.txt`, alevin has to have some low confidence CB to learn from, so I am guessing in your case any value from 0.15-0.20 should ideally work. Having said that, I am still exploring why even setting `freqThreshold` to 0, alevin not considers all `3M` CB for processing, I guess there is some kind of filter which is coming into the picture but I might need a bit more time to explore that. I will update here once I figure it out. Thanks again for raising the issue and investing your time in improving alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:177,Deployability,release,release,177,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:260,Deployability,release,release,260,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:325,Deployability,update,updated,325,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:215,Integrability,depend,dependencies,215,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:26,Usability,feedback,feedbacks,26,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:1876,Availability,error,error,1876,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:2078,Deployability,pipeline,pipeline,2078,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:558,Safety,predict,predicting,558,"Hi @alexmascension ,. Thanks for confirming. I'll paste my response, I sent you earlier, here too. In case it's helpful to some other user. > Hi Alex,. >Thanks again for forwarding the data. I think I have the solution for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was ove",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:380,Testability,log,log,380,"Hi @alexmascension ,. Thanks for confirming. I'll paste my response, I sent you earlier, here too. In case it's helpful to some other user. > Hi Alex,. >Thanks again for forwarding the data. I think I have the solution for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was ove",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:1086,Testability,log,logs,1086," response, I sent you earlier, here too. In case it's helpful to some other user. > Hi Alex,. >Thanks again for forwarding the data. I think I have the solution for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:1936,Testability,log,logs,1936,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:2022,Usability,progress bar,progress bar,2022,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771:70,Safety,predict,predicted,70,"Hi @ChelseaCHENX ,. Thanks for confirming the counts of the number of predicted cells.; As much as I'd love to give you an exact answers, but in realty w/ current settings it requires a little more exploratory analysis. Whitelisting traditionally is done by making some greedy choices and generally can results in different number of predicted cells, and having an exact answer is difficult to have. For example, if you run alevin with `--dumpFeatures` and plot the frequency of CB, as dumped in the `raw_cb_frequency.txt`, you will observe a monotonically non-increasing function. Different tools try to get the ""knee"" in the distribution, so as alevin, as the first round of whitelisting. For cellranger, at least in my understanding, they try to take the top X% (I think it's 10) of the value suggested through `expectCells` command as high confidence and use all the CB which has the frequency greater than the lowest frequency of the high confidence barcodes for quantification. To counter the greediness of the CB calling, we in our suggested method for alevin, proposed a naive bayes based approach by learning features from not only CB frequency but various other features. There had been other methods like ""emptyDrops"" which you can try for more fine-grained whitelisting post quantification using alevin quants. Having said that, if you use expectCells with bigger value, alevin will start to include more and more cells. However as the frequency of the new CB which gets included as high confidence with each new iteration drops exponentially, and even though the new CB gets merged to a high confidence barcode its chance of affecting the quantification also drops. In summary, if you are sure about your experiment to have more cells then it's ideal to increase the value otherwise, I think, with the increased expectCells value the quants can potentially be effected but most probably not by a lot. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771:334,Safety,predict,predicted,334,"Hi @ChelseaCHENX ,. Thanks for confirming the counts of the number of predicted cells.; As much as I'd love to give you an exact answers, but in realty w/ current settings it requires a little more exploratory analysis. Whitelisting traditionally is done by making some greedy choices and generally can results in different number of predicted cells, and having an exact answer is difficult to have. For example, if you run alevin with `--dumpFeatures` and plot the frequency of CB, as dumped in the `raw_cb_frequency.txt`, you will observe a monotonically non-increasing function. Different tools try to get the ""knee"" in the distribution, so as alevin, as the first round of whitelisting. For cellranger, at least in my understanding, they try to take the top X% (I think it's 10) of the value suggested through `expectCells` command as high confidence and use all the CB which has the frequency greater than the lowest frequency of the high confidence barcodes for quantification. To counter the greediness of the CB calling, we in our suggested method for alevin, proposed a naive bayes based approach by learning features from not only CB frequency but various other features. There had been other methods like ""emptyDrops"" which you can try for more fine-grained whitelisting post quantification using alevin quants. Having said that, if you use expectCells with bigger value, alevin will start to include more and more cells. However as the frequency of the new CB which gets included as high confidence with each new iteration drops exponentially, and even though the new CB gets merged to a high confidence barcode its chance of affecting the quantification also drops. In summary, if you are sure about your experiment to have more cells then it's ideal to increase the value otherwise, I think, with the increased expectCells value the quants can potentially be effected but most probably not by a lot. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771:1109,Usability,learn,learning,1109,"Hi @ChelseaCHENX ,. Thanks for confirming the counts of the number of predicted cells.; As much as I'd love to give you an exact answers, but in realty w/ current settings it requires a little more exploratory analysis. Whitelisting traditionally is done by making some greedy choices and generally can results in different number of predicted cells, and having an exact answer is difficult to have. For example, if you run alevin with `--dumpFeatures` and plot the frequency of CB, as dumped in the `raw_cb_frequency.txt`, you will observe a monotonically non-increasing function. Different tools try to get the ""knee"" in the distribution, so as alevin, as the first round of whitelisting. For cellranger, at least in my understanding, they try to take the top X% (I think it's 10) of the value suggested through `expectCells` command as high confidence and use all the CB which has the frequency greater than the lowest frequency of the high confidence barcodes for quantification. To counter the greediness of the CB calling, we in our suggested method for alevin, proposed a naive bayes based approach by learning features from not only CB frequency but various other features. There had been other methods like ""emptyDrops"" which you can try for more fine-grained whitelisting post quantification using alevin quants. Having said that, if you use expectCells with bigger value, alevin will start to include more and more cells. However as the frequency of the new CB which gets included as high confidence with each new iteration drops exponentially, and even though the new CB gets merged to a high confidence barcode its chance of affecting the quantification also drops. In summary, if you are sure about your experiment to have more cells then it's ideal to increase the value otherwise, I think, with the increased expectCells value the quants can potentially be effected but most probably not by a lot. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:428,Security,validat,validateMappings,428,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:828,Security,validat,validateMappings,828,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:1205,Security,validat,validateMappings,1205,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:1425,Security,validat,validateMappings,1425,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:554,Testability,test,tested,554,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:58,Usability,clear,clear,58,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:1189,Usability,simpl,simply,1189,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2324,Availability,down,down-weight,2324,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:346,Deployability,release,release,346,"Hi @leagleag,. Thank you for your question. This is because with selective alignment (`--validateMappings`) salmon is making use of [range-factorized equivalence classes](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977), which do not directly correspond to the ""standard"" notion of equivalence classes. In fact, as of the next release, it will _always_ make use of these equivalence classes by default. This leads to potentially confusing results when used in conjunction with `--dumpEq`. Specifically, the range-factorized equivalence relation group fragments not only by the transcripts to which they map, but also with respect to the conditional probabilities of having generated that fragment & alignment score given each transcript. Practically, what happens is that the space of conditional probabilities is quantized, and an equivalence relation is defined based on both the transcript set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different co",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:1462,Deployability,release,release,1462,"rized equivalence relation group fragments not only by the transcripts to which they map, but also with respect to the conditional probabilities of having generated that fragment & alignment score given each transcript. Practically, what happens is that the space of conditional probabilities is quantized, and an equivalence relation is defined based on both the transcript set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2794,Deployability,update,update,2794,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2852,Deployability,release,release,2852,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:89,Security,validat,validateMappings,89,"Hi @leagleag,. Thank you for your question. This is because with selective alignment (`--validateMappings`) salmon is making use of [range-factorized equivalence classes](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977), which do not directly correspond to the ""standard"" notion of equivalence classes. In fact, as of the next release, it will _always_ make use of these equivalence classes by default. This leads to potentially confusing results when used in conjunction with `--dumpEq`. Specifically, the range-factorized equivalence relation group fragments not only by the transcripts to which they map, but also with respect to the conditional probabilities of having generated that fragment & alignment score given each transcript. Practically, what happens is that the space of conditional probabilities is quantized, and an equivalence relation is defined based on both the transcript set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different co",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:1906,Usability,simpl,simple,1906,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2899,Usability,clear,clear,2899,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214:186,Availability,avail,available,186,"Sounds good, just wanted to give you the heads up, as we are working on some other part of the salmon pipeline, currently I can't give you an ETA when would the new version of salmon be available. If For the time being the choice are either you can compile the develop branch of salmon or I can forward you a linux usable salmon binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214:102,Deployability,pipeline,pipeline,102,"Sounds good, just wanted to give you the heads up, as we are working on some other part of the salmon pipeline, currently I can't give you an ETA when would the new version of salmon be available. If For the time being the choice are either you can compile the develop branch of salmon or I can forward you a linux usable salmon binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214:315,Usability,usab,usable,315,"Sounds good, just wanted to give you the heads up, as we are working on some other part of the salmon pipeline, currently I can't give you an ETA when would the new version of salmon be available. If For the time being the choice are either you can compile the develop branch of salmon or I can forward you a linux usable salmon binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:132,Deployability,install,install,132,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:167,Deployability,install,installer,167,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:183,Deployability,install,install-,183,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:244,Deployability,install,installer,244,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:409,Deployability,install,install,409,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:473,Deployability,install,install,473,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:988,Deployability,install,installed,988,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:1018,Deployability,install,install,1018,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:64,Modifiability,config,configure,64,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:274,Modifiability,config,config,274,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:315,Modifiability,config,config,315,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:667,Performance,perform,performance,667,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:576,Usability,simpl,simply,576,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038:213,Deployability,install,installed,213,The Arabidopsis example in the getting started guide ([https://combine-lab.github.io/salmon/getting_started/](https://combine-lab.github.io/salmon/getting_started/)) seems to work fine on FreeBSD 13.0 with salmon installed via miniconda per instructions above.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038:47,Usability,guid,guide,47,The Arabidopsis example in the getting started guide ([https://combine-lab.github.io/salmon/getting_started/](https://combine-lab.github.io/salmon/getting_started/)) seems to work fine on FreeBSD 13.0 with salmon installed via miniconda per instructions above.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932:327,Integrability,depend,depend,327,"Hi @pinin4fjords ,; Thanks for raising an important question and running alevin for the training.; I think there is a confusion regarding the `quantmerge` command. That command works only with bulk RNA-seq quants not with alevin output. To answer your question of running multiple alevin instance for multiple file pair, might depend on what are the separate files from, are they from separate lanes or are they separated based on cellular barcode ? The basic intuition is after initial barcode assignment, alevin works on each cell disjointly meaning as long as you are confident that each file pair is cell disjoint then at the end you can just cat the output of the alevin quants. Also, depending on what's the training about you can think of multiple workarounds like you can use very small 100 cell (7 million reads) datasets from 10x and combine it all together in one file if size and multiple files is a problem.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932:690,Integrability,depend,depending,690,"Hi @pinin4fjords ,; Thanks for raising an important question and running alevin for the training.; I think there is a confusion regarding the `quantmerge` command. That command works only with bulk RNA-seq quants not with alevin output. To answer your question of running multiple alevin instance for multiple file pair, might depend on what are the separate files from, are they from separate lanes or are they separated based on cellular barcode ? The basic intuition is after initial barcode assignment, alevin works on each cell disjointly meaning as long as you are confident that each file pair is cell disjoint then at the end you can just cat the output of the alevin quants. Also, depending on what's the training about you can think of multiple workarounds like you can use very small 100 cell (7 million reads) datasets from 10x and combine it all together in one file if size and multiple files is a problem.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932:460,Usability,intuit,intuition,460,"Hi @pinin4fjords ,; Thanks for raising an important question and running alevin for the training.; I think there is a confusion regarding the `quantmerge` command. That command works only with bulk RNA-seq quants not with alevin output. To answer your question of running multiple alevin instance for multiple file pair, might depend on what are the separate files from, are they from separate lanes or are they separated based on cellular barcode ? The basic intuition is after initial barcode assignment, alevin works on each cell disjointly meaning as long as you are confident that each file pair is cell disjoint then at the end you can just cat the output of the alevin quants. Also, depending on what's the training about you can think of multiple workarounds like you can use very small 100 cell (7 million reads) datasets from 10x and combine it all together in one file if size and multiple files is a problem.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540400660:200,Usability,clear,clear,200,"Thanks @k3yavi for the clarification. In my example case the files are not cell disjoint, being multiple lanes run from the same library. Obviously I can use just one lane for the training, but to be clear: in the real world in this situation all files for a library need to be run together, right?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540400660
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-545201602:447,Usability,simpl,simple,447,"Hi @roryk ; I know about tximport but is there any way to generate the input data for DESeq2 without using R? I am processing the data on one platform and then transfer to another platform for R/DESeq2 analysis. I would like to be able to generate the output of the first part (salmon) without using an R library. . If it is not possible and I have run R to get the count matrix for DEseq2, I can figure out a way to do it. DESeq2 input file is a simple matrix of counts and ""salmon quantmerge"" already generates this, can you please explain to me why an external library is required ? Is there something I am missing that tximport package is doing to the data? Does tximport takes into account gene lengths or library size to generate the output? . Thanks; Best Regards; Hamdi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-545201602
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286:59,Deployability,update,update,59,"I also wonder why salmon not output original reads counts. update:. Because it is more accurate. DeSeq2 should accept it. As for now, maybe we could simply round to the nearest integer. > NumReads — This is salmon’s estimate of the number of reads mapping to each transcript that was quantified. It is an “estimate” insofar as it is the expected number of reads that have originated from each transcript given the structure of the uniquely mapping and multi-mapping reads and the relative abundance estimates for each transcript.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286:149,Usability,simpl,simply,149,"I also wonder why salmon not output original reads counts. update:. Because it is more accurate. DeSeq2 should accept it. As for now, maybe we could simply round to the nearest integer. > NumReads — This is salmon’s estimate of the number of reads mapping to each transcript that was quantified. It is an “estimate” insofar as it is the expected number of reads that have originated from each transcript given the structure of the uniquely mapping and multi-mapping reads and the relative abundance estimates for each transcript.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682:295,Deployability,release,release,295,"If you can already use DESeq2, then using tximport should not make it any harder at all. Given the tximport data, getting it into DESeq2 is as easy as. ```; dds <- DESeqDataSetFromTximport(txi, sampleTable, ~condition); ```. as shown in the [tximport vignette](https://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html#Introduction). . Regarding outputting ""original read counts""; salmon *does* output the estimates for the number of reads deriving from each transcript. If the question is, why is this number not an integer, that's because the best estimate (the maximum likelihood estimate) is often not integral. Tools that simply count reads (e.g. HTSeq) produce integer counts, but these are in no way ""original read counts"" for the corresponding genes, and are usually less accurate (farther from the true number of fragments deriving from a transcript / gene) than the estimates produced by salmon. The fact that the best estimate is often not an integer is a direct result of the fact one is considering a statistical model and taking expectations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682:658,Usability,simpl,simply,658,"If you can already use DESeq2, then using tximport should not make it any harder at all. Given the tximport data, getting it into DESeq2 is as easy as. ```; dds <- DESeqDataSetFromTximport(txi, sampleTable, ~condition); ```. as shown in the [tximport vignette](https://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html#Introduction). . Regarding outputting ""original read counts""; salmon *does* output the estimates for the number of reads deriving from each transcript. If the question is, why is this number not an integer, that's because the best estimate (the maximum likelihood estimate) is often not integral. Tools that simply count reads (e.g. HTSeq) produce integer counts, but these are in no way ""original read counts"" for the corresponding genes, and are usually less accurate (farther from the true number of fragments deriving from a transcript / gene) than the estimates produced by salmon. The fact that the best estimate is often not an integer is a direct result of the fact one is considering a statistical model and taking expectations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:790,Deployability,release,release,790,"Hi @tamuanand,. Thank you for the detailed questions! Let me elaborate a bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:1916,Deployability,update,update,1916,"t results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alignment against the full genome on the same machine, you will need both versions, as quasi-mapping is supported only in the [RapMap](https://github.com/COMBINE-lab/RapMap/tree/develop-salmon), while indexing something on the scale of the genome when not using the [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index has tremendous memory requirements (as is not recommended ). 5 & 6) To re-iterate @k3yavi's answer --- the extra flags used in the pre-print were only for the purpose of holding as many variables fixed as possible when comparing different approaches. It continues to be recommended to use the VBEM over the EM; it seems to perform better with respect to the ways in which we can measure and such improvements have also been documented in [other work](https://www.ncbi.nlm.nih.gov/pubmed/23821651). The _main_ effect of `--mi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:633,Integrability,depend,depending,633,"Hi @tamuanand,. Thank you for the detailed questions! Let me elaborate a bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:2602,Modifiability,variab,variables,2602," decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alignment against the full genome on the same machine, you will need both versions, as quasi-mapping is supported only in the [RapMap](https://github.com/COMBINE-lab/RapMap/tree/develop-salmon), while indexing something on the scale of the genome when not using the [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index has tremendous memory requirements (as is not recommended ). 5 & 6) To re-iterate @k3yavi's answer --- the extra flags used in the pre-print were only for the purpose of holding as many variables fixed as possible when comparing different approaches. It continues to be recommended to use the VBEM over the EM; it seems to perform better with respect to the ways in which we can measure and such improvements have also been documented in [other work](https://www.ncbi.nlm.nih.gov/pubmed/23821651). The _main_ effect of `--mimicBT2` is to discard orphan alignments for the purposes of quantification. This is a more strict requirement than the default behavior of allowing orphans if there is no satisfactory alignment of both ends of a fragment. However, there is no obvious reason why it is better behavior than accounting for these orphan fragments (when appropriately adjusting the conditional probability given their distance from the transcript boundaries, as salmon does).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:1041,Performance,perform,perform,1041,"bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alig",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:2739,Performance,perform,perform,2739," decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alignment against the full genome on the same machine, you will need both versions, as quasi-mapping is supported only in the [RapMap](https://github.com/COMBINE-lab/RapMap/tree/develop-salmon), while indexing something on the scale of the genome when not using the [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index has tremendous memory requirements (as is not recommended ). 5 & 6) To re-iterate @k3yavi's answer --- the extra flags used in the pre-print were only for the purpose of holding as many variables fixed as possible when comparing different approaches. It continues to be recommended to use the VBEM over the EM; it seems to perform better with respect to the ways in which we can measure and such improvements have also been documented in [other work](https://www.ncbi.nlm.nih.gov/pubmed/23821651). The _main_ effect of `--mimicBT2` is to discard orphan alignments for the purposes of quantification. This is a more strict requirement than the default behavior of allowing orphans if there is no satisfactory alignment of both ends of a fragment. However, there is no obvious reason why it is better behavior than accounting for these orphan fragments (when appropriately adjusting the conditional probability given their distance from the transcript boundaries, as salmon does).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:1061,Usability,simpl,simply,1061,"bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alig",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:1090,Performance,perform,performance,1090,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:463,Safety,sanity check,sanity check,463,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:784,Safety,detect,detection,784,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:317,Usability,guid,guides,317,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:329,Usability,guid,guides,329,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1587,Availability,avail,available,1587,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1761,Availability,robust,robust,1761,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:188,Deployability,release,release,188,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:148,Modifiability,flexible,flexible,148,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1783,Modifiability,flexible,flexible,1783,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:358,Usability,learn,learned,358,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:644,Usability,intuit,intuitive,644,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1221,Usability,simpl,simply,1221,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:2085,Integrability,protocol,protocol,2085,"odel for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how much difference it can create in generating the estimates. . I Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:1990,Safety,avoid,avoid,1990,"approach right. I believe, It takes time and understanding to develop a good model for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:2013,Testability,log,logic,2013,"odel for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how much difference it can create in generating the estimates. . I Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:2595,Testability,test,test,2595,"odel for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how much difference it can create in generating the estimates. . I Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:1206,Usability,learn,learn,1206,"ort as many use cases as possible but since it involves multiples developers it's hard to know the intrinsic details about every use-case by all the developers. Having said that, salmon is primarily designed for bulk RNA-seq quantification and as the help page you shared says -- `noLengthCorrection` is experimental. . I agree with all three comments above regarding the paper I shared. In fact, that was my reading as well, like I said I am not sure about the intricacies involved with QuantSeq ""technology"" and that's why I forwarded to you for confirming. I understand the difference between Lexogen and Quantseq but what I meant was more data specific knowledge as I _personally_ don't find ""one model fits all"" kind of approach right. I believe, It takes time and understanding to develop a good model for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:1813,Usability,intuit,intuition,1813,"approach right. I believe, It takes time and understanding to develop a good model for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:243,Integrability,protocol,protocol,243,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:441,Integrability,protocol,protocol,441,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:469,Integrability,protocol,protocol,469,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1009,Integrability,depend,dependent,1009,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:481,Performance,perform,performing,481,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:616,Security,validat,validation,616,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:704,Testability,test,tested,704,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1400,Testability,test,test,1400,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:368,Usability,simpl,simply,368,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1090,Usability,learn,learning,1090,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1462,Usability,feedback,feedback,1462,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:513,Integrability,protocol,protocol,513,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:1028,Integrability,protocol,protocol,1028,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:685,Performance,optimiz,optimization,685,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:956,Performance,optimiz,optimize,956,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:292,Security,access,access,292,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:363,Testability,test,testing,363,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:722,Testability,test,testing,722,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:1074,Usability,feedback,feedback,1074,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:391,Availability,failure,failure,391,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2451,Availability,error,errors,2451,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2865,Availability,error,error,2865,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:54,Deployability,install,install,54,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:144,Deployability,install,installs,144,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:206,Deployability,install,installed,206,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:564,Deployability,release,releases,564,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2814,Deployability,install,installed,2814,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:250,Integrability,depend,dependency,250,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1994,Integrability,message,message,1994,"-p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.l",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:804,Performance,load,load,804,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1474,Testability,test,tests,1474,"_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/sof",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1623,Testability,test,tests,1623," -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/so",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2573,Testability,log,log,2573,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2683,Testability,log,log,2683,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2729,Testability,log,logs,2729,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2985,Testability,log,log,2985,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:3053,Testability,log,log,3053,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:3072,Testability,log,log,3072,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:3141,Testability,log,log,3141,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:312,Usability,guid,guidance,312,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554:994,Deployability,pipeline,pipeline,994,"Hi @Zhuxitong, . You can easily change the k-mer length used for indexing by passing the desired value to the `-k` option of the `index` command. So, that part isn't technically a problem. The bigger issue is that Ribo-seq data doesn't follow the same basic model as RNA-seq data. That is, the coverage variation in RNA-seq is more often an issue to be corrected (e.g. evidence of bias during library prep / sequencing), whereas it is integral to the interpretation of Ribo-seq data (i.e. the peaks are primary features of interest). Therefore, it's not clear to me that using any RNA-seq abundance estimation software on Ribo-seq data ""off-the-shelf"" is conceptually the right thing to do, though you are welcome to experiment with it. However, there is some interesting work on combining transcript abundance profiles with Ribo-seq data to infer isoform-level information in the Ribo-seq data. For example, [this recent pre-print](https://www.biorxiv.org/content/10.1101/582031v3) provides a pipeline for this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554
https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554:554,Usability,clear,clear,554,"Hi @Zhuxitong, . You can easily change the k-mer length used for indexing by passing the desired value to the `-k` option of the `index` command. So, that part isn't technically a problem. The bigger issue is that Ribo-seq data doesn't follow the same basic model as RNA-seq data. That is, the coverage variation in RNA-seq is more often an issue to be corrected (e.g. evidence of bias during library prep / sequencing), whereas it is integral to the interpretation of Ribo-seq data (i.e. the peaks are primary features of interest). Therefore, it's not clear to me that using any RNA-seq abundance estimation software on Ribo-seq data ""off-the-shelf"" is conceptually the right thing to do, though you are welcome to experiment with it. However, there is some interesting work on combining transcript abundance profiles with Ribo-seq data to infer isoform-level information in the Ribo-seq data. For example, [this recent pre-print](https://www.biorxiv.org/content/10.1101/582031v3) provides a pipeline for this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:2154,Availability,down,downstream,2154,"xpect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this inf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:3218,Availability,down,downstream,3218,"which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again for the detailed report! We'll look into the logging issue, and please let me know if my description above answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:303,Deployability,update,updates,303,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:745,Deployability,update,updates,745,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:863,Performance,perform,performs,863,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:1849,Performance,perform,performing,1849,"xpect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this inf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:2301,Performance,optimiz,optimization,2301,"tstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again f",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:2557,Performance,perform,performing,2557,"which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again for the detailed report! We'll look into the logging issue, and please let me know if my description above answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:3308,Testability,log,logging,3308,"which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again for the detailed report! We'll look into the logging issue, and please let me know if my description above answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:522,Usability,learn,learned,522,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:111,Testability,test,test,111,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:232,Testability,test,test,232,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:415,Testability,test,test,415,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:784,Testability,test,test,784,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:677,Usability,simpl,simply,677,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940:16,Deployability,update,update,16,"Ok all; another update. The issue I raise above still exists (differences between calls to `ksw_extz` and `ksw_extz2sse`). *However*, I think that what is happening in this case is actually explained more simply. That is, the positions being reported by salmon are _correct_ given the optimal alignment. Specifically, salmon is performing an end-to-end alignment of the read, and the optimal alignment here includes an indel of length 3 in the initial portion of the read. If we were outputting the CIGAR string along with the position, then the bases would line up because the ""off by 3"" issue that happens above for the reads would be addressed when walking the CIGAR. However, we don't (currently) output the CIGAR — rather, we output a decoy CIGAR that does not represent the optimal alignment as computed by ksw2. So, if we assume all matches / mismatches (an indel-free prefix for this read), then we see the position shift noted in the initial bug report. I think the easiest solution, for the time being at least, is to report the position as if the prefix before the first MEM is indel free under the optimal alignment (even if it is not and the optimal score reflects that). However, if there are other suggestions for the best way to address this, I'm open to those as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940:328,Performance,perform,performing,328,"Ok all; another update. The issue I raise above still exists (differences between calls to `ksw_extz` and `ksw_extz2sse`). *However*, I think that what is happening in this case is actually explained more simply. That is, the positions being reported by salmon are _correct_ given the optimal alignment. Specifically, salmon is performing an end-to-end alignment of the read, and the optimal alignment here includes an indel of length 3 in the initial portion of the read. If we were outputting the CIGAR string along with the position, then the bases would line up because the ""off by 3"" issue that happens above for the reads would be addressed when walking the CIGAR. However, we don't (currently) output the CIGAR — rather, we output a decoy CIGAR that does not represent the optimal alignment as computed by ksw2. So, if we assume all matches / mismatches (an indel-free prefix for this read), then we see the position shift noted in the initial bug report. I think the easiest solution, for the time being at least, is to report the position as if the prefix before the first MEM is indel free under the optimal alignment (even if it is not and the optimal score reflects that). However, if there are other suggestions for the best way to address this, I'm open to those as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940:205,Usability,simpl,simply,205,"Ok all; another update. The issue I raise above still exists (differences between calls to `ksw_extz` and `ksw_extz2sse`). *However*, I think that what is happening in this case is actually explained more simply. That is, the positions being reported by salmon are _correct_ given the optimal alignment. Specifically, salmon is performing an end-to-end alignment of the read, and the optimal alignment here includes an indel of length 3 in the initial portion of the read. If we were outputting the CIGAR string along with the position, then the bases would line up because the ""off by 3"" issue that happens above for the reads would be addressed when walking the CIGAR. However, we don't (currently) output the CIGAR — rather, we output a decoy CIGAR that does not represent the optimal alignment as computed by ksw2. So, if we assume all matches / mismatches (an indel-free prefix for this read), then we see the position shift noted in the initial bug report. I think the easiest solution, for the time being at least, is to report the position as if the prefix before the first MEM is indel free under the optimal alignment (even if it is not and the optimal score reflects that). However, if there are other suggestions for the best way to address this, I'm open to those as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566:538,Availability,down,down,538,"From my somewhat superficial understanding, I have feeling that it is the only the soft clipping (deletion at either end) that causes the problem (and not INDELs in general). I think it might be better if the CIGAR string contained the soft clipping operations (the POS would still need to be shifted) of course. Right now when one visualizes the BAM file various distracting artifacts manifest themselves with both salmon and kallisto even when the POS field is correct. See the image below:. ![Alignments](https://www.ialbert.me/static/down/pseudo_alignments/pseudo_aln.png). (Top Kallisto, second Salmon, bottom Hisat. ). The soft clipped sequences are not marked as such, therefore lead to ugly misalignment at the ends, that in turn dominate the visualization. . Ideally, the pseudo-bam should look a little more like Hisat, I don't know how feasible that is though, perhaps knowing that only the ends need to be fixed makes for a simpler solution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566:936,Usability,simpl,simpler,936,"From my somewhat superficial understanding, I have feeling that it is the only the soft clipping (deletion at either end) that causes the problem (and not INDELs in general). I think it might be better if the CIGAR string contained the soft clipping operations (the POS would still need to be shifted) of course. Right now when one visualizes the BAM file various distracting artifacts manifest themselves with both salmon and kallisto even when the POS field is correct. See the image below:. ![Alignments](https://www.ialbert.me/static/down/pseudo_alignments/pseudo_aln.png). (Top Kallisto, second Salmon, bottom Hisat. ). The soft clipped sequences are not marked as such, therefore lead to ugly misalignment at the ends, that in turn dominate the visualization. . Ideally, the pseudo-bam should look a little more like Hisat, I don't know how feasible that is though, perhaps knowing that only the ends need to be fixed makes for a simpler solution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:867,Deployability,release,release,867,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:434,Testability,test,test,434,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:545,Testability,test,tested,545,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:773,Testability,test,testing,773,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:605,Usability,feedback,feedback,605,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:401,Energy Efficiency,schedul,scheduling,401,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:250,Modifiability,config,configured,250,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:599,Testability,log,log,599,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:737,Testability,log,logs,737,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:1243,Testability,log,log,1243,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:1339,Testability,log,log,1339,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:1492,Usability,clear,clear,1492,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:502,Availability,redundant,redundant,502,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:411,Performance,perform,perform,411,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:502,Safety,redund,redundant,502,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:462,Security,validat,validateMappings,462,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:203,Usability,clear,clear,203,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090:201,Deployability,install,installs,201,"it's not clear if the right fix is to pin the boost version [here](https://github.com/bioconda/bioconda-recipes/blob/master/recipes/salmon/meta.yaml), or just to go with the ""always use conda-forge on installs"" strategy in the documentation. I will consult with experts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090
https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090:9,Usability,clear,clear,9,"it's not clear if the right fix is to pin the boost version [here](https://github.com/bioconda/bioconda-recipes/blob/master/recipes/salmon/meta.yaml), or just to go with the ""always use conda-forge on installs"" strategy in the documentation. I will consult with experts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:3023,Availability,robust,robust,3023,"h is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). This will tell you if something strange might be going on in those samples, like extensive rRNA contamination, that would lead to the observed mapping rates. You could also get this information using a tool like STAR, by asking it to produce both genomic and transcriptomi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2780,Energy Efficiency,adapt,adapters,2780," described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). Thi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2780,Integrability,adapter,adapters,2780," described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). Thi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2780,Modifiability,adapt,adapters,2780," described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). Thi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2690,Performance,perform,performs,2690,"efore used for quantification by default. Specifically, starting with 0.14, ""dovetail"" alignments [(as described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:1147,Security,validat,validate,1147," to generally be similar to that of quasi-mapping, but there are some important exceptions. You can find some aggregate statistics in supplementary figure 1 of the [pre-print that introduces selective alignment](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf); ; ![image](https://user-images.githubusercontent.com/361470/73905651-08d4fd00-486e-11ea-91f4-9f167f54f676.png). Here, ""orcale"" is a method that aligns the reads both to the transcriptome with Bowtie2 and the genome with STAR, and removes reads from the Bowtie2 `bam` that seem to be spuriously aligned to the transcriptome (they align to the genome outside of an annotated transcriptome with a better score than that assigned by Bowtie2 within the transcriptome). Here, you can see that in most cases most methods map a similar number of reads, but there are definitely samples where methods map more reads than the oracle, and sometimes quasi-mapping maps quite a few more. This is, to a large extent, because it doesn't validate those mappings and some of them may be spurious (i.e. the exact matches used to find the mapping in the given location would not support a high quality alignment at that location). (2) This is certainly possible that some samples get very little to no mapping. _However_, there are a few points worth noting about how the data are processed that is worth being aware of before you write such samples off. * There is a change in default behavior between salmon < 0.13 and >= 0.13 with which mappings are considered as ""concordant"" and therefore used for quantification by default. Specifically, starting with 0.14, ""dovetail"" alignments [(as described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you c",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2407,Usability,simpl,simply,2407,"how the data are processed that is worth being aware of before you write such samples off. * There is a change in default behavior between salmon < 0.13 and >= 0.13 with which mappings are considered as ""concordant"" and therefore used for quantification by default. Specifically, starting with 0.14, ""dovetail"" alignments [(as described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:303,Deployability,install,install,303,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:342,Deployability,install,install,342,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:414,Deployability,install,install,414,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:793,Testability,log,logout,793,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:484,Usability,simpl,simplest,484,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:511,Usability,simpl,simply,511,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:486,Deployability,release,releases,486,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:869,Deployability,release,release,869,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:1440,Deployability,release,release,1440,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:90,Integrability,depend,depends,90,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:1231,Usability,responsiv,responsive,1231,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443:211,Deployability,release,releases,211,"Thank you for verifying @zhangchipku, and thank you very much for the kind words! We appreciate the feedback and input from our users like yourself. We'll prioritize the soft-clipping functionality for upcoming releases (maybe even the next if we can make that work in time). For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443:332,Energy Efficiency,efficient,efficient,332,"Thank you for verifying @zhangchipku, and thank you very much for the kind words! We appreciate the feedback and input from our users like yourself. We'll prioritize the soft-clipping functionality for upcoming releases (maybe even the next if we can make that work in time). For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443:100,Usability,feedback,feedback,100,"Thank you for verifying @zhangchipku, and thank you very much for the kind words! We appreciate the feedback and input from our users like yourself. We'll prioritize the soft-clipping functionality for upcoming releases (maybe even the next if we can make that work in time). For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3155,Availability,down,downstream,3155,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3382,Availability,down,downstream,3382,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:266,Energy Efficiency,adapt,adapter,266,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:529,Energy Efficiency,adapt,adapter,529,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:754,Energy Efficiency,adapt,adapter-trimming,754,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:853,Energy Efficiency,adapt,adapters,853,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:929,Energy Efficiency,adapt,adapter,929,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:997,Energy Efficiency,adapt,adapter-trimming,997,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1042,Energy Efficiency,adapt,adapter-trimming,1042,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1161,Energy Efficiency,adapt,adapter,1161,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1255,Energy Efficiency,adapt,adapter,1255,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1324,Energy Efficiency,adapt,adapters,1324,"mming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1401,Energy Efficiency,adapt,adapter,1401,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1583,Energy Efficiency,adapt,adapter,1583,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1805,Energy Efficiency,adapt,adapter,1805,"quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mappin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3423,Energy Efficiency,adapt,adapter,3423,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3494,Energy Efficiency,adapt,adapters,3494,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:266,Integrability,adapter,adapter,266,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:529,Integrability,adapter,adapter,529,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:754,Integrability,adapter,adapter-trimming,754,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:853,Integrability,adapter,adapters,853,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:929,Integrability,adapter,adapter,929,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:997,Integrability,adapter,adapter-trimming,997,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1042,Integrability,adapter,adapter-trimming,1042,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1161,Integrability,adapter,adapter,1161,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1255,Integrability,adapter,adapter,1255,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1324,Integrability,adapter,adapters,1324,"mming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1401,Integrability,adapter,adapter,1401,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1583,Integrability,adapter,adapter,1583,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1805,Integrability,adapter,adapter,1805,"quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mappin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3423,Integrability,adapter,adapter,3423,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3494,Integrability,adapter,adapters,3494,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:266,Modifiability,adapt,adapter,266,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:529,Modifiability,adapt,adapter,529,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:754,Modifiability,adapt,adapter-trimming,754,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:853,Modifiability,adapt,adapters,853,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:929,Modifiability,adapt,adapter,929,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:997,Modifiability,adapt,adapter-trimming,997,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1042,Modifiability,adapt,adapter-trimming,1042,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1161,Modifiability,adapt,adapter,1161,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1255,Modifiability,adapt,adapter,1255,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1324,Modifiability,adapt,adapters,1324,"mming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1401,Modifiability,adapt,adapter,1401,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1583,Modifiability,adapt,adapter,1583,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1805,Modifiability,adapt,adapter,1805,"quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mappin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3423,Modifiability,adapt,adapter,3423,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3494,Modifiability,adapt,adapters,3494,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1917,Performance,throughput,throughput,1917,"trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.bios",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1409,Safety,detect,detection,1409,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1431,Safety,detect,detect,1431,"igure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1591,Safety,detect,detection,1591,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:56,Testability,log,logic,56,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:226,Testability,log,logic,226,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3297,Testability,log,logic,3297,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:2391,Usability,simpl,simply,2391,"detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I us",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756:286,Integrability,depend,depending,286,"Thanks @rob-p I am glad you and others have found it useful. The actual bbmap.sh and bbduk.sh commands for SE data are in the links to Phil Ewels' multiqc GH. . Just like salmon indexing kmer size choice, one can tinker with the **_```k, hdist, minq and other parameters```_** of bbduk depending on how good/bad the data is. Needless to state, bbduk is the swiss-army-knife for sequence reads quality assessment with whole range of parameters to tweak . https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/ suggests; ![image](https://user-images.githubusercontent.com/8467214/78302368-a3695980-7508-11ea-990d-4b6e008e3f07.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756:511,Usability,guid,guide,511,"Thanks @rob-p I am glad you and others have found it useful. The actual bbmap.sh and bbduk.sh commands for SE data are in the links to Phil Ewels' multiqc GH. . Just like salmon indexing kmer size choice, one can tinker with the **_```k, hdist, minq and other parameters```_** of bbduk depending on how good/bad the data is. Needless to state, bbduk is the swiss-army-knife for sequence reads quality assessment with whole range of parameters to tweak . https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/ suggests; ![image](https://user-images.githubusercontent.com/8467214/78302368-a3695980-7508-11ea-990d-4b6e008e3f07.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756:523,Usability,guid,guide,523,"Thanks @rob-p I am glad you and others have found it useful. The actual bbmap.sh and bbduk.sh commands for SE data are in the links to Phil Ewels' multiqc GH. . Just like salmon indexing kmer size choice, one can tinker with the **_```k, hdist, minq and other parameters```_** of bbduk depending on how good/bad the data is. Needless to state, bbduk is the swiss-army-knife for sequence reads quality assessment with whole range of parameters to tweak . https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/ suggests; ![image](https://user-images.githubusercontent.com/8467214/78302368-a3695980-7508-11ea-990d-4b6e008e3f07.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:60,Modifiability,variab,variable,60,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:231,Modifiability,variab,variable,231,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:313,Modifiability,variab,variable,313,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:297,Usability,simpl,simply,297,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839:22,Performance,perform,performs,22,"Hi,; Basically Alevin performs CB sequence correction within 1 distance hamming ball, the intuition being the set of real CB should ideally be more than 1 edit distance away.; Here I think the x axis gives you the count of reads for a CB before sequence correction and on y axis post sequence correction. Hope it helps",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839
https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839:90,Usability,intuit,intuition,90,"Hi,; Basically Alevin performs CB sequence correction within 1 distance hamming ball, the intuition being the set of real CB should ideally be more than 1 edit distance away.; Here I think the x axis gives you the count of reads for a CB before sequence correction and on y axis post sequence correction. Hope it helps",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890:662,Safety,detect,detected,662,"Thank you for the clear and thorough explanation, @rob-p . Now I understand exactly why this is happening. I like your idea for the “throw-away” run for Salmon, and the short example command you sketched out is exactly what I had said in mind as I read your words. Reworking the core Salmon algorithm to do some gymnastics with re-processing the first 10,000 reads would not be elegant or worth your time. I think the workaround you proposed is a perfectly good solution. If in the long run many other people find this useful, perhaps an easier fix would be to make a new command in Salmon that just bails after the first 10k reads automatically and returns the detected library orientation upon termination of the command; e.g. in Bash:. `mylibtype=$(salmon quant —getLibType -r reads.fq.gz)`; `salmon quant —libType $mylibtype -r reads.fq.gz`. Thank you for the great software and for being so attentive to detail and our questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890:18,Usability,clear,clear,18,"Thank you for the clear and thorough explanation, @rob-p . Now I understand exactly why this is happening. I like your idea for the “throw-away” run for Salmon, and the short example command you sketched out is exactly what I had said in mind as I read your words. Reworking the core Salmon algorithm to do some gymnastics with re-processing the first 10,000 reads would not be elegant or worth your time. I think the workaround you proposed is a perfectly good solution. If in the long run many other people find this useful, perhaps an easier fix would be to make a new command in Salmon that just bails after the first 10k reads automatically and returns the detected library orientation upon termination of the command; e.g. in Bash:. `mylibtype=$(salmon quant —getLibType -r reads.fq.gz)`; `salmon quant —libType $mylibtype -r reads.fq.gz`. Thank you for the great software and for being so attentive to detail and our questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890
https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699:626,Safety,avoid,avoid,626,"Hi @mej54,. First, thanks for using salmon and for providing detailed feedback! There are two main points I'd like to make in response to the points you raise. . First, v0.9.1 is _very_ old, and there have been a large number of bug fixes and substantial improvements to salmon since that version (though it's much better than people who are still using 0.8.2 from, like, 3 years ago!). Specifically, I'd highly recommend upgrading to the latest version (1.1.0, with 1.2.0 coming out shortly). We've added (and made standard) selective-alignment, which is a procedure that provides alignment scoring for the assigned reads to avoid spurious mappings that arise with fast lightweight mapping procedures. Second, the observation of mismatching bases at the provided alignment location is the expected behavior with the mappings written by salmon with the `--writeMappings` option. Specifically, while newer versions of salmon (0.15.0 and greater) will do alignment scoring and removal of low score alignments by default, salmon still does not compute or write out a full CIGAR string for its alignments. Instead, it uses a _score-only_ dynamic program to compute the optimal alignment score at the given location, but it ""spoofs"" the CIGAR string. Thus, if there is e.g. a small indel in the read, this will show up in an IGV visualization as a large number of mismatches after that indeed location. I'm not sure that is what is happening in the screenshot you show above, and, in fact, may of these mappings may disappear with selective-alignment. However, it will definitely still be possible to see a cigar string showing full matches, where there are mismatches in IGV. This is intended behavior due to score-only alignment. However, it's also true that newer versions of salmon will report the alignment score in an `AS` tag, so that you can see how high the alignment quality was at the particular location.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699
https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699:70,Usability,feedback,feedback,70,"Hi @mej54,. First, thanks for using salmon and for providing detailed feedback! There are two main points I'd like to make in response to the points you raise. . First, v0.9.1 is _very_ old, and there have been a large number of bug fixes and substantial improvements to salmon since that version (though it's much better than people who are still using 0.8.2 from, like, 3 years ago!). Specifically, I'd highly recommend upgrading to the latest version (1.1.0, with 1.2.0 coming out shortly). We've added (and made standard) selective-alignment, which is a procedure that provides alignment scoring for the assigned reads to avoid spurious mappings that arise with fast lightweight mapping procedures. Second, the observation of mismatching bases at the provided alignment location is the expected behavior with the mappings written by salmon with the `--writeMappings` option. Specifically, while newer versions of salmon (0.15.0 and greater) will do alignment scoring and removal of low score alignments by default, salmon still does not compute or write out a full CIGAR string for its alignments. Instead, it uses a _score-only_ dynamic program to compute the optimal alignment score at the given location, but it ""spoofs"" the CIGAR string. Thus, if there is e.g. a small indel in the read, this will show up in an IGV visualization as a large number of mismatches after that indeed location. I'm not sure that is what is happening in the screenshot you show above, and, in fact, may of these mappings may disappear with selective-alignment. However, it will definitely still be possible to see a cigar string showing full matches, where there are mismatches in IGV. This is intended behavior due to score-only alignment. However, it's also true that newer versions of salmon will report the alignment score in an `AS` tag, so that you can see how high the alignment quality was at the particular location.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699
https://github.com/COMBINE-lab/salmon/issues/494#issuecomment-601087573:189,Usability,clear,clear,189,"> Yep use --end 5 --umiLength 8 --barcodeLength 18. Thanks again, I forgot, however, to specify that the sequence is composed first of BC and then of UMI (BC + UMI) (I'm not sure if it was clear in the issue).; Does the command remain the same?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/494#issuecomment-601087573
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:19,Deployability,patch,patch,19,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:131,Deployability,patch,patch,131,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:502,Deployability,install,install,502,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:733,Deployability,install,install,733,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1383,Deployability,install,install,1383,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1514,Deployability,release,releases,1514,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1578,Deployability,release,releases,1578,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1685,Deployability,patch,patches,1685,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1712,Deployability,release,releases,1712,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:311,Integrability,message,message,311,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:389,Integrability,message,message,389,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:34,Usability,simpl,simple,34,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:339,Availability,error,error,339,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:393,Availability,down,download,393,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:729,Availability,error,error,729,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:209,Deployability,install,install,209,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:286,Deployability,install,installing,286,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:439,Deployability,update,update,439,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:479,Deployability,update,update,479,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:543,Deployability,install,install,543,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:693,Deployability,install,installation,693,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:581,Performance,cache,cache,581,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:37,Usability,simpl,simple,37,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:612,Availability,down,downstream,612,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:670,Security,access,access,670,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:225,Usability,simpl,simply,225,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:680,Usability,simpl,simply,680,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:823,Deployability,release,released,823,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:1049,Security,hash,hash,1049,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:530,Usability,simpl,simply,530,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:1589,Usability,clear,clears,1589,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:430,Availability,error,errors,430,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:437,Availability,down,downstream,437,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:594,Availability,error,error,594,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:129,Testability,log,log,129,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:39,Usability,simpl,simple,39,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649:925,Availability,ping,pinging,925,"Ok, I figured it out :) — these two *decoy* references are (1) identical with each other and (2) collide with *another* decoy reference. Currently, the way we process decoys, we don't allow duplicate decoys (it makes even less sense to allow duplicate decoys than to allow duplicate transcripts). However, the reason indexing worked with `k=17` is not because of `k` but because of the `--keepDuplicates` flag. With that flag, these decoys get added. I think the right thing for us to do on our part is to remove duplicate *decoys* if they appear in the reference and the user has not passed `--keepDuplicates`. . However, for the time being, I think the best thing to do is simply to remove `AABR07022993.1` and `AABR07023006.1` from the `toplevel` file and from `decoys.txt`, since the sequence they contain is already represented in the decoy part of the index. This will represent a full and comprehensive SAF index. I'm pinging @k3yavi to see if he has any good idea about the easiest way to cull these duplicate refs from the input files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649:675,Usability,simpl,simply,675,"Ok, I figured it out :) — these two *decoy* references are (1) identical with each other and (2) collide with *another* decoy reference. Currently, the way we process decoys, we don't allow duplicate decoys (it makes even less sense to allow duplicate decoys than to allow duplicate transcripts). However, the reason indexing worked with `k=17` is not because of `k` but because of the `--keepDuplicates` flag. With that flag, these decoys get added. I think the right thing for us to do on our part is to remove duplicate *decoys* if they appear in the reference and the user has not passed `--keepDuplicates`. . However, for the time being, I think the best thing to do is simply to remove `AABR07022993.1` and `AABR07023006.1` from the `toplevel` file and from `decoys.txt`, since the sequence they contain is already represented in the decoy part of the index. This will represent a full and comprehensive SAF index. I'm pinging @k3yavi to see if he has any good idea about the easiest way to cull these duplicate refs from the input files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355:49,Usability,feedback,feedback,49,"Hi @tamuanand and @uros-sipetic,. Thanks for the feedback on this! I just cut v1.2.1 which ""fixes"" the behavior. It will simply discard any duplicate _decoy_ sequences, which resolves this problem without requiring manual intervention.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355:121,Usability,simpl,simply,121,"Hi @tamuanand and @uros-sipetic,. Thanks for the feedback on this! I just cut v1.2.1 which ""fixes"" the behavior. It will simply discard any duplicate _decoy_ sequences, which resolves this problem without requiring manual intervention.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-617275380:35,Usability,clear,clear,35,@rob-p Thank you very much for the clear explanation. That answers my question.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-617275380
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047:169,Integrability,message,message,169,"Hi @summerrfair ,. I can't see anything obviously wrong with the command line. Do you have a small example of the transcripts.fa and myseq.bam file you could share? The message indicates that salmon thinks its running in mapping-based mode (with input fastq files), but you are clearly running in alignment based mode. Is the behavior any different if you put the -a argument first?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047:278,Usability,clear,clearly,278,"Hi @summerrfair ,. I can't see anything obviously wrong with the command line. Do you have a small example of the transcripts.fa and myseq.bam file you could share? The message indicates that salmon thinks its running in mapping-based mode (with input fastq files), but you are clearly running in alignment based mode. Is the behavior any different if you put the -a argument first?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-622566744:1360,Usability,feedback,feedback,1360,"Hi Jason,. Thanks for the kind words, and for the detailed issue! This is sort of a tough one, since salmon tries to work out the relative abundance of these different transcripts in the way that maximizes the likelihood of the observed data (or, more specifically, maximized the ELBO in the variational Bayesian framework). Of course, you seem to know that already :). One thought that comes to mind, though, is the following. The default settings for salmon favor sparsity of the solution pretty strongly — it is important to explain the data with as few distinct transcripts as possible. While this often seems a nice thing to do, it can tend to lead to the type of behavior that you are seeing. The way to modify this is to alter the `--vbPrior` parameter to salmon. Basically, this number encodes the prior observation weight that should be attributed to each isoform _before_ accounting for the data. The default value for this parameter is 0.01. Small values (<< 1) are sparsity inducing, while larger values are not (and values close to 1 and above actually penalize sparsity). You could try quantifying with a few (larger) values of this parameter to see if any of them give allocations among these isoforms that seem to make more sense to you (or that agree more strongly with any alternative assays). Actually, I'd be very interested in hearing any feedback you have about this if you find a setting that is more in line with what you expect!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-622566744
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1011,Availability,down,downstream,1011,"Thanks for the feedback!. I tried playing with the vbPrior setting and observed that, as you noted, higher increases of the vbPrior tended to flatten out the read apportionment, such that as the vbPrior increased the two transcripts became increasingly similar in their final expression (presumably they would eventually hit 50/50). It's good to know how that settings affects my data, but this is not quite what I was hoping for... . Ideally, the short transcript would get nearly *all* of the reads, rather than splitting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large de",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:2158,Deployability,pipeline,pipeline,2158,"tting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. So, for now my workaround is to just modify the transcripts so they are non-overlapping in the transcriptome fasta or to manually count reads after looking at the alignments, but I'd love to hear any more thoughts you have on this problem. Thanks,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1814,Safety,detect,detect,1814,"tting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. So, for now my workaround is to just modify the transcripts so they are non-overlapping in the transcriptome fasta or to manually count reads after looking at the alignments, but I'd love to hear any more thoughts you have on this problem. Thanks,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:15,Usability,feedback,feedback,15,"Thanks for the feedback!. I tried playing with the vbPrior setting and observed that, as you noted, higher increases of the vbPrior tended to flatten out the read apportionment, such that as the vbPrior increased the two transcripts became increasingly similar in their final expression (presumably they would eventually hit 50/50). It's good to know how that settings affects my data, but this is not quite what I was hoping for... . Ideally, the short transcript would get nearly *all* of the reads, rather than splitting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large de",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1049,Usability,intuit,intuition,1049," read apportionment, such that as the vbPrior increased the two transcripts became increasingly similar in their final expression (presumably they would eventually hit 50/50). It's good to know how that settings affects my data, but this is not quite what I was hoping for... . Ideally, the short transcript would get nearly *all* of the reads, rather than splitting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1631,Usability,intuit,intuition,1631,"tting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. So, for now my workaround is to just modify the transcripts so they are non-overlapping in the transcriptome fasta or to manually count reads after looking at the alignments, but I'd love to hear any more thoughts you have on this problem. Thanks,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:510,Availability,down,down,510,"Hi Jason,. Thanks for the super-detailed feedback! A couple of thoughts / questions:. > FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:684,Availability,down,downstream,684,"Hi Jason,. Thanks for the super-detailed feedback! A couple of thoughts / questions:. > FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:2001,Deployability,pipeline,pipeline,2001,"er, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and nei",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:4212,Energy Efficiency,efficient,efficiently,4212,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:4305,Energy Efficiency,efficient,efficiently,4305,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1657,Safety,detect,detect,1657," not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitiga",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:41,Usability,feedback,feedback,41,"Hi Jason,. Thanks for the super-detailed feedback! A couple of thoughts / questions:. > FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1113,Usability,intuit,intuition,1113,"ias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1182,Usability,intuit,intuition,1182,"together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1464,Usability,learn,learn,1464,"sue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-sy",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1575,Usability,learn,learns,1575,"alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:2911,Usability,intuit,intuitions,2911,"d handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would b",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:3922,Usability,clear,clear,3922,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:4109,Usability,intuit,intuition,4109,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2001,Availability,down,downstream,2001,"nto_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. Yes! I've been analyzing a large dataset and my real motivating problem was not really the example I posted above, but distinguishing between pre-processed and fully-processed non-coding RNA transcripts. I'm attaching an image showing an example ncRNA; the two tracks are the same data, but the lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:4650,Availability,down,downstream,4650,"the unique-region-derived prior... . But as I think about it... I realize I don't *really* know the underlying algorithmic details of the existing implements. But it would be **amazing** if you could incorporate this type of information into Salmon. I really hope some progress can be made here! . Thanks again for helping me out and showing interest in the motivating problem!. P.S.,; As a total aside, I've been working with this large yeast RNAseq dataset and eventually reached the same conclusions as the selective alignment paper and other recent ones; that is, the most important aspect for getting good transcript-level quantifications is not aligning to the genome vs. aligning to the transcriptome, but rather having an accurate transcriptome annotation to begin with. I saw **huge** gains from updating my transcriptome annotation to include UTRs, especially given differences in coverage bias between samples... for example, if the actual transcript is 500 bp but the gene body is only 200 bp, slight coverage biases can propagate non-linearly and cause huge problems downstream. This got me thinking... if the end goal is differential expression analysis (and obviously this is not *always* the end goal), what if we just discard the notion of a pre-defined transcriptome and stick with equivalence classes, then do differential expression analysis on the equivalence classes themselves (perhaps calculated against the whole genome... this is feasible in yeast, maybe not in humans), then only after discovering significant differential expression one could work backwards to interpret the changes. Is this a crazy idea? Or not crazy at all and already routine? I know salmon can dump the counts to each equivalence class already so it's not hard for me to *do* what I just described, but I'm wondering if you have any opinions/insights into this idea. Thanks again!. ![snr40_isoforms](https://user-images.githubusercontent.com/10292386/81047610-a17c1880-8e6f-11ea-8012-a6695afd68db.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:3445,Deployability,update,updates,3445,"e of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique sequences between the transcripts... the read depth over unique regions updates the prior on the overall transcript abundance, and the otherwise non-unique reads get apportioned in accordance with the unique-region-derived prior... . But as I think about it... I realize I don't *really* know the underlying algorithmic details of the existing implements. But it would be **amazing** if you could incorporate this type of information into Salmon. I really hope some progress can be made here! . Thanks again for helping me out and showing interest in the motivating problem!. P.S.,; As a total aside, I've been working with this large yeast RNAseq dataset and eventually reached the same conclusions as the selective alignment paper and other recent ones; that is, the most important aspect for getting good transcript-level quantifications is not aligning to the genome vs. aligning to the transcriptome, but rather having an accurate transcriptome annotation to begin with. I saw **huge** gains from updating my transcriptome annotation to include UTRs, especially given differences in coverage bias between samples... for example,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2613,Energy Efficiency,efficient,efficiently,2613,"l have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique sequences between the transcripts... the read depth over unique regions updates the prior on the overall transcript abundance, and the otherwise non-unique reads get ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:5236,Integrability,rout,routine,5236,"the unique-region-derived prior... . But as I think about it... I realize I don't *really* know the underlying algorithmic details of the existing implements. But it would be **amazing** if you could incorporate this type of information into Salmon. I really hope some progress can be made here! . Thanks again for helping me out and showing interest in the motivating problem!. P.S.,; As a total aside, I've been working with this large yeast RNAseq dataset and eventually reached the same conclusions as the selective alignment paper and other recent ones; that is, the most important aspect for getting good transcript-level quantifications is not aligning to the genome vs. aligning to the transcriptome, but rather having an accurate transcriptome annotation to begin with. I saw **huge** gains from updating my transcriptome annotation to include UTRs, especially given differences in coverage bias between samples... for example, if the actual transcript is 500 bp but the gene body is only 200 bp, slight coverage biases can propagate non-linearly and cause huge problems downstream. This got me thinking... if the end goal is differential expression analysis (and obviously this is not *always* the end goal), what if we just discard the notion of a pre-defined transcriptome and stick with equivalence classes, then do differential expression analysis on the equivalence classes themselves (perhaps calculated against the whole genome... this is feasible in yeast, maybe not in humans), then only after discovering significant differential expression one could work backwards to interpret the changes. Is this a crazy idea? Or not crazy at all and already routine? I know salmon can dump the counts to each equivalence class already so it's not hard for me to *do* what I just described, but I'm wondering if you have any opinions/insights into this idea. Thanks again!. ![snr40_isoforms](https://user-images.githubusercontent.com/10292386/81047610-a17c1880-8e6f-11ea-8012-a6695afd68db.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2406,Safety,detect,detect,2406,"he lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique seq",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:1409,Testability,log,log,1409,".pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). Excellent papers! Definitely going to give those tools a try on my data. > So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. Yes! I've been analyzing a large dataset and my real motivating problem was not really the example I posted above, but distinguishing between pre-processed and fully-processed non-coding RNA transcripts. I'm attaching an image showing an example ncRNA; the two tracks are the same data, but the lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity mea",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2256,Testability,benchmark,benchmark,2256," between pre-processed and fully-processed non-coding RNA transcripts. I'm attaching an image showing an example ncRNA; the two tracks are the same data, but the lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth varianc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2510,Usability,intuit,intuition,2510,"l have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique sequences between the transcripts... the read depth over unique regions updates the prior on the overall transcript abundance, and the otherwise non-unique reads get ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:824,Availability,failure,failure,824,"Hi @mohsenzakeri ,. Sorry for the slow reply! I’ve done some sleuthing and have (seemingly) figured out what’s going on. I compared tons of overlapping transcript scenarios and played with the salmon options and concluded the following:. 1. **Success scenarios:** Generally, with default options, Salmon does an excellent job at assigning reads to overlapping transcripts the same way that a human would. Whether or not the transcripts overlap slightly or one is entirely contained within another is irrelevant. 2. **Failure scenarios:** In some scenarios with overlapping transcripts, read assignment can be very strange and unintuitive, especially when one of the transcript isoforms is much more abundant than the other (the abundant transcript tends to “steal” reads from the less abundant one). 3. Both the success and failure scenarios are **due to the length bias model** used when estimating the abundance of transcripts. **Totally disabling this with the —noLengthCorrection flag** (NOT the —no**Effective**LengthCorrection flag) **actually *creates* the transcript within a transcript failure scenario that I mistakenly thought was the original issue.** That is, when length bias modeling is turned off, then longer transcripts will always get assigned *all* of the reads that multimap to shorter transcripts.; -Therefore... if you *did* want to tackle the transcript within a transcript scenario to build a coverage bias model, you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:1095,Availability,failure,failure,1095," I’ve done some sleuthing and have (seemingly) figured out what’s going on. I compared tons of overlapping transcript scenarios and played with the salmon options and concluded the following:. 1. **Success scenarios:** Generally, with default options, Salmon does an excellent job at assigning reads to overlapping transcripts the same way that a human would. Whether or not the transcripts overlap slightly or one is entirely contained within another is irrelevant. 2. **Failure scenarios:** In some scenarios with overlapping transcripts, read assignment can be very strange and unintuitive, especially when one of the transcript isoforms is much more abundant than the other (the abundant transcript tends to “steal” reads from the less abundant one). 3. Both the success and failure scenarios are **due to the length bias model** used when estimating the abundance of transcripts. **Totally disabling this with the —noLengthCorrection flag** (NOT the —no**Effective**LengthCorrection flag) **actually *creates* the transcript within a transcript failure scenario that I mistakenly thought was the original issue.** That is, when length bias modeling is turned off, then longer transcripts will always get assigned *all* of the reads that multimap to shorter transcripts.; -Therefore... if you *did* want to tackle the transcript within a transcript scenario to build a coverage bias model, you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive/folders/1LcJNa4PHNoYqGsnkRx0YxvNXnNJCVyq9?u",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6637,Availability,down,down,6637," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6417,Integrability,depend,depends,6417," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:1952,Security,access,access,1952,"otally disabling this with the —noLengthCorrection flag** (NOT the —no**Effective**LengthCorrection flag) **actually *creates* the transcript within a transcript failure scenario that I mistakenly thought was the original issue.** That is, when length bias modeling is turned off, then longer transcripts will always get assigned *all* of the reads that multimap to shorter transcripts.; -Therefore... if you *did* want to tackle the transcript within a transcript scenario to build a coverage bias model, you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive/folders/1LcJNa4PHNoYqGsnkRx0YxvNXnNJCVyq9?usp=sharing. 1. **Success scenarios with default options:** . In the below IGV snapshots, I show the read alignments for one sample. The top GTF annotation is the default gene annotation, and the GTF at the bottom shows the new transcript isoforms I made and quantified on (this index is called ""extras""). For each example I ran salmon on the transcripts from the default or extra index, with standard options (only --validateMappings), with or without the --noLengthCorrection flag. **I'm showing only the number of reads** assigned to each transcript, not the TPM. I also tried this on more samples and transcript scenarios and saw the same trends. **Nested transcript isoforms:** ; ![AGP1_example](https://user-images.githubusercontent.com/10292386/86509506-45654000-bd9d-11ea-839f-6637620c3247.png). <img width=""430"" alt=""AGP1_table"" src=""https://user-images.githubusercontent.com/1029",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:2462,Security,validat,validateMappings,2462," you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive/folders/1LcJNa4PHNoYqGsnkRx0YxvNXnNJCVyq9?usp=sharing. 1. **Success scenarios with default options:** . In the below IGV snapshots, I show the read alignments for one sample. The top GTF annotation is the default gene annotation, and the GTF at the bottom shows the new transcript isoforms I made and quantified on (this index is called ""extras""). For each example I ran salmon on the transcripts from the default or extra index, with standard options (only --validateMappings), with or without the --noLengthCorrection flag. **I'm showing only the number of reads** assigned to each transcript, not the TPM. I also tried this on more samples and transcript scenarios and saw the same trends. **Nested transcript isoforms:** ; ![AGP1_example](https://user-images.githubusercontent.com/10292386/86509506-45654000-bd9d-11ea-839f-6637620c3247.png). <img width=""430"" alt=""AGP1_table"" src=""https://user-images.githubusercontent.com/10292386/86509511-48f8c700-bd9d-11ea-8203-c37eeaf4820d.png"">. Looking first at the ""extras"" index + default options, almost all the reads are assigned to AGP1_long1, which indeed seems to be the best fit for the actual gene body + UTRs (the default AGP1 transcript is only protein coding ORF, no UTRs). Even though all of the reads from AGP1_long1 would also multimap to AGP1_long2, etc., AGP1_long1 is assigned all the reads because it has the highest reads per kb. Presumably the few reads assigned to AGP1_",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:5122,Usability,intuit,intuition,5122,"thubusercontent.com/10292386/86509800-6b8bdf80-bd9f-11ea-9a9f-bbfea99e2703.png); <img width=""485"" alt=""KCC4_table"" src=""https://user-images.githubusercontent.com/10292386/86509801-6d55a300-bd9f-11ea-9c69-8e269fc3ab1c.png"">. In this example, there are two regions in KCC4 with obviously different coverage. Ideally we would be able to have a default KCC4 transcript and a truncated isoform in the salmon index, and it would assign the reads appropriately, even though all of the reads that map to the truncated form would also multimap to the long form. Again, you can see in the table that salmon assigns reads parsimoniously to both transcripts with the default options, but with the length bias modeling turned off ALL of the reads are assigned to the long transcript. I also added a third transcript to the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6540,Usability,intuit,intuition,6540," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6647,Usability,simpl,simple,6647," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6811,Usability,clear,cleared,6811," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:556,Availability,recover,recovering,556,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:1113,Availability,failure,failure,1113,"problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in treating those reads as un-mapped. Furthermore, this problem was more evident when we tried that approach on other larger samples, it seemed that could effect the",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:414,Deployability,update,updates,414,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:2553,Deployability,update,update,2553,"ection, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in treating those reads as un-mapped. Furthermore, this problem was more evident when we tried that approach on other larger samples, it seemed that could effect the expression of a lot transcripts very significantly. Specially, on the real samples where the coverage are often not uniform and detecting a zero coverage region on a transcript is more common due to un-annotated transcripts in the samples and etc. Currently, we are actively looking for more thorough solutions for this problem to deal with the coverage profile of transcripts. I'll try to update you more as we make more progress about this. Thank you again for the detailed explanation, hope to get back at you soon. Best,; Mohsen",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:556,Safety,recover,recovering,556,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:2291,Safety,detect,detecting,2291,"ection, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in treating those reads as un-mapped. Furthermore, this problem was more evident when we tried that approach on other larger samples, it seemed that could effect the expression of a lot transcripts very significantly. Specially, on the real samples where the coverage are often not uniform and detecting a zero coverage region on a transcript is more common due to un-annotated transcripts in the samples and etc. Currently, we are actively looking for more thorough solutions for this problem to deal with the coverage profile of transcripts. I'll try to update you more as we make more progress about this. Thank you again for the detailed explanation, hope to get back at you soon. Best,; Mohsen",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:106,Usability,clear,clear,106,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/520#issuecomment-625604855:38,Usability,guid,guidance,38,"Dear Rob,. Thank you so much for your guidance. I appreciate you taking the time to help me. I was able to run salmon, no problem. All the best,; Craig",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/520#issuecomment-625604855
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638:261,Performance,perform,perform,261,"Hi @PlantDr430,. Thanks for the context! As always, we'd be interested in learning anything interesting you find about the general behavior of salmon in different contexts and with different parameter settings etc. Out of curiosity, when you mention that genes perform ""better"" with one or another `--scoreExp`, is it the case that this is data where you have some sort of ground truth expectation for the abundance of the primary vs. spliced forms? If so, super interesting!. One other thought I had about this. While it is true, as I mentioned in my original post, that the conditioning on the transcripts is _fundamental_ in the case of salmon and other transcript expression tools that don't, themselves, try to assemble new transcripts, it's not necessarily true that there is no evidence in the quantifications that something my be awry. Specifically, I noticed that you are using posterior confidence estimation (bootstrapping). We actually have a [recent paper](https://www.biorxiv.org/content/10.1101/2020.04.07.029967v1.full) that discusses how to use the uncertainty estimates from salmon (though we rely on the Gibbs sampler rather than bootstrapping) to group together transcripts whose abundances cannot be individually estimated with confidence (with evidenced provided by the posterior samples). It might be useful to identify such cases in your analysis. Let me know if there's any other way I can help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638:74,Usability,learn,learning,74,"Hi @PlantDr430,. Thanks for the context! As always, we'd be interested in learning anything interesting you find about the general behavior of salmon in different contexts and with different parameter settings etc. Out of curiosity, when you mention that genes perform ""better"" with one or another `--scoreExp`, is it the case that this is data where you have some sort of ground truth expectation for the abundance of the primary vs. spliced forms? If so, super interesting!. One other thought I had about this. While it is true, as I mentioned in my original post, that the conditioning on the transcripts is _fundamental_ in the case of salmon and other transcript expression tools that don't, themselves, try to assemble new transcripts, it's not necessarily true that there is no evidence in the quantifications that something my be awry. Specifically, I noticed that you are using posterior confidence estimation (bootstrapping). We actually have a [recent paper](https://www.biorxiv.org/content/10.1101/2020.04.07.029967v1.full) that discusses how to use the uncertainty estimates from salmon (though we rely on the Gibbs sampler rather than bootstrapping) to group together transcripts whose abundances cannot be individually estimated with confidence (with evidenced provided by the posterior samples). It might be useful to identify such cases in your analysis. Let me know if there's any other way I can help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653:2438,Modifiability,variab,variables,2438,"here, for instance. What do the flags and qualities represent?. It is just a SAM file without CIGAR strings. The flags have the same (normal) interpretation for SAM records. However the CIGAR strings are not meaningful (apart from what is required for the file to undergo valid processing with samtools). The records additionally contain tags about the number of targets to which a fragment multi-maps, and the alignment score of the read pair to the current target (in the `AS` flag). The records themself are just normal SAM records, but with a trivial CIGAR strong. > More importantly, is there a way to filter the pseudobam files to find the reads corresponding to the counts/NumReads in the quant.sf output file? Do the normal samtools quality and flag filters work to subset e.g. uniquely-mapped reads, or do those concepts not really apply to these pseudobams?. There is no easy way to filter so the above condition is satisfied, as the NumReads are obtained by proportional allocation of the reads according to the underlying probabilistic model of salmon. Specifically, the NumReads column of the quantification file corresponds to summing over the _expectation_ of all of the latent variables that represent fragment to transcript assignment so that, apart from uniquely-mapped reads, there is no way to say that a fragment _definitely_ came from a transcript. However, you should still be able to easily filter out uniquely mapped reads, and you can interpret them in a relatively unambiguous way. Also, you could filter on the `AS` tag as well. For a given read, if there is a single transcript where the `AS` value is much larger than the others for this read, it is overwhelmingly likely that the read originated from the transcript with the unique best `AS` score. @shangguandong1996 : The SAM file _does_ contain positions (and orientations, and alignment scores) for each read. It is simply that the positions are with respect to the transcriptome and not with respect to the genome.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653:3146,Usability,simpl,simply,3146,"here, for instance. What do the flags and qualities represent?. It is just a SAM file without CIGAR strings. The flags have the same (normal) interpretation for SAM records. However the CIGAR strings are not meaningful (apart from what is required for the file to undergo valid processing with samtools). The records additionally contain tags about the number of targets to which a fragment multi-maps, and the alignment score of the read pair to the current target (in the `AS` flag). The records themself are just normal SAM records, but with a trivial CIGAR strong. > More importantly, is there a way to filter the pseudobam files to find the reads corresponding to the counts/NumReads in the quant.sf output file? Do the normal samtools quality and flag filters work to subset e.g. uniquely-mapped reads, or do those concepts not really apply to these pseudobams?. There is no easy way to filter so the above condition is satisfied, as the NumReads are obtained by proportional allocation of the reads according to the underlying probabilistic model of salmon. Specifically, the NumReads column of the quantification file corresponds to summing over the _expectation_ of all of the latent variables that represent fragment to transcript assignment so that, apart from uniquely-mapped reads, there is no way to say that a fragment _definitely_ came from a transcript. However, you should still be able to easily filter out uniquely mapped reads, and you can interpret them in a relatively unambiguous way. Also, you could filter on the `AS` tag as well. For a given read, if there is a single transcript where the `AS` value is much larger than the others for this read, it is overwhelmingly likely that the read originated from the transcript with the unique best `AS` score. @shangguandong1996 : The SAM file _does_ contain positions (and orientations, and alignment scores) for each read. It is simply that the positions are with respect to the transcriptome and not with respect to the genome.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711:226,Availability,ping,ping,226,"The `AS:i:-2147483648` is a sentinel value basically meaning the alignment was below the minimum acceptable quality. You can simply ignore those (its the min signed 32-bit integer). Let me think about your other question (and ping @mikelove), and get back to you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711:125,Usability,simpl,simply,125,"The `AS:i:-2147483648` is a sentinel value basically meaning the alignment was below the minimum acceptable quality. You can simply ignore those (its the min signed 32-bit integer). Let me think about your other question (and ping @mikelove), and get back to you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:266,Integrability,protocol,protocol,266,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:333,Integrability,protocol,protocol,333,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:29,Testability,test,test,29,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:482,Usability,guid,guide,482,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530:46,Modifiability,portab,portable,46,"Thanks @k3yavi !If you can forward me a Linux portable binary that would be great. Whenever I try to compile something on my computer, I fail half of the time . I have Ubuntu 18.04. I will ask permission to share with you part of the data and get back to you. Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes? And do you recommend using the `--naiveEqclass`; option when there are only 64 guide sequences as features?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530:420,Usability,guid,guide,420,"Thanks @k3yavi !If you can forward me a Linux portable binary that would be great. Whenever I try to compile something on my computer, I fail half of the time . I have Ubuntu 18.04. I will ask permission to share with you part of the data and get back to you. Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes? And do you recommend using the `--naiveEqclass`; option when there are only 64 guide sequences as features?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:126,Availability,down,download,126,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:178,Availability,avail,available,178,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:1337,Availability,error,error,1337,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:213,Deployability,release,release,213,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:1215,Safety,avoid,avoiding,1215,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:545,Usability,guid,guide,545,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:1491,Usability,guid,guide,1491,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:618,Availability,down,downstream,618,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:3175,Performance,optimiz,optimizer,3175,alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:26:05.872] [alevinLog] [info] Total [32m95377[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:26:06.746] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:26:06.880] [alevinLog] [info] Total 1.2299% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:26:10.886] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:26:10.924] [alevinLog] [info] Total 118774 CB got sequence corrected; > [2020-06-04 12:26:10.936] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:26:10.936] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:26:10.936] [alevinLog] [info] Used Barcodes except Whitelist: 88156; > [2020-06-04 12:26:11.113] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:26:11.113] [alevinLog] [info] parsing read library format; > [2020-06-04 12:27:21.373] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:27:22.086] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.086] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 12:27:22.412] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:27:22.418] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:27:22.418] [alevinLog] [info] Finished optimizer. Run 2: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcod,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:3944,Performance,perform,performing,3944,"[alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:26:11.113] [alevinLog] [info] parsing read library format; > [2020-06-04 12:27:21.373] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:27:22.086] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.086] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 12:27:22.412] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:27:22.418] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:27:22.418] [alevinLog] [info] Finished optimizer. Run 2: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 200000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > [2020-06-04 12:40:45.455] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:40:45.456] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:40:45.461] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:42:01.202] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:42:01.202] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:4035,Performance,optimiz,optimizer,4035,"ng to Quantify; > ; > [2020-06-04 12:26:11.113] [alevinLog] [info] parsing read library format; > [2020-06-04 12:27:21.373] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:27:22.086] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.086] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 23937.00 UMI after deduplicating.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 91 BiDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:27:22.409] [alevinLog] [warning] Skipped 82268 barcodes due to No mapped read; > [2020-06-04 12:27:22.412] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:27:22.418] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:27:22.418] [alevinLog] [info] Finished optimizer. Run 2: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 200000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > [2020-06-04 12:40:45.455] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:40:45.456] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:40:45.456] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:40:45.461] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:42:01.202] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:42:01.202] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:6012,Performance,optimiz,optimizer,6012,200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:42:02.738] [alevinLog] [info] Total [32m197328[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:42:03.656] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:42:03.830] [alevinLog] [info] Total 0.780192% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:42:13.353] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-04 12:42:13.555] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:42:13.556] [alevinLog] [info] parsing read library format; > [2020-06-04 12:43:22.789] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:43:23.499] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.499] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 24009.00 UMI after deduplicating.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 89 BiDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [warning] Skipped 184123 barcodes due to No mapped read; > [2020-06-04 12:43:23.840] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:43:23.846] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:43:23.846] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:6782,Performance,perform,performing,6782,200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:42:02.738] [alevinLog] [info] Total [32m197328[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:42:03.656] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:42:03.830] [alevinLog] [info] Total 0.780192% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:42:13.353] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-04 12:42:13.555] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:42:13.556] [alevinLog] [info] parsing read library format; > [2020-06-04 12:43:22.789] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:43:23.499] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.499] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 24009.00 UMI after deduplicating.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 89 BiDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [warning] Skipped 184123 barcodes due to No mapped read; > [2020-06-04 12:43:23.840] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:43:23.846] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:43:23.846] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:6873,Performance,optimiz,optimizer,6873,200250[0m / [31m52200250[0m.; > [2020-06-04 12:42:01.300] [alevinLog] [info] Forcing to use 200000 cells; > [2020-06-04 12:42:02.037] [alevinLog] [info] Throwing 0 barcodes with < 1 reads; > [2020-06-04 12:42:02.738] [alevinLog] [info] Total [32m197328[0m(has [32m101[0m low confidence) barcodes; > [2020-06-04 12:42:03.656] [alevinLog] [info] Done True Barcode Sampling; > [2020-06-04 12:42:03.830] [alevinLog] [info] Total 0.780192% reads will be thrown away because of noisy Cellular barcodes.; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done populating Z matrix; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total 0 CB got sequence corrected; > [2020-06-04 12:42:13.353] [alevinLog] [info] Done indexing Barcodes; > [2020-06-04 12:42:13.353] [alevinLog] [info] Total Unique barcodes found: 604589; > [2020-06-04 12:42:13.353] [alevinLog] [info] Used Barcodes except Whitelist: 0; > [2020-06-04 12:42:13.555] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; > ; > [2020-06-04 12:42:13.556] [alevinLog] [info] parsing read library format; > [2020-06-04 12:43:22.789] [alevinLog] [info] Starting optimizer; > ; > [2020-06-04 12:43:23.499] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.499] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 24009.00 UMI after deduplicating.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 89 BiDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [info] Total 82 UniDirected Edges.; > [2020-06-04 12:43:23.835] [alevinLog] [warning] Skipped 184123 barcodes due to No mapped read; > [2020-06-04 12:43:23.840] [alevinLog] [info] Clearing EqMap; Might take some time.; > [2020-06-04 12:43:23.846] [alevinLog] [warning] Num Low confidence barcodes too less 1 < 100.Can't performing whitelisting; Skipping; > [2020-06-04 12:43:23.846] [alevinLog] [info] Finished optimizer,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:66,Testability,log,logs,66,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:526,Testability,log,log,526,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:1180,Testability,test,testing,1180,"two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alevinLog] [info] # Barcodes Used: [32m52200250[0m / [31m52200250[0m.; > [2020-06-04 12:26:04.435] [alevinLog] [info] Forcing to use 100000 cells; > [2020-",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199:324,Usability,guid,guide,324,"Hi Avi,. Thanks for the detailed reply. I was able to run it (see logs below), but I had to use `ISR`, not `ISF` to get it to work. I also had to add these two settings as well; `--freqThreshold 1 --lowRegionMinNumBarcodes 100`. . I am not sure why the `ISF` option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. 1. Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000? ; 2. For the downstream analysis of such data, I usually work with both the read and UMI counts, but `quants_mat.gz` only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the ` --dumpEq` or `--dumpBfh` flags? Can *tximport* be used for this or do I need to use the Python [parser]([https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.pyl]) first?. I will be sending you some reads from the experiments for unit testing shortly. Thanks!. Run 1: `salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21 --maxNumBarcodes 100000 --freqThreshold 1 --lowRegionMinNumBarcodes 100`. > 20-06-04 12:24:47.610] [alevinLog] [info] set CITE-seq minScoreFraction parameter to : 0.797619; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found 64 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); > [2020-06-04 12:24:47.610] [alevinLog] [info] Filled with 64 txp to gene entries ; > [2020-06-04 12:24:47.610] [alevinLog] [info] Found all transcripts to gene mappings; > [2020-06-04 12:24:47.616] [alevinLog] [info] Processing barcodes files (if Present) ; > ; > [2020-06-04 12:26:04.322] [alevinLog] [info] Done barcode density calculation.; > [2020-06-04 12:26:04.322] [alev",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638991199
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220:857,Safety,detect,detect,857,"Hi Rob,. The joint distribution of the read and UMI counts can contain important information. The majority of observations (CB + guide combination) lie along a well defined experiment-specific mean trend whose slope is given by the coverage ( ratio of reads to UMIs). Also the same regularity can be observed when aggregating across the cell barcodes. See figure below. The points below the black horizontal line are cells with less than 100 reads. ![image](https://user-images.githubusercontent.com/9895004/83791774-30937080-a668-11ea-9b44-937ba8f69b34.png). At the guide level, it would look like this . ![image](https://user-images.githubusercontent.com/9895004/83792096-941d9e00-a668-11ea-9b26-976332f639fe.png). In general, I often find myself needing to work with read counts. For example, the read counts can be used to estimate the hopping rate and detect hopped reads in multiplexed scRNAseq data as we show in this recent paper https://www.nature.com/articles/s41467-020-16522-z . Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220:129,Usability,guid,guide,129,"Hi Rob,. The joint distribution of the read and UMI counts can contain important information. The majority of observations (CB + guide combination) lie along a well defined experiment-specific mean trend whose slope is given by the coverage ( ratio of reads to UMIs). Also the same regularity can be observed when aggregating across the cell barcodes. See figure below. The points below the black horizontal line are cells with less than 100 reads. ![image](https://user-images.githubusercontent.com/9895004/83791774-30937080-a668-11ea-9b44-937ba8f69b34.png). At the guide level, it would look like this . ![image](https://user-images.githubusercontent.com/9895004/83792096-941d9e00-a668-11ea-9b26-976332f639fe.png). In general, I often find myself needing to work with read counts. For example, the read counts can be used to estimate the hopping rate and detect hopped reads in multiplexed scRNAseq data as we show in this recent paper https://www.nature.com/articles/s41467-020-16522-z . Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220:567,Usability,guid,guide,567,"Hi Rob,. The joint distribution of the read and UMI counts can contain important information. The majority of observations (CB + guide combination) lie along a well defined experiment-specific mean trend whose slope is given by the coverage ( ratio of reads to UMIs). Also the same regularity can be observed when aggregating across the cell barcodes. See figure below. The points below the black horizontal line are cells with less than 100 reads. ![image](https://user-images.githubusercontent.com/9895004/83791774-30937080-a668-11ea-9b44-937ba8f69b34.png). At the guide level, it would look like this . ![image](https://user-images.githubusercontent.com/9895004/83792096-941d9e00-a668-11ea-9b26-976332f639fe.png). In general, I often find myself needing to work with read counts. For example, the read counts can be used to estimate the hopping rate and detect hopped reads in multiplexed scRNAseq data as we show in this recent paper https://www.nature.com/articles/s41467-020-16522-z . Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639005220
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1151,Availability,down,down,1151,"h the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read coun",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1841,Availability,down,downstream,1841,"ingle-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the --dumpEq or --dumpBfh flags? Can tximport be used for this or do I need to use the Python parser first?. Congratulations on the awesome paper :). We were actually discussing yesterday about your paper and potentially modifying alevin to include model for correcting index-hoping, although it's still in discussion phase. To answer your question, thanks for the feature request, I can add that feature on the weekend if it's urgent. However, you can also generate that with the current version using the `--dumpBfh` flag and `bfh.txt` file. Unfortunately, since it's the first use case where we need the read-count matrix, we haven't included the support in `tximport` yet. However, like you said one can use the [read_bfh](https://g",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:764,Testability,log,log,764,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1245,Testability,log,logs,1245," sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the --dumpEq or --dumpBfh flags? Can tximport be used for this or do I need to use the Python parser firs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:1669,Testability,test,testing,1669,"ncreasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matrix of read counts as well. It would be a great feature to add. For now, what is easiest way to get the cell x feature matrix of read counts if I use the --dumpEq or --dumpBfh flags? Can tximport be used for this or do I need to use the Python parser first?. Congratulations on the awesome paper :). We were actually discussing yesterday about your paper and potentially modifying alevin to include model for correcting index-hoping, although it's still in discussion phase. To answer your question, thanks for the feature request, I can add that feature on the weekend if it's urgent. However, you can also generate that with the current version using the `--dumpBfh` flag and `bfh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:142,Usability,guid,guide,142,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:293,Usability,guid,guide,293,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:423,Usability,guid,guideRNA,423,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052:562,Usability,guid,guide,562,"Hi @rfarouni ,. Thanks for the detailed answer.; > I am not sure why the ISF option didn't work, but probably it has something to do with the guide sequences I was provided. At any rate, I have a few other questions I hope you can help me answer. I'm not sure about this, it's possible if the guide sequences were already reverse-complemented then the above behavior would makes sense. I am a little less familiar with the guideRNA based ECCITE-seq data, although the mRNA library should be 5' and the sequence does come from forward strand but do we expect the guide RNA to be on the forward strand as well ? Unclear . I'll ask around at nygc and would let you know. > Why does increasing --maxNumBarcodes to 200000 results in no barcodes getting corrected? (See log for Run 2 below). What is the rationale for the current default of 100000?. That's again a great question. In short single-cell world is expanding rapidly and alevin was initially designed to work with 10x 3' data and some of the restriction are outdated with combinatorial indexing based multiplexed experiments. To be honest, 100k was just a random high enough number that was put down to throw away the obvious junk data. Having said that, you would notice that in both the logs you attached a significant fraction of barcodes are thrown away i.e., `Skipped 82268 barcodes due to No mapped read`, which is like ~82% of the 100k barcodes. Even if you include the 200k almost everything was thrown away, `[warning] Skipped 184123 barcodes due to No mapped read`. Although your point is important one why things are not getting sequence corrected with 200k, unfortunately I might have to do some more testing on that front to give more precise answer but in your case I'd advise keeping the default 100k bound, unless you are doing combinatorially-indexed data . > For the downstream analysis of such data, I usually work with both the read and UMI counts, but quants_mat.gz only contains the UMI counts. Can Alevin a produce a matri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639048052
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909:1184,Energy Efficiency,efficient,efficiently,1184,"Hi Avi,. Yes I just asked and the guide sequences were reverse complemented. I was looking through the results and comparing it with the output of another alignment software. I noticed that there are substantially fewer UMI per guide (in cell) throughout ( see figures for comparison). . ![image](https://user-images.githubusercontent.com/9895004/83803410-7eb16f80-a67a-11ea-832d-562c88dafef3.png) ; ![image](https://user-images.githubusercontent.com/9895004/83803427-8709aa80-a67a-11ea-9ea4-f66ca447a65c.png). Also, the number of UMIs per cell barcode is consistently lower and there is around 796 barcodes that are not found in the 10X whitelist, the majority of which tend to have 1 UMI count only. Here is tally, where the TRUE column indicates the barcode is found in the whitelist. The row names indicate the total number of UMIs; ; ![image](https://user-images.githubusercontent.com/9895004/83803984-7279e200-a67b-11ea-8578-fc863f94f714.png). It would be great if you can implement the index hopping correction in Alevin. The software we have works fine if the number of samples is not too large. If had known how to code in C++, I would have implemented part of the code more efficiently using Rcpp. Please let me know if you ever decide to add this feature to Salmon. I am more than happy to help. Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909:34,Usability,guid,guide,34,"Hi Avi,. Yes I just asked and the guide sequences were reverse complemented. I was looking through the results and comparing it with the output of another alignment software. I noticed that there are substantially fewer UMI per guide (in cell) throughout ( see figures for comparison). . ![image](https://user-images.githubusercontent.com/9895004/83803410-7eb16f80-a67a-11ea-832d-562c88dafef3.png) ; ![image](https://user-images.githubusercontent.com/9895004/83803427-8709aa80-a67a-11ea-9ea4-f66ca447a65c.png). Also, the number of UMIs per cell barcode is consistently lower and there is around 796 barcodes that are not found in the 10X whitelist, the majority of which tend to have 1 UMI count only. Here is tally, where the TRUE column indicates the barcode is found in the whitelist. The row names indicate the total number of UMIs; ; ![image](https://user-images.githubusercontent.com/9895004/83803984-7279e200-a67b-11ea-8578-fc863f94f714.png). It would be great if you can implement the index hopping correction in Alevin. The software we have works fine if the number of samples is not too large. If had known how to code in C++, I would have implemented part of the code more efficiently using Rcpp. Please let me know if you ever decide to add this feature to Salmon. I am more than happy to help. Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909:228,Usability,guid,guide,228,"Hi Avi,. Yes I just asked and the guide sequences were reverse complemented. I was looking through the results and comparing it with the output of another alignment software. I noticed that there are substantially fewer UMI per guide (in cell) throughout ( see figures for comparison). . ![image](https://user-images.githubusercontent.com/9895004/83803410-7eb16f80-a67a-11ea-832d-562c88dafef3.png) ; ![image](https://user-images.githubusercontent.com/9895004/83803427-8709aa80-a67a-11ea-9ea4-f66ca447a65c.png). Also, the number of UMIs per cell barcode is consistently lower and there is around 796 barcodes that are not found in the 10X whitelist, the majority of which tend to have 1 UMI count only. Here is tally, where the TRUE column indicates the barcode is found in the whitelist. The row names indicate the total number of UMIs; ; ![image](https://user-images.githubusercontent.com/9895004/83803984-7279e200-a67b-11ea-8578-fc863f94f714.png). It would be great if you can implement the index hopping correction in Alevin. The software we have works fine if the number of samples is not too large. If had known how to code in C++, I would have implemented part of the code more efficiently using Rcpp. Please let me know if you ever decide to add this feature to Salmon. I am more than happy to help. Rick,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639088909
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897:828,Deployability,integrat,integration,828,"Hi @rfarouni ,. Is it possible to visualize the above two plots on the same scale ? ; Regarding the few cells not from 10x whitelist, I should have been more clear last time. ; Basically, what I meant earlier when I said that 10x data is clean is that we do observe some cells from the non whitelist file _but_ they have very few UMI and we discard them anyway. I am guessing here your motivation is a bit different i.e. considering very low confidence (even with 1 UMI) barcodes, while generally we discard anything below 10 as noise. Thanks a lot for offering to help with index-hopping idea. I agree, it'd be great to include the model in the alevin framework. Currently I just got the gist of your paper, let us go through the paper in a bit more detail and we'll get back to you as soon as we have some free cycles for the integration.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897:828,Integrability,integrat,integration,828,"Hi @rfarouni ,. Is it possible to visualize the above two plots on the same scale ? ; Regarding the few cells not from 10x whitelist, I should have been more clear last time. ; Basically, what I meant earlier when I said that 10x data is clean is that we do observe some cells from the non whitelist file _but_ they have very few UMI and we discard them anyway. I am guessing here your motivation is a bit different i.e. considering very low confidence (even with 1 UMI) barcodes, while generally we discard anything below 10 as noise. Thanks a lot for offering to help with index-hopping idea. I agree, it'd be great to include the model in the alevin framework. Currently I just got the gist of your paper, let us go through the paper in a bit more detail and we'll get back to you as soon as we have some free cycles for the integration.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897:158,Usability,clear,clear,158,"Hi @rfarouni ,. Is it possible to visualize the above two plots on the same scale ? ; Regarding the few cells not from 10x whitelist, I should have been more clear last time. ; Basically, what I meant earlier when I said that 10x data is clean is that we do observe some cells from the non whitelist file _but_ they have very few UMI and we discard them anyway. I am guessing here your motivation is a bit different i.e. considering very low confidence (even with 1 UMI) barcodes, while generally we discard anything below 10 as noise. Thanks a lot for offering to help with index-hopping idea. I agree, it'd be great to include the model in the alevin framework. Currently I just got the gist of your paper, let us go through the paper in a bit more detail and we'll get back to you as soon as we have some free cycles for the integration.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639137897
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:81,Testability,log,log,81,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:109,Testability,log,logs,109,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:201,Testability,log,log,201,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:221,Testability,log,logs,221,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:254,Testability,log,log,254,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938:136,Usability,clear,clear,136,"Oh wow 14k v 126k is indeed a big difference, is it possible to share the Alevin log for your run ? From the logs you attached it's not clear what's the mapping rate. May I also ask to look at another log file inside the logs folder, called salmon_quant.log. that would have more information regarding the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639161938
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639204641:69,Usability,guid,guide,69,"Lemme work with the reads you forwarded, is it possible to share the guide sequence as well ? Otherwise I won't be able to check the mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639204641
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956:124,Deployability,update,update,124,I will be trying your suggestion out. I might be able to share with you a toy dataset with a fewer number of guides. I will update you as soon as I get it.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956:109,Usability,guid,guides,109,I will be trying your suggestion out. I might be able to share with you a toy dataset with a fewer number of guides. I will update you as soon as I get it.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-639618956
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397:25,Deployability,update,updates,25,"Thanks @rfarouni for the updates. > With --minScoreFraction 0.607 I get a way much better mapping rate. I wonder if there is way to determine the optimal value empirically?. Glad to hear that, may I ask what percent of the reads are mapping now ? It's not clear from the alevin logs you shared but I think the total number of deduplicated UMIs are similar to your baseline experiment. I think defining an optimal empirical threshold is a great idea but the issue is that 21 length barcodes are kind of in the middle i.e. a tad longer than the regular barcodes and somewhat smaller than a full read. The full read alignment process indeed allows more erroneous reads to map but 21 is a bit too short to work with. @rob-p might have more thoughts on this one. > But now there are a lot of barcodes that are not in the whitelist. Thanks again for checking this, it is indeed concerning. However, as I was mentioning earlier in a regular single-cell experiment we end up throwing away almost all of these very low frequency count cellular barcodes. I'd say even 45 reads CBs are most probably a noise and will be filtered away, because only a fraction of the reads will map and after deduplication it'll result in significantly low count in 1 cellular barcode. > Also with the default setting of --freqThreshold, no CB correction gets done. I can check why is this happening, let me know once you have a toy dataset to play with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397:278,Testability,log,logs,278,"Thanks @rfarouni for the updates. > With --minScoreFraction 0.607 I get a way much better mapping rate. I wonder if there is way to determine the optimal value empirically?. Glad to hear that, may I ask what percent of the reads are mapping now ? It's not clear from the alevin logs you shared but I think the total number of deduplicated UMIs are similar to your baseline experiment. I think defining an optimal empirical threshold is a great idea but the issue is that 21 length barcodes are kind of in the middle i.e. a tad longer than the regular barcodes and somewhat smaller than a full read. The full read alignment process indeed allows more erroneous reads to map but 21 is a bit too short to work with. @rob-p might have more thoughts on this one. > But now there are a lot of barcodes that are not in the whitelist. Thanks again for checking this, it is indeed concerning. However, as I was mentioning earlier in a regular single-cell experiment we end up throwing away almost all of these very low frequency count cellular barcodes. I'd say even 45 reads CBs are most probably a noise and will be filtered away, because only a fraction of the reads will map and after deduplication it'll result in significantly low count in 1 cellular barcode. > Also with the default setting of --freqThreshold, no CB correction gets done. I can check why is this happening, let me know once you have a toy dataset to play with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397:256,Usability,clear,clear,256,"Thanks @rfarouni for the updates. > With --minScoreFraction 0.607 I get a way much better mapping rate. I wonder if there is way to determine the optimal value empirically?. Glad to hear that, may I ask what percent of the reads are mapping now ? It's not clear from the alevin logs you shared but I think the total number of deduplicated UMIs are similar to your baseline experiment. I think defining an optimal empirical threshold is a great idea but the issue is that 21 length barcodes are kind of in the middle i.e. a tad longer than the regular barcodes and somewhat smaller than a full read. The full read alignment process indeed allows more erroneous reads to map but 21 is a bit too short to work with. @rob-p might have more thoughts on this one. > But now there are a lot of barcodes that are not in the whitelist. Thanks again for checking this, it is indeed concerning. However, as I was mentioning earlier in a regular single-cell experiment we end up throwing away almost all of these very low frequency count cellular barcodes. I'd say even 45 reads CBs are most probably a noise and will be filtered away, because only a fraction of the reads will map and after deduplication it'll result in significantly low count in 1 cellular barcode. > Also with the default setting of --freqThreshold, no CB correction gets done. I can check why is this happening, let me know once you have a toy dataset to play with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640093397
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640698401:80,Usability,clear,clear,80,"Yes, absolutely, above I meant in scRNA-seq context, my apologies if it was not clear.; Here, you are right we might have to think of ways to provide whitelist cellular barcode. One another thing you can try is providing the full 10x expected whitelisted cellular barcodes to alevin through `--whitelist` command.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-640698401
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638961902:58,Usability,clear,clear,58,"Hi @rob-p ,. Thank you for your explanation, that is very clear and helpful. Yes, the transcriptome annotation was not originally from my RNA-seq data and I may try using stringtie to discover new isoforms, probably that would cover more of my reads.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-638961902
https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-1453502593:430,Usability,guid,guide,430,"Hello, @rob-p and first of all big thanks from me (and the community in general!) for being so reactive and helpful, and that's not even mentioning the tool you and your colleagues provided, which is very cool!. > do transcript assembly in these samples (using e.g. scallop or StringTie2) and then re-quantify using salmon under the expanded annotation. Could you please point me (and others who might be reading this topic) to a guide on how to convert this purported _de novo_ transcriptome to common gene/transcriptome names already known for the organism?? I am not even sure how that would work since many of us use well-known model organisms like mice or fruit flies, for which both the genome and transcriptome are well-described... What kind of genes/transcripts will I be looking at, exactly, after assembling this particular _de novo_ transcriptome which in theory came from mus musculus but in practice is only applicable to this particular single experiment???. Best regards,; Emile",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/533#issuecomment-1453502593
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370:251,Safety,detect,detection,251,"Hi @deevdevil88,. The challenges I faced with this issue made me switch over to `kallisto` which has some nice advantages as far as speed. I didn't see any obvious affects on quality for my samples although I did have to re-implement some of the auto-detection that `alevin` and `salmon` do for you. . I personally observed some strange behaviour with Soupx - visually apparent differences in gene expression between samples that at the time I felt were artefactual of the adjustment by Soupx. I eventually rolled-my-own strategy where I omitted ambient outlier genes from differential expression. Ambient outliers were defined by taking droplets with UMI counts <10 with using a boxplot in R to define outliers. The osca.bioconductor.org [recommendations](https://osca.bioconductor.org/multi-sample-comparisons.html#ambient-problems) ended up being very similar. They also describe some of the pitfalls of adjusting counts. Best of luck! Always appreciative of all the great work and responsiveness of @k3yavi and the team!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370
https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370:985,Usability,responsiv,responsiveness,985,"Hi @deevdevil88,. The challenges I faced with this issue made me switch over to `kallisto` which has some nice advantages as far as speed. I didn't see any obvious affects on quality for my samples although I did have to re-implement some of the auto-detection that `alevin` and `salmon` do for you. . I personally observed some strange behaviour with Soupx - visually apparent differences in gene expression between samples that at the time I felt were artefactual of the adjustment by Soupx. I eventually rolled-my-own strategy where I omitted ambient outlier genes from differential expression. Ambient outliers were defined by taking droplets with UMI counts <10 with using a boxplot in R to define outliers. The osca.bioconductor.org [recommendations](https://osca.bioconductor.org/multi-sample-comparisons.html#ambient-problems) ended up being very similar. They also describe some of the pitfalls of adjusting counts. Best of luck! Always appreciative of all the great work and responsiveness of @k3yavi and the team!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/538#issuecomment-646058370
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382:80,Availability,fault,fault,80,"Yes, I wonder if there is some sort of simple formatting issue that could be at fault here. Having a look at the file(s) would make it much easier to diagnose.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382
https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382:39,Usability,simpl,simple,39,"Yes, I wonder if there is some sort of simple formatting issue that could be at fault here. Having a look at the file(s) would make it much easier to diagnose.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/540#issuecomment-648250382
https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-674765288:141,Usability,usab,usable,141,@hariiyer16 I have not! The index works in an older version of salmon but not the newer one even though it was built in a way that should be usable by the newer Salmon version.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/559#issuecomment-674765288
https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718:423,Availability,error,error,423,"Hi @Cold7,. Thanks for the report. So, could you provide the full output that you get on the terminal when you run this? Your command line looks fine to me. Since version 1.0.0, `--validateMappings` has become the default behavior and so this flag technically has no effect (it is marked as ""deprecated""). However, the argument parser should _absolutely_ accept it, and it's not clear to me why it might be giving you this error. The full output from the terminal may help to diagnose this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718
https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718:181,Security,validat,validateMappings,181,"Hi @Cold7,. Thanks for the report. So, could you provide the full output that you get on the terminal when you run this? Your command line looks fine to me. Since version 1.0.0, `--validateMappings` has become the default behavior and so this flag technically has no effect (it is marked as ""deprecated""). However, the argument parser should _absolutely_ accept it, and it's not clear to me why it might be giving you this error. The full output from the terminal may help to diagnose this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718
https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718:379,Usability,clear,clear,379,"Hi @Cold7,. Thanks for the report. So, could you provide the full output that you get on the terminal when you run this? Your command line looks fine to me. Since version 1.0.0, `--validateMappings` has become the default behavior and so this flag technically has no effect (it is marked as ""deprecated""). However, the argument parser should _absolutely_ accept it, and it's not clear to me why it might be giving you this error. The full output from the terminal may help to diagnose this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/563#issuecomment-680250718
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661:683,Availability,recover,recoverOrphans,683,"I have a similar problem.; Attached are:; 1. gtf file, where clearly, the gene_ id and transcript_id are provided; 2. quant files are as followed for gene and transcripts; 3. my command as followed:; ---. /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant \; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index \; -l ISR \; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz \; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz \; --seqBias \; --gcBias \; --posBias \; --incompatPrior 0.0 \; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf \; --recoverOrphans \; --allowDovetail \; --threads $NSLOTS \; --dumpEq \; --minScoreFraction 0.65 \; --writeMappings /gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM \; --fldMean 250 \; --fldSD 25 \; --writeOrphanLinks \; --writeUnmappedNames \; --quiet \; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ---. [fewLines.gtf.txt](https://github.com/COMBINE-lab/salmon/files/5383013/fewLines.gtf.txt); [quant.genes.txt](https://github.com/COMBINE-lab/salmon/files/5382998/quant.genes.txt); [quant.txt](https://github.com/COMBINE-lab/salmon/files/5382999/quant.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661:683,Safety,recover,recoverOrphans,683,"I have a similar problem.; Attached are:; 1. gtf file, where clearly, the gene_ id and transcript_id are provided; 2. quant files are as followed for gene and transcripts; 3. my command as followed:; ---. /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant \; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index \; -l ISR \; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz \; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz \; --seqBias \; --gcBias \; --posBias \; --incompatPrior 0.0 \; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf \; --recoverOrphans \; --allowDovetail \; --threads $NSLOTS \; --dumpEq \; --minScoreFraction 0.65 \; --writeMappings /gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM \; --fldMean 250 \; --fldSD 25 \; --writeOrphanLinks \; --writeUnmappedNames \; --quiet \; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ---. [fewLines.gtf.txt](https://github.com/COMBINE-lab/salmon/files/5383013/fewLines.gtf.txt); [quant.genes.txt](https://github.com/COMBINE-lab/salmon/files/5382998/quant.genes.txt); [quant.txt](https://github.com/COMBINE-lab/salmon/files/5382999/quant.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661
https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661:61,Usability,clear,clearly,61,"I have a similar problem.; Attached are:; 1. gtf file, where clearly, the gene_ id and transcript_id are provided; 2. quant files are as followed for gene and transcripts; 3. my command as followed:; ---. /gpfsdata/apps/salmon-latest_linux_x86_64/bin/salmon quant \; -i /gpfshome/hockchuan/SALMON/GCF_900626175.2_cs10_index \; -l ISR \; -1 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_1.fastq.gz \; -2 /gpfsdata/JangiLab/hockchuan/170302/2.Trimmomatic_output/clean_HEADBANDSTEM_2.fastq.gz \; --seqBias \; --gcBias \; --posBias \; --incompatPrior 0.0 \; --geneMap /gpfsdata/JangiLab/hockchuan/cs10_reference_genome/GCF_900626175.2_cs10_genomic.gtf \; --recoverOrphans \; --allowDovetail \; --threads $NSLOTS \; --dumpEq \; --minScoreFraction 0.65 \; --writeMappings /gpfshome/hockchuan/SALMON/MAP/HEADBANDSTEM \; --fldMean 250 \; --fldSD 25 \; --writeOrphanLinks \; --writeUnmappedNames \; --quiet \; -o /gpfshome/hockchuan/SALMON/HEADBANDSTEM_quant; ---. [fewLines.gtf.txt](https://github.com/COMBINE-lab/salmon/files/5383013/fewLines.gtf.txt); [quant.genes.txt](https://github.com/COMBINE-lab/salmon/files/5382998/quant.genes.txt); [quant.txt](https://github.com/COMBINE-lab/salmon/files/5382999/quant.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/569#issuecomment-708949661
https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-710738799:326,Usability,clear,clear,326,"Hi @Vivianstats ,. Thanks for reaching out. You can certainly dump the CB and UMI tagged Bam file, however, the problem is we can't mark a subset of reads as deduplicated. Alevin's algorithm does not deduplicate UMI at the read level instead we deduplicate at the level of an arboreacence. Basically, the problems is it's not clear which alignment / read should be marked as primary because of following reasons:. 1.) Alevin does fractional assignment of ambiguous reads.; 2.) If there are multiple equally good alignment of a read which alignment should be marked primary for the deduplication ? ; 3.) Even in the UMI tools world, a UMI from a single gene can come from a range of genomic loci, because of the random process of sequence fragmentation. I am not quite sure what UMI tools does, I can check with the developers, but I feel randomly marking one among multiple equally good choice can hide the full picture. Hope it helps. @rob-p feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/574#issuecomment-710738799
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:541,Deployability,integrat,integrate,541,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:541,Integrability,integrat,integrate,541,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:1409,Testability,test,testing,1409,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:697,Usability,simpl,simplicity,697,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553:1239,Usability,simpl,simply,1239,"Hi @uros-sipetic!. Unfortunately, as you suggest, there really is no good way to infer the fragment length distribution from only single-end reads. Rather, this flag determines how the conditional probability of single-end fragments near the beginning (if in the rc orientation) or end (if in the forward orientation) of the transcript are determined. A single-end read does not have any known fragment length. But we do know that e.g. fragments very close to the end or beginning of the transcript are rather unlikely. In this case, we can integrate (sum) over all possibilities to assign a conditional probability. This is what salmon does. For a single-end read (assume forward orientation for simplicity) at position i on a transcript of length n, we consider the conditional fragment length probability to be given by F_n(n-i), where f_n is the conditional fragment length distribution conditioned on the transcript length (maximum observable length) being n and F_n is the cumulative distribution function of f_n. Intuitively, this means that fragments very close to transcript ends will get a smaller conditional probability, while those farther from the end will get larger conditional probabilities. The `--noSingleFragProb` flag simply turns off this conditional probability all together. It is _not_ recommended to disable the single-end fragment length probability modeling. We have evidence from testing that it improves quantification accuracy. Thus, I would suggest _not_ setting the `--noSingleFragProb` flag. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/575#issuecomment-711061553
https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531:388,Availability,down,down,388,"Hi @yeodynasty,. There are two different ways to tackle this question. The first relies on the fact that the correction employed by salmon for GC bias is done via the adjustment of transcript effective lengths. Here, you could compare the effective length in the quant.sf file to the effective length you would get ignoring GC-fragment (or other bias). Granted, the latter is not written down in the file here, but it is straightforward to calculate since salmon also writes out the fragment length distribution. ; The effective length discarding bias estimates is simply the transcript length, minus the mean of the conditional fragment length distribution (the fragment length distribution from 0 up to the transcript length, re-normalized to be an appropriate probability distribution). If you look at the differences between these values, you can infer how much bias correction was applied. Specifically, when the bias-corrected length is longer than the non bias-corrected length, then these transcripts are over-represented in sequencing and the bias correction aims to reduce their estimated abundance. On the other hand, when the bias-corrected length is shorter than the non bias-corrected length, then these transcripts are under-represented in sequencing and the bias correction aims to increase their estimated abundance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531
https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531:1076,Energy Efficiency,reduce,reduce,1076,"Hi @yeodynasty,. There are two different ways to tackle this question. The first relies on the fact that the correction employed by salmon for GC bias is done via the adjustment of transcript effective lengths. Here, you could compare the effective length in the quant.sf file to the effective length you would get ignoring GC-fragment (or other bias). Granted, the latter is not written down in the file here, but it is straightforward to calculate since salmon also writes out the fragment length distribution. ; The effective length discarding bias estimates is simply the transcript length, minus the mean of the conditional fragment length distribution (the fragment length distribution from 0 up to the transcript length, re-normalized to be an appropriate probability distribution). If you look at the differences between these values, you can infer how much bias correction was applied. Specifically, when the bias-corrected length is longer than the non bias-corrected length, then these transcripts are over-represented in sequencing and the bias correction aims to reduce their estimated abundance. On the other hand, when the bias-corrected length is shorter than the non bias-corrected length, then these transcripts are under-represented in sequencing and the bias correction aims to increase their estimated abundance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531
https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531:565,Usability,simpl,simply,565,"Hi @yeodynasty,. There are two different ways to tackle this question. The first relies on the fact that the correction employed by salmon for GC bias is done via the adjustment of transcript effective lengths. Here, you could compare the effective length in the quant.sf file to the effective length you would get ignoring GC-fragment (or other bias). Granted, the latter is not written down in the file here, but it is straightforward to calculate since salmon also writes out the fragment length distribution. ; The effective length discarding bias estimates is simply the transcript length, minus the mean of the conditional fragment length distribution (the fragment length distribution from 0 up to the transcript length, re-normalized to be an appropriate probability distribution). If you look at the differences between these values, you can infer how much bias correction was applied. Specifically, when the bias-corrected length is longer than the non bias-corrected length, then these transcripts are over-represented in sequencing and the bias correction aims to reduce their estimated abundance. On the other hand, when the bias-corrected length is shorter than the non bias-corrected length, then these transcripts are under-represented in sequencing and the bias correction aims to increase their estimated abundance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/578#issuecomment-717267531
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923:120,Availability,down,downstream,120,"Thanks @tamuanand for the (as always) detailed and clear question! Since this directly involves `tximport` and `DESeq2` downstream, let me also ping @mikelove here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923:144,Availability,ping,ping,144,"Thanks @tamuanand for the (as always) detailed and clear question! Since this directly involves `tximport` and `DESeq2` downstream, let me also ping @mikelove here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923:51,Usability,clear,clear,51,"Thanks @tamuanand for the (as always) detailed and clear question! Since this directly involves `tximport` and `DESeq2` downstream, let me also ping @mikelove here.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719143923
https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719806601:17,Usability,simpl,simpler,17,"To keep the code simpler:. ```; txi <- tximport(files, type = ""salmon"", tx2gene = tx2gene, countsFromAbundance = ""lengthScaledTPM""); # then below...; dds <- DESeqDataSetFromTximport(txi, sampleTable, ~condition); ```. DESeq2 will do the right thing based on the value of `txi$countsFromAbundance`. This is the point of the importer functions. We also have them in tximeta for edgeR and limma. (You can still use tximeta with organisms other than human, mouse, or fly, you just have to run `makeLinkedTxome` and point to the GTF for your organism. It's just one step really.)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/581#issuecomment-719806601
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:124,Deployability,update,updated,124,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:388,Deployability,update,update,388,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:732,Deployability,update,update,732,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:104,Integrability,message,message,104,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:400,Integrability,message,message,400,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:471,Integrability,message,messages,471,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:149,Security,validat,validateMappings,149,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:499,Testability,log,logger,499,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670:704,Usability,clear,clear,704,"Hi @ACastanza,. Thanks for reporting both of these. For the first, I think it is just the case that the message needs to be updated. In fact, the `--validateMappings` flag is now deprecated since selective-alignment is used by default (and can't be turned off, except in the single-cell mapping context, with the `--sketchMode` flag, which is currently only in the develop branch). We'll update that message.; Regarding the misplaced newline, the issue is that the other messages are written by the logger, which is asynchronous. So, sometimes it will get to the appropriate place and write a newline before the fragment counter starts, and sometimes it won't. I'll look into if there is a way to better clear the line, even if the update is asynchronous. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/587#issuecomment-729044670
https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492:353,Deployability,update,update,353,"Hi @guidohooiveld,. Yes, the original cutoff was set to accomodate TITIN, which, at the time, was the longest human transcript. Note, this warning doesn't prevent the transcript from being indexed, it just notifies the user its exceptionally long. I think updating the warning threshold makes sense so that gencode indexes cleanly in this regard. We'll update this. Thanks for the report!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492
https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492:4,Usability,guid,guidohooiveld,4,"Hi @guidohooiveld,. Yes, the original cutoff was set to accomodate TITIN, which, at the time, was the longest human transcript. Note, this warning doesn't prevent the transcript from being indexed, it just notifies the user its exceptionally long. I think updating the warning threshold makes sense so that gencode indexes cleanly in this regard. We'll update this. Thanks for the report!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/591#issuecomment-733736492
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:2032,Energy Efficiency,green,green,2032,"umGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both transcripts in each sample. The green line at the top is what you get if you sum the abundances of these two transcripts. The main point is that the inferential relative variance (adjusted ratio of the variance over the mean) is _much_ smaller for the sum of these transcripts than for either individually. This is strong evidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you ar",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:4127,Energy Efficiency,reduce,reduce,4127,"r resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance, end up being assigned very different abundances in different samples / over different runs. Sorry for the information dump, but I wanted to lay out what might be going on, how to assess it, and what some potential solutions might be. If you dive in to start investigating this, feel free to reach out in this issue along the way if yo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3323,Integrability,depend,depending,3323,"vidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-un",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:201,Performance,optimiz,optimization,201,"Hi @reganhayward,. Thank you for the detailed report. It's interesting that this happens when running with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:784,Performance,optimiz,optimization,784,"Hi @reganhayward,. Thank you for the detailed report. It's interesting that this happens when running with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:1036,Performance,perform,perform,1036,"ing with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both transcripts in each sample. The green line at the top is what you get if you sum the abundances of ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:4377,Performance,perform,performing,4377,"elective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance, end up being assigned very different abundances in different samples / over different runs. Sorry for the information dump, but I wanted to lay out what might be going on, how to assess it, and what some potential solutions might be. If you dive in to start investigating this, feel free to reach out in this issue along the way if you get stuck or have follow-up questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:4740,Safety,avoid,avoid,4740,"elective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance, end up being assigned very different abundances in different samples / over different runs. Sorry for the information dump, but I wanted to lay out what might be going on, how to assess it, and what some potential solutions might be. If you dive in to start investigating this, feel free to reach out in this issue along the way if you get stuck or have follow-up questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:2769,Security,access,access,2769,"485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both transcripts in each sample. The green line at the top is what you get if you sum the abundances of these two transcripts. The main point is that the inferential relative variance (adjusted ratio of the variance over the mean) is _much_ smaller for the sum of these transcripts than for either individually. This is strong evidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:917,Testability,test,test,917,"Hi @reganhayward,. Thank you for the detailed report. It's interesting that this happens when running with STAR but not when running with selective alignment. However, salmon will attempt to solve the optimization problem with the alignments it is given, regardless of if those come from STAR or from it's built-in selective alignment. While I would generally expect these to be similar, the alignment algorithms are different; see [e.g. the differences between SA/SAF & STAR here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). Nonetheless, it is possible that for a small subset of transcripts, the probabilistic allocations are _so_ ambiguous, that you get large swings in the resulting quantification estimates based on tiny variations in where the optimization starts (which is, itself, stochastic due to the asynchronous nature of salmon's online inference phase). One way we can test this hypothesis is as follows. You can run salmon with `--numGibbsSamples 100` and `-d`. This will tell salmon to perform posterior Gibbs sampling (`--numGibbsSamples 100`) and to dump the range-factorized equivalence classes used for offline quantification (`-d`). The Gibbs sampling files will contain the traces for the transcripts in question over the various iterations of the sampling procedure. Transcripts where there is a tremendous amount of ambiguity will tend to have highly anti-correlated posterior samples, and similarly, if you were to consider the abundance output of these transcripts as a *group*, there would be a large reduction in inferential relative variance. In fact, we [wrote a whole paper on this topic](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485). Consider this example from that paper:. ![image](https://user-images.githubusercontent.com/361470/101438021-706d3600-38df-11eb-9ada-a54ea9092d2d.png). The x-axis is samples from the Gibbs chains, and the y-values denote the estimated number of reads assigned to both t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3599,Testability,test,test,3599," Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3086,Usability,simpl,simply,3086,"top is what you get if you sum the abundances of these two transcripts. The main point is that the inferential relative variance (adjusted ratio of the variance over the mean) is _much_ smaller for the sum of these transcripts than for either individually. This is strong evidence that they are _inherently_ uncertain given the read evidence and alignments used for quantification. The tool described in that paper, called [`terminus`](https://github.com/COMBINE-lab/terminus), is a tool for automatically finding such groups of transcripts. Anyway, once you have the Gibbs samples in hand, we can walk you though how to do some assessment of these transcripts (tagging @hiraksarkar here since he's most likely to have access to scripts that will let us look at the posterior samples from individual transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115:3867,Usability,simpl,simply,3867," transcripts). Similarly, if you can provide the quantification directory, we can help examine this too. If this is the case, that the posterior distributions are highly anti-correlated, it is likely that the ambiguity you are seeing is simply inherent given the alignments salmon is being provided. If you have the quantification folder resulting from the same sample using selective alignment, we could compare and contrast the two. At that point, there are a few options depending on how deeply you want to dive. You could try to see how STAR and selective alignment are mapping differently to these transcripts. One potential difference is that STAR is _a lot_ more happy to softclip reads, which selective alignment won't do by default (you can test the effect with the `--softclipOverhangs` to allow selective alignment to softclip reads that hang off the transcript end or `--softclip` to allow softclips anywhere). Note that selective alignment may _still_ be a bit more conservative than STAR about softclips simply because of the nature of the scoring function it uses. This might give you a sense if one of these alignment methodologies is more consistent with your expectations in this case. Another option is to consider doing a grouping with `terminus`. This will reduce the set of ""genes"" that you can call as DE, because it will be happy to group together transcripts from different genes. However, it should help considerably in eliminating DE from highly-uncertain point estimates. Finally, you might consider performing DE with swish (cc @mikelove as he might have some input here) rather than DESeq2 (though we've typically been using swish at the transcript level rather than the gene level). Unlike DESeq2, swish will explicitly take into account the inferential uncertainty in the abundance estimates, using the Gibbs samples produced by salmon. This will allow it to avoid spurious DE calls that might otherwise occur when you have highly uncertain transcripts that, by chance,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/600#issuecomment-740363115
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:828,Integrability,depend,depending,828,"Hi @rmurray2,. Thanks again for the detailed question (I answered them in reverse order, so that's why I'm saying ""again"" here). There are a few things going on that could be leading to differences. They are, in the order I think they will have an effect on the result:. * You are using RSEM in a mode that is mapping the reads to the entire genome (using STAR) and then projecting the resulting alignments to the transcriptome. You are using salmon in a way that is performing selective alignment against the transcriptome only. We have recently published [a paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8) discussing in detail the effect that some of these choices can have on transcript and gene-level abundance estimation. In general, if you don't include the genome as a mapping target, depending on your sample, there may be certain reads that are assigned to the transcriptome even though they have a better alignment to some other genomic location. This is independent of e.g. salmon and RSEM, and you'd observe the same thing if you ran RSEM using e.g. Bowtie2 as the aligner aligning against the transcriptome. Luckily, you can control this source of variation. Salmon, like RSEM, can accept alignments to the transcriptome produced by STAR. If you want to see how big of an effect this is having in your sample, you can align reads to the genome using STAR (and project them to the transcriptome) to produce a BAM file that salmon can quantify. You can check RSEM's script to see exactly how it invokes STAR, but the parameters are something like `--outFilterType BySJout --alignSJoverhangMin 8 --outFilterMultimapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3543,Integrability,depend,depending,3543,"e relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). * Finally, and likely the smallest source of potential differences, is that there are other implementation details that differ between salmon and RSEM (e.g. exactly how th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3660,Modifiability,variab,variable,3660,"efault. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). * Finally, and likely the smallest source of potential differences, is that there are other implementation details that differ between salmon and RSEM (e.g. exactly how the fragment length distribution is used to compute the effective transcript length, exactly how the alignment score of a read is used to as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:467,Performance,perform,performing,467,"Hi @rmurray2,. Thanks again for the detailed question (I answered them in reverse order, so that's why I'm saying ""again"" here). There are a few things going on that could be leading to differences. They are, in the order I think they will have an effect on the result:. * You are using RSEM in a mode that is mapping the reads to the entire genome (using STAR) and then projecting the resulting alignments to the transcriptome. You are using salmon in a way that is performing selective alignment against the transcriptome only. We have recently published [a paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8) discussing in detail the effect that some of these choices can have on transcript and gene-level abundance estimation. In general, if you don't include the genome as a mapping target, depending on your sample, there may be certain reads that are assigned to the transcriptome even though they have a better alignment to some other genomic location. This is independent of e.g. salmon and RSEM, and you'd observe the same thing if you ran RSEM using e.g. Bowtie2 as the aligner aligning against the transcriptome. Luckily, you can control this source of variation. Salmon, like RSEM, can accept alignments to the transcriptome produced by STAR. If you want to see how big of an effect this is having in your sample, you can align reads to the genome using STAR (and project them to the transcriptome) to produce a BAM file that salmon can quantify. You can check RSEM's script to see exactly how it invokes STAR, but the parameters are something like `--outFilterType BySJout --alignSJoverhangMin 8 --outFilterMultimapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2662,Performance,optimiz,optimization,2662,"imapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2863,Performance,optimiz,optimizing,2863," BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the align",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2271,Safety,avoid,avoiding,2271,"this is having in your sample, you can align reads to the genome using STAR (and project them to the transcriptome) to produce a BAM file that salmon can quantify. You can check RSEM's script to see exactly how it invokes STAR, but the parameters are something like `--outFilterType BySJout --alignSJoverhangMin 8 --outFilterMultimapNmax 20 --alignSJDBoverhangMin 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --eadFilesCommand zcat --outSAMtype BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3282,Testability,test,test,3282,"appings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://gen",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:2839,Usability,simpl,simply,2839," BAM Unsorted --quantMode TranscriptomeSAM --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan IndelSoftclipSingleend`; note that last parameter that I will come back to later. Also, the paper referenced above also describes a new capability present in recent versions of salmon that allow it to index the entire genome (as well as the transcriptome) to have the former act as a decoy. This allows avoiding what might otherwise be spurious mappings that result when one considers only the transcriptome as a source of mapping. There are a number of ways to proceed on this front, but this is a good place to first check for discrepancy (and the paper gives a good overview of the relative tradeoffs and merits of different alignment approaches). * Salmon and RSEM use related but distinct optimization algorithms by default. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the align",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590:3672,Usability,simpl,simply,3672,"efault. RSEM uses the EM algorithm, and salmon uses the variational Bayesian EM algorithm. The latter tends to induce more sparse solutions. This is simply because they are optimizing slightly different objectives. It is very difficult to say in general if one is ""better"" than the other in a blanket way, but [there is previous literature to support that the VBEM may be more accurate](https://academic.oup.com/bioinformatics/article/29/18/2292/239795). However, while RSEM only implements the EM algorithm, salmon actually implements and provides a switch to use either. So, if you want to test the effect of this difference, you can run salmon with the `--useEM` algorithm. This will tell salmon to use the ""classic"" EM algorithm and will eliminate this source of variation. * As with the other question you asked, there may be a _small_ discrepancy depending on when enforcement of a stranded library kicks in under salmon's `A` library type. You can eliminate that variable by simply providing `-l SF` to match the library type being used with RSEM. * Coming back to the `IndelSoftclipSingleend` parameter I mentioned in the first point; RSEM disallows indels in the alignments that it quantifies. This means that to produce RSEM-compatible input, STAR must not align reads that contain indels. While this won't generally have a big effect for many transcripts, it can certainly affect the abundance estimates for transcripts in your sample where the sample you are quantifying has (indel) variation with respect to the reference annotation. We touch upon that a bit as well in the [paper I mentioned above](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8). * Finally, and likely the smallest source of potential differences, is that there are other implementation details that differ between salmon and RSEM (e.g. exactly how the fragment length distribution is used to compute the effective transcript length, exactly how the alignment score of a read is used to as",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/612#issuecomment-758004590
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:2177,Availability,down,downstream,2177,"e result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the preci",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3540,Availability,robust,robustly,3540,"against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:4274,Availability,reliab,reliably,4274,"alse confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to be able to have e.g. exactly the same numerical output for a particular sample, we feel that doing so might convey a false sense of certainty in the resulting estimates (and it would also be very difficult to do, technically, given the streaming asynchronous phase of the method). This also means, of course, that you should be wary of the precision between runs even for methods that produce their estimates in 100% deterministic ways (e.g. RSEM, etc.); you may get identical or near identical numbers, but without an estimate of the uncertainty, th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:120,Deployability,release,released,120,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:181,Deployability,release,released,181,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:2654,Deployability,update,update,2654,"transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3758,Deployability,release,release,3758,"riments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to be able to have e.g. exactly the same numerical output for a particular sample, we feel that doing so might convey a fal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1568,Modifiability,variab,variables,1568,"ssue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actua",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1207,Performance,optimiz,optimization,1207,"r. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you obser",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1861,Performance,optimiz,optimization,1861,"mon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samp",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3631,Performance,perform,perform,3631,"pdate order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:553,Safety,detect,detection,553,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:421,Testability,test,test,421,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:3914,Testability,test,test,3914," fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the sample, it cannot and should not be removed. Having exact replication of a sample at a numerical threshold below the inferential uncertainty for a transcript conveys false confidence in the precision of the estimate. This is why, for transcript-level analysis, we highly recommend having salmon produce posterior gibbs samples (with the `--numGibbsSamples` flag). This will draw samples from the posterior distribution over the abundance estimates and allow determination of what inferences can be made robustly and what cannot. We have spent a good deal of time thinking about how to properly perform statistical inference on these uncertain quantities, and so I'd point you at [swish](https://bioconductor.org/packages/release/bioc/vignettes/fishpond/inst/doc/swish.html), which is a tool for differential analysis at the transcript level that makes uses of a non-parametric test over the inferential replicates (Gibbs samples) to incorporate uncertainty into the differential analysis. We also developed a tool [terminus](https://academic.oup.com/bioinformatics/article/36/Supplement_1/i102/5870485) that makes use of the Gibbs samples and point estimates of salmon to group together transcripts whose individual abundances cannot be reliably inferred given the fragments in the sample. While the best way to properly assess, propagate and handle uncertainty in transcript-level inference is still, in my opinion, an active area of research in the field, these are some solutions we've come up with to address this challenge so far. And while, as a computer scientist myself, I _certainly_ appreciate the desire to be able to have e.g. exactly the same numerical output for a particular sample, we feel that doing so might convey a false sense of certainty in the resulting estimates (and it would also be very difficult to do, technically, given the streaming asynchronous p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:885,Usability,simpl,simply,885,"Hi @rmurray2,. Thank you for the report. First, I just want to mention that I don't believe v0.99.0 to be an officially released version number. That is, there was a v0.14.x and a (released in source only v0.15.0), and then the versions moved to 1.0.0 and beyond. However, this behavior certainly isn't related to that. There are 2 things going on that can lead to this effect. The first one, which is relatively easy to test, is that there may be small changes in when the inferred library type starts to be enforced (if it is not `IU`) when auto type detection is used (see [this issue and comments therein](https://github.com/COMBINE-lab/salmon/issues/489)). The second and more fundamental thing going on is that the observed behavior is intended. Even with a single thread of execution provided to salmon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:1753,Usability,simpl,simply,1753,"mon for mapping and quantification, there is a separate background thread that simply consumes reads from file and puts them in memory for quantification, and while e.g. pairing information between files is guaranteed to be preserved, exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samp",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858:2016,Usability,simpl,simply,2016," exact read order is not. This can lead to differences in the order in which reads are processed and, as a result, differences in the initialization conditions of the optimization. The ultimate result is that for transcripts that have large inferential uncertainty, different numbers of reads can be assigned between runs. We have thought *a lot* about this behavior, what it means, and how the `NumRead` values should best be communicated to users. At the end of the day, the `NumReads` constitute the expected value of latent variables inferred in a _very_ high-dimensional space (# of parameters is at least the number of transcripts). Therefore, there are certain transcripts, whose estimated number of reads simply have _tremendous_ inferential uncertainty — and small perturbations in the initial conditions of the optimization will lead to different estimated values for their abundances. For those transcripts where you observe such fluctuations between runs, this is simply evidence that the precision that can be confidently placed on those estimates is below the degree of variation you observe. Treating these transcripts in downstream analysis as more certain can easily lead to spurious inferences regarding things like differential transcript expression or usage. . One can make an argument for trying to provide a way to enforce removal of this variation (which, granted, would be a challenge). However, the reason we decided against even attempting this is because it doesn't properly address any issue with respect to an actual biological analysis. That is, even if you could fix, precisely, the update order and initialization conditions for a specific sample to eliminate any variation between runs, almost all experiments consist of multiple samples. In other samples, the same transcript fractions could give rise to a slightly different set of observed fragments that induce exactly the same type of variation under uncertainty; and since that uncertainty is baked into the samp",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/613#issuecomment-757989858
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938:429,Safety,risk,risk,429,"Just in case it helps, I've written a script to splice out cell barcode linker sequences and shift them to before the polyA. In the process of doing this, it also does a 2-distance hamming correction of cell barcode and linker regions. All operations assume there are no INDELs:. https://gitlab.com/gringer/bioinfscripts/-/blob/master/synthSquish.pl. [usual disclaimers apply: I cannot guarantee that this works; use at your own risk]. This script could be used as a stop-gap measure to pre-process Rhapsody reads for use with Alevin via the undocumented custom length settings [--end 5, --barcodeLength 27, --umiLength 8]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938
https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938:542,Usability,undo,undocumented,542,"Just in case it helps, I've written a script to splice out cell barcode linker sequences and shift them to before the polyA. In the process of doing this, it also does a 2-distance hamming correction of cell barcode and linker regions. All operations assume there are no INDELs:. https://gitlab.com/gringer/bioinfscripts/-/blob/master/synthSquish.pl. [usual disclaimers apply: I cannot guarantee that this works; use at your own risk]. This script could be used as a stop-gap measure to pre-process Rhapsody reads for use with Alevin via the undocumented custom length settings [--end 5, --barcodeLength 27, --umiLength 8]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/628#issuecomment-776417938
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:764,Availability,error,errors,764,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:824,Availability,error,error,824,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:1270,Availability,error,error,1270,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628:1165,Usability,clear,clears,1165,"Hi @danphillips28,. Oh ok! Now I see what's going on. We really should be more stringent about catching the issue here. . The problem is that using k > 32 is not permitted in the current implementation. This is because we use a 64-bit machine word to encode k-mers, and you can only fit up to 32 nucleotides in a word. In reality, anything > 31 is not allowed, since the k-mer should be odd so that the orientation can be inferred from the canonical (lexicographically smaller) version. There is nothing fundamentally problematic about using a larger k, it's just that it would require some modifications throughout the codebase we've not yet made. Further, we've not really found cases where having such large k really help. Specifically, the larger k, the fewer errors you need to render a read unmappable (if there is an error in every k-mer, you can't map the read). On the other hand, selective alignment will do a good job of filtering out poor matches where there is insufficient similarity between the read and reference. Thus, we set a default value of k=31. You can go lower (with any odd value), but it's not currently possible to go higher. I hope this clears things up, and thanks for bringing this to our attention. I think the indexer should bail with an error in this case, until (and unless) this feature is supported. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779418628
https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779421124:91,Usability,simpl,simply,91,"Hi Rob,. Thanks! That's brilliant. Just what I needed. Now I've realised what I've done. I simply hadn't checked the manual for a while and in my mind the default (max) k had morphed from 31 into 35 (silly me, really need to check the manual more). . All the best, and thanks again!; Dan",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/632#issuecomment-779421124
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:768,Availability,error,error,768,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:2240,Availability,error,error,2240," protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrection` flag *is* better for ONT data, though results without this flag are sub-optimal, they are not unusable. We have let ONT know about this, and I would suspect they will address it (perhaps they'll even accept a PR?). Finally, a long read error model has been created and will _hopefully_ make it to the next version of salmon. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:861,Deployability,release,release,861,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:283,Integrability,depend,dependent,283,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:340,Integrability,protocol,protocols,340,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:1444,Integrability,depend,depends,1444," protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrection` flag *is* better for ONT data, though results without this flag are sub-optimal, they are not unusable. We have let ONT know about this, and I would suspect they will address it (perhaps they'll even accept a PR?). Finally, a long read error model has been created and will _hopefully_ make it to the next version of salmon. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:703,Performance,optimiz,optimize,703,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:457,Testability,test,tested,457,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147:968,Usability,simpl,simple,968,"Dear @callumparr,. Thank you for bringing this up. So you are correct that the `--noLengthCorrection` flag should be passed to salmon when quantifying data that does not have a ""fragmentation effect"", that is, where the number of fragments we expect to draw from a transcript is not dependent upon the length of that transcript. In the ONT protocols, it is usually the case that we get 1 read -> 1 transcript, even if we don't read the whole thing. We have tested the effect of this in ONT data with spike ins, and have verified that using `--noLengthCorrection` does generally lead to improved accuracy with respect to quantification estimates. We have informed ONT of this, and I would guess they may optimize the flags that are used soon (we have also developed an error model that works correctly for these long reads, and that should make it into the next release of salmon). Regarding the effect this has on the `NumReads` values reported by salmon, it's not as simple as with the `TPM` estimates. The length affects the assigned reads through the probabilistic model on which inference is done. With the length effect we have that P(f | t_i) ∝ P( position | f, t_i ) * P( alignment | f, t_i) --- forgetting the alignment term for the time being, we have that with length correction P( position | f, t_i ) ∝ 1 / l_i and without length correction the l_i term goes away. In other words, the probability of allocating reads has a term that depends on the effective length when the `--noLengthCorrection` flag is not passed, but that term goes away when it is passed. This is not quite as drastic as with TPM where the normalization includes the length directly in the normalization (note, however, that when the `--noLengthCorrection` flag is passed, this adjusts the TPM as well). Further, the `NumReads` is still better than TPM in this regard because it still encodes the effect size (i.e. `NumReads` will sum to the total number of aligned reads). Anyway TLDR: Passing the `--noLengthCorrectio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/651#issuecomment-821995147
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1514,Deployability,integrat,integrated,1514,"at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the pr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1514,Integrability,integrat,integrated,1514,"at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the pr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2336,Integrability,protocol,protocol,2336,"cript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:3164,Integrability,protocol,protocols,3164,"he premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. do",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:4276,Integrability,message,message,4276," subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. dotPlot.fname <- '../ouput/dotPlots.pdf'. ### Read exon annotations ####; message('Loading Ensembl exon annotation (1-22, X, Y, MT)...'). chromosomes <- c(1:22, 'X', 'Y', 'MT'). anot <- import(anot.gtf, feature = 'exon') %>% sort; anot <- anot[seqnames(anot) %in% chromosomes, ]. # append gene and transcript version numbers to IDs; anot$gene_id <- paste(anot$gene_id, anot$gene_version, sep = '.'); anot$transcript_id <- paste(anot$transcript_id, anot$transcript_version, sep = '.'). ### Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_');",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:4737,Integrability,message,message,4737,"ttr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. dotPlot.fname <- '../ouput/dotPlots.pdf'. ### Read exon annotations ####; message('Loading Ensembl exon annotation (1-22, X, Y, MT)...'). chromosomes <- c(1:22, 'X', 'Y', 'MT'). anot <- import(anot.gtf, feature = 'exon') %>% sort; anot <- anot[seqnames(anot) %in% chromosomes, ]. # append gene and transcript version numbers to IDs; anot$gene_id <- paste(anot$gene_id, anot$gene_version, sep = '.'); anot$transcript_id <- paste(anot$transcript_id, anot$transcript_version, sep = '.'). ### Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <-",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:5692,Integrability,message,message,5692,"# Create premature transcript annotations ####; message('Creating premature transcript annotation...'). anot.pre <- split(anot, anot$transcript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$prema",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7388,Integrability,message,message,7388,"emature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7632,Integrability,message,message,7632,"!grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(anot.pre$premature_group, '_premature') # premature rna indicator. ### Dot plots ####; smoothDot <- function(s1, s2, w = 10) {. s1a <- sapply(; X = 1:(length(s1) - w + 1),; function(z) paste(s1[ z:(z + w - 1) ], collapse = ''); ). s2a <- sapply(;",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7943,Integrability,message,message,7943,"not.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(anot.pre$premature_group, '_premature') # premature rna indicator. ### Dot plots ####; smoothDot <- function(s1, s2, w = 10) {. s1a <- sapply(; X = 1:(length(s1) - w + 1),; function(z) paste(s1[ z:(z + w - 1) ], collapse = ''); ). s2a <- sapply(; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ], collapse = ''); ). s2b <- sapply( # considering the reversed y sequence per window; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ] %>% rev, collapse = ''); ). outer(s1a, s2a, FUN = '==') | outer(s1a, s2b, FUN = '=='); }. ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:8029,Integrability,message,message,8029,"nes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group] <- reverseComplement(; premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$premature_group]; ). names(premature.tx) <- anot.pre$premature_group # paste0(anot.pre$premature_group, '_premature') # premature rna indicator. ### Dot plots ####; smoothDot <- function(s1, s2, w = 10) {. s1a <- sapply(; X = 1:(length(s1) - w + 1),; function(z) paste(s1[ z:(z + w - 1) ], collapse = ''); ). s2a <- sapply(; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ], collapse = ''); ). s2b <- sapply( # considering the reversed y sequence per window; X = 1:(length(s2) - w + 1),; function(z) paste(s2[ z:(z + w - 1) ] %>% rev, collapse = ''); ). outer(s1a, s2a, FUN = '==') | outer(s1a, s2b, FUN = '=='); }. to.plot <- lapply(; setNames(nm = chosenOnes),; function(x) {. # out <- outer(as.vector",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:989,Performance,load,load,989,"ob-p ,. Edit: I have resolved the problem. It is not a problem with Biostrings or GRanges. It turns out that when subsetting the premature sequences, the subsetted sequences do not retain the names of the GRanges used to subset them therefore my code could not identify minus strand transcripts and get their reverse complements. Apologies for any confusion!; ---; Thank you very much for the prompt response and for taking the time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1062,Performance,load,loaded,1062,"d sequences do not retain the names of the GRanges used to subset them therefore my code could not identify minus strand transcripts and get their reverse complements. Apologies for any confusion!; ---; Thank you very much for the prompt response and for taking the time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:1393,Performance,load,loaded,1393," time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:11816,Performance,load,loaded,11816,"_blank() # element_rect(fill = 'grey'); ) +; facet_wrap(paste('tx strand:', tx_strand) ~ paste('tx ID:', tx_id) + paste('myFasta seq type:', myFastaSeqType), ncol = 3, scales = 'free'). print(ggp). # dev.off(). ```. R session info:; ```; R version 4.0.2 (2020-06-22); Platform: x86_64-pc-linux-gnu (64-bit); Running under: CentOS Linux 7 (Core). Matrix products: default; BLAS/LAPACK: [hidden]/easybuild/software/2017/Core/imkl/2018.3.222/compilers_and_libraries_2018.3.222/linux/mkl/lib/intel64_lin/libmkl_gf_lp64.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 ; [5] LC_MONETARY=en_CA.UTF-8 LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C ; [9] LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] parallel stats4 stats graphics grDevices utils datasets methods base . other attached packages:; [1] ggplot2_3.3.3 reshape2_1.4.4 Biostrings_2.58.0 XVector_0.30.0 rtracklayer_1.50.0 ; [6] GenomicRanges_1.42.0 GenomeInfoDb_1.26.7 IRanges_2.24.1 S4Vectors_0.28.1 BiocGenerics_0.36.1 ; [11] magrittr_2.0.1 data.table_1.14.0 . loaded via a namespace (and not attached):; [1] SummarizedExperiment_1.20.0 tidyselect_1.1.0 purrr_0.3.4 lattice_0.20-41 ; [5] colorspace_2.0-0 vctrs_0.3.7 generics_0.1.0 yaml_2.2.1 ; [9] utf8_1.2.1 XML_3.99-0.6 rlang_0.4.10 pillar_1.6.0 ; [13] glue_1.4.2 withr_2.4.1 DBI_1.1.1 BiocParallel_1.24.1 ; [17] matrixStats_0.58.0 GenomeInfoDbData_1.2.4 lifecycle_1.0.0 plyr_1.8.6 ; [21] stringr_1.4.0 zlibbioc_1.36.0 MatrixGenerics_1.2.1 munsell_0.5.0 ; [25] gtable_0.3.0 labeling_0.4.2 Biobase_2.50.0 fansi_0.4.2 ; [29] Rcpp_1.0.6 scales_1.1.1 DelayedArray_0.16.3 farver_2.1.0 ; [33] Rsamtools_2.6.0 digest_0.6.27 stringi_1.5.3 dplyr_1.0.5 ; [37] grid_4.0.2 tools_4.0.2 bitops_1.0-6 RCurl_1.98-1.3 ; [41] tibble_3.1.0 crayon_1.4.1 pkgconfig_2.0.3 ellipsis_0.3.1 ; [45] Matrix_1.2-18 assertthat_0.2.1 rstudioapi_0.13 R6_2.5.0 ; [49] GenomicAlignments_1.26.0 compiler_4.0.2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:6145,Safety,avoid,avoid,6145," anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:7109,Safety,sanity check,sanity check,7109,"erparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same for pre and mature RNA counterparts); all(; sort(paste(strand(anot), anot$transcript_id) %>% unique) ==; sort(paste(strand(anot.pre), anot.pre$premature_group) %>% unique); ) %>% print. ### Mature transcript sequences ####; message('Creating mature transcript sequences...'). # subset pos sorted exons, split by tx ID, concatenate exon seq per transcript using unlist; mature.tx <- lapply(; X = split(dna[anot], anot$transcript_id),; FUN = unlist; ) %>% DNAStringSet. message('... now getting reverse complements of mature transcripts on the minus strand...'). mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id] <- reverseComplement(; mature.tx[names(mature.tx) %in% anot[strand(anot) == '-', ]$transcript_id]; ). ### Premature transcript sequences ####; message('Creating premature transcript sequences...'). premature.tx <- dna[anot.pre]. message('... now getting reverse complements of premature transcripts on the minus strand...'). premature.tx[names(premature.tx) %in% anot.pre[strand(anot.pre) == '-', ]$",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:443,Security,validat,validate,443,"Dear @rob-p ,. Edit: I have resolved the problem. It is not a problem with Biostrings or GRanges. It turns out that when subsetting the premature sequences, the subsetted sequences do not retain the names of the GRanges used to subset them therefore my code could not identify minus strand transcripts and get their reverse complements. Apologies for any confusion!; ---; Thank you very much for the prompt response and for taking the time to validate Salmon's functionality. Indeed, Salmon is not the problem here. After taking a closer look at my transcript fasta, I noticed a problem with it, as you suggested. Long story short, half the premature transcripts had the wrong orientation and complementarity. Long story:. Oddly, the mature sequences were fine even though I used an identical approach to subset premature and mature transcripts from the genome reference!. Briefly my approach relied on three R packages rtracklayer, GenomicRanges, and Biostrings. 1. I used rtracklayer to load a gtf formatted exon annotations acquired from Ensembl. The file is loaded as a GenomicRanges object which essentially describes the locus of each exon (the transcribed strand [+ or -], chromosome, start and end positions relative to the reference strand) and its associated gene and transcript. 2. I used the GRanges object to generate pre-RNA coordinates that span all exons of a transcript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I ha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2328,Testability,test,test,2328,"cript. 3. I loaded the reference genome fasta acquired from Ensembl using the Biostrings package. GRanges and Biostrings are tightly integrated, allowing me to subset sequences from a Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2979,Testability,test,test,2979," reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:2516,Usability,feedback,feedback,2516,"Biostrings object using the GRanges object. **I believe the problem lies here.** It appears that when subsetting the mature exonic sequences from Biostrings using GRanges, the strand field in the GRanges object **was not** utilized. I.e., I needed to get the reverse complement of the extracted sequences for transcripts on the minus strand. I had done that and assumed that this behaviour would be consistent. However, for reasons I have not been able to pinpoint (potentially a bug), the strand information **was accounted for** when I used GRanges to subset the premature sequences. I **did not** need to get the reverse complement of the premature sequences on the minus strand as I had to do for the mature sequences. Yet, I did that anyway. I initially did test my protocol to ensure it produced identical transcript sequences to Gencode, but I only did this for mature sequences. All seemed fine for both + and - strand transcripts. After your feedback, I compared the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potenti",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:3591,Usability,guid,guidance,3591," the premature sequences in my transcript fasta against the transcript fasta from Gencode. As you can see in the smoothed dot plots below, the premature sequences of transcripts on the minus strand are in the wrong orientation and have the wrong complementarity!. ![Screenshot from 2021-04-24 00-35-05](https://user-images.githubusercontent.com/10429333/115947355-0d96c880-a495-11eb-92a6-d8d2233c8d2b.png). I included my R code below for this test case for anyone who might stumble upon this issue. Under the code headers “Mature transcript sequences” and “Premature transcript sequences”, you can observe that I used identical protocols for sequence subsetting, yet in the mature case the strand information in the GRanges seems to be disregarded when subsetting from Biostrings, but in the premature case the strand information is used. Of course, this problem is out of the scope of this forum so it will be okay to close this issue. I will reach out to the developers of GenomicRanges and Biostrings to point out this potential problem and seek their guidance. Thank you again for all your help. Rached. ```; # setwd('wd'). options(scipen = 9999). libraries <- lapply(; X = c('data.table', 'magrittr', 'rtracklayer', 'Biostrings', 'reshape2', 'ggplot2'),; FUN = library, character.only = TRUE; ). ### Inputs ####; anot.gtf <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.101.gtf.gz' # Ensembl GTF; genome.fasta <- '../../shared_data/annotations/Ensembl/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz' # Genome fasta from Ensembl; gencode.tx.fasta <- '../../shared_data/annotations/Gencode/gencode.v35.transcripts.fa.gz' # Gencode transcript FASTA. dotPlot.fname <- '../ouput/dotPlots.pdf'. ### Read exon annotations ####; message('Loading Ensembl exon annotation (1-22, X, Y, MT)...'). chromosomes <- c(1:22, 'X', 'Y', 'MT'). anot <- import(anot.gtf, feature = 'exon') %>% sort; anot <- anot[seqnames(anot) %in% chromosomes, ]. # append gene and transcript version numbers to IDs; a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:5783,Usability,simpl,simplify,5783,"ranscript_id); anot.pre <- anot.pre[lengths(anot.pre) > 1] %>% range %>% unlist %>% sort # only consider transcripts with > 1 exon. anot.pre$transcript_range <- as.character(anot.pre); anot.pre$gene_id <- anot[match(names(anot.pre), anot$transcript_id), ]$gene_id. # collapse replicate pre-mature transcripts per gene...; names(anot.pre) <- anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source =",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:6211,Usability,simpl,simplicity,6211," anot.pre$premature_group <- sapply(; split(; names(anot.pre),; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ),; paste, collapse = ';'; )[; paste(anot.pre$gene_id, anot.pre$transcript_range, sep = '_'); ]. # ... need to convert GR to data.table before unique because unique method for GR class ignores metadata and rownames; anot.pre <- as.data.table(anot.pre) %>% unique %>% makeGRangesFromDataFrame(., keep.extra.columns = T); names(anot.pre) <- anot.pre$premature_group. ### Read human genome sequence ####; message('Loading genome sequence...'). dna <- readDNAStringSet(filepath = genome.fasta). # simplify chromosome names; names(dna) <- sapply(strsplit(names(dna), ' '), '[', 1). dna <- dna[chromosomes] # subset chrom 1-22, X, Y, MT. ### Read Gencode transcript sequences ####; gencode <- readDNAStringSet(gencode.tx.fasta); names(gencode) <- gsub(; pattern = '\\|.*', replacement = '',; x = names(gencode); ). ### Sample transcripts on + and - strand (and avoid premature transcripts with multiple mature counterparts for simplicity); anot.pre <- anot.pre[order(width(anot.pre), decreasing = F), ]. chosenOnesP <- anot.pre[; strand(anot.pre) == '+' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnesM <- anot.pre[; strand(anot.pre) == '-' & !grepl(';', anot.pre$premature_group) & anot.pre$premature_group %in% names(gencode) & !duplicated(anot.pre$premature_group) &; anot.pre$premature_group %in% anot[anot$transcript_source == 'ensembl_havana']$transcript_id; ]$premature_group[1]. chosenOnes <- c(chosenOnesP, chosenOnesM). # subset chosed ones; anot.ori <- anot; anot.pre.ori <- anot.pre. anot <- anot[anot$transcript_id %in% chosenOnes, ]; anot.pre <- anot.pre[anot.pre$premature_group %in% chosenOnes, ]. # sanity check (make sure strand information is the same ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191:10493,Usability,guid,guide,10493,"er,; s2 = as.vector(gencode[[x]]) %>% as.character; ). out2 <- reshape2::melt(out2) %>% as.data.table; colnames(out2) <- c('myFasta', 'GencodeMatureFasta', 'seqMatch'); out2[ , myFastaSeqType := 'premature']. # my premature complement vs gencode mature; out3 <- smoothDot(; s1 = as.vector(premature.tx[[x]] %>% complement) %>% as.character,; s2 = as.vector(gencode[[x]]) %>% as.character; ). out3 <- reshape2::melt(out3) %>% as.data.table; colnames(out3) <- c('myFasta', 'GencodeMatureFasta', 'seqMatch'); out3[ , myFastaSeqType := 'premature (complement)']. out <- rbind(out1, out2, out3); out[ , tx_strand := strand(anot[which(anot$transcript_id == x)[1], ]) %>% as.character]. return(out); }; ) %>% rbindlist(., idcol = 'tx_id'). # pdf(dotPlot.fname, width = 8, height = 6). ggp <- ggplot(mapping = aes(x = myFasta, y = GencodeMatureFasta, fill = seqMatch), data = to.plot) +; geom_tile(width = 1, height = 1) +; scale_fill_manual(; values = c('TRUE' = 'black', 'FALSE' = 'white'),; guide = F; ) + scale_x_continuous(; expand = c(0, 0); ) + scale_y_continuous(; expand = c(0, 0); ) +; theme_bw(base_size = 10) +; theme(; panel.grid = element_blank(), panel.background = element_blank() # element_rect(fill = 'grey'); ) +; facet_wrap(paste('tx strand:', tx_strand) ~ paste('tx ID:', tx_id) + paste('myFasta seq type:', myFastaSeqType), ncol = 3, scales = 'free'). print(ggp). # dev.off(). ```. R session info:; ```; R version 4.0.2 (2020-06-22); Platform: x86_64-pc-linux-gnu (64-bit); Running under: CentOS Linux 7 (Core). Matrix products: default; BLAS/LAPACK: [hidden]/easybuild/software/2017/Core/imkl/2018.3.222/compilers_and_libraries_2018.3.222/linux/mkl/lib/intel64_lin/libmkl_gf_lp64.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 ; [5] LC_MONETARY=en_CA.UTF-8 LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C ; [9] LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] parallel stats4",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826035191
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612:319,Availability,fault,fault,319,"Thank you @rached-97, for the incredibly thorough follow-up to the original issue. I agree that the work you put into not only reporting the issue thoroughly to begin with, but following up with your findings, will certainly help others who might encounter related issues in the future. Since salmon seems not to be at fault here, I'll close the issue. I do recommend taking this over to the bioconductor forums, where the community is _incredibly_ responsive; I imagine you'll have a resolution in no time. Thanks again for the excellent report and follow-up!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612
https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612:449,Usability,responsiv,responsive,449,"Thank you @rached-97, for the incredibly thorough follow-up to the original issue. I agree that the work you put into not only reporting the issue thoroughly to begin with, but following up with your findings, will certainly help others who might encounter related issues in the future. Since salmon seems not to be at fault here, I'll close the issue. I do recommend taking this over to the bioconductor forums, where the community is _incredibly_ responsive; I imagine you'll have a resolution in no time. Thanks again for the excellent report and follow-up!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/655#issuecomment-826117612
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753:210,Deployability,update,updated,210,"Hi @aedavids,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a _decoy_ sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753:237,Deployability,update,update,237,"Hi @aedavids,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a _decoy_ sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753:48,Usability,undo,undocumented,48,"Hi @aedavids,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a _decoy_ sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:555,Deployability,update,updated,555,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:582,Deployability,update,update,582,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:290,Usability,undo,undocumented,290,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332:395,Usability,undo,undocumented,395,"thanks. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833861332
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:520,Availability,down,downstream,520,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1272,Deployability,update,updated,1272,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1299,Deployability,update,update,1299,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1007,Usability,undo,undocumented,1007,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924:1112,Usability,undo,undocumented,1112,"Hi Rob. I thought about this a little bit more. I am confused. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. It seems like I need to use something like. Grep -v “$d”. to find the reads that did not map. Is this correct? I have been given the task of exploring our unmapped reads. Running grep is not a big deal. I just want to make sure I do not mess up my downstream analysis. By the way our lab is a huge fan of Salmon. ctrl.1.unmapped]$ cut -d "" "" -f 2 aux_info/unmapped_names.txt | sort | uniq -c; 519916 d; 39097 m1; 34534 m2; 747447 u. Kind regards. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Thursday, May 6, 2021 at 1:53 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Thanks for catching that this is undocumented! This means that the mapping type was determined as mapping to a decoy sequence. When we added this output into the code, the documentation wasn't updated accordingly. We'll update the documentation. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833857753>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAQ35L6PU3DKXYIM4ODTML6TPANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-833928924
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-841492751:2120,Usability,simpl,simply,2120,"ecoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file.; ; Well, it's the case the decoys are *not* be quantified. That is, only the target transcripts will appear in the `quant.sf` file, no decoys should be present there. The main purpose of the decoys is to account for reads _not_ from target transcripts that might otherwise be sequenced in the sample.; ; The reason we report the decoy mapping fragments in the unmapped names file is, as I said, a historical contingency. Basically, since we're not mapping the decoys to targets and counting them toward quantification, one might be interested in knowing _where_ the decoy sequences come from. At some point, the easiest way to do this was just to place the name of these fragments in the unmapped names file (with the `d` tag) and then grab the reads and go fishing with them in some other way. However, I totally understand why including them in the unmapped names file is confusing. During selective-alignment, if we assign a fragment as best mapping to a decoy, it doesn't get assigned to a quantifiable target, but it's not technically unmapped in the same sense as the other unmapped reads. That is, we know it comes from the decoy sequence, that the alignment score is at least the minimum required, and that it maps better to the decoy than to any non-decoy target. That's quite different that ""truly"" unmapped fragments where we find no mapping for the fragment within the required score threshold. Anyway, I hope the answer is useful for you. If you want to select only unmapped reads that were matched to *neither* your target sequences (transcripts) *nor* to the decoy sequences, then your `grep` command should do the trick. However, if you want to look at all of the reads that simply didn't contribute to the counts in the `quant.sf` file, then you'd want to look at everything in the unmapped names file. Let me know if you have any other questions!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-841492751
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628:323,Usability,undo,undocumented,323,"Hi Rob. Thanks for the explination. Andy. From: Rob Patro ***@***.***>; Reply-To: COMBINE-lab/salmon ***@***.***>; Date: Friday, May 14, 2021 at 1:41 PM; To: COMBINE-lab/salmon ***@***.***>; Cc: ""andrew e. davidson"" ***@***.***>, Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] salmon --writeUnmappedNames produced undocumented result (#657). Hi @aedavids<https://github.com/aedavids>,. Sorry for the delay in replying to your reply here. So I think what you suggest is the right solution, and there are some strange historical reasons we write the decoy names out in the unmapped file. We build the index from decoys. My understanding is that only reads that map to the decoy will be quantified. I am surprised to see that the name for mapped reads would show up in the unmapped_names.txt file. Well, it's the case the decoys are not be quantified. That is, only the target transcripts will appear in the quant.sf file, no decoys should be present there. The main purpose of the decoys is to account for reads not from target transcripts that might otherwise be sequenced in the sample. The reason we report the decoy mapping fragments in the unmapped names file is, as I said, a historical contingency. Basically, since we're not mapping the decoys to targets and counting them toward quantification, one might be interested in knowing where the decoy sequences come from. At some point, the easiest way to do this was just to place the name of these fragments in the unmapped names file (with the d tag) and then grab the reads and go fishing with them in some other way. However, I totally understand why including them in the unmapped names file is confusing. During selective-alignment, if we assign a fragment as best mapping to a decoy, it doesn't get assigned to a quantifiable target, but it's not technically unmapped in the same sense as the other unmapped reads. That is, we know it comes from the decoy sequence, that the alignment score is at least the minimum required, and tha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628
https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628:2473,Usability,simpl,simply,2473,"be present there. The main purpose of the decoys is to account for reads not from target transcripts that might otherwise be sequenced in the sample. The reason we report the decoy mapping fragments in the unmapped names file is, as I said, a historical contingency. Basically, since we're not mapping the decoys to targets and counting them toward quantification, one might be interested in knowing where the decoy sequences come from. At some point, the easiest way to do this was just to place the name of these fragments in the unmapped names file (with the d tag) and then grab the reads and go fishing with them in some other way. However, I totally understand why including them in the unmapped names file is confusing. During selective-alignment, if we assign a fragment as best mapping to a decoy, it doesn't get assigned to a quantifiable target, but it's not technically unmapped in the same sense as the other unmapped reads. That is, we know it comes from the decoy sequence, that the alignment score is at least the minimum required, and that it maps better to the decoy than to any non-decoy target. That's quite different that ""truly"" unmapped fragments where we find no mapping for the fragment within the required score threshold. Anyway, I hope the answer is useful for you. If you want to select only unmapped reads that were matched to neither your target sequences (transcripts) nor to the decoy sequences, then your grep command should do the trick. However, if you want to look at all of the reads that simply didn't contribute to the counts in the quant.sf file, then you'd want to look at everything in the unmapped names file. Let me know if you have any other questions!. Best,; Rob. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-841492751>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AN3VWAUROTOCPHB6SKMA2ETTNWDF5ANCNFSM44HLOFXQ>.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/657#issuecomment-843255628
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025:92,Deployability,release,release,92,"Hi @curtisd0886,. So, issues relevant to processing this data should be resolved in the new release (v1.5.1). However, for technical reasons in the way different modes are handled internally, we had to simplify the mixing and matching of certain different options. Specifically, one can no longer use the `--citeseq` flag in conjunction with the custom geometry flags. So, if you have non-standard `--citeseq` geometry, the recommendation is to just use the new barcode specification format (e.g. `--umi-geometry`, `--bc-geometry` and `--read-geometry`), along with a couple of other flags. Specifically, you should explicitly provide `--keepCBFraction 1.0` and a tgMap file (even if it is just a trivial one mapping each feature to itself). @k3yavi can elaborate further if I've overlooked anything. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025
https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025:202,Usability,simpl,simplify,202,"Hi @curtisd0886,. So, issues relevant to processing this data should be resolved in the new release (v1.5.1). However, for technical reasons in the way different modes are handled internally, we had to simplify the mixing and matching of certain different options. Specifically, one can no longer use the `--citeseq` flag in conjunction with the custom geometry flags. So, if you have non-standard `--citeseq` geometry, the recommendation is to just use the new barcode specification format (e.g. `--umi-geometry`, `--bc-geometry` and `--read-geometry`), along with a couple of other flags. Specifically, you should explicitly provide `--keepCBFraction 1.0` and a tgMap file (even if it is just a trivial one mapping each feature to itself). @k3yavi can elaborate further if I've overlooked anything. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/670#issuecomment-860704025
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4633,Availability,fault,fault,4633,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4832,Availability,recover,recovered,4832,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:1867,Energy Efficiency,reduce,reduce,1867,"mber of splices: GC/AG |	39101; Number of splices: AT/AC |	13983; Number of splices: Non-canonical |	478779; Mismatch rate per base, % |	0.56%; Deletion rate per base |	0.03%; Deletion average length |	4.89; Insertion rate per base |	0.03%; Insertion average length |	4.88; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	1029261; % of reads mapped to multiple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some can have a size between 100pb -150pb. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I got this final.out:; ```; Started job on |	Jul 05 14:25:19; Started mapping on |	Jul 05 14:25:23; Finished on |	Jul 05 16:37:44; Mapping speed, Million of reads per hour |	38.78. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	70090369; Uniquely mapped reads % |	81.93%; Average mapped length |	191.51; Number of splices: Total |	1068826; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	470490; Number of splices: GC/AG |	43525",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4832,Safety,recover,recovered,4832,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:393,Testability,log,log,393,"Hi @rob-p,; I am aware of that, but we were off on bwa anyway. I decided to follow your advice and used STAR with this order:. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I then got this final.out:; ```. Started job on |	Jul 05 07:51:09; Started mapping on |	Jul 05 07:51:13; Finished on |	Jul 05 10:01:38; Mapping speed, Million of reads per hour |	39.36. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	36980651; Uniquely mapped reads % |	43.23%; Average mapped length |	283.47; Number of splices: Total |	943061; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	411198; Number of splices: GC/AG |	39101; Number of splices: AT/AC |	13983; Number of splices: Non-canonical |	478779; Mismatch rate per base, % |	0.56%; Deletion rate per base |	0.03%; Deletion average length |	4.89; Insertion rate per base |	0.03%; Insertion average length |	4.88; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	1029261; % of reads mapped to multiple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:2368,Testability,log,log,2368,"iple loci |	1.20%; Number of reads mapped to too many loci |	565; % of reads mapped to too many loci |	0.00%; UNMAPPED READS:; Number of reads unmapped: too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	47533174; % of reads unmapped: too short |	55.56%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%. ```. I filtered it by samtools -f 2 -F 3840 . and Salmon gave me this result which is still very weak: 24323638 counts. So I decided to reduce the parameters as indicated in this link: https://github.com/alexdobin/STAR/issues/169; Because I trimmed my sequence and some can have a size between 100pb -150pb. ` ""STAR --runThreadN {threads} --runMode alignReads --genomeDir {input.ref} --readFilesIn {input.fq1} {input.fq2} --readFilesCommand zcat --outSAMtype BAM Unsorted SortedByCoordinate --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --quantMode TranscriptomeSAM GeneCounts --outFileNamePrefix {output} --outStd Log {log} ""`. I got this final.out:; ```; Started job on |	Jul 05 14:25:19; Started mapping on |	Jul 05 14:25:23; Finished on |	Jul 05 16:37:44; Mapping speed, Million of reads per hour |	38.78. Number of input reads |	85547657; Average input read length |	298; UNIQUE READS:; Uniquely mapped reads number |	70090369; Uniquely mapped reads % |	81.93%; Average mapped length |	191.51; Number of splices: Total |	1068826; Number of splices: Annotated (sjdb) |	0; Number of splices: GT/AG |	470490; Number of splices: GC/AG |	43525; Number of splices: AT/AC |	15865; Number of splices: Non-canonical |	538946; Mismatch rate per base, % |	1.29%; Deletion rate per base |	0.03%; Deletion average length |	4.76; Insertion rate per base |	0.03%; Insertion average length |	5.23; MULTI-MAPPING READS:; Number of reads mapped to multiple loci |	15205492; % of reads mapped to multiple loci |	17.77%; Number o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664:4711,Usability,usab,usable,4711,"too many mismatches |	0; % of reads unmapped: too many mismatches |	0.00%; Number of reads unmapped: too short |	0; % of reads unmapped: too short |	0.00%; Number of reads unmapped: other |	4006; % of reads unmapped: other |	0.00%; CHIMERIC READS:; Number of chimeric reads |	0; % of chimeric reads |	0.00%; ```. It was really better but I am afraid that I have really low quality (I try the parameter 0.3 when I wrote these lines ), I filtered again with samtools -f 2 -F3840 and the salmon counts which is still very low : 24323720 counts. I used samtools flagstat to see what happens after the filter and we get this?; ```; 48983692 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 48983692 + 0 mapped (100.00% : N/A); 48983692 + 0 paired in sequencing; 24491846 + 0 read1; 24491846 + 0 read2; 48983692 + 0 properly paired (100.00% : N/A); 48983692 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. I don't understand why I'm losing so many counts, is it because I'm filtering? But still I have to filter to get the properly pairs... For the sorting it's totally my fault I read the doc wrong but even by not sorting I get very low results not usable less than 26%. The experimentation is done on oak, on 4 times 3 late samples and 3 early samples of dormancy were recovered and we made a TruSeq stranded illumina on these samples. I use a gene model built by my team with the 25808 genes that the oak has as reference. For this part ""Is this a polyA selection or ribosomal depletion prep"" I don't know, I'll find out. To be honest I am totally lost because I don't understand what's wrong in my analysis.... Thank you very much for your help once again . Kisekya. EDIT:. I discover that I have 59 millions of duplicates in my data...; I tried to delete it after filtering my proper pair I get bad records 38% of mapping ...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/676#issuecomment-874540664
https://github.com/COMBINE-lab/salmon/issues/678#issuecomment-1138623879:98,Usability,clear,clearly,98,"I am not sure what this `-m` flag refers to. It is not currently an option, and doesn't appear to clearly have been one in the past either.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/678#issuecomment-1138623879
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672:297,Availability,error,error,297,"Yep, I think that's right and it is expected that every sample would have different number of cells/cellular barcodes. The general idea is to use a frequency distribution to separate high quality barcodes from low quality post quantification. I'm sorry that you are facing issues with v2, but the error simply means you are providing the full list of 737k barcodes which is not an expected behavior for the `--whitelist` flag and in a typical 10x/ Dropseq based experiment one would expect ~10-12k cells/CB.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672:303,Usability,simpl,simply,303,"Yep, I think that's right and it is expected that every sample would have different number of cells/cellular barcodes. The general idea is to use a frequency distribution to separate high quality barcodes from low quality post quantification. I'm sorry that you are facing issues with v2, but the error simply means you are providing the full list of 737k barcodes which is not an expected behavior for the `--whitelist` flag and in a typical 10x/ Dropseq based experiment one would expect ~10-12k cells/CB.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-878695672
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:25,Availability,error,error,25,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:137,Availability,error,error,137,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:1494,Availability,error,errors,1494,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:31,Integrability,message,message,31,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:143,Integrability,message,message,143,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952:238,Usability,clear,clear,238,"The issue relates to the error message, so maybe I will close the issue in terms of being able to run the program without generating the error message. However, I think something still doesn't seem right, and I thought I should make that clear. <table>; <tbody>; <tr>; <th align=""center"">Method</th>; <th align=""center"">SRR13313130</th>; <th align=""center"">10x_pbmc_5k</th>; </tr>; <tr>; 	 <td align=""left"">CellRanger</td>; <td align=""center"">9,974 cells</td>; <td align=""center"">4,956 cells</td>; </tr>; <tr>; 	 <td align=""left"">STARsolo</td>; <td align=""center"">7,587 cells</br><i>(Summary.csv)</i></td>; <td align=""center"">4,586 cells</br><i>(Summary.csv)</i></td>; </tr>; <tr>; 	 <td align=""left"">Alevin</td>; <td align=""center""><b>814 cell barcodes?</b></td>; <td align=""center""> 856,224 cell barcodes</td>; </tr>; <tr>; 	 <td align=""left"">Kallisto</td>; <td align=""center"">79,254 cells</br><i>(BUSpaRse)</i></td>; <td align=""center"">47,598 cells</br><i>(BUSpaRse)</i></td>; </tr>; <tr>. </tbody>; </table>. For Alevin and Kallisto, I am not so worried about the exact values for cell barcodes (versus cells), since I was expecting extra work was needed to estimate a cell count from a distribution of measurements for each cell barcode. However, the number of cell barcodes should be larger than the number of cells, and that seems to match for everything except Alevin for this sample (SRR13313130). In other words, for sample, I think that this is the command that generates the fewest errors/warnings/notes:. `/path/to/salmon alevin -l ISF --chromium -1 $R1 -2 $R2 -i $REF -p 4 -o $ID --tgMap $MAP`. However, I think the cell barcode count is too small. **Is there anything else that you would recommend trying?**",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/682#issuecomment-882751952
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:1803,Availability,down,downstream,1803,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:1469,Performance,perform,perform,1469,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:390,Security,hash,hash,390,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669:191,Usability,simpl,simply,191,"Hi @jashapiro,. So there are definitely a few things going on here. The first is that you correctly diagnosed the missing cmd_info.json information when `alevin` is run in RAD mode. That was simply an oversight, and there is no reason that file shouldn't have been written. Second, there is also useful information that belongs in `meta_info.json` in the `aux_info` directory (like the SHA hash of the reference sequences); that was also missing but has now been added.; ; In addition to salmon's `alevin` command, each step of `alevin-fry` also writes some useful metadata when it executes. For example, there is a json file written by the `generate-permit-list` step, one written by the `collate` step, and one written by the `quant` step. We've never run into the problem of the output of `alevin-fry` overwriting the output of `alevin` because we use a directory structure where the output quantifications reside in a separate directory from the input RAD file. However, I can now see that if you're writing the quants in the same place as the input, then there will be a conflict in the file names, and the existing files will be overwritten with the new ones. I agree that both tools output useful information. I'm a *bit* ambivalent about assuming the salmon-generated files exist, and merging them into one output file, as I think there might be cases where those files aren't present and `alevin-fry` should still run properly since it doesn't require them to perform it's processing. One option would be to rename the `alevin-fry` output files to prefix/postfix them so they don't collide with the salmon files even if they live in the same directory. Then, one could (now or later) write a small command to merge the relevant json files into a unified output if that would be more convenient downstream. Let me know your thoughts. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883497669
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316:325,Deployability,pipeline,pipeline,325,"Hi @rob-p,. Ah, I hadn't looked carefully at the outputs, so I was overlooking the fact that the `meta_info.json` for `salmon alevin` is in the `aux_info` subdirectory, while the one for `alevin-fry quant` is in the output directory, so I don't _think_ there will be a conflict. (We have been using the same directory in our pipeline for simplicity.) However, I do think having them named the same thing is a potential source of confusion. Is there a reason not to name the file produced by `alevin-fry quant` something more like `quant.json` or `quant_info.json` to be more in parallel with the `collate.json` and `generate_permit_list.json` files generated at those steps? Either way, I agree that merging the files within `alevin-fry` is probably _not_ the best solution. . The `cmd_info.json` file, seems a special case: I am not sure what the ultimate goal for that file is; it seems now to be included ""for R compatibility"" though I am not fully clear on what that means (with `.mtx` input we don't need it, but `tximeta`/`tximport` may be looking for it?). If the final quant output directory does need the file, it would seem to make sense to copy it along somehow from the `salmon alevin --rad` output directory (with a stop along the way in the `collate` output I guess?). Presumably the `aux_info` would also be desired for `tximeta` if/when `alevin-fry` support is implemented there?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316:338,Usability,simpl,simplicity,338,"Hi @rob-p,. Ah, I hadn't looked carefully at the outputs, so I was overlooking the fact that the `meta_info.json` for `salmon alevin` is in the `aux_info` subdirectory, while the one for `alevin-fry quant` is in the output directory, so I don't _think_ there will be a conflict. (We have been using the same directory in our pipeline for simplicity.) However, I do think having them named the same thing is a potential source of confusion. Is there a reason not to name the file produced by `alevin-fry quant` something more like `quant.json` or `quant_info.json` to be more in parallel with the `collate.json` and `generate_permit_list.json` files generated at those steps? Either way, I agree that merging the files within `alevin-fry` is probably _not_ the best solution. . The `cmd_info.json` file, seems a special case: I am not sure what the ultimate goal for that file is; it seems now to be included ""for R compatibility"" though I am not fully clear on what that means (with `.mtx` input we don't need it, but `tximeta`/`tximport` may be looking for it?). If the final quant output directory does need the file, it would seem to make sense to copy it along somehow from the `salmon alevin --rad` output directory (with a stop along the way in the `collate` output I guess?). Presumably the `aux_info` would also be desired for `tximeta` if/when `alevin-fry` support is implemented there?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316
https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316:952,Usability,clear,clear,952,"Hi @rob-p,. Ah, I hadn't looked carefully at the outputs, so I was overlooking the fact that the `meta_info.json` for `salmon alevin` is in the `aux_info` subdirectory, while the one for `alevin-fry quant` is in the output directory, so I don't _think_ there will be a conflict. (We have been using the same directory in our pipeline for simplicity.) However, I do think having them named the same thing is a potential source of confusion. Is there a reason not to name the file produced by `alevin-fry quant` something more like `quant.json` or `quant_info.json` to be more in parallel with the `collate.json` and `generate_permit_list.json` files generated at those steps? Either way, I agree that merging the files within `alevin-fry` is probably _not_ the best solution. . The `cmd_info.json` file, seems a special case: I am not sure what the ultimate goal for that file is; it seems now to be included ""for R compatibility"" though I am not fully clear on what that means (with `.mtx` input we don't need it, but `tximeta`/`tximport` may be looking for it?). If the final quant output directory does need the file, it would seem to make sense to copy it along somehow from the `salmon alevin --rad` output directory (with a stop along the way in the `collate` output I guess?). Presumably the `aux_info` would also be desired for `tximeta` if/when `alevin-fry` support is implemented there?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/688#issuecomment-883524316
https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790:259,Deployability,release,releases,259,"Hi @saipra003,. Thank you for posting the issue, and also following up with the resolution. It’s not immediately clear why there would have been an issue with 1.2.1, but we’ll be sure to make not of this for anyone else who runs into such an issue with older releases. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790
https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790:113,Usability,clear,clear,113,"Hi @saipra003,. Thank you for posting the issue, and also following up with the resolution. It’s not immediately clear why there would have been an issue with 1.2.1, but we’ll be sure to make not of this for anyone else who runs into such an issue with older releases. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/690#issuecomment-886279790
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:329,Availability,avail,available,329,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:51,Deployability,pipeline,pipeline,51,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:103,Deployability,pipeline,pipeline,103,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:33,Integrability,protocol,protocol,33,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:149,Integrability,protocol,protocol,149,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:247,Testability,test,testing,247,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597:362,Usability,clear,clear,362,"Hey @jeremymsimon! I checked the protocol and the [pipeline code](https://github.com/yjzhang/split-seq-pipeline/blob/master/split_seq/tools.py). The protocol you described is v1 and the Parsebio is v2. I have implemented v2 in salmon and would be testing it this week. v1 can be similarly implemented. I read the paper and other available resources but I am not clear about the random hexamer usage and it's effects on the barcode. Can you please explain what you meant by BC1s being paired and what's the use of random hexamer, please? Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-936331597
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:482,Availability,down,down,482,"Thanks @Gaura! Sounds promising. . Can you clarify what differences you saw between v1/v2 protocols? My understanding was that the only changes were slight differences in barcode positions within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of sal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:90,Integrability,protocol,protocols,90,"Thanks @Gaura! Sounds promising. . Can you clarify what differences you saw between v1/v2 protocols? My understanding was that the only changes were slight differences in barcode positions within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of sal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:554,Integrability,protocol,protocol,554,"Thanks @Gaura! Sounds promising. . Can you clarify what differences you saw between v1/v2 protocols? My understanding was that the only changes were slight differences in barcode positions within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of sal",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:1273,Integrability,rout,routes,1273,"within the barcode read, ie something that could be handled with different `bc-geometry`, but maybe that's all you meant in terms of differing implementations. Regarding the BC1 and how it could be two sequences for the same sample - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of salmon/alevin as a preprocessing step, like what my slow perl script can do, or it can be handled internally. Having alevin do the collapsing would likely be a lot faster and means the FASTQs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722:2333,Usability,clear,clear,2333,"mple - this is confusing to explain in text format, but all comes down to the sequential nature of how the cells acquire barcodes in this protocol. We start with a 96-well plate, where each of the top 48 wells contain BOTH an oligo-dT barcode and random hexamer barcode. The samples then get added to each well. Biochemistry happens. Then you pool all the cells together, split them back out into 48 wells again, and each well gets its own BC2. Then repeat for BC3. . So a given transcript may get amplified via one of two amplification primers (oligo-dT or random hexamer), but after that, will get a single BC2 sequence and BC3 sequence added after that. In Fig 1A of the Rosenberg paper, it's as though there isn't _just_ an orange sequence carrying out reverse transcription, there are actually two different (known) sequences associated with different routes of amplification per cell. . The net effect is that a given cell can contain transcripts that have a sequence like this:; AACGTGAT-CTGTAGCC-ACACAGAA. or like this:; GATAGACA-CTGTAGCC-ACACAGAA. where maybe the first sequence was amplified by oligo-dT and the second was amplified via a random hexamer. Because they have the same BC2 and BC3 sequence, and the BC1 sequences match a known pairing, we know they come from the same cell and therefore the data should be merged. . Any lab running these experiments will have a table of known pairings (ie the two barcodes added to each of first 48 wells), so that they can be merged and treated as though they came from the same cell. This can either be handled upstream of salmon/alevin as a preprocessing step, like what my slow perl script can do, or it can be handled internally. Having alevin do the collapsing would likely be a lot faster and means the FASTQs don't need any editing, which would be preferable, but I would understand if that is out of scope for you all. . Hopefully that explanation is clear, but if you have any other questions on this I'd be happy to meet and discuss",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-937918722
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:395,Availability,down,downstream,395,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:1617,Performance,bottleneck,bottleneck,1617,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:1711,Testability,test,test,1711,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108:295,Usability,simpl,simply,295,"Hi @jeremymsimon,. I've discussed the support for SPLiT-seq/ParseBio with @Gaura in some depth. Honestly, I think the cleanest solution right now is just to have a more streamlined (and streaming) way to match / replace the random hexamers upstream of alevin-fry. By my understanding, if we can simply replace barcode 1 appropriately (as your Perl script currently does), everything should work downstream in alevin/alevin-fry.; ; To that end, I've thrown together a small rust program based on your Perl script. Currently that lives [here](https://github.com/COMBINE-lab/splitp). It reads the same basic parameters as the Perl script, and writes its output to stdout so that it can be used with named pipes. For example, something like:; ; ```; <normal salmon command> -1 read_file_1.fq -2 <(splitp --read-file read_file_2.fq --bc-map bcSharing_example.txt --start 79 --end 86 --one-hamming); ```. which will transform the second fastq file and stream the transformed reads out which can then be read by alevin-fry. One important thing to note is that while *alevin* requires the input reads to be a real file (i.e. you can't stream reads in because it does 2 passes), if you are mapping these reads for processing with *alevin-fry* you can use the process substitution trick above. As you hinted, this program works considerably faster than the Perl script. For example, for the first 10,000,000 reads in `SRR6750042`, the Perl script took 2m 48s to transform the reads and `splitp` took ~6s (if the output wasn't being written to a file on disk it took <4s). This should generally be fast enough to not be a speed bottleneck. So, perhaps the next step is to try to help you walk through this approach with a test dataset (and ideally using alevin-fry) to see if things are turning out as expected?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-961598108
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970:82,Safety,detect,detected,82,"Okay my alevin run finished, and I got a mapping rate of just 6.1% and 2254 cells detected. My `lib_format_counts.json` contains the following:. ```; {; ""read_files"": ""[ SRR10174292_2.fastq.gz, SRR10174292_1.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 15259749,; ""num_assigned_fragments"": 15259749,; ""num_frags_with_concordant_consistent_mappings"": 0,; ""num_frags_with_inconsistent_or_orphan_mappings"": 61866895,; ""strand_mapping_bias"": 0.0,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 0,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 0,; ""SF"": 0,; ""SR"": 0,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. so lots of fragments are discarded for one reason or another, and it's not clear whether the library type assignment is working properly, sort of like my initial example above. Separately, I'm running zUMIs on the same files and will report back with those data when the run is complete",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970:709,Usability,clear,clear,709,"Okay my alevin run finished, and I got a mapping rate of just 6.1% and 2254 cells detected. My `lib_format_counts.json` contains the following:. ```; {; ""read_files"": ""[ SRR10174292_2.fastq.gz, SRR10174292_1.fastq.gz]"",; ""expected_format"": ""ISR"",; ""compatible_fragment_ratio"": 1.0,; ""num_compatible_fragments"": 15259749,; ""num_assigned_fragments"": 15259749,; ""num_frags_with_concordant_consistent_mappings"": 0,; ""num_frags_with_inconsistent_or_orphan_mappings"": 61866895,; ""strand_mapping_bias"": 0.0,; ""MSF"": 0,; ""OSF"": 0,; ""ISF"": 0,; ""MSR"": 0,; ""OSR"": 0,; ""ISR"": 0,; ""SF"": 0,; ""SR"": 0,; ""MU"": 0,; ""OU"": 0,; ""IU"": 0,; ""U"": 0; }; ```. so lots of fragments are discarded for one reason or another, and it's not clear whether the library type assignment is working properly, sort of like my initial example above. Separately, I'm running zUMIs on the same files and will report back with those data when the run is complete",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985038970
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1307,Availability,error,errors,1307," here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:2489,Availability,error,errors,2489,"tead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps that can be achieved with some Levenshtein distance flexibility using the entire 24bp barcode sequence detected...? . Anyway sorry for the brainstorming dump, but the short answer is: we're probably stuck losing a bunch of reads due to positional errors like this",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:535,Modifiability,flexible,flexible,535,"@Gaura this sort of frameshift in the barcodes is a known issue, and can be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:183,Safety,detect,detection,183,"@Gaura this sort of frameshift in the barcodes is a known issue, and can be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:277,Safety,detect,detect,277,"@Gaura this sort of frameshift in the barcodes is a known issue, and can be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1226,Safety,detect,detection,1226," here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1604,Safety,detect,detection,1604,"sible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps that can be achieved with some Levenshtein distance flexibility using the entire 24bp barcode sequence detected...? . Anyway sorry for the brainstorming dump, but the short answer is: we're probably stuck losing a bunch of ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:2345,Safety,detect,detected,2345,"tead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps that can be achieved with some Levenshtein distance flexibility using the entire 24bp barcode sequence detected...? . Anyway sorry for the brainstorming dump, but the short answer is: we're probably stuck losing a bunch of reads due to positional errors like this",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1019,Testability,test,test,1019,"be computationally challenging (at least for existing methods). zUMIs, for example, does an automatic barcode detection based on fixed barcode positions like we're doing here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883:1176,Usability,simpl,simplest,1176," here with alevin, so it would mis-detect cells like the shifted ones you pasted above. For SPLiT-seq, we do know exactly which barcodes go into the wells, however, so it is technically possible to restrict based on all possible known combinations of barcodes instead and be more positionally flexible. But deciding how many indel bases are allowable, and presumably doing multiple passes through the data to establish an include-list could be time-consuming. Further, the zUMIs developer rightly mentions [in this thread](https://github.com/sdparekh/zUMIs/issues/63) that there are likely going to be _many_ unused barcode combinations this way, so lots of time could be spent looking for ""cells"" that don't actually exist in the data. The authors of the paper from which our test dataset was derived describe in their methods using a Drop-seq computational framework, so I'm not sure which approach theirs is more similar to. . The simplest approach here is certainly the automatic detection, but it will come at the cost of losing meaningful reads to frameshift errors. . My guess is this falls well out of the scope of alevin, but if you're interested in improving on that, there may be a middle ground between the two approaches above, one that I'm not sure if your group or others have attempted for other methods: we could essentially do a 2-pass barcode detection. The first pass would restrict based on positions like we're already doing, and establish an include-list of possible barcodes seen in the data. Then we could pass through the barcode sequences a second time, looking only for those sequence combinations, but allowing 1-2bp flexibility in the positions they occur, potentially rescuing some of the ones missed during the first pass. This would get around the issue of searching for thousands (or more) barcodes that never exist. . However, for your above sequences in red, we would still need to somehow collapse the barcodes `GATAGACA`, `ATAGACAT`, and `ATAGACAG`, but perhaps t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-985554883
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:141,Availability,error,errors,141,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:1087,Availability,error,errors,1087,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:511,Energy Efficiency,efficient,efficiently,511,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:565,Integrability,protocol,protocols,565,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:326,Safety,avoid,avoid,326,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:287,Testability,log,logic,287,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837:608,Usability,simpl,simple,608,"Hi @jeremymsimon — @Gaura is going to take a look at unfiltered permit listing and will share those results here later. Regarding frameshift errors, I think that's certainly out of scope for the alevin -> fry phase, but that type of thing *could* be in scope for `splitp`. Basically, my logic / reasoning is this: I'd like to avoid further complicating the already immense salmon/alevin codebase with special implementations handling problems outside of their core function (e.g. mapping reads to the reference efficiently and quantifying UMIs/barcode). Since most protocols (and the most common) have quite simple barcode geometry, it makes sense for this code to live there. I'm fully supportive of enabling support for more complex barcode geometries and preprocessing requirements if there are folks whom it would help, but it feels like that essential complexity belongs upstream of alevin / fry, so that by the time the reads get to alevin, it can assume a straightforward geometry. So TLDR : I think we'd be willing to investigate what is required to address potential frameshift errors, and how much of a difference that makes, but I think that analysis and eventual implementation (if we decide it's worth it), belongs in `splitp`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/699#issuecomment-988030837
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:156,Availability,error,error,156,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:11,Deployability,install,installing,11,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:78,Deployability,install,install,78,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:53,Integrability,message,message,53,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359:64,Usability,simpl,simple,64,I tried re-installing salmon today after seeing your message. A simple `conda install salmon` worked for me this time. I don't know why it was giving me an error back then and not one now though....,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1137154359
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:271,Availability,error,error,271,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:326,Availability,error,errors,326,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:49,Deployability,install,install,49,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:509,Modifiability,config,config,509,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474:363,Usability,simpl,simple,363,"```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels; channels:; - conda-forge; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171204474
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:17,Availability,error,error,17,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:597,Availability,error,error,597,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:652,Availability,error,errors,652,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:375,Deployability,install,install,375,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:150,Modifiability,config,config,150,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:188,Modifiability,config,config,188,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:226,Modifiability,config,config,226,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:835,Modifiability,config,config,835,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:67,Testability,log,login,67,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414:689,Usability,simpl,simple,689,"Yes ! it's other error that I can't find . but I try : . ```; Last login: Thu Jun 30 15:14:51 on ttys002; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; Warning: 'conda-forge' already in 'channels' list, moving to the top; Benjamin@u932-ulm-2-57030119-6834 ~ % conda install salmon; Collecting package metadata (current_repodata.json): failed. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/osx-arm64/current_repodata.json>; Elapsed: -. An HTTP error occurred when trying to retrieve this URL.; HTTP errors are often intermittent, and a simple retry will get you on your way.; 'https://conda.anaconda.org/conda-forge/osx-arm64'; ```. ```; Benjamin@u932-ulm-2-57030119-6834 ~ % conda config --show channels ; channels:; - conda-forge; - bioconda; - defaults; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/724#issuecomment-1171223414
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1022365298:360,Usability,clear,clear,360,"Hi @k3yavi . Thank you very much for the answer. . I am interested in the equivalence class counts (ECC) per cell and the transcripts that belong to each equivalence class. I think bfh.txt is file that contains that information, but I couldn't figure out how the file is structured. I looked at the function you linked to, but the schema hasn't become totally clear. Could you provide some more information? . Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1022365298
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923:162,Availability,down,down,162,"Hi @k3yavi . Thank you for linking me to your script to parse the bfh file from alevin. I think I could figure out the structure of the bfh file. I will write it down underneath with the help of an example. Could you confirm it is correct? . Everything up and until the listing of the barcodes is clear. I will start with a line from after the barcodes. ; ""7	90480	90486	107507	107990	108641	109149	112915	1	1	105	1	TGGGATTT	1"". I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. . My goal is create a expression matrix where the ECs are the rows and the columns are the cells. If I want the UMI counts, do I need to count the number of reads associated with each UMI or just the number of UMIs per cell? . Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923:297,Usability,clear,clear,297,"Hi @k3yavi . Thank you for linking me to your script to parse the bfh file from alevin. I think I could figure out the structure of the bfh file. I will write it down underneath with the help of an example. Could you confirm it is correct? . Everything up and until the listing of the barcodes is clear. I will start with a line from after the barcodes. ; ""7	90480	90486	107507	107990	108641	109149	112915	1	1	105	1	TGGGATTT	1"". I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. . My goal is create a expression matrix where the ECs are the rows and the columns are the cells. If I want the UMI counts, do I need to count the number of reads associated with each UMI or just the number of UMIs per cell? . Thanks in advance.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1028272923
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067:664,Integrability,depend,depends,664,"Hi @tmms1 ,. > I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. This is correct !. Unfortunately you goal is not very clear to me and the output matrix depends on that. I understand that you wan't a matrix of dimension |eq_class X cells| but if you wan't the values in the matrix to be read count then you have to add the counts of all the UMIs in class across the cells; and if you wan't the values in the matrix to be the frequency of the unique UMIs then just count the UMIs you wan't instead of reads.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067
https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067:630,Usability,clear,clear,630,"Hi @tmms1 ,. > I think that each such line corresponds to an equivalence class (EC). The first entry on each row is the number of transcripts in the EC. This is followed by the transcripts (more correctly, indices you can use to obtain the transcripts). Then you have the number of reads with in the EC, followed by the number of barcodes (~cells). For each barcode, you have an index that can be used to retrieve the identity of the barcode, followed by the number of UMIs within that barcode, the sequence of the UMI and lastly the number of reads associated with that UMI. This is correct !. Unfortunately you goal is not very clear to me and the output matrix depends on that. I understand that you wan't a matrix of dimension |eq_class X cells| but if you wan't the values in the matrix to be read count then you have to add the counts of all the UMIs in class across the cells; and if you wan't the values in the matrix to be the frequency of the unique UMIs then just count the UMIs you wan't instead of reads.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/728#issuecomment-1029328067
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732:44,Testability,test,test,44,"I can try '^' and '$' after lunch. I didn't test any other library, since boost was already a pre-requisite for salmon and my focus was on getting it to work first. But now that it is done, other libraries can be tried. However, at this moment, I'm not clear about the effort and speed-up ratio.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732:253,Usability,clear,clear,253,"I can try '^' and '$' after lunch. I didn't test any other library, since boost was already a pre-requisite for salmon and my focus was on getting it to work first. But now that it is done, other libraries can be tried. However, at this moment, I'm not clear about the effort and speed-up ratio.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1013348732
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721:12,Performance,perform,performance,12,"1/3 loss in performance seems significant, given that presumably the code does something else than just parsing UMIs. I am looking at Boost own comparison and benchmarks, and on long inputs (20MB) it is competitive with PCRE2. But with short inputs (20-30 characters) PCRE2 is consistently faster (by about 30% :thinking: ). And if PCRE2 is feature full, not sure it is the fastest either, especially for simple regexp.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721:159,Testability,benchmark,benchmarks,159,"1/3 loss in performance seems significant, given that presumably the code does something else than just parsing UMIs. I am looking at Boost own comparison and benchmarks, and on long inputs (20MB) it is competitive with PCRE2. But with short inputs (20-30 characters) PCRE2 is consistently faster (by about 30% :thinking: ). And if PCRE2 is feature full, not sure it is the fastest either, especially for simple regexp.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721:405,Usability,simpl,simple,405,"1/3 loss in performance seems significant, given that presumably the code does something else than just parsing UMIs. I am looking at Boost own comparison and benchmarks, and on long inputs (20MB) it is competitive with PCRE2. But with short inputs (20-30 characters) PCRE2 is consistently faster (by about 30% :thinking: ). And if PCRE2 is feature full, not sure it is the fastest either, especially for simple regexp.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1023320721
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961:1049,Usability,simpl,simple,1049,"Hi all (@Gaura / @gmarcais),. While this has languished somewhat as we try to figure out what regex engine to include and how best to package it, I wanted to mention that I am attempting something similar in rust (which has a canonical regex crate which, I believe, is supposed to be among the fast ones). That repo is over on [seq_geom_xform](https://github.com/COMBINE-lab/seq_geom_xform) and it relies on [seq_geom_parser](https://github.com/COMBINE-lab/seq_geom_parser). I like @Gaura's geometry string specification, so we're going with that for time time being. If you want to chime in or start a discussion on either of those repos, please do let me know if you have any other thoughts on this generalized scheme. The purpose of the `seq_geom_xform` crate is actually that it will be *both* a rust library (to allow parsing complex geometry descriptions as a regex and extract the relevant sequence) *and* a stand-alone executable that can do streaming sequence transformation from a ""complex"" barcode geometry (e.g. the sciseq3 above) to a ""simple"" geometry (fixed position and fixed length barcode and UMI). Thus, one could imagine (at the cost of sticking a rust executable in the invocation here) replacing this feature by a streaming invocation to `seq_geom_xform` that would take the compressed fastq files as input along with the geometry specification, and which would output two streams (one for each read) with a simplified geometry that could be parsed in the simpler format. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961:1430,Usability,simpl,simplified,1430,"Hi all (@Gaura / @gmarcais),. While this has languished somewhat as we try to figure out what regex engine to include and how best to package it, I wanted to mention that I am attempting something similar in rust (which has a canonical regex crate which, I believe, is supposed to be among the fast ones). That repo is over on [seq_geom_xform](https://github.com/COMBINE-lab/seq_geom_xform) and it relies on [seq_geom_parser](https://github.com/COMBINE-lab/seq_geom_parser). I like @Gaura's geometry string specification, so we're going with that for time time being. If you want to chime in or start a discussion on either of those repos, please do let me know if you have any other thoughts on this generalized scheme. The purpose of the `seq_geom_xform` crate is actually that it will be *both* a rust library (to allow parsing complex geometry descriptions as a regex and extract the relevant sequence) *and* a stand-alone executable that can do streaming sequence transformation from a ""complex"" barcode geometry (e.g. the sciseq3 above) to a ""simple"" geometry (fixed position and fixed length barcode and UMI). Thus, one could imagine (at the cost of sticking a rust executable in the invocation here) replacing this feature by a streaming invocation to `seq_geom_xform` that would take the compressed fastq files as input along with the geometry specification, and which would output two streams (one for each read) with a simplified geometry that could be parsed in the simpler format. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961
https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961:1478,Usability,simpl,simpler,1478,"Hi all (@Gaura / @gmarcais),. While this has languished somewhat as we try to figure out what regex engine to include and how best to package it, I wanted to mention that I am attempting something similar in rust (which has a canonical regex crate which, I believe, is supposed to be among the fast ones). That repo is over on [seq_geom_xform](https://github.com/COMBINE-lab/seq_geom_xform) and it relies on [seq_geom_parser](https://github.com/COMBINE-lab/seq_geom_parser). I like @Gaura's geometry string specification, so we're going with that for time time being. If you want to chime in or start a discussion on either of those repos, please do let me know if you have any other thoughts on this generalized scheme. The purpose of the `seq_geom_xform` crate is actually that it will be *both* a rust library (to allow parsing complex geometry descriptions as a regex and extract the relevant sequence) *and* a stand-alone executable that can do streaming sequence transformation from a ""complex"" barcode geometry (e.g. the sciseq3 above) to a ""simple"" geometry (fixed position and fixed length barcode and UMI). Thus, one could imagine (at the cost of sticking a rust executable in the invocation here) replacing this feature by a streaming invocation to `seq_geom_xform` that would take the compressed fastq files as input along with the geometry specification, and which would output two streams (one for each read) with a simplified geometry that could be parsed in the simpler format. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/734#issuecomment-1447552961
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:581,Availability,mask,masked,581,"Thanks a lot @rob-p and @k3yavi .; I wanted your advise on something more intricate. Asking since you people are the pioneers in this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It cons",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:1156,Availability,avail,available,1156,"n this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It consumes genome fasta (one file given through -g), transcriptome fasta (-t) and the annotation (GTF file given through -a) to creat",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:679,Usability,simpl,simple,679,"Thanks a lot @rob-p and @k3yavi .; I wanted your advise on something more intricate. Asking since you people are the pioneers in this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It cons",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973:705,Usability,simpl,simplify,705,"Thanks a lot @rob-p and @k3yavi .; I wanted your advise on something more intricate. Asking since you people are the pioneers in this aspect of problem solving.; I have to determine the expression of GFP and Transposon sequence in the transcriptome.; I read the material posted on the link https://salmon.readthedocs.io/en/latest/salmon.html.; It instructs this to be done in 2 different ways-; There are two options for generating a decoy-aware transcriptome:. The first is to compute a set of decoy sequences by mapping the annotated transcripts you wish to index against a hard-masked version of the organism’s genome. This can be done with e.g. MashMap2, and we provide some simple scripts to greatly simplify this whole process. Specifically, you can use the generateDecoyTranscriptome.sh script, whose instructions you can find in this README. The second is to use the entire genome of the organism as the decoy sequence. This can be done by concatenating the genome to the end of the transcriptome you want to index and populating the decoys.txt file with the chromosome names. Detailed instructions on how to prepare this type of decoy sequence is available here. This scheme provides a more comprehensive set of decoys, but, obviously, requires considerably more memory to build the index. I tried the 2nd approach. Combined the GFP,Transposon and the genome FASTA files, indexed it , constructed the decoy according to the given instructions given here https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/.; When I ran Salmon (version 1.2.1_linux_x86_64) it did not report anything in the quant files (I know that these samples have high GFP and Transposon expression in these samples). The 1st approach is giving me problems to the construction of the GTF file and then memory usage. The instructions say - generateDecoyTranscriptome.sh — Located in the scripts/ directory, this is a preprocessing script for creating augmented hybrid fasta file for salmon index. It cons",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1022416973
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469:387,Deployability,pipeline,pipelines,387,"Hi @amitpande74 ,. May be you are already aware of this but just to make it clear, the idea behind decoy indexing is to ""exclude"" those sequences from the transcriptome quantification and that's why you don't see the indexed transposon sequence in the salmon output. The motivation behind such indexing is to remove false mapping reads i.e. exclude RNA-seq reads from the quantification pipelines which maps better to the decoy sequences compared to the provided transcriptome. . I hope that clarifies your doubt and if you wan't to quantify GFP sequences then you have to concatenate them with the transcriptome sequence ""not"" the decoy sequence, although I just wanted to give you heads up that this analysis goes into an unexplored territory as we personally have not explored such use cases. Unless Rob have more thoughts on it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469
https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469:76,Usability,clear,clear,76,"Hi @amitpande74 ,. May be you are already aware of this but just to make it clear, the idea behind decoy indexing is to ""exclude"" those sequences from the transcriptome quantification and that's why you don't see the indexed transposon sequence in the salmon output. The motivation behind such indexing is to remove false mapping reads i.e. exclude RNA-seq reads from the quantification pipelines which maps better to the decoy sequences compared to the provided transcriptome. . I hope that clarifies your doubt and if you wan't to quantify GFP sequences then you have to concatenate them with the transcriptome sequence ""not"" the decoy sequence, although I just wanted to give you heads up that this analysis goes into an unexplored territory as we personally have not explored such use cases. Unless Rob have more thoughts on it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/737#issuecomment-1024668469
https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1019515038:242,Usability,clear,clear,242,"@k3yavi — might it be worthwhile exploring the effect of changing the min score fraction here, or enabling softclipping? I do recall that this seems in the ballpark of drop-seq data mapping to the annotated (spliced) transcriptome, but is it clear _why_ the mapping rates for this technology are so low?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/738#issuecomment-1019515038
https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023:214,Testability,log,log,214,"Hi @bzmby ,. I am sure you are aware of this but just wanted to clear that salmon is primarily designed for transcriptome quantification.; Ideally, there should not be a problem with indexing genome, also from the log you shared it looks like a warning. ; Having said that if you will index the genome then at the end of the day you will get quantification of the chromosomes, is that what you wan't ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023
https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023:64,Usability,clear,clear,64,"Hi @bzmby ,. I am sure you are aware of this but just wanted to clear that salmon is primarily designed for transcriptome quantification.; Ideally, there should not be a problem with indexing genome, also from the log you shared it looks like a warning. ; Having said that if you will index the genome then at the end of the day you will get quantification of the chromosomes, is that what you wan't ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/741#issuecomment-1024655023
https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498:187,Deployability,release,release,187,"Hi @taylorreiter,. I was wrong — there was simply a bug that, in single end mode, everything was being written out with the `u` flag. This is now fixed in develop. It will be in the next release. Sorry about that!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498
https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498:43,Usability,simpl,simply,43,"Hi @taylorreiter,. I was wrong — there was simply a bug that, in single end mode, everything was being written out with the `u` flag. This is now fixed in develop. It will be in the next release. Sorry about that!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/748#issuecomment-1146205498
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742:64,Deployability,update,update,64,"Hi @yagam-fluent,. Thanks for the excellent question. We should update the documentation regarding this option. Basically, in `salmon alevin`, we assumptions about expected read orientation are applied as ""hard filters"". That is, the behavior is equivalent to `--incompatPrior 0`, so that aligninments not in the prescribed orientation are simply not considered as invalid alignments. This is because in the case of single-cell processing, we (the community in general) currently do not have as sophisticated of probabilistic models for resolving UMI origins and gene abundances, and so algorithms typically do not take into account a ""wrong orientation"" probability. So, in `salmon alevin` if you are using alevin itself for the quantification, then hard filtering will be applied based on the expectations of `--libType`. On the other hand, if you are using `salmon alevin` to simply map the reads for subsequent processing with [`alevin-fry`](https://github.com/COMBINE-lab/alevin-fry) (i.e. `salmon alevin .... --rad` or `salmon alevin ... --sketch`), then *no* filtering is applied to mapping orientation, and instead you filter reads by orientation later in `alevin-fry`'s `generate-permit-list` step. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742:340,Usability,simpl,simply,340,"Hi @yagam-fluent,. Thanks for the excellent question. We should update the documentation regarding this option. Basically, in `salmon alevin`, we assumptions about expected read orientation are applied as ""hard filters"". That is, the behavior is equivalent to `--incompatPrior 0`, so that aligninments not in the prescribed orientation are simply not considered as invalid alignments. This is because in the case of single-cell processing, we (the community in general) currently do not have as sophisticated of probabilistic models for resolving UMI origins and gene abundances, and so algorithms typically do not take into account a ""wrong orientation"" probability. So, in `salmon alevin` if you are using alevin itself for the quantification, then hard filtering will be applied based on the expectations of `--libType`. On the other hand, if you are using `salmon alevin` to simply map the reads for subsequent processing with [`alevin-fry`](https://github.com/COMBINE-lab/alevin-fry) (i.e. `salmon alevin .... --rad` or `salmon alevin ... --sketch`), then *no* filtering is applied to mapping orientation, and instead you filter reads by orientation later in `alevin-fry`'s `generate-permit-list` step. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742:879,Usability,simpl,simply,879,"Hi @yagam-fluent,. Thanks for the excellent question. We should update the documentation regarding this option. Basically, in `salmon alevin`, we assumptions about expected read orientation are applied as ""hard filters"". That is, the behavior is equivalent to `--incompatPrior 0`, so that aligninments not in the prescribed orientation are simply not considered as invalid alignments. This is because in the case of single-cell processing, we (the community in general) currently do not have as sophisticated of probabilistic models for resolving UMI origins and gene abundances, and so algorithms typically do not take into account a ""wrong orientation"" probability. So, in `salmon alevin` if you are using alevin itself for the quantification, then hard filtering will be applied based on the expectations of `--libType`. On the other hand, if you are using `salmon alevin` to simply map the reads for subsequent processing with [`alevin-fry`](https://github.com/COMBINE-lab/alevin-fry) (i.e. `salmon alevin .... --rad` or `salmon alevin ... --sketch`), then *no* filtering is applied to mapping orientation, and instead you filter reads by orientation later in `alevin-fry`'s `generate-permit-list` step. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038325742
https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038381121:282,Usability,clear,clear,282,"Thanks Rob. I am using alevin with libType of ISR. When I looked at the SAM file after setting the writeMapping option, I observed that ~1% had the ""reverse alignment"" flag set to 1 (the only flags field that had this flag set was 341, all the rest had that bit set to 0). It's not clear to me whether alevin uses those reads for UMI counting, and if so, what's the best way to turn off this behavior.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/749#issuecomment-1038381121
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001:46,Availability,ping,pinging,46,"Hi @SeBaBInf,. Thanks for reporting this. I'm pinging @k3yavi for his thoughts here. Two quick thoughts though -- the first is that the abstract for this paper mentions 5' tagged end sequencing, thus it might be necessary to swap the reads so that the biological and technical reads are in the expected order. Second, it's likely also worth seeing if and how the data look different if you process with alevin-fry rather than alevin. I'll let @k3yavi provide more detailed guidance here. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001
https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001:473,Usability,guid,guidance,473,"Hi @SeBaBInf,. Thanks for reporting this. I'm pinging @k3yavi for his thoughts here. Two quick thoughts though -- the first is that the abstract for this paper mentions 5' tagged end sequencing, thus it might be necessary to swap the reads so that the biological and technical reads are in the expected order. Second, it's likely also worth seeing if and how the data look different if you process with alevin-fry rather than alevin. I'll let @k3yavi provide more detailed guidance here. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/763#issuecomment-1073004001
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1127073879:164,Usability,simpl,simply,164,"Hi @Liripo - if I'm understanding correctly, your UMI/barcodes are on R2 but `alevin` is incorrectly extracting them from the R1 file? If so, you should be able to simply reverse your inputs, e.g.:. ```; salmon alevin -i ../../GRch38_splici_idx \; -l A \; -1 2.fq.gz \; -2 1.fq.gz \; -p 32 \; --splitseqV1 \; -o alevin_out \; --tgMap ../../transcriptome_splici_fl86/transcriptome_splici_fl86_t2g.tsv \; --dumpFeatures --whitelist ../white_barcode.txt; ```; I've needed to do this for my own projects sometimes, since our cDNA/barcode reads are opposite that of the original Rosenberg paper, and it works fine. Can you give that a try and see if it solves your issue? Or if I'm misunderstanding, can you elaborate more on what you expected the output to look like?. And seconding @rob-p's suggestion above - you should be using `alevin` -> `alevin-fry`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1127073879
https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1185670648:9,Usability,learn,learn,9,Great to learn that. Let us know if you have any other issue. :),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/774#issuecomment-1185670648
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719:1778,Performance,perform,performing,1778,"al `1,099,008` are being discarded because they have only dovetail mappings. . We discard dovetail mappings by default, but you can admit them with `--allowDovetail`. The other `2,776,678` fragments are discarded because, though there are seeds for mapping that match, they do not have sufficiently high alignment score to be allowed for mapping. This default behavior, too, can be modified. The main flags that affect the behavior here are `--minScoreFraction`, where a lower number allows lower-quality alignments through and also the `--softclipOverhangs` flag which will decrease the penalty on alignments that overhang the end of an annotated transcript. However, it's worth noting that this is up to `3,875,686` more reads that might be mappable. This number is non-trivial, but quite far from the 90% rate of STAR. The rest of the reads, however, simply don't have support for alignment against the annotated transcriptome. **This suggests to me that STAR is probably aligning a lot of reads outside of annotated genes**. If you build the salmon index [using a full decoy of the genome](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/), then you might be able to evaluate intergenic mapping in the output in terms of `Number of fragments discarded because they are best-mapped to decoys`. However, in that case, these reads still won't contribute to transcript expression, as they do not align to annotated transcripts. Finally, if you suspect these reads might be coming from genes expressed in your sample but not present in the annotation, you might consider performing a transcript assembly on your data, using a tool like [scallop2](https://github.com/Shao-Group/scallop2) or [stringtie](https://github.com/gpertea/stringtie). Best,; Rob. P.S. I'm closing the thread, since I think the above answers your direct question, but please feel free to continue commenting here (for discussion) or to open up another issue if there are follow-ups that are salmon-related.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719
https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719:1039,Usability,simpl,simply,1039,"re being discarded because they have no alignment to annotated transcripts above the minimum allowable score, and an additional `1,099,008` are being discarded because they have only dovetail mappings. . We discard dovetail mappings by default, but you can admit them with `--allowDovetail`. The other `2,776,678` fragments are discarded because, though there are seeds for mapping that match, they do not have sufficiently high alignment score to be allowed for mapping. This default behavior, too, can be modified. The main flags that affect the behavior here are `--minScoreFraction`, where a lower number allows lower-quality alignments through and also the `--softclipOverhangs` flag which will decrease the penalty on alignments that overhang the end of an annotated transcript. However, it's worth noting that this is up to `3,875,686` more reads that might be mappable. This number is non-trivial, but quite far from the 90% rate of STAR. The rest of the reads, however, simply don't have support for alignment against the annotated transcriptome. **This suggests to me that STAR is probably aligning a lot of reads outside of annotated genes**. If you build the salmon index [using a full decoy of the genome](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/), then you might be able to evaluate intergenic mapping in the output in terms of `Number of fragments discarded because they are best-mapped to decoys`. However, in that case, these reads still won't contribute to transcript expression, as they do not align to annotated transcripts. Finally, if you suspect these reads might be coming from genes expressed in your sample but not present in the annotation, you might consider performing a transcript assembly on your data, using a tool like [scallop2](https://github.com/Shao-Group/scallop2) or [stringtie](https://github.com/gpertea/stringtie). Best,; Rob. P.S. I'm closing the thread, since I think the above answers your direct question, but please feel fr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/775#issuecomment-1126361719
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:330,Availability,error,error,330,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:102,Performance,perform,performing,102,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:136,Performance,optimiz,optimization,136,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:458,Performance,optimiz,optimization,458,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:772,Performance,optimiz,optimization,772,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:404,Usability,simpl,simple,404,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478:513,Usability,clear,clearing,513,"Hi @Jensen416,. Thank you for reporting this. Certain versions of the GCC compiler are not capable of performing full program link time optimization (`lto`) for this codebase. This is a known issue — and there are other programs that exhibit this same behavior. This is something that GCC must fix upstream — an internal compiler error is something that really shouldn't happen. Luckily, the solution is simple; just don't use whole program inter procedural optimization. Try using this `cmake` invocation (after clearing out your build directory):. ```; cmake -DNO_IPO=TRUE -DFETCH_BOOST=TRUE -DTBB_INSTALL_DIR= ~/anaconda3/pkgs/tbb-2021.5.0-hd09550d_0/ -DCMAKE_INSTALL_PREFIX= ~/salmon/; ```. The `-DNO_IPO` tells `cmake` to invoke the compiler without inter procedural optimization (i.e. `lto`). Let me know if this works for you. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/778#issuecomment-1134776478
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:116,Modifiability,extend,extending,116,"Thank you @rob-p for commenting, I presumed that could be the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:751,Security,validat,validateMappings,751,"Thank you @rob-p for commenting, I presumed that could be the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:902,Security,validat,validateMappings,902,"Thank you @rob-p for commenting, I presumed that could be the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:1130,Security,validat,validateMappings,1130,"the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of pre-mRNAs + mature RNAs in each locus.. What do you think?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463:1938,Usability,guid,guided,1938,"the case for reads from unspliced pre-mRNAs that are even extending a small fraction into the introns (hence better scoring on the decoys). The 2 FASTQ files for one of the samples I was describing above can be found as R4171*.fastq.gz at this globus link: http://research.libd.org/globus/jhpce_bsp2-dlpfc/index.html. I used just the main chromosomes with Gencode v41 annotation (slightly ""curated"" to remove read-through and ""retained intron"" annotated transcripts). I am attaching 3 `meta_info.json` outputs for the 3 ways I ran salmon on this sample:. - [tx_only.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006627/tx_only.meta_info.json.gz) : no decoys, **without** `--validateMappings`; - [gentrome_full.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006628/gentrome_full.meta_info.json.gz) : with `--validateMappings`, decoys are full chromosome sequences appended to the transcripts file, ; - [gentrome_mashed.meta_info.json.gz](https://github.com/COMBINE-lab/salmon/files/11006629/gentrome_mashed.meta_info.json.gz) : with `--validateMappings`, decoys prepared with mashmap as instructed [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md). It would be great to be able to use Salmon's ""wicked fast"" mapping engine to estimate intronic and intergenic reads at the same time, so I'm considering to make better use of the `writeMappings` output for that purpose, by preparing the decoys in a specific way (extracting intronic and intergenic sequences as distinctively labeled decoys and count the mappings to each label from Salmon's SAM output -- would that work?). I am wondering, due to pre-mRNAs found in rRNA-depletion (ribo-zero) samples, it might be better to artifically add the unspliced transcripts into the mix along with the ""reference"" annotation transcripts, so they also get quantified during the EM-guided probabilistic distribution of reads across this mix of pre-mRNAs + mature RNAs in each locus.. What do you think?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/779#issuecomment-1474451463
https://github.com/COMBINE-lab/salmon/issues/782#issuecomment-1142131363:857,Usability,simpl,simply,857,"Hi @jonahcullen,. No need to apologize! We should better document these numbers. Basically, the elements you point out : `num_decoy_fragments`, `num_dovetail_fragments` and `num_fragments_filtered_vm` are the fragments where alignment was *attempted* but subsequently failed. In these cases because (1) the read best mapped to a decoy, (2) the best alignment was dovetailed or (3) no alignment passed the alignment score threshold. In addition to this, fragments can fail to align when no sufficiently good seed is found such that alignment is not even attempted. This can happen e.g. if no 31-mer from the read matches the transcriptome/genome, or if the only matching 31-mers are degenerate in terms of their frequency (appear thousands of times and are therefore not useful for alignment). So, the most likely occurrence here is that these ~1M fragments simply had no alignment attempted. Let me know if this answers your question. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/782#issuecomment-1142131363
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:211,Availability,error,error,211,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:19,Deployability,update,update,19,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:422,Testability,test,tested,422,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835:119,Usability,guid,guide,119,"Dear Rob,; a brief update:; 1) with the flag -DNO_IPO=TRUE the compilation worked perfectly. thank you . 2)following a guide found at stackoverlow ([Find which assembly instruction caused an Illegal Instruction error without debugging], I discover that the illegal instruction is **vfmsubsd**. ; I am not an expert at all in the field, but googling it seems to be a standard SSE instruction.; I am surprised indeed.; cpus tested: ; Intel Xeon Gold 5220 (72) ; Intel Xeon Gold 5317 (48); Intel i7-10750H (12). Best and thanks again; Silvano. Program terminated with signal SIGILL, Illegal instruction.; #0 0x00007fa222c47396 in __ieee754_pow_fma4 () from /dataraw/mouse/salmon-1.8.0_linux_x86_64/bin/../lib/libm.so.6. 0x7fa222c47396 <__ieee754_pow_fma4+182> vfmsubsd %xmm3,%xmm6,%xmm3,%xmm7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/783#issuecomment-1145687835
https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516:124,Deployability,update,update,124,"@mousepixels Apologies for slow reply, I found myself circling back to this same issue with another project and thought I'd update the thread. Seems like there was some guidance all along regarding dealing with ONT data. See this [link ](https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/). . To summarize, looks like they advise _-N 100 -p 1.0_ for minimap2, which coincidently is what I have been doing as well. Hope that's helpful if you haven't already come up with a strategy.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516
https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516:169,Usability,guid,guidance,169,"@mousepixels Apologies for slow reply, I found myself circling back to this same issue with another project and thought I'd update the thread. Seems like there was some guidance all along regarding dealing with ONT data. See this [link ](https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/). . To summarize, looks like they advise _-N 100 -p 1.0_ for minimap2, which coincidently is what I have been doing as well. Hope that's helpful if you haven't already come up with a strategy.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/790#issuecomment-2028185516
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:380,Availability,repair,repair,380,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:515,Availability,repair,repair-guide,515,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:509,Usability,guid,guide,509,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752:522,Usability,guid,guide,522,"In this case, it is likely treating the left and right reads as orphans when mapping. Therefore, you're losing basically all of the benefit of having paired-end reads (which can be considerable) and also increasing the probability of spurious mapping (orphans generally map much more ambiguously than properly paired reads). Since you have the paired-end files, you should try to repair them (using something like BBMap's [re-pair tool](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/repair-guide/)) to get back properly-paired FASTQ files for analysis. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/793#issuecomment-1220164752
https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290:582,Testability,test,test,582,"Hi @biobenkj,. Congratulations on publishing your new single-cell technology, and thanks for your interest in adding support to alevin(fry). . After adding the functionality to provide custom geometry for UMI and cellular barcode sequence through command line flags like `--umi-geometry` and `--barcode-geometry,` our general guidelines have been shifted against adding technology-specific command line flags to the alevin codebase. Rob might have more comments on that. Regarding the 0-length cell barcode, I recommend first trying to add the dummy CB before the UMI sequence as a test case. If it helps with your use case, we can discuss adding the ; 0-length cellular barcode functionality to the main codebase. Previously, paired-end read processing was not possible under the alevin framework, but with the publication of alevin-fry, the support for paired-end read (I think) has been added. @DongzeHE and @Gaura might have better thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290
https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290:326,Usability,guid,guidelines,326,"Hi @biobenkj,. Congratulations on publishing your new single-cell technology, and thanks for your interest in adding support to alevin(fry). . After adding the functionality to provide custom geometry for UMI and cellular barcode sequence through command line flags like `--umi-geometry` and `--barcode-geometry,` our general guidelines have been shifted against adding technology-specific command line flags to the alevin codebase. Rob might have more comments on that. Regarding the 0-length cell barcode, I recommend first trying to add the dummy CB before the UMI sequence as a test case. If it helps with your use case, we can discuss adding the ; 0-length cellular barcode functionality to the main codebase. Previously, paired-end read processing was not possible under the alevin framework, but with the publication of alevin-fry, the support for paired-end read (I think) has been added. @DongzeHE and @Gaura might have better thoughts on this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/805#issuecomment-1282467290
https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1323774035:81,Usability,clear,clear,81,"Pinging @mikelove, who is certainly the person to give the best answer here. One clear difference to note though is that TPM is a length-normalized measure, while CPM is not. This alone means they will exhibit nontrivial differences.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/812#issuecomment-1323774035
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443:86,Availability,error,error,86,"Hello,; Thank you for your guidance on this question. However, I encountered the same error despite using the latest versions of Trinityrnaseq and salmon. I ran Trinity version 2.15.1 to generate Fasta files. I attempted to use Salmon version 1.10.1 for indexing, but I encountered this exception. Upon checking the Docker link provided in the comment, I found that the salmon version listed is 1.5.0. So, I tried using Salmon 1.5.0 and encountered the same error. Could you please advise me on how to resolve this issue? Thank you. ```; $./trinityrnaseq-v2.15.1/Trinity --seqType fq --max_memory 6 --samples_file sample.txt --min_kmer_cov 2 --no_parallel_norm_stats --output trinity_test_0210_1019 --CPU 6. ... $ ~/tools/salmon-latest_linux_x86_64/bin/salmon index --index quasi --type quasi --transcripts ~/first_try_Gall/trinity_test_0210_1019.Trinity.fasta ; Version Info: This is the most recent version of salmon.; Exception : [Error: RapMap-based indexing is not supported in this version of salmon.]; /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443:458,Availability,error,error,458,"Hello,; Thank you for your guidance on this question. However, I encountered the same error despite using the latest versions of Trinityrnaseq and salmon. I ran Trinity version 2.15.1 to generate Fasta files. I attempted to use Salmon version 1.10.1 for indexing, but I encountered this exception. Upon checking the Docker link provided in the comment, I found that the salmon version listed is 1.5.0. So, I tried using Salmon 1.5.0 and encountered the same error. Could you please advise me on how to resolve this issue? Thank you. ```; $./trinityrnaseq-v2.15.1/Trinity --seqType fq --max_memory 6 --samples_file sample.txt --min_kmer_cov 2 --no_parallel_norm_stats --output trinity_test_0210_1019 --CPU 6. ... $ ~/tools/salmon-latest_linux_x86_64/bin/salmon index --index quasi --type quasi --transcripts ~/first_try_Gall/trinity_test_0210_1019.Trinity.fasta ; Version Info: This is the most recent version of salmon.; Exception : [Error: RapMap-based indexing is not supported in this version of salmon.]; /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443
https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443:27,Usability,guid,guidance,27,"Hello,; Thank you for your guidance on this question. However, I encountered the same error despite using the latest versions of Trinityrnaseq and salmon. I ran Trinity version 2.15.1 to generate Fasta files. I attempted to use Salmon version 1.10.1 for indexing, but I encountered this exception. Upon checking the Docker link provided in the comment, I found that the salmon version listed is 1.5.0. So, I tried using Salmon 1.5.0 and encountered the same error. Could you please advise me on how to resolve this issue? Thank you. ```; $./trinityrnaseq-v2.15.1/Trinity --seqType fq --max_memory 6 --samples_file sample.txt --min_kmer_cov 2 --no_parallel_norm_stats --output trinity_test_0210_1019 --CPU 6. ... $ ~/tools/salmon-latest_linux_x86_64/bin/salmon index --index quasi --type quasi --transcripts ~/first_try_Gall/trinity_test_0210_1019.Trinity.fasta ; Version Info: This is the most recent version of salmon.; Exception : [Error: RapMap-based indexing is not supported in this version of salmon.]; /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /home/ubuntu/tools/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/832#issuecomment-1990328443
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:398,Deployability,configurat,configurations,398,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:27,Modifiability,config,configure,27,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:312,Modifiability,config,configure,312,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:398,Modifiability,config,configurations,398,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294:228,Usability,simpl,simple,228,"Using the rest of the same configure flags without `-DUSE_SHARED_LIBS=TRUE`, the build does not link properly. I think you should try building without these extra flags. Since the LTO seems not to be a problem on this system, a simple `cmake .. && make` should work. In the mean time, I'll try and pare back the configure command line to find the maximum viable interpolation between our different configurations. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464012294
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:31,Integrability,depend,dependency,31,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:305,Integrability,depend,dependency,305,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:508,Integrability,depend,dependencies,508,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885:161,Usability,clear,clear,161,"So, it turns out that with the dependency setup that is pulled in, I can't even get `salmon` to build without `USE_SHARED=TRUE`. I think at this point, it's not clear the segfault is due to something that is broken / can be fixed in the salmon code itself. Rather, it's likely due to a misversioning of a dependency that is pulled in and then linked against. For the time being, I think the options are to do a more ""standard"" build (i.e. like the first one I suggested that pull in only that minimal set of dependencies and let salmon itself statically link the rest), or to try and look at the upstream shared libraries being linked and figure out which of them is misversioned. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464124885
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:942,Availability,down,download,942,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:121,Integrability,depend,dependencies,121,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988:516,Usability,simpl,simply,516,"Hi @rob-p,; thanks a lot for your investigation. Could you please be more verbose on those incorrect Build-Depends? What dependencies can be removed (if not used they should not really harm, thought but you are correct that it makes sense to remove these) and more importantly which can not be used. For instance if we can't use libstaden as packaged we have a problem. All preconditions for a Debian package have to be packaged first. Fetching something from network is not permitted at package build time.; Thus I simply tried changing the cmake options to. ```; $ cmake -DCMAKE_BUILD_TYPE=Release -DUSE_SHARED_LIBS=TRUE -DBZIP2_LIBRARIES=-lbz2 -DBZIP2_INCLUDE_DIR=/usr/include -DLIBLZMA_INCLUDE_DIR=/usr/include/ -DLIBLZMA_LIBRARY=lzma -DCMAKE_MODULE_PATH=/usr/share/cmake/Modules -DTBB_WILL_RECONFIGURE=FALSE -DBOOST_WILL_RECONFIGURE=FALSE ..; ```. which does not change the SEGFAULT problem. If the issue belongs to something we need to download from somewhere please let me know what looks suspicious to you. This would be helpful since we could either add it to the Debian package source in debian/missing-sources ... or rather fix the predependency that would break salmon.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464471988
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:700,Availability,down,downloaded,700,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:1045,Availability,down,downloading,1045,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:1105,Availability,down,downloaded,1105,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:109,Deployability,upgrade,upgrade,109,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:197,Deployability,update,update,197,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:244,Deployability,upgrade,upgraded,244,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:1160,Deployability,install,installed,1160,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:983,Testability,test,test,983,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371:204,Usability,simpl,simply,204,"I think we should sort out this issue step by step. If you say `libstaden` has an important bugfix we should upgrade to the latest version in any case. Do you have a link to this bug? I admit this update simply slipped through - we should have upgraded this in the beginning of this year. Usually we try to follow upstream closely (which we failed for salmon blatantly for several reasons - one is the close connection to pufferfish).; Regarding `pufferfish`: We tried hard to get `pufferfish` packaged but failed (due to the use of other versions of `spdlog`, `cereal`, and `fmt`) However, since we can't run `fetchPufferfish.sh` *inside the build process* I was running it separately and added the downloaded source in [debian/external/pufferfish](https://salsa.debian.org/med-team/salmon/-/tree/master/debian/external/pufferfish) So I think the requirement of salmon should be fulfilled. I confirm your feeling that pufferfish is important for the current issue.; However, in the test I did when opening this bug report I did not do that pre-downloading of pufferfish since I was building right in the downloaded source tarball. `libpufferfish-dev` was not installed by `apt build-dep salmon` since this package does not exist.; Kind regards, Andreas.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464878371
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:626,Availability,down,downloaded,626,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:486,Deployability,install,install,486,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:755,Deployability,release,release,755,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:771,Deployability,install,installed,771,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:811,Deployability,install,install,811,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1180,Deployability,update,updated,1180,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1252,Deployability,install,installed,1252,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1302,Deployability,install,install,1302,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:390,Integrability,depend,dependencies,390,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:418,Integrability,depend,dependency,418,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:512,Integrability,depend,dependencies,512,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:570,Testability,test,testing,570,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:244,Usability,clear,clearly,244,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376:1280,Usability,simpl,simple,1280,"Hi @tillea,. So I went through the list of deps pulled in by `apt build-dep salmon` and the minimal set I gave above. I tried to make the smallest number of changes I could to the `apt build-dep salmon` list while also removing things that are clearly outdated (we no longer use jellyfish, rapmap, etc. and we use the header-only version of spdlog). As a result I came up with this list of dependencies. The offending dependency seems to be `libcereal-dev`. Specifically, I was able to install just this list of dependencies (minus `libcereal-dev`) atop a clean `debian:testing` and get a working version where the only thing downloaded from the internet was the appropriate version of the pufferfish files grabbed by `fetchPufferfish.sh` in the `1.10.0` release. Once I installed `libcereal-dev` with `apt-get install`, and rebuilt, then I got the segfault mentioned at the top of this issue. So, it seems that we either have to let `salmon` build it's own libcereal, or figure out what the problem is with the library upstream. Please let me know if this you observe this same behavior as well (also @nileshpatra may want to try this out). If so, perhaps we can get `libstaden` updated upstream, and then use this as the new dep list for `salmon`. I installed these deps with a simple `xargs apt-get install -y < deps_sorted_updated.txt` (without `libcereal-dev` for the working version, and with it included, as below, for the segfault). Best,; Rob. [deps_sorted_updated.txt](https://github.com/COMBINE-lab/salmon/files/10949233/deps_sorted_updated.txt)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/835#issuecomment-1464986376
https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720:1190,Performance,optimiz,optimization,1190,"Hi @apredeus,. In short, what you explain in the first paragraph is right and is the expected behavior. However, to simplify the parsing algorithm (i.e. to ensure that BAM input can be parsed in bounded memory), both `salmon` and `RSEM` require that all of the alignments for a given read are adjacent within the input BAM file. If this is violated, they will be treated as different reads. In other words, if you have something like:. ```; read1:aln1; read1:aln2; read1:aln3; read2:aln1; read2:aln2; read2:aln3; ```. then in total, 2 ""reads"" worth of mass will be assigned (probabilistically across the targets). However, if you have. ```; read1:aln1; read1:aln2; read2:aln1; read1:aln3; read2:aln2; read2:aln3; ```. Then there will be *4* total reads assigned. Each time the query name (read name modulo 1/2 of a paired-end read) changes in the BAM stream, it is assumed to be a new read, and its alignments are dealt with separately. Both Bowtie2 and STAR (when projecting genomic alignments to the transcriptome) will follow this convention by default, but I'm not certain the same is true for other aligners. Again, this restriction is present in both `RSEM` and `salmon`, and it's an optimization that is made because otherwise there can be unbounded distance in the worst case between the different alignments for a read and so the parser would either have to hold all alignments in memory (which is very bad), or make many passes over the input BAM (which is also very bad) to perform quantification. Let me know if you think this may be the issue in your case. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720
https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720:1485,Performance,perform,perform,1485,"Hi @apredeus,. In short, what you explain in the first paragraph is right and is the expected behavior. However, to simplify the parsing algorithm (i.e. to ensure that BAM input can be parsed in bounded memory), both `salmon` and `RSEM` require that all of the alignments for a given read are adjacent within the input BAM file. If this is violated, they will be treated as different reads. In other words, if you have something like:. ```; read1:aln1; read1:aln2; read1:aln3; read2:aln1; read2:aln2; read2:aln3; ```. then in total, 2 ""reads"" worth of mass will be assigned (probabilistically across the targets). However, if you have. ```; read1:aln1; read1:aln2; read2:aln1; read1:aln3; read2:aln2; read2:aln3; ```. Then there will be *4* total reads assigned. Each time the query name (read name modulo 1/2 of a paired-end read) changes in the BAM stream, it is assumed to be a new read, and its alignments are dealt with separately. Both Bowtie2 and STAR (when projecting genomic alignments to the transcriptome) will follow this convention by default, but I'm not certain the same is true for other aligners. Again, this restriction is present in both `RSEM` and `salmon`, and it's an optimization that is made because otherwise there can be unbounded distance in the worst case between the different alignments for a read and so the parser would either have to hold all alignments in memory (which is very bad), or make many passes over the input BAM (which is also very bad) to perform quantification. Let me know if you think this may be the issue in your case. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720
https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720:116,Usability,simpl,simplify,116,"Hi @apredeus,. In short, what you explain in the first paragraph is right and is the expected behavior. However, to simplify the parsing algorithm (i.e. to ensure that BAM input can be parsed in bounded memory), both `salmon` and `RSEM` require that all of the alignments for a given read are adjacent within the input BAM file. If this is violated, they will be treated as different reads. In other words, if you have something like:. ```; read1:aln1; read1:aln2; read1:aln3; read2:aln1; read2:aln2; read2:aln3; ```. then in total, 2 ""reads"" worth of mass will be assigned (probabilistically across the targets). However, if you have. ```; read1:aln1; read1:aln2; read2:aln1; read1:aln3; read2:aln2; read2:aln3; ```. Then there will be *4* total reads assigned. Each time the query name (read name modulo 1/2 of a paired-end read) changes in the BAM stream, it is assumed to be a new read, and its alignments are dealt with separately. Both Bowtie2 and STAR (when projecting genomic alignments to the transcriptome) will follow this convention by default, but I'm not certain the same is true for other aligners. Again, this restriction is present in both `RSEM` and `salmon`, and it's an optimization that is made because otherwise there can be unbounded distance in the worst case between the different alignments for a read and so the parser would either have to hold all alignments in memory (which is very bad), or make many passes over the input BAM (which is also very bad) to perform quantification. Let me know if you think this may be the issue in your case. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/844#issuecomment-1518485720
https://github.com/COMBINE-lab/salmon/issues/850#issuecomment-2052164298:567,Usability,guid,guidance,567,"Hello @Starahoush, greetings from Brazil! I'm an undergraduate student in Biomedical Informatics at the Federal University of Paraná, currently involved in a scientific initiation project in an immunology lab. I'm working extensively with FASTQ files from samples sequenced on the BD Rhapsody V1 platform and I've been facing a challenge: a significant portion of the reads are being discarded due to ""noisy cellular barcodes"", with around 50% of the reads affected. Could you please share if you've encountered a similar situation in your experiment or provide some guidance on how to address this issue? I appreciate your attention and assistance!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/850#issuecomment-2052164298
https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832:254,Availability,error,error,254,"Hi @Alecrim24,. It looks like the list of r1 files are being interpreted as a single, long, filename. Same with the list of r2 files. Any idea why that's the case? They should be a space-separated list (of course, there *are* a ton of them here, but the error clearly suggests they are being interpreted as a single, long, filename). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832
https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832:260,Usability,clear,clearly,260,"Hi @Alecrim24,. It looks like the list of r1 files are being interpreted as a single, long, filename. Same with the list of r2 files. Any idea why that's the case? They should be a space-separated list (of course, there *are* a ton of them here, but the error clearly suggests they are being interpreted as a single, long, filename). --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/870#issuecomment-1694754832
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:340,Availability,ping,ping,340,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:526,Integrability,wrap,wrapper,526,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:597,Integrability,interface,interfaced,597,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:131,Usability,simpl,simply,131,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:516,Usability,simpl,simpleaf,516,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:612,Usability,simpl,simpleaf,612,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996:713,Usability,user experience,user experience,713,"Hi @davidaknowles,. Assuming that the total size of all cell barcodes doesn't exceed 32 nucleotides, then it should be possible to simply specify them using a custom geometry string. Of course for that to work, we need to know where the 4th barcode is located, so that we can generate the correct custom geometry string to extract it. I'll ping @DongzeHE and @k3yavi here to see if either of them are familiar with this chemistry already. As always, I'd also suggest running this through `alevin-fry` (or using the `simpleaf` wrapper). While we continue to support `alevin`, `alevin-fry` (largely interfaced by `simpleaf`) is where most of our development effort is currently going, and hopefully we can make the user experience there as smooth and easy as possible!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1732428996
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:207,Integrability,wrap,wrapper,207,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:181,Security,expose,exposed,181,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:197,Usability,simpl,simpleaf,197,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331:223,Usability,simpl,simplify,223,"Hi @davidaknowles,. Indeed — the barcode extraction will happen either in `salmon alevin` or, if you are using the new `piscem` module for mapping prior to quantification (both are exposed in the `simpleaf` wrapper tool to simplify single-cell processing with `alevin-fry`), then it will happen there. We have a new very general and much more capable module in the works that will be able to handle all manners of single-cell geometry, but nothing about the Parse library seems beyond the capabilities of the current geometry processing code. If you share some reads, we can also try and take a look and figure out where the last BC resides. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/874#issuecomment-1733872331
https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632:937,Availability,error,error,937,"Hi @rob-p, ; Thank you for getting back to me so quickly and thank you for the explanation. ; I found this info about the `--eqclasses ` parameter in the Salmon quant help manual for alignment-based mode:; <img width=""626"" alt=""Screenshot 2023-10-03 at 11 40 56"" src=""https://github.com/COMBINE-lab/salmon/assets/76558077/dc98c406-759f-4543-8cdd-5299d24775eb"">; Everything else I tried was a guess.; I wasn’t sure how to get the right file, so I thought it might be the **eq_classes.txt.gz** file, made using the `--dumpEq` option. I read about [—dumpEq option ](https://salmon.readthedocs.io/en/latest/salmon.html#dumpeq) and I found some explanation of [equivalence class file](https://salmon.readthedocs.io/en/latest/file_formats.html#equivalence-class-file). So, I made this file using the `--dumpEq` option and then used it with `--eqclasses` option, but I made mistake by also providing a BAM file with `-a` option. When I got the error, i thought maybe this file was supposed to be used instead of the alignment BAM file, not together with, and now i understand.; I'm still not clear: is this file only used when I want to analyze the same sample multiple times, with different options?; Also, I noticed the `--eqclasses` parameter isn’t in the help manual for salmon quant in mapping-based mode because it’s not there when I run `salmon quant --help-reads`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632
https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632:1085,Usability,clear,clear,1085,"Hi @rob-p, ; Thank you for getting back to me so quickly and thank you for the explanation. ; I found this info about the `--eqclasses ` parameter in the Salmon quant help manual for alignment-based mode:; <img width=""626"" alt=""Screenshot 2023-10-03 at 11 40 56"" src=""https://github.com/COMBINE-lab/salmon/assets/76558077/dc98c406-759f-4543-8cdd-5299d24775eb"">; Everything else I tried was a guess.; I wasn’t sure how to get the right file, so I thought it might be the **eq_classes.txt.gz** file, made using the `--dumpEq` option. I read about [—dumpEq option ](https://salmon.readthedocs.io/en/latest/salmon.html#dumpeq) and I found some explanation of [equivalence class file](https://salmon.readthedocs.io/en/latest/file_formats.html#equivalence-class-file). So, I made this file using the `--dumpEq` option and then used it with `--eqclasses` option, but I made mistake by also providing a BAM file with `-a` option. When I got the error, i thought maybe this file was supposed to be used instead of the alignment BAM file, not together with, and now i understand.; I'm still not clear: is this file only used when I want to analyze the same sample multiple times, with different options?; Also, I noticed the `--eqclasses` parameter isn’t in the help manual for salmon quant in mapping-based mode because it’s not there when I run `salmon quant --help-reads`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/877#issuecomment-1744787632
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:891,Integrability,wrap,wrapper,891,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:753,Performance,scalab,scalable,753,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:309,Usability,simpl,simpleaf,309,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139:905,Usability,simpl,simpleaf,905,"Hi @mugpeng,. This is because it is covered by the custom geometry specification (as laid out in the docs). I agree it's nice to have a specific flag for each geometry, rather than to have to e.g. specify the custom geometry each time. We are working on good solutions to that at a higher level (e.g. in our `simpleaf` tool where users can register their own custom geometry specifications and refer to them by name). However, in `salmon`/`alevin` right now, the named geometries are hard-coded, and so to have a specific `--indropV2` flag, that would have to be added to the argument parser and then mapped to the specific underlying geometry in the code. This isn't hard, but as the number of different chemistries proliferates, it's not ultimately a scalable solution. So, the current recommendation would be to use the custom geometry flags as specified in the documentation, or adopt a wrapper like `simpleaf` and add `indropV2` to your custom geometry specification library. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/878#issuecomment-1758171139
https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757735883:234,Usability,clear,clear,234,"Hey Rob, not sure if I understood your answer. ; During cDNA synthesis, we use random 9-mer (not 6-mer). My impression from the documentation is that -seqbias can only be used if cDNA synthesis was done using 6-mer. This point is not clear in your answer for me. ; Could you please clarify?; Many thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/880#issuecomment-1757735883
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719:145,Testability,test,test,145,"I'm going to cc @dpryan79 on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test `simpleaf`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719:151,Usability,simpl,simpleaf,151,"I'm going to cc @dpryan79 on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test `simpleaf`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784122719
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021:73,Testability,test,testing,73,"conda create never seems to even get out of the gate ... a little bit of testing strongly suggests that version of salmon can't be found :. conda create -n owlVsunicorn -c bioconda owlVsunicorn; Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. this is the first time I've encountered an issue where something that is ""supposed"" to be there can't be found. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 10:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; State change ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). I'm going to cc @dpryan79<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_dpryan79&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=hPsmfTmSAfiwDhS2JFQyD0EUHl5m5sbj_n46DYj6tdM&e=> on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test simpleaf. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784122719&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=6Y-rQOzzAA-t9QV8NyfcMVeySD2an4xeN1HsDqa6VpQ&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUDFRQIHN4AMNBHWCN3YBZOUTAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PN",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021:1420,Testability,test,test,1420,"environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. this is the first time I've encountered an issue where something that is ""supposed"" to be there can't be found. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 10:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; State change ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). I'm going to cc @dpryan79<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_dpryan79&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=hPsmfTmSAfiwDhS2JFQyD0EUHl5m5sbj_n46DYj6tdM&e=> on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test simpleaf. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784122719&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=6Y-rQOzzAA-t9QV8NyfcMVeySD2an4xeN1HsDqa6VpQ&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUDFRQIHN4AMNBHWCN3YBZOUTAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZDENZRHE&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=ckSFRx1FekMV-wL0KtdZFPdtgCB1DiAziHIsdrF0cKQ&e=>.; You are receiving this because you mo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021:1425,Usability,simpl,simpleaf,1425,"environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): / Killed. this is the first time I've encountered an issue where something that is ""supposed"" to be there can't be found. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 10:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; State change ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). I'm going to cc @dpryan79<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_dpryan79&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=hPsmfTmSAfiwDhS2JFQyD0EUHl5m5sbj_n46DYj6tdM&e=> on this — does it just not finish? It seems to work within our GitHub CI, where we have to grab the prebuilt salmon to test simpleaf. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784122719&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=6Y-rQOzzAA-t9QV8NyfcMVeySD2an4xeN1HsDqa6VpQ&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUDFRQIHN4AMNBHWCN3YBZOUTAVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZDENZRHE&d=DwMFaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=mSC4skssR3kpqIDO5fXv00Vk9PrFQeVf0OH62EOgXZ9hbM31qjkaQra0z60JiEA_&s=ckSFRx1FekMV-wL0KtdZFPdtgCB1DiAziHIsdrF0cKQ&e=>.; You are receiving this because you mo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784126021
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784137337:473,Usability,usab,usable,473,"Hi @adamfreedman,. I think this is just conda being very very very slow. For me, the below command, replacing `mamba` with `conda` (but keeping the switched channel order) eventually did work, but took several minutes to ""collect package metadata"". However, the following works fine for me (and finishes in ~1 minute):. ```; mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2; ```. Can you use the `mamba` resolver in your environment? Conda has become hardly usable over the years, but `mamba` works quite well as a fast replacement. I'll also note that I swapped the order of `conda-forge` and `bioconda` as the docs specify that `bioconda` should preferably come last in the list of channels. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784137337
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:46,Deployability,install,install,46,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:192,Modifiability,sandbox,sandbox,192,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:223,Performance,latency,latency,223,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:192,Testability,sandbox,sandbox,192,"I think you're right wrt conda. I was able to install 1.10.2 with mamba fairly easily. We've been moving away from conda (towards mamba) but this didn't cross my mind when I was playing in my sandbox. Might be some cluster latency issues combined with conda's snail's pace causing the problem on our end. Thx for the quick replies!. Adam H. Freedman, PhD; Data Scientist; Faculty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835:1378,Usability,usab,usable,1378,"ulty of Arts & Sciences Informatics Group; Harvard University; 38 Oxford St; Cambridge, MA 02138; phone: +001 310 415 7145; ________________________________; From: Rob Patro ***@***.***>; Sent: Sunday, October 29, 2023 11:01 AM; To: COMBINE-lab/salmon ***@***.***>; Cc: Freedman, Adam ***@***.***>; Mention ***@***.***>; Subject: Re: [COMBINE-lab/salmon] anaconda version of salmon outdated, missing decoys option (Issue #895). Hi @adamfreedman<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_adamfreedman&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=kxY9gCLGWZJp-dp7l31S6M5u2RuUTeWXVrKmaydpo5o&e=>,. I think this is just conda being very very very slow (and potentially broken). The following works fine for me (and finishes in ~1 minute):. mamba create -n salmon -c conda-forge -c bioconda salmon=1.10.2. Can you use the mamba resolver in your environment? Conda has become hardly usable over the years, but mamba works quite well as a fast replacement. I'll also note that I swapped the order of conda-forge and bioconda as the docs specify that bioconda should preferably come last in the list of channels. --Rob. —; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_COMBINE-2Dlab_salmon_issues_895-23issuecomment-2D1784137337&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=GNiCXqUbJLM16QBJ5PNAqv-rsgDdpCpcvezPXO_riWk&e=>, or unsubscribe<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_ADBMMUCOMVRRPOAZQL2EIITYBZVT5AVCNFSM6AAAAAA6UYYPGOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTOOBUGEZTOMZTG4&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=MITI_LEJgyr1a24IMFAlSaZIPxMpOUT21T7L3fg4CjA&m=40O3raH84f_BIZ3HF7nqTYSO2FehGrGHL9b7sqT7LIpWZjFmA3BLxNDTHoz420jp&s=54-iPwwQkGRgqbmGQptKb3",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/895#issuecomment-1784196835
https://github.com/COMBINE-lab/salmon/issues/910#issuecomment-1918166379:31,Usability,clear,clear,31,"Updated Expected behavior: ; A clear and concise description of what you expected to happen.; I aim to retain all gene IDs, and for those represented by multiple lines, I intend to calculate the sum of values for each unique gene ID. I came across a few posts regarding this issue, but have not found a good solution for salmon quantmerge yet",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/910#issuecomment-1918166379
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:48,Availability,error,error,48,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:357,Availability,error,error,357,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:613,Availability,error,errors,613,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:851,Availability,error,errors,851,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:958,Availability,error,errors,958,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:1059,Availability,error,errors,1059,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:554,Usability,learn,learns,554,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254:914,Usability,learn,learned,914,"Hello! I did some work with the oxford nanopore error model last summer. There's a blog post about the ONT long read quantification here: https://combine-lab.github.io/salmon-tutorials/2021/ont-long-read-quantification/ . In terms of length correction, the --ont flag basically turns off length correction (since it doesn't really apply to long reads). The error model that the current version of salmon uses for the --ont flag (found in src/ONTAlignmentModel.cpp) basically bins reads by length (into 4 bins by default, I believe). Then for each bin it learns a binomial/geometric distribution for the number of errors (mismatches or indels) in the alignment of the reads in the bin, as well as distributions for the number of bases softclipped at the beginning and end of the read. It then uses these models to penalize reads that have an amount of errors/softclips that is very different from the center of the learned distribution, only if the number of errors/softclips is larger than what we expect for that bin (since a smaller than expected number of errors in the alignment is generally a good, not a bad sign for how likely the read is to map to this transcript). I'm not the original author/creator of this model, so I don't have all the details on specifics of how it works/the design decisions that went into it, but let me know if you have any other questions I can answer!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/924#issuecomment-2148242254
https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517:36,Testability,log,logic,36,"@tdsone I _think_ this is the right logic: https://github.com/COMBINE-lab/salmon/blob/master/include/LibraryTypeDetector.hpp. . The main source of my confusion on this post was that I think Salmon just chucks back 'IU' for read numbers below 50k. It confused me less on realistic read numbers. I've simplified [quite a bit](https://github.com/nf-core/rnaseq/blob/bc6189f09954c0d00a71ac43b2ccf69ef22bbd82/subworkflows/local/utils_nfcore_rnaseq_pipeline/main.nf#L587) to just work with the strandedness component, using the numbers from lib_format_counts.json. It seems to produce results broadly as expected, but might be a bit naive, for example the numbers are mappings rather than fragments which could throw things off a bit.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517
https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517:299,Usability,simpl,simplified,299,"@tdsone I _think_ this is the right logic: https://github.com/COMBINE-lab/salmon/blob/master/include/LibraryTypeDetector.hpp. . The main source of my confusion on this post was that I think Salmon just chucks back 'IU' for read numbers below 50k. It confused me less on realistic read numbers. I've simplified [quite a bit](https://github.com/nf-core/rnaseq/blob/bc6189f09954c0d00a71ac43b2ccf69ef22bbd82/subworkflows/local/utils_nfcore_rnaseq_pipeline/main.nf#L587) to just work with the strandedness component, using the numbers from lib_format_counts.json. It seems to produce results broadly as expected, but might be a bit naive, for example the numbers are mappings rather than fragments which could throw things off a bit.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/938#issuecomment-2175817517
