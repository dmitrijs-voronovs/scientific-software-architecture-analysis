id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1485,Security,authenticat,authentication,1485,"g additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports ",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1881,Security,authenticat,authenticated,1881," the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanoth",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2420,Security,access,access,2420," Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception o",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:3481,Security,authenticat,authentication,3481,"tely in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it ",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:3577,Security,access,access,3577,"tely in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it ",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:3708,Security,access,access,3708,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:3757,Security,access,access,3757,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:3956,Security,authoriz,authorization,3956,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:4140,Security,password,password,4140,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:4300,Security,access,access,4300,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:4535,Security,access,access,4535,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:309,Usability,guid,guidance,309,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:967,Usability,simpl,simplest,967," is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme ext",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2683,Usability,simpl,simply,2683,"o/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:55,Availability,avail,available,55,"In order to run a workflow, Cromwell uses the backends available to it to create jobs and monitor them until they are complete. When no new jobs can start and all jobs have finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An e",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:460,Availability,failure,failure,460,"In order to run a workflow, Cromwell uses the backends available to it to create jobs and monitor them until they are complete. When no new jobs can start and all jobs have finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An e",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:923,Availability,failure,failure,923,"In order to run a workflow, Cromwell uses the backends available to it to create jobs and monitor them until they are complete. When no new jobs can start and all jobs have finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An e",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1041,Availability,failure,failure,1041,"omwell uses the backends available to it to create jobs and monitor them until they are complete. When no new jobs can start and all jobs have finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1338,Availability,failure,failure,1338,"at to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous examp",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1595,Availability,failure,failure,1595,"oon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retr",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1910,Availability,failure,failures,1910,"flow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1930,Availability,failure,failures,1930,"nfiguration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort.",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1951,Availability,failure,failures,1951,"nfiguration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort.",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1988,Availability,failure,failure,1988,"nfiguration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort.",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2023,Availability,failure,failure,2023,"view#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2138,Availability,failure,failure,2138,"ndent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endp",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2270,Availability,failure,failure,2270," are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#a",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2419,Availability,failure,failure,2419,"o complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. A",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2490,Availability,failure,failure,2490,"l then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2542,Availability,failure,failure,2542,"ded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the work",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2582,Availability,failure,failure,2582,"mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2777,Availability,failure,failure,2777,"ailed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an a",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4750,Availability,failure,failure,4750,"mwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of t",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:5696,Availability,failure,failure,5696," with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gra",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:7034,Availability,down,down,7034,""". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently C",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:7453,Availability,down,down,7453,"y:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortA",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4897,Deployability,upgrade,upgrade,4897,"well will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backe",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:6209,Deployability,configurat,configuration,6209,"fferent instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process st",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:90,Energy Efficiency,monitor,monitor,90,"In order to run a workflow, Cromwell uses the backends available to it to create jobs and monitor them until they are complete. When no new jobs can start and all jobs have finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An e",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:678,Energy Efficiency,monitor,monitor,678,"In order to run a workflow, Cromwell uses the backends available to it to create jobs and monitor them until they are complete. When no new jobs can start and all jobs have finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An e",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1408,Energy Efficiency,green,green,1408,"at to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous examp",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1651,Energy Efficiency,green,green,1651,"oon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retr",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1684,Energy Efficiency,green,green,1684,"oon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retr",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2201,Energy Efficiency,green,green,2201,"ndent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endp",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2861,Energy Efficiency,green,green,2861,"ailed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an a",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2908,Energy Efficiency,green,green,2908,"ng). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status o",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1176,Integrability,depend,depends,1176,"e finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will b",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1201,Integrability,depend,depends,1201,"e finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will b",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:2528,Integrability,depend,depend,2528,"ded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the work",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3985,Integrability,depend,dependent,3985,"odes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progre",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:5682,Integrability,message,message,5682,"Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:5893,Integrability,message,message,5893,"._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to retu",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:6707,Integrability,message,message,6707,"s failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:8806,Integrability,message,message,8806,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9198,Integrability,depend,depending,9198,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3223,Modifiability,config,configured,3223,"ill keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)).",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:6209,Modifiability,config,configuration,6209,"fferent instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process st",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:8647,Modifiability,config,configured,8647,"(called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-p",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9354,Modifiability,config,configurable,9354,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:7346,Performance,queue,queued,7346,"cess. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9224,Performance,load,load,9224,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3033,Safety,abort,abort,3033," is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . In the example above, if B's failure is retryable then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular c",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3105,Safety,abort,aborting,3105,"able then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://c",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3147,Safety,abort,abort,3147,"able then B will be retried (shaded yellow and green). The workflow will keep running normally, regardless of which failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://c",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3250,Safety,abort,abort,3250,"ch failure mode is enabled. ![](CWP_B_retryable_fail_then_success.png). Using the previous example, let's imagine **B** failed from a **non-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is fin",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3414,Safety,abort,abort,3414,"n-retryable** failure (shaded red). After B failed, **A** fails from a **retryable** failure. Now Cromwell's behavior will depend on the failure mode. **`NoNewCalls`** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some wo",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3560,Safety,abort,aborting,3560,"** ; If the failure mode is `NoNewCalls`, then A **will not be** retried (yellow). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend i",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3631,Safety,abort,abort,3631,"low). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3692,Safety,abort,aborted,3692,"low). A1 (grey) will not start, because A did not complete. ![](NCC_B_fail_A_retryable.png). **`ContinueWhilePossible`** ; If the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3770,Safety,abort,abort,3770,"the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempte",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3803,Safety,abort,abort,3803,"the failure mode is **`ContinueWhilePossible`**, then A **will be** retried (yellow and green). If A is successful then A1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempte",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3905,Safety,abort,aborted,3905,"1 will start (green). ![](CWP_B_fail_A_retryable.png). ## Abort. In both [Run](../Modes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example t",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:3960,Safety,abort,abort,3960,"odes/#run) and [Server](../Modes/#server) mode, you can abort a running workflow. This section explains what that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progre",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4087,Safety,abort,abort,4087,"t that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4138,Safety,abort,abort,4138,"t that entails. When aborting a workflow, either through the `/abort` endpoint or by terminating the [Cromwell run process](../Modes) (if [configured](../Configuring#abort) to do so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4292,Safety,abort,aborted,4292,"o so), Cromwell does the following:. 1. Changes the status of the workflow to `Aborting`,; 2. Does not start any new jobs,; 3. Asks every running job to abort,; 4. Waits for all running jobs to complete,; 5. Finalizes the workflow,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workfl",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4525,Safety,abort,aborted,4525,"low,; 6. Changes the status of the workflow to `Aborted`. The action of aborting a job is backend specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normal",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4581,Safety,abort,abort,4581,"specific. Cromwell can only ask a backend to abort a job and wait for the backend to notify it when it is aborted. . For example, if you are running Cromwell on the Google backend and abort a job, Google will send an abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were nev",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:4834,Safety,abort,aborting,4834," abort request to the Google Pipelines API. When Pipelines API indicates that the status of the job is aborted, Cromwell will mark it as such.; Remember that abort action is entirely dependent on the backend. In this particular case, Pipelines API does not guarantee the success of an abort request (see [Pipelines API documentation on abort](https://cloud.google.com/genomics/reference/rest/v1alpha2/operations/cancel)). You'll also notice that the workflow is finalized even though being aborted.; Finalization is the last step in the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:5369,Safety,abort,abort,5369,"the execution of a workflow and a chance for each backend to do some work before the workflow is terminated.; Backends won't be denied the chance to finalize the workflow even if it's being aborted. _Note that by the time the backend is asked to abort a job, the job may have succeeded or failed already. In this case Cromwell will report the job's status (successful or failed)._. _If a job fails with a retryable failure (e.g is preempted), it will **not** be attempted again when the workflow is aborting._. ## Restart. When Cromwell restarts (for example to upgrade to a new version) it will reconnect to all workflows that were in progress. On the Google and HPC backends only, Cromwell will additionally attempt to reconnect to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initi",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:6122,Safety,avoid,avoid,6122," to all running jobs. Note that a workflow; does not ""belong"" to any one Cromwell instance (it belongs to the cluster), so a different instance in a horizontal cluster might reconnect to the workflow instead of the original. If the workflow was in state `Aborting`, Cromwell will ask all running jobs to abort again. No new jobs will be started. Once all jobs have been reconnected to, the workflow will keep running normally. During the reconnection process Cromwell might ask backends to reconnect to jobs that were never started before the restart. In that case, the job will be mark as failed with an explanation message. This failure is benign and only an artifact of the fact that Cromwell was restarted. ; If the backend does not support reconnection to an existing job, jobs will be marked as failed with an explanation message as well. The backend status of the jobs will be ""Unknown"". ## Graceful Shutdown. When Cromwell is run as a server, it will by default attempt to gracefully shutdown, stopping its different services in a specific order to avoid losing critical data.; This behavior, documented below, can be turned off in the configuration via `system.graceful-server-shutdown = false`. Upon receiving a `SIGINT` or `SIGTERM` signal, the JVM will initiate its shutdown process. Prior to this Cromwell will attempt to shutdown its own services in the following way:. 1. Workflows in `Submitted` state are no longer started; 2. Cromwell unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connection",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:7530,Safety,risk,risk,7530,"unbinds from the address/port it was listening on. From this point the Cromwell server is unreachable via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceR",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:7666,Safety,timeout,timeout,7666," via the endpoints.; 3. All actors generating data that needs to be persisted receive a message asking them to gracefully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:7724,Safety,timeout,timeout,7724,"fully stop.; This means that they are given some time (see below for how much and how to change it) to return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:8605,Safety,abort,abort,8605,"(called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-p",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:8632,Safety,timeout,timeout,8632,"(called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-p",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:8737,Safety,abort,aborting,8737,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:8925,Safety,abort,abort,8925,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9024,Safety,abort,abort-all-workflows,9024,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9044,Safety,timeout,timeout,9044,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9087,Safety,timeout,timeouts,9087,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9341,Safety,timeout,timeouts,9341,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9574,Safety,timeout,timeout,9574,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:9644,Safety,timeout,timeout,9644,"as its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort"") and wait for a response. In the case where you want to be able to give more time to abort, as it will likely involve more work, you can edit the value of `coordinated-shutdown.phases.abort-all-workflows.timeout` which defaults to 1 hour.; Phases timeouts default to 5 seconds, except the stop-io-activity phase which defaults to 30 minutes. This is because depending on the Database load at the time of the shutdown, it might take a significant amount of time to flush all pending writes. All of the timeouts are configurable in the `akka.coordinated-shutdown.phases` section ([see the latest `reference.conf`](https://raw.githubusercontent.com/akka/akka/master/akka-actor/src/main/resources/reference.conf)).; To change the default timeout, change the value of `akka.coordinated-shutdown.default-phase-timeout`.; ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:7803,Testability,log,logic,7803,"return to a known ""consistent"" state.; For example, an actor waiting for a response from the database before sending information to the metadata will wait for that response before shutting itself down.; 4. All active connections from the REST endpoints are completed and closed. At this point any client that made a request before the shutdown process started should have received a response.; 5. All actors responsible for data persistence are in turn being asked to gracefully shutdown. ; For example, all queued up metadata writes are executed.; 6. Database connection pools are shutdown.; 7. Actor system shuts down.; 8. JVM exits.; ; This multi-stage process is designed to minimize the risk of data loss during shutdown. However in order to prevent this process from lasting forever, each stage (called phase) has its own timeout.; If the phase does not complete within the given timeout, actors will be forcefully stopped and the next phase will start. This logic is implemented using [Akka Coordinated Shutdown Extension](http://doc.akka.io/docs/akka/current/scala/actors.html#coordinated-shutdown). Currently Cromwell is running [version 2.5.4](https://doc.akka.io/docs/akka/2.5.4/scala/actors.html#coordinated-shutdown).; It comes with a set of pre-defined phases, that can be added on and modified. Those phases can be linked together to form a Graph. Cromwell shutdown graphs looks as such:. ![Scaladoc](CromwellShutdownProcess.png). Pre-defined but unused phases have been omitted (cluster related phases for example that are irrelevant in Cromwell). You'll notice the presence of a `PhaseAbortAllWorkflows` phase. This phase is at the same level as the `PhaseServiceRequestsDone` phase which corresponds to our step #3 above.; The reason for a specific abort phase is so that its timeout can be configured differently than the normal shutdown phase. Indeed, stopping all workflows and aborting them is very similar from an outside perspective. We send a message (resp. ""Stop"" and ""Abort",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md:1095,Usability,simpl,simple,1095,"they are complete. When no new jobs can start and all jobs have finished (Success, Failure, Aborted), the workflow terminates. In an ideal situation, all jobs succeed and the workflow succeeds.; Unfortunately things don't always go as planned. Here is what to expect from Cromwell when things go off the rails. ## Failure Modes. Cromwell supports two failure modes, which specify how Cromwell behaves when a job fails during the execution of a workflow. * `NoNewCalls` **(default)** ; 	* Cromwell does not start any new call as soon as a job fails. Cromwell will still monitor the rest of the jobs until they complete (successfully or not). ; * `ContinueWhilePossible` ; 	* Cromwell attempts to run as many jobs as possible until no more can be started. When all running jobs are complete, the workflow fails. The failure mode can be set in the [Configuration](../Configuring/) or [Workflow Options](../wf_options/Overview#workflow-failure). _For example:_. ![](ABdependency.png). This simple diagram represents 4 jobs:. * Job A and Job B are independent; ; * Job A1 depends on A; ; * Job B1 depends on B. Let's look at the case where A and B are both running, and for some reason B fails (shaded red). **`NoNewCalls`** ; If the failure mode is `NoNewCalls` Cromwell waits for A to complete (shaded green). Regardless of whether A is successful or not, Cromwell then fails the workflow, without starting A1 or B1 (shaded grey). ![](NNC_B_fail.png). **`ContinueWhilePossible`** ; If the failure mode is `ContinueWhilePossible` and A succeeds (green), then Cromwell starts A1 (green) and waits for it to complete. At this point all jobs that can run have completed. B1 (grey) cannot run since B failed (red), therefore Cromwell fails the workflow without starting B1. ![](CWP_B_fail.png). ### Retryable failures. Retryable failures are **not** failures that can trigger a workflow failure. An example of a retryable failure is when a [preemptible VM](../RuntimeAttributes/#preemptible) is preempted. . ",MatchSource.DOCS,docs/execution/ExecutionTwists.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/execution/ExecutionTwists.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:46,Deployability,configurat,configuration,46,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:774,Deployability,configurat,configuration,774,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:1148,Deployability,pipeline,pipelines,1148,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:46,Modifiability,config,configuration,46,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:613,Modifiability,config,configured,613,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:774,Modifiability,config,configuration,774,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:1002,Modifiability,config,config,1002,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:1201,Modifiability,config,config,1201,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:1223,Modifiability,config,config,1223,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:291,Security,authenticat,authentication,291,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md:336,Security,authenticat,authenticate,336,"# Data Repository Service (DRS). The Cromwell configuration for DRS is as follows:. **Filesystem Configuration**. ```hocon; drs {; # A reference to a potentially different auth required to contact DRS Resolution service.; auth = ""application-default""; }; ```. The `auth` field refers to the authentication schema that should be used to authenticate requests to DRS Resolution service. The `drs` section needs to be added to; - `engine.filesystems` block; - [PapiV2](http://cromwell.readthedocs.io/en/develop/backends/Google) backend's `filesystems` block. **Localization Configuration**. DRS localization must be configured with the docker image to use. ```hocon; drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```. **Example**. A sample configuration for DRS filesystem might look like:. ```hocon; engine {; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }. backend {; # ... other global backend config here, probably just setting the default ...; providers {; # ... other providers here ...; Papi {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # ... other config here ...; filesystems {; # ... other filesystems here, probably gcs, and then ...; drs {; auth = ""application-default""; }; }; }; }; }; }. drs {; localization {; docker-image = ""broadinstitute/drs-localizer:latest""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/DataRepositoryService.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/DataRepositoryService.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:394,Availability,avail,available,394,"# Filesystems. Most workflows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = """,MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:1046,Availability,failure,failures,1046,"lows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = ""cromwell.filesystems.htt",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:5412,Availability,echo,echo,5412,"d: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. This time the `read_string` function is in the context of a task run by the backend.; The filesystem configuration used will be the one in the `config` section of the Pipelines API backend. ### Supported Filesystems. - Shared File System (SFS). - Google Cloud Storage (GCS) - [Cromwell Doc](GoogleCloudStorage.md) / [Google Doc](https://cloud.google.com/storage/). - Simple Storage Service (S3) - [Amazon Doc](https://aws.amazon.com/documentation/s3/). - HTTP - support for `http` or `https` URLs for [workflow inputs only](http://cromwell.readthedocs.io/en/develop/filesystems/HTTP). - File Transfer Protocol (FTP) - [Cromwell Doc](FileTransferProtocol.md); ",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:295,Deployability,configurat,configuration,295,"# Filesystems. Most workflows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = """,MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:2238,Deployability,configurat,configuration,2238,"responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = ""cromwell.filesystems.http.HttpPathBuilderFactory""; }; }; ```. It defines the filesystems that can be accessed by Cromwell.; Those filesystems can be referenced by their name (`drs`, `gcs`, `s3`, `http` and `local`) in other parts of the configuration. **Note:**; - **S3 filesystem is experimental.** ; - **DRS filesystem has initial support only. Also, currently it works only with [GCS filesystem](../GoogleCloudStorage) in [PapiV2 backend](http://cromwell.readthedocs.io/en/develop/backends/Google).**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; Stri",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:3458,Deployability,configurat,configuration,3458,".**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not*",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:3680,Deployability,configurat,configuration,3680,"any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4168,Deployability,configurat,configuration,4168,"; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../t",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4361,Deployability,configurat,configuration,4361,"he content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; ech",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4470,Deployability,configurat,configuration,4470,"he content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; ech",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4596,Deployability,configurat,configuration,4596,"need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. Th",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:5760,Deployability,configurat,configuration,5760,"d: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. This time the `read_string` function is in the context of a task run by the backend.; The filesystem configuration used will be the one in the `config` section of the Pipelines API backend. ### Supported Filesystems. - Shared File System (SFS). - Google Cloud Storage (GCS) - [Cromwell Doc](GoogleCloudStorage.md) / [Google Doc](https://cloud.google.com/storage/). - Simple Storage Service (S3) - [Amazon Doc](https://aws.amazon.com/documentation/s3/). - HTTP - support for `http` or `https` URLs for [workflow inputs only](http://cromwell.readthedocs.io/en/develop/filesystems/HTTP). - File Transfer Protocol (FTP) - [Cromwell Doc](FileTransferProtocol.md); ",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:246,Modifiability,config,configurable,246,"# Filesystems. Most workflows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = """,MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:295,Modifiability,config,configuration,295,"# Filesystems. Most workflows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = """,MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:309,Modifiability,inherit,inherited,309,"# Filesystems. Most workflows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = """,MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:510,Modifiability,config,config,510,"# Filesystems. Most workflows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = """,MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:888,Modifiability,config,config,888,"# Filesystems. Most workflows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = """,MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:961,Modifiability,config,config,961,"lows represent their inputs and outputs in the form of files. Those files are stored in filesystems. There exists many filesystems. This section describes which filesystems Cromwell supports. ## Overview. Filesystems are configurable. The `reference.conf`, which is the configuration inherited by any Cromwell instance, contains the following:. ```hocon; # Filesystems available in this Crowmell instance; # They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends; # There is a default built-in local filesytem that can also be referenced as ""local"" as well.; filesystems {; drs {; class = ""cromwell.filesystems.drs.DrsPathBuilderFactory""; # Use to share a unique global object across all instances of the factory; global {; # Class to instantiate and propagate to all factories. Takes a single typesafe config argument; class = ""cromwell.filesystems.drs.DrsFileSystemConfig""; config {; resolver {; url = https://drshub-url-here""; # The number of times to retry failures connecting or HTTP 429 or HTTP 5XX responses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = ""cromwell.filesystems.htt",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:2238,Modifiability,config,configuration,2238,"responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = ""cromwell.filesystems.http.HttpPathBuilderFactory""; }; }; ```. It defines the filesystems that can be accessed by Cromwell.; Those filesystems can be referenced by their name (`drs`, `gcs`, `s3`, `http` and `local`) in other parts of the configuration. **Note:**; - **S3 filesystem is experimental.** ; - **DRS filesystem has initial support only. Also, currently it works only with [GCS filesystem](../GoogleCloudStorage) in [PapiV2 backend](http://cromwell.readthedocs.io/en/develop/backends/Google).**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; Stri",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:2931,Modifiability,config,configured,2931,"athBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = ""cromwell.filesystems.http.HttpPathBuilderFactory""; }; }; ```. It defines the filesystems that can be accessed by Cromwell.; Those filesystems can be referenced by their name (`drs`, `gcs`, `s3`, `http` and `local`) in other parts of the configuration. **Note:**; - **S3 filesystem is experimental.** ; - **DRS filesystem has initial support only. Also, currently it works only with [GCS filesystem](../GoogleCloudStorage) in [PapiV2 backend](http://cromwell.readthedocs.io/en/develop/backends/Google).**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for informa",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:2977,Modifiability,config,configures,2977,"p.HttpPathBuilderFactory""; }; }; ```. It defines the filesystems that can be accessed by Cromwell.; Those filesystems can be referenced by their name (`drs`, `gcs`, `s3`, `http` and `local`) in other parts of the configuration. **Note:**; - **S3 filesystem is experimental.** ; - **DRS filesystem has initial support only. Also, currently it works only with [GCS filesystem](../GoogleCloudStorage) in [PapiV2 backend](http://cromwell.readthedocs.io/en/develop/backends/Google).**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:3458,Modifiability,config,configuration,3458,".**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not*",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:3680,Modifiability,config,configuration,3680,"any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4168,Modifiability,config,configuration,4168,"; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../t",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4213,Modifiability,inherit,inherited,4213,"; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../t",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4361,Modifiability,config,configuration,4361,"he content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; ech",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4470,Modifiability,config,configuration,4470,"he content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; ech",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4596,Modifiability,config,configuration,4596,"need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. Th",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:5039,Modifiability,config,configure,5039,"y_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. This time the `read_string` function is in the context of a task run by the backend.; The filesystem configuration used will be the one in the `config` section of the Pipelines API backend. ### Supported Filesystems. - Shared File System (SFS). - Google Cloud Storage (GCS) - [Cromwell Doc](GoogleCloudStorage.md) / [Google Doc](https://cloud.google.com/storage/). - Simple Storage ",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:5760,Modifiability,config,configuration,5760,"d: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. This time the `read_string` function is in the context of a task run by the backend.; The filesystem configuration used will be the one in the `config` section of the Pipelines API backend. ### Supported Filesystems. - Shared File System (SFS). - Google Cloud Storage (GCS) - [Cromwell Doc](GoogleCloudStorage.md) / [Google Doc](https://cloud.google.com/storage/). - Simple Storage Service (S3) - [Amazon Doc](https://aws.amazon.com/documentation/s3/). - HTTP - support for `http` or `https` URLs for [workflow inputs only](http://cromwell.readthedocs.io/en/develop/filesystems/HTTP). - File Transfer Protocol (FTP) - [Cromwell Doc](FileTransferProtocol.md); ",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:5803,Modifiability,config,config,5803,"d: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. This time the `read_string` function is in the context of a task run by the backend.; The filesystem configuration used will be the one in the `config` section of the Pipelines API backend. ### Supported Filesystems. - Shared File System (SFS). - Google Cloud Storage (GCS) - [Cromwell Doc](GoogleCloudStorage.md) / [Google Doc](https://cloud.google.com/storage/). - Simple Storage Service (S3) - [Amazon Doc](https://aws.amazon.com/documentation/s3/). - HTTP - support for `http` or `https` URLs for [workflow inputs only](http://cromwell.readthedocs.io/en/develop/filesystems/HTTP). - File Transfer Protocol (FTP) - [Cromwell Doc](FileTransferProtocol.md); ",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:2102,Security,access,accessed,2102,"sponses, default 3.; num-retries = 3; # How long to wait between retrying HTTP 429 or HTTP 5XX responses, default 10 seconds.; wait-initial = 30 seconds; # The maximum amount of time to wait between retrying HTTP 429 or HTTP 5XX responses, default 30 seconds.; wait-maximum = 60 seconds; # The amount to multiply the amount of time to wait between retrying HTTP or 429 or HTTP 5XX responses.; # Default 1.25, and will never multiply the wait time more than wait-maximum.; wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = ""cromwell.filesystems.http.HttpPathBuilderFactory""; }; }; ```. It defines the filesystems that can be accessed by Cromwell.; Those filesystems can be referenced by their name (`drs`, `gcs`, `s3`, `http` and `local`) in other parts of the configuration. **Note:**; - **S3 filesystem is experimental.** ; - **DRS filesystem has initial support only. Also, currently it works only with [GCS filesystem](../GoogleCloudStorage) in [PapiV2 backend](http://cromwell.readthedocs.io/en/develop/backends/Google).**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backen",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:2592,Security,access,accessible,2592,"wait-mulitiplier = 1.25; # The randomization factor to use for creating a range around the wait interval.; # A randomization factor of 0.5 results in a random period ranging between 50% below and 50% above the wait; # interval. Default 0.1.; wait-randomization-factor = 0.1; }; }; }; }; gcs {; class = ""cromwell.filesystems.gcs.GcsPathBuilderFactory""; }; s3 {; class = ""cromwell.filesystems.s3.S3PathBuilderFactory""; }; http {; class = ""cromwell.filesystems.http.HttpPathBuilderFactory""; }; }; ```. It defines the filesystems that can be accessed by Cromwell.; Those filesystems can be referenced by their name (`drs`, `gcs`, `s3`, `http` and `local`) in other parts of the configuration. **Note:**; - **S3 filesystem is experimental.** ; - **DRS filesystem has initial support only. Also, currently it works only with [GCS filesystem](../GoogleCloudStorage) in [PapiV2 backend](http://cromwell.readthedocs.io/en/develop/backends/Google).**. Also note that the local filesystem (the one on which Cromwell runs on) is implicitly accessible but can be disabled. ; To do so, add the following to any `filesystems` stanza in which the local filesystem should be disabled: `local.enabled: false`. ### Engine Filesystems. Cromwell is conceptually divided in an engine part and a backend part. One Cromwell instance corresponds to an ""engine"" but can have multiple backends configured.; The `engine.filesystems` section configures filesystems that Cromwell can use when it needs to interact with files outside of the context of a backend. For instance, consider the following WDL:. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""/Users/me/my_file.txt""); output {; String out = s; }; }; ```. This workflow is valid WDL and does not involve any backend, or even a task. However it does involve interacting with a filesystem to retrieve the content of `my_file.txt`; With a default configuration Cromwell will be able to run this workflow because the local filesystem is enabled by defaul",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4824,Testability,assert,assert,4824,"-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: input_file = ""gs://mybucket/my_file.txt"" }; }; ```. Suppose this workflow is submitted to a Cromwell running a Pipelines API backend. This time the `read_string` function is in the context of a task run by the backend.; The filesystem configuration used will be the one in the `config` s",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md:4518,Usability,simpl,simply,4518,"he local filesystem is enabled by default.; If the file is located on a different filesystem (a cloud filesystem for instance), we would need to modify the configuration to tell Cromwell how to interact with this filesystem:. ```hocon; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; ```. (See the [Google section](../backends/Google.md) for information about the `auth` field.). We can now run this workflow. ```wdl; version 1.0. workflow my_workflow {; String s = read_string(""gs://mybucket/my_file.txt""); output {; String out = s; }; }; ```. #### Default ""engine"" Filesystems. If you don't change anything in your own configuration file, the following default is inherited from `reference.conf`:; ```; engine {; filesystems {; local {; enabled: true; }; http {; enabled: true; }; }; }; ```. **Note**: since our configuration files are HOCON, to disable filesystems you *must* add `enabled: false` into your ; overriding configuration file. It is **not** sufficient to simply omit a filesystem from your stanza. . For example: adding this to your configuration file will remove the `http` filesystem and leave `local` for use in the ; engine:; ```; engine {; filesystems {; http {; enabled: false; }; }; }; ```. Whereas this example will leave `http` unchanged and merely re-assert the default enabling of `local`. In other; words, **this will do nothing**:; ```; engine {; filesystems {; local {; enabled: true; }; }; }; ```. ### Backend Filesystems. Similarly to the engine, you can also configure backend filesystems individually. Some backends might require the use of a specific filesystem.; For example, the [Pipelines API](../tutorials/PipelinesApi101.md) backend requires Google Cloud Storage.; Let's take another example:. ```wdl; version 1.0. task my_pipelines_task {; input {; File input_file; }; String content = read_string(input_file); ; command {; echo ~{content}; }; ; runtime {; docker: ""ubuntu""; }; }; workflow my_workflow {; call my_pipelines_task { input: ",MatchSource.DOCS,docs/filesystems/Filesystems.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/Filesystems.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3371,Availability,down,downloading,3371,"ure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Exampl",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:281,Deployability,configurat,configuration,281,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:715,Deployability,configurat,configuration,715,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:854,Deployability,configurat,configuration,854,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:1046,Deployability,configurat,configuration,1046,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2366,Deployability,configurat,configuration,2366,"| | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and F",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2464,Deployability,configurat,configuration,2464,"| | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2542,Deployability,configurat,configuration,2542,"-----------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a fi",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3747,Deployability,configurat,configuration,3747,"r per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; b",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3873,Deployability,configurat,configuration,3873,"on.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_p",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4191,Deployability,configurat,configuration,4191,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4416,Deployability,configurat,configuration,4416,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3887,Integrability,depend,dependent,3887,"on.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_p",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:231,Modifiability,config,configured,231,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:281,Modifiability,config,configuration,281,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:715,Modifiability,config,configuration,715,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:854,Modifiability,config,configuration,854,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:1046,Modifiability,config,configuration,1046,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2366,Modifiability,config,configuration,2366,"| | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and F",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2396,Modifiability,config,configure,2396,"| | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2464,Modifiability,config,configuration,2464,"| | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2542,Modifiability,config,configuration,2542,"-----------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a fi",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2591,Modifiability,config,config,2591,"| | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; c",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3747,Modifiability,config,configuration,3747,"r per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; b",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3873,Modifiability,config,configuration,3873,"on.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_p",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4191,Modifiability,config,configuration,4191,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4416,Modifiability,config,configuration,4416,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4495,Modifiability,config,config,4495,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4803,Modifiability,config,config,4803,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:843,Performance,cache,cache-ttl,843,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2829,Performance,cache,cache,2829,"2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2874,Performance,cache,cache-ttl,2874," | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:1037,Safety,timeout,timeout,1037,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:2997,Safety,timeout,timeout,2997,"--------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3026,Safety,timeout,timeout,3026,"--------------------+ | |; | | | | | | | |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; +---------------------------------------------------------------------------------------------------+; ```. ## Global Configuration. The FTP filesystem supports a global configuration. The goal is to configure values that will apply Cromwell-wide, as opposed to other configuration that could be backend (or engine) specific (see below). Default configuration:. ```hocon; filesystems.ftp.global.config = {; # This value should be sufficiently high to cover for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:3599,Safety,timeout,timeout,3599,"for the duration of the longest expected I/O operation.; # It is a time to live value after which a filesystem (unique per user per server) will be closed and evicted from the cache if unused for the specified duration.; cache-ttl = 1 day; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.fil",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:384,Security,secur,secure,384,"# File Transfer Protocol (FTP). Cromwell supports communication with basic FTP servers (not FTPS, and not SFTP either).; Please read the [#Filesystems](Filesystems.md) section before to get an explanation of how filesystems can be configured in general.; For a full example of FTP configuration see [here](#Example). **Note**: Be aware that FTP is in many cases very inefficient, not secure and generally doesn't scale well. If possible, consider using an alternate file system. ## Overview. Cromwell handles FTP connections as follows:. - Cromwell maintains one FTP FileSystem per FTP server per user; - An FTP FileSystem maintains a pool of connections with a fixed size (see `max-connection-per-server-per-user` configuration below) to that server, for that user; - When an FTP FileSystem hasn't been used in a certain amount of time (see `cache-ttl` configuration below), the associated connections are closed and it is destroyed; - In a given pool, when a connection has been idle for a certain amount of time (see `idle-connection-timeout` configuration below), it is closed. ```; Cromwell; +---------------------------------------------------------------------------------------------------+; | |; | FileSystem 1 FileSystem 2 FileSystem 3 |; | +-----------------------------+ +-----------------------------+ +-----------------------------+ |; | | | | | | | |; | | ftp://user1@server1.com | | ftp://user2@server1.com | | ftp://user1@server2.com | |; | | | | | | | |; | | Connection Pool | | Connection Pool | | Connection Pool | |; | | +-------------------------+ | | +-------------------------+ | | +-------------------------+ | |; | | | | | | | | | | | | | |; | | | - Connection 1 | | | | - Connection 1 | | | | - Connection 1 | | |; | | | | | | | | | | | | | |; | | | - Connection 2 | | | | - Connection 2 | | | | - Connection 2 | | |; | | | | | | | | | | | | | |; | | | - Connection 3 | | | | - Connection 3 | | | | - Connection 3 | | |; | | | | | | | | | | | | | |; | | +-------------------",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4076,Security,password,password,4076,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4088,Security,password,password,4088,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4153,Security,authenticat,authentication,4153,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4286,Security,password,password,4286,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4315,Security,authenticat,authentication,4315,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4695,Security,password,password,4695,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md:4854,Security,password,password,4854,"; ; # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout; # obtain-connection-timeout = 1 hour; ; # Maximum number of connections that will be established per user per ftp server. This is across the entire Cromwell instance.; # Setting this number allows to workaround FTP server restrictions for the number of connections for a single user per IP address.; # Has to be >= 2 to allow copying (Copying and FTP file requires downloading and uploading it); max-connection-per-server-per-user = 30; ; # Time after which a connection will be closed if idle. This is to try to free connections from a filesystem when it's not heavily used.; idle-connection-timeout = 1 hour; ; # FTP connection port to use; connection-port: 21; ; # FTP connection mode; connection-mode = ""passive""; }. ```. Note that this configuration being global, it applies the same way regardless of the FTP server. There is currently no good way to make this configuration dependent on the server. ## Instance Configuration. Configuration that can be applied to a backend or engine filesystem stanza:. ```hocon; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```. A default authentication can be provided in the configuration. It can be overridden using the following workflow options: `ftp-username`, `ftp-password`, `ftp-account`; If authentication is omitted the connection is established as an anonymous user. ## Example. Example of configuration mixing the above two sections:. ```hocon; filesystems.ftp.global.config {; # We only override what changes from the default; max-connection-per-server-per-user = 20; }. # Configure the default auth for the engine; engine.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }. # Configure the default auth for the local backend; backend.providers.Local.config.filesystems.ftp {; auth {; username = ""me""; password = ""my_password""; }; }; ```; ",MatchSource.DOCS,docs/filesystems/FileTransferProtocol.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/FileTransferProtocol.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:175,Deployability,configurat,configuration,175,"# Google Cloud Storage (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:2192,Deployability,configurat,configuration,2192,"e `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of th",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:2937,Deployability,pipeline,pipelines-api-,2937,"ld behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of the extra cost of running workflows access requester pays buckets.; ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:175,Modifiability,config,configuration,175,"# Google Cloud Storage (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:2192,Modifiability,config,configuration,2192,"e `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of th",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:513,Performance,cache,cache,513,"# Google Cloud Storage (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:596,Performance,cache,cached,596,"# Google Cloud Storage (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:914,Performance,cache,cache,914,"# Google Cloud Storage (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:1319,Performance,cache,cached,1319,"g files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default pr",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:870,Security,access,accessible,870,"# Google Cloud Storage (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:1004,Security,authenticat,authentication,1004,"ge (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:1049,Security,authenticat,authenticate,1049,"ge (GCS). ## Overview . Cromwell supports workflows referencing objects stored in [Google Cloud Storage](https://cloud.google.com/storage/).; The Cromwell configuration for GCS is as follow:. ```hocon; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for requests on buckets with requester pays enabled; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:1923,Security,access,access,1923,"on-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.m",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:2263,Security,authenticat,authenticated,2263,"e `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of th",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:2409,Security,access,access,2409,"ld behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of the extra cost of running workflows access requester pays buckets.; ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:2681,Security,access,access,2681,"ld behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of the extra cost of running workflows access requester pays buckets.; ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:3119,Security,access,access,3119,"ld behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of the extra cost of running workflows access requester pays buckets.; ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:3239,Security,access,access,3239,"ld behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of the extra cost of running workflows access requester pays buckets.; ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:3094,Testability,log,logs,3094,"ld behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the credentials used need to have the `serviceusage.services.use` permission on this project. . **Important Note #2**: Pipelines API version 1 does **not** support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will **not** work.; For full requester pays support, use the [Pipelines API v2 Cromwell backend](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#pipelines-api-v2). . **Important Note #3**: Access to requester pays buckets from Cromwell is seamless, this also means that Cromwell will not report in the logs or metadata when it access a bucket with requester pays. It is the user's responsibility to be aware of the extra cost of running workflows access requester pays buckets.; ",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md:1454,Usability,simpl,simply,1454,"led; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; ```. - The `auth` field refers to the authentication schema that should be used to authenticate requests. See [here](../backends/Google.md) for more info.; - The `project` field has to do with the Requester Pays feature (see below).; - The `caching.duplication-strategy` field determines how Cromwell should behave w.r.t output files when call is being cached. The default strategy `copy` is to copy the file to its new call location. As mentioned, `reference` will not copy the file and simply point the results to the existing location.; See the [Call Caching documentation](../cromwell_features/CallCaching.md) for more information. ## Requester Pays. GCS has a feature called Requester Pays (RP). This section describes how Cromwell supports it and the consequences on cost. Please first read the [official documentation](https://cloud.google.com/storage/docs/requester-pays) if you're not already familiar with it. The billing project Cromwell uses to access a bucket with requester pays is determined as follows:. - If a `google_project` was set in the [workflow options](../wf_options/Google.md) when the workflow was submitted, this value is used; - Otherwise, the value of the `project` field in the `gcs` filesystem configuration is used; - Otherwise, if the machine Cromwell runs on is authenticated using gcloud and a default project is set, this value will be used. **Important Note #1**: In order for a project to be billable to access a bucket with requester pays, the cr",MatchSource.DOCS,docs/filesystems/GoogleCloudStorage.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/GoogleCloudStorage.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:420,Deployability,configurat,configuration,420,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:514,Deployability,configurat,configuration,514,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:420,Modifiability,config,configuration,420,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:514,Modifiability,config,configuration,514,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:775,Modifiability,config,configured,775,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:831,Modifiability,config,configured,831,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:924,Modifiability,config,config,924,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:1164,Modifiability,config,config,1164,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md:621,Usability,simpl,simple,621,"# HTTP Inputs. ## Overview. For shared filesystem and Google Pipelines API (PAPI) version 2 backends Cromwell can support workflow inputs specified by `http` and `https` URLs.; Please note this is not true ""filesystem"" support for HTTP URLs;; if inputs to a workflow are specified by HTTP URLs the outputs of steps will nevertheless appear at local or GCS paths and not HTTP; URLs. ### Configuration. Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. If there is a need to turn off this `http` filesystem in the default `Local` backend the following Java property; allows for this: `-Dbackend.providers.Local.config.filesystems.http.enabled=false`. ### Caveats. Using HTTP inputs in Cromwell can produce some unexpected behavior:; - Files specified by HTTP URIs will be renamed locally, so programs that rely on file extensions or other filenaming conventions may not function properly.; - Files located in the same remote HTTP-defined directory will not be colocated locally. This can cause problems if a program is expecting an index file (e.g. `.fai`) to appear in the same directory as the associated data file (e.g. `.fa`) without specifying the index location.; ",MatchSource.DOCS,docs/filesystems/HTTP.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/filesystems/HTTP.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:286,Availability,checkpoint,checkpoints,286,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:368,Availability,checkpoint,checkpoint,368,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:475,Availability,checkpoint,checkpointFile,475,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:540,Availability,checkpoint,checkpoint,540,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:631,Availability,checkpoint,checkpoint,631,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:792,Availability,checkpoint,checkpoint,792,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:912,Availability,checkpoint,checkpoint,912,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1017,Availability,checkpoint,checkpoint,1017,"her. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1313,Availability,checkpoint,checkpoint,1313,"arly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1367,Availability,checkpoint,checkpoint,1367,"ntire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minut",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1602,Availability,checkpoint,checkpoint,1602,"every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. c",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1797,Availability,checkpoint,checkpointFile,1797,"d. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_ch",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1943,Availability,checkpoint,checkpointFile,1943,"example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1998,Availability,checkpoint,checkpoint-aware,1998,"ccrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoi",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2115,Availability,checkpoint,checkpoint,2115,"M and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to C",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2252,Availability,checkpoint,checkpointing,2252,"able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2305,Availability,checkpoint,checkpointFile,2305,"able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2468,Availability,checkpoint,checkpointed,2468,"ue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribut",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2653,Availability,checkpoint,checkpoint,2653,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2672,Availability,recover,recovery,2672,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2892,Availability,echo,echo,2892,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2964,Availability,echo,echo,2964,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:3083,Availability,checkpoint,checkpointFile,3083,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:3162,Availability,checkpoint,checkpoint,3162,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:3180,Availability,checkpoint,checkpointFile,3180,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:3258,Availability,checkpoint,checkpoint,3258,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:3312,Availability,checkpoint,checkpointing,3312,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:3439,Availability,checkpoint,checkpointFile,3439,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:276,Energy Efficiency,schedul,scheduled,276,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:971,Energy Efficiency,charge,charges,971,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1080,Energy Efficiency,charge,charges,1080,"her. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1446,Energy Efficiency,charge,charges,1446,"ntire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minut",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1650,Energy Efficiency,charge,charges,1650,"every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. c",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1158,Integrability,depend,depending,1158,"her. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:93,Performance,optimiz,optimization,93,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1249,Performance,perform,performance,1249,"arly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1874,Performance,cache,cache,1874,"d. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_ch",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1959,Performance,optimiz,optimization,1959,"example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:388,Safety,avoid,avoiding,388,"# The 'Checkpoint File' Optimization. ## Overview. Available in Cromwell 55 and higher. This optimization provides a way to mitigate the problem of long-running tasks getting preempted partway through execution. It allows the user to save intermediates of a task at regularly scheduled checkpoints. After an interruption, the task can be restarted from the last saved checkpoint, thereby avoiding having to re-run the entire computation again. ### Description. Specifying a `checkpointFile` value in a task's `runtime` section designates a checkpoint file which will periodically be; copied to cloud storage every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is che",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:1508,Safety,abort,aborted,1508,"every 10 minutes. This checkpoint file will then be restored automatically on subsequent attempts if the job is interrupted. Once the final output has been successfully generated, the checkpoint file will be deleted. To use this feature effectively, the WDL task must be written intentionally to use the checkpoint file. See example below. . ### Effects on cloud charges. Charges will accrue from storing the checkpoint file during the running of the task, and additional charges may apply to the transfer between the VM and the cloud storage bucket depending on their locations. These costs should be minor, especially balanced against the performance and cost benefits of being able to restore from the checkpoint when a worker VM gets preempted. Since the checkpoint file is deleted after successful completion of the task, no further charges will accrue after completion. However, if the task is aborted or otherwise stopped externally, ie through interruption of Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. c",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md:2672,Safety,recover,recovery,2672,"f Cromwell's operation, the checkpoint file will NOT be deleted and storage charges will continue to accrue indefinitely, or until the file is deleted manually. . ### Effect on Call Caching. The presence or absence of the `checkpointFile` attribute is not considered when determining whether to call cache. . ### Example. The following WDL demonstrates the use of the `checkpointFile` optimization. It has a command that is checkpoint-aware:. * It starts by attempting to restore state from the `my_checkpoint` file (or starts at `1` if the checkpoint is empty); * Then it counts up to 100, printing out the current counter value and a date timestamp at each value. To make the checkpointing work, the `runtime` section specifies `checkpointFile: ""my_checkpoint""`. ```wdl; version 1.0. workflow count_wf {; # Count to 2100 at 1/second => 35 minutes to complete, but; # luckily the state can be checkpointed every 10 minutes in; # case of preemption: ; call count { input: count_to = 2100 }; }. task count {; input {; Int count_to; }. command <<<; # Note: Cromwell will stage the checkpoint file on recovery attempts.; # This task checks the 'my_checkpoint' file for a counter value, or else; # initializes the counter at '1':; FROM_CKPT=$(cat my_checkpoint | tail -n1 | awk '{ print $1 }'); FROM_CKPT=${FROM_CKPT:-1}. echo '--' >> my_checkpoint; for i in $(seq $FROM_CKPT ~{count_to}); do; echo $i $(date) >> my_checkpoint; sleep 1; done; >>>. runtime {; docker: ""ubuntu:latest""; preemptible: 3; # Note: This checkpointFile attribute is what signals to Cromwell to save; # the designated checkpoint file:; checkpointFile: ""my_checkpoint""; }. output {; # Note: This task also uses the checkpoint as its output. This is not; # required for checkpointing to work:; Array[String] out = read_lines(""my_checkpoint""); }; }; ```. ## Backend Support. Cromwell supports the `checkpointFile` attribute on the following backends:. * The Google PAPIv2 (alpha1) backend; * The Google Life Sciences (beta) backend; ",MatchSource.DOCS,docs/optimizations/CheckpointFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/CheckpointFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md:127,Performance,optimiz,optimization,127,"# The 'localization_optional' Optimization. Available in Cromwell version 33 and higher. ## Scope. The 'localization_optional' optimization can be applied to a task's individual input declarations containing files, specifically `File` and `File?` values and any complex types containing them. ; It allows you to save time and money by identifying files which do not need to be localized for the task to succeed. ## Condition. The optimization signals to Cromwell that a task has been written in such a way that:; ; * The task **will work** if Cromwell does localize the specified file inputs; * For example if a file is localized for a local dockerized execution environment. **And**:. * The task will **also** work if Cromwell **does not** localize the same file input; * For example the file remains in a cloud object store and the command is constructed using its URL rather than a local path. ## Effect on File Localization. If the [backend](#backend-support) has been set up to respect `localization_optional`, Cromwell will ; choose not to localize the appropriate file input. ### Effect on Call Caching:. None! . Files marked for optional localization are still treated in exactly the same way as other `File` inputs for call caching. ## Language Support. ### WDL 1.0 (or later). In a WDL 1.0 `task`, this optimization is specified by adding a `localization_optional` field to ; an input's entry in the task's `parameter_meta` section. Here's an example:. ```wdl; task nio_task {; input {; File foo_file; File bar_file; }; ; parameter_meta {; foo_file: {; description: ""a foo file"",; localization_optional: true; }; bar_file: {; description: ""a bar file""; }; }; ; command <<<; # This tool must work for **BOTH** local file paths **AND** object store URL values:; java -jar my_tool_1.jar ~{foo_file}; ; # Because the optimization is not applied to 'bar_file' in parameter_meta, this file **WILL** be localized:; java -jar my_tool_2.jar ~{bar_file}; >>>; }; ```. ## Backend Support. This optimiza",MatchSource.DOCS,docs/optimizations/FileLocalization.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md:430,Performance,optimiz,optimization,430,"# The 'localization_optional' Optimization. Available in Cromwell version 33 and higher. ## Scope. The 'localization_optional' optimization can be applied to a task's individual input declarations containing files, specifically `File` and `File?` values and any complex types containing them. ; It allows you to save time and money by identifying files which do not need to be localized for the task to succeed. ## Condition. The optimization signals to Cromwell that a task has been written in such a way that:; ; * The task **will work** if Cromwell does localize the specified file inputs; * For example if a file is localized for a local dockerized execution environment. **And**:. * The task will **also** work if Cromwell **does not** localize the same file input; * For example the file remains in a cloud object store and the command is constructed using its URL rather than a local path. ## Effect on File Localization. If the [backend](#backend-support) has been set up to respect `localization_optional`, Cromwell will ; choose not to localize the appropriate file input. ### Effect on Call Caching:. None! . Files marked for optional localization are still treated in exactly the same way as other `File` inputs for call caching. ## Language Support. ### WDL 1.0 (or later). In a WDL 1.0 `task`, this optimization is specified by adding a `localization_optional` field to ; an input's entry in the task's `parameter_meta` section. Here's an example:. ```wdl; task nio_task {; input {; File foo_file; File bar_file; }; ; parameter_meta {; foo_file: {; description: ""a foo file"",; localization_optional: true; }; bar_file: {; description: ""a bar file""; }; }; ; command <<<; # This tool must work for **BOTH** local file paths **AND** object store URL values:; java -jar my_tool_1.jar ~{foo_file}; ; # Because the optimization is not applied to 'bar_file' in parameter_meta, this file **WILL** be localized:; java -jar my_tool_2.jar ~{bar_file}; >>>; }; ```. ## Backend Support. This optimiza",MatchSource.DOCS,docs/optimizations/FileLocalization.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md:1313,Performance,optimiz,optimization,1313,"r. ## Scope. The 'localization_optional' optimization can be applied to a task's individual input declarations containing files, specifically `File` and `File?` values and any complex types containing them. ; It allows you to save time and money by identifying files which do not need to be localized for the task to succeed. ## Condition. The optimization signals to Cromwell that a task has been written in such a way that:; ; * The task **will work** if Cromwell does localize the specified file inputs; * For example if a file is localized for a local dockerized execution environment. **And**:. * The task will **also** work if Cromwell **does not** localize the same file input; * For example the file remains in a cloud object store and the command is constructed using its URL rather than a local path. ## Effect on File Localization. If the [backend](#backend-support) has been set up to respect `localization_optional`, Cromwell will ; choose not to localize the appropriate file input. ### Effect on Call Caching:. None! . Files marked for optional localization are still treated in exactly the same way as other `File` inputs for call caching. ## Language Support. ### WDL 1.0 (or later). In a WDL 1.0 `task`, this optimization is specified by adding a `localization_optional` field to ; an input's entry in the task's `parameter_meta` section. Here's an example:. ```wdl; task nio_task {; input {; File foo_file; File bar_file; }; ; parameter_meta {; foo_file: {; description: ""a foo file"",; localization_optional: true; }; bar_file: {; description: ""a bar file""; }; }; ; command <<<; # This tool must work for **BOTH** local file paths **AND** object store URL values:; java -jar my_tool_1.jar ~{foo_file}; ; # Because the optimization is not applied to 'bar_file' in parameter_meta, this file **WILL** be localized:; java -jar my_tool_2.jar ~{bar_file}; >>>; }; ```. ## Backend Support. This optimization is currently only applied to localization in the Pipelines API (GCE) backends.; ",MatchSource.DOCS,docs/optimizations/FileLocalization.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md:1823,Performance,optimiz,optimization,1823,"r. ## Scope. The 'localization_optional' optimization can be applied to a task's individual input declarations containing files, specifically `File` and `File?` values and any complex types containing them. ; It allows you to save time and money by identifying files which do not need to be localized for the task to succeed. ## Condition. The optimization signals to Cromwell that a task has been written in such a way that:; ; * The task **will work** if Cromwell does localize the specified file inputs; * For example if a file is localized for a local dockerized execution environment. **And**:. * The task will **also** work if Cromwell **does not** localize the same file input; * For example the file remains in a cloud object store and the command is constructed using its URL rather than a local path. ## Effect on File Localization. If the [backend](#backend-support) has been set up to respect `localization_optional`, Cromwell will ; choose not to localize the appropriate file input. ### Effect on Call Caching:. None! . Files marked for optional localization are still treated in exactly the same way as other `File` inputs for call caching. ## Language Support. ### WDL 1.0 (or later). In a WDL 1.0 `task`, this optimization is specified by adding a `localization_optional` field to ; an input's entry in the task's `parameter_meta` section. Here's an example:. ```wdl; task nio_task {; input {; File foo_file; File bar_file; }; ; parameter_meta {; foo_file: {; description: ""a foo file"",; localization_optional: true; }; bar_file: {; description: ""a bar file""; }; }; ; command <<<; # This tool must work for **BOTH** local file paths **AND** object store URL values:; java -jar my_tool_1.jar ~{foo_file}; ; # Because the optimization is not applied to 'bar_file' in parameter_meta, this file **WILL** be localized:; java -jar my_tool_2.jar ~{bar_file}; >>>; }; ```. ## Backend Support. This optimization is currently only applied to localization in the Pipelines API (GCE) backends.; ",MatchSource.DOCS,docs/optimizations/FileLocalization.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md:1993,Performance,optimiz,optimization,1993,"r. ## Scope. The 'localization_optional' optimization can be applied to a task's individual input declarations containing files, specifically `File` and `File?` values and any complex types containing them. ; It allows you to save time and money by identifying files which do not need to be localized for the task to succeed. ## Condition. The optimization signals to Cromwell that a task has been written in such a way that:; ; * The task **will work** if Cromwell does localize the specified file inputs; * For example if a file is localized for a local dockerized execution environment. **And**:. * The task will **also** work if Cromwell **does not** localize the same file input; * For example the file remains in a cloud object store and the command is constructed using its URL rather than a local path. ## Effect on File Localization. If the [backend](#backend-support) has been set up to respect `localization_optional`, Cromwell will ; choose not to localize the appropriate file input. ### Effect on Call Caching:. None! . Files marked for optional localization are still treated in exactly the same way as other `File` inputs for call caching. ## Language Support. ### WDL 1.0 (or later). In a WDL 1.0 `task`, this optimization is specified by adding a `localization_optional` field to ; an input's entry in the task's `parameter_meta` section. Here's an example:. ```wdl; task nio_task {; input {; File foo_file; File bar_file; }; ; parameter_meta {; foo_file: {; description: ""a foo file"",; localization_optional: true; }; bar_file: {; description: ""a bar file""; }; }; ; command <<<; # This tool must work for **BOTH** local file paths **AND** object store URL values:; java -jar my_tool_1.jar ~{foo_file}; ; # Because the optimization is not applied to 'bar_file' in parameter_meta, this file **WILL** be localized:; java -jar my_tool_2.jar ~{bar_file}; >>>; }; ```. ## Backend Support. This optimization is currently only applied to localization in the Pipelines API (GCE) backends.; ",MatchSource.DOCS,docs/optimizations/FileLocalization.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/FileLocalization.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md:458,Modifiability,portab,portability,458,"# Optimization. These optimizations are *Cromwell-specific* functionality which can be triggered from within your workflow descriptions. You can think of them as ways of telling Cromwell that the task or workflow has a certain property which allows Cromwell to do something clever. ; . ## Portability Warning. Note that these optimizations are *outside* of the language specifications and so not all workflow engines will respect them.; In order to maintain portability of workflows, write defensively with respect to these optimizations: . * Remember that a Cromwell instance might have your optimization turned off.; * Remember that your workflow might need to run on a version of Cromwell which predates the optimization.; * Remember that to share your WDL most widely, it will need to be able to run on engines other than Cromwell - and those engines won't necessarily respect these optimizations.; ",MatchSource.DOCS,docs/optimizations/optimizations.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md:22,Performance,optimiz,optimizations,22,"# Optimization. These optimizations are *Cromwell-specific* functionality which can be triggered from within your workflow descriptions. You can think of them as ways of telling Cromwell that the task or workflow has a certain property which allows Cromwell to do something clever. ; . ## Portability Warning. Note that these optimizations are *outside* of the language specifications and so not all workflow engines will respect them.; In order to maintain portability of workflows, write defensively with respect to these optimizations: . * Remember that a Cromwell instance might have your optimization turned off.; * Remember that your workflow might need to run on a version of Cromwell which predates the optimization.; * Remember that to share your WDL most widely, it will need to be able to run on engines other than Cromwell - and those engines won't necessarily respect these optimizations.; ",MatchSource.DOCS,docs/optimizations/optimizations.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md:326,Performance,optimiz,optimizations,326,"# Optimization. These optimizations are *Cromwell-specific* functionality which can be triggered from within your workflow descriptions. You can think of them as ways of telling Cromwell that the task or workflow has a certain property which allows Cromwell to do something clever. ; . ## Portability Warning. Note that these optimizations are *outside* of the language specifications and so not all workflow engines will respect them.; In order to maintain portability of workflows, write defensively with respect to these optimizations: . * Remember that a Cromwell instance might have your optimization turned off.; * Remember that your workflow might need to run on a version of Cromwell which predates the optimization.; * Remember that to share your WDL most widely, it will need to be able to run on engines other than Cromwell - and those engines won't necessarily respect these optimizations.; ",MatchSource.DOCS,docs/optimizations/optimizations.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md:524,Performance,optimiz,optimizations,524,"# Optimization. These optimizations are *Cromwell-specific* functionality which can be triggered from within your workflow descriptions. You can think of them as ways of telling Cromwell that the task or workflow has a certain property which allows Cromwell to do something clever. ; . ## Portability Warning. Note that these optimizations are *outside* of the language specifications and so not all workflow engines will respect them.; In order to maintain portability of workflows, write defensively with respect to these optimizations: . * Remember that a Cromwell instance might have your optimization turned off.; * Remember that your workflow might need to run on a version of Cromwell which predates the optimization.; * Remember that to share your WDL most widely, it will need to be able to run on engines other than Cromwell - and those engines won't necessarily respect these optimizations.; ",MatchSource.DOCS,docs/optimizations/optimizations.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md:593,Performance,optimiz,optimization,593,"# Optimization. These optimizations are *Cromwell-specific* functionality which can be triggered from within your workflow descriptions. You can think of them as ways of telling Cromwell that the task or workflow has a certain property which allows Cromwell to do something clever. ; . ## Portability Warning. Note that these optimizations are *outside* of the language specifications and so not all workflow engines will respect them.; In order to maintain portability of workflows, write defensively with respect to these optimizations: . * Remember that a Cromwell instance might have your optimization turned off.; * Remember that your workflow might need to run on a version of Cromwell which predates the optimization.; * Remember that to share your WDL most widely, it will need to be able to run on engines other than Cromwell - and those engines won't necessarily respect these optimizations.; ",MatchSource.DOCS,docs/optimizations/optimizations.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md:711,Performance,optimiz,optimization,711,"# Optimization. These optimizations are *Cromwell-specific* functionality which can be triggered from within your workflow descriptions. You can think of them as ways of telling Cromwell that the task or workflow has a certain property which allows Cromwell to do something clever. ; . ## Portability Warning. Note that these optimizations are *outside* of the language specifications and so not all workflow engines will respect them.; In order to maintain portability of workflows, write defensively with respect to these optimizations: . * Remember that a Cromwell instance might have your optimization turned off.; * Remember that your workflow might need to run on a version of Cromwell which predates the optimization.; * Remember that to share your WDL most widely, it will need to be able to run on engines other than Cromwell - and those engines won't necessarily respect these optimizations.; ",MatchSource.DOCS,docs/optimizations/optimizations.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md:887,Performance,optimiz,optimizations,887,"# Optimization. These optimizations are *Cromwell-specific* functionality which can be triggered from within your workflow descriptions. You can think of them as ways of telling Cromwell that the task or workflow has a certain property which allows Cromwell to do something clever. ; . ## Portability Warning. Note that these optimizations are *outside* of the language specifications and so not all workflow engines will respect them.; In order to maintain portability of workflows, write defensively with respect to these optimizations: . * Remember that a Cromwell instance might have your optimization turned off.; * Remember that your workflow might need to run on a version of Cromwell which predates the optimization.; * Remember that to share your WDL most widely, it will need to be able to run on engines other than Cromwell - and those engines won't necessarily respect these optimizations.; ",MatchSource.DOCS,docs/optimizations/optimizations.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/optimizations.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md:748,Availability,echo,echo,748,"# The 'volatile' Optimization. Available in Cromwell version 49 and higher. ### Effect on Call Caching:. The 'volatile' optimization is applied to tasks in their `meta` section.; Call caching will be disabled for any call to that task during the execution of the workflow. . This is particularly useful if:. * One task can produce stochastic results but you still want to use call caching in the rest of the workflow.; * You want to guarantee that a task is never call cached for any other reason. ## Language Support. ### WDL. In a WDL `task`, this optimization is specified by adding a `volatile` field to ; the task's `meta` section. Here's an example:. ```wdl; version 1.0; ; task make_random_int {; ; meta {; volatile: true; }; ; command <<<; echo $RANDOM; >>>. output {; Int random = read_string(stdout()); }; }; ```. ## Backend Support. The volatile keyword applies equally to all backends.; ",MatchSource.DOCS,docs/optimizations/VolatileTasks.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md:120,Performance,optimiz,optimization,120,"# The 'volatile' Optimization. Available in Cromwell version 49 and higher. ### Effect on Call Caching:. The 'volatile' optimization is applied to tasks in their `meta` section.; Call caching will be disabled for any call to that task during the execution of the workflow. . This is particularly useful if:. * One task can produce stochastic results but you still want to use call caching in the rest of the workflow.; * You want to guarantee that a task is never call cached for any other reason. ## Language Support. ### WDL. In a WDL `task`, this optimization is specified by adding a `volatile` field to ; the task's `meta` section. Here's an example:. ```wdl; version 1.0; ; task make_random_int {; ; meta {; volatile: true; }; ; command <<<; echo $RANDOM; >>>. output {; Int random = read_string(stdout()); }; }; ```. ## Backend Support. The volatile keyword applies equally to all backends.; ",MatchSource.DOCS,docs/optimizations/VolatileTasks.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md:469,Performance,cache,cached,469,"# The 'volatile' Optimization. Available in Cromwell version 49 and higher. ### Effect on Call Caching:. The 'volatile' optimization is applied to tasks in their `meta` section.; Call caching will be disabled for any call to that task during the execution of the workflow. . This is particularly useful if:. * One task can produce stochastic results but you still want to use call caching in the rest of the workflow.; * You want to guarantee that a task is never call cached for any other reason. ## Language Support. ### WDL. In a WDL `task`, this optimization is specified by adding a `volatile` field to ; the task's `meta` section. Here's an example:. ```wdl; version 1.0; ; task make_random_int {; ; meta {; volatile: true; }; ; command <<<; echo $RANDOM; >>>. output {; Int random = read_string(stdout()); }; }; ```. ## Backend Support. The volatile keyword applies equally to all backends.; ",MatchSource.DOCS,docs/optimizations/VolatileTasks.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md
https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md:550,Performance,optimiz,optimization,550,"# The 'volatile' Optimization. Available in Cromwell version 49 and higher. ### Effect on Call Caching:. The 'volatile' optimization is applied to tasks in their `meta` section.; Call caching will be disabled for any call to that task during the execution of the workflow. . This is particularly useful if:. * One task can produce stochastic results but you still want to use call caching in the rest of the workflow.; * You want to guarantee that a task is never call cached for any other reason. ## Language Support. ### WDL. In a WDL `task`, this optimization is specified by adding a `volatile` field to ; the task's `meta` section. Here's an example:. ```wdl; version 1.0; ; task make_random_int {; ; meta {; volatile: true; }; ; command <<<; echo $RANDOM; >>>. output {; Int random = read_string(stdout()); }; }; ```. ## Backend Support. The volatile keyword applies equally to all backends.; ",MatchSource.DOCS,docs/optimizations/VolatileTasks.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/optimizations/VolatileTasks.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:4101,Availability,echo,echo,4101,"n>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the j",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:4357,Availability,echo,echo,4357,"ateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1270,Deployability,install,installing,1270,"ment to run workflows using Cromwell on AWS Batch. ### Let's get started!. To create all the resources for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch qu",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2146,Deployability,deploy,deploying,2146,"amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2321,Deployability,configurat,configuration,2321,"amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2652,Deployability,deploy,deployed,2652,"](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attri",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:3869,Deployability,configurat,configuration,3869,"ng config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:5000,Energy Efficiency,monitor,monitor,5000,"or manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on AWS!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials and guides interesting to tackle next:. * [Server Mode](/tutorials/ServerMode); * [AWS Batch Backend](/backends/AWSBatch); * [Persisting Data Between Restarts](/tutorials/PersistentServer); ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:4439,Integrability,message,message,4439,"cution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on AWS!""; },; ""id"": ""08213b40-bcf5",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:4565,Integrability,message,message,4565,"and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on AWS!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials and guides in",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:5378,Integrability,message,message,5378,"or manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on AWS!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials and guides interesting to tackle next:. * [Server Mode](/tutorials/ServerMode); * [AWS Batch Backend](/backends/AWSBatch); * [Persisting Data Between Restarts](/tutorials/PersistentServer); ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:234,Modifiability,config,configured,234,"## Getting started on AWS with AWS Batch (beta). ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have configured your local environment to run workflows using Cromwell on AWS Batch. ### Let's get started!. To create all the resources for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:623,Modifiability,config,configure,623,"## Getting started on AWS with AWS Batch (beta). ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have configured your local environment to run workflows using Cromwell on AWS Batch. ### Let's get started!. To create all the resources for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:824,Modifiability,config,configuring-the-aws-environment,824,"## Getting started on AWS with AWS Batch (beta). ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have configured your local environment to run workflows using Cromwell on AWS Batch. ### Let's get started!. To create all the resources for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:885,Modifiability,config,configuring-cromwell,885,"## Getting started on AWS with AWS Batch (beta). ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have configured your local environment to run workflows using Cromwell on AWS Batch. ### Let's get started!. To create all the resources for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1374,Modifiability,config,configure,1374,"ces for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell.",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1406,Modifiability,config,configure,1406,"he [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1662,Modifiability,config,configuring-authentication,1662,"d run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where yo",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2321,Modifiability,config,configuration,2321,"amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2382,Modifiability,config,configure,2382,"igure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executio",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2909,Modifiability,config,config,2909,"WS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more informati",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:3307,Modifiability,config,config,3307,"configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:3869,Modifiability,config,configuration,3869,"ng config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2261,Performance,queue,queue,2261,"amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2755,Performance,queue,queue,2755," the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manip",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:2879,Performance,queue,queue-arn,2879,"WS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where your resources are deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more informati",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:3650,Performance,queue,queueArn,3650,"e deployed.; - S3 bucket name where Cromwell will store its execution files.; - The ARN of the AWS Batch queue you want to use for your tasks. You can replace the placeholders (`<your region>`, `<your-s3-bucket-name>` and `<your-queue-arn>`) in the following config:. ##### `aws.conf`. ```hocon; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""<your-region>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are with",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:4985,Performance,queue,queue,4985,"or manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on AWS!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials and guides interesting to tackle next:. * [Server Mode](/tutorials/ServerMode); * [AWS Batch Backend](/backends/AWSBatch); * [Persisting Data Between Restarts](/tutorials/PersistentServer); ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:736,Security,authenticat,authenticating-a-local-cromwell-server-with-aws,736,"## Getting started on AWS with AWS Batch (beta). ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have configured your local environment to run workflows using Cromwell on AWS Batch. ### Let's get started!. To create all the resources for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1497,Security,access,access,1497,"ion/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1542,Security,authenticat,authentication,1542,"ion/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1606,Security,authenticat,authentication,1606,"d run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where yo",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1674,Security,authenticat,authentication,1674,"d run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, the stack will output the S3 bucket name and two AWS Batch queue ARNs (default and high-priority) used in the Cromwell configuration. #### Configuring Cromwell. Now we're going to configure Cromwell to use the AWS resources we just created by updating a `*.conf` file to use the `AWSBackend` at runtime. This requires three pieces of information:. - The [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) where yo",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:5222,Testability,log,logs,5222,"or manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on AWS!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials and guides interesting to tackle next:. * [Server Mode](/tutorials/ServerMode); * [AWS Batch Backend](/backends/AWSBatch); * [Persisting Data Between Restarts](/tutorials/PersistentServer); ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:1214,Usability,guid,guide,1214,"nd of this tutorial you'll have configured your local environment to run workflows using Cromwell on AWS Batch. ### Let's get started!. To create all the resources for running a Cromwell server on AWS using CloudFormation, launch the [Cromwell Full Stack Deployment](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/). Alternatively, this page will walk through the specific steps to configure and run a local Cromwell server using AWS Batch. 1. [Authenticating a local Cromwell server with AWS](#authenticating-a-local-cromwell-server-with-aws); 2. [Configuring the AWS environment](#configuring-the-aws-environment); 3. [Configuring Cromwell](#configuring-cromwell); 4. [Workflow Source Files](#workflow-source-files); 5. [Running Cromwell and AWS](#running-cromwell-and-aws); 6. [Outputs](#outputs). #### Authenticating a local Cromwell server with AWS. The easiest way to allow a local Cromwell server to talk to AWS is to:. 1. Install the AWS CLI through Amazon's [user guide](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).; 2. [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) by calling `aws configure` (provide your `Access Key` and `Secret Access Key` when prompted). Cromwell can access these credentials through the default authentication provider. For more options, see the [Configuring authentication of Cromwell with AWS](/backends/AWSBatch#configuring-authentication) section below. #### Configuring the AWS environment. Next you'll need the following setup in your AWS account:; - The core set of resources (S3 Bucket, IAM Roles, AWS Batch); - Custom Compute Resource (Launch Template or AMI) with Cromwell Additions. Information and instructions to setup an AWS environment to work properly with Cromwell can be found on [AWS for Genomics Workflow](https://docs.opendata.aws/genomics-workflows/core-env/introduction/). By deploying the CloudFormation templates provided by AWS, ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:4074,Usability,simpl,simple,4074,"n>""; }. engine {; filesystems {; s3.auth = ""default""; }; }. backend {; default = ""AWSBatch""; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ""s3://<your-s3-bucket-name>/cromwell-execution"". // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". default-runtime-attributes {; queueArn: ""<your arn here>""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the j",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md:5565,Usability,guid,guides,5565,"or manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```. For more information about this configuration or how to change the behaviour of AWS Batch, visit the [AWS Backend](/backends/AWSBatch) page. #### Workflow Source Files . Lastly, create an example workflow to run. We're going to define a simple workflow that will `echo` a string to the console and return the result to Cromwell. Within AWS Batch (like other cloud providers), we're required to specify a Docker container for every task. ##### `hello.wdl`. ```wdl; task hello {; String addressee = ""Cromwell""; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on AWS!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello; output { hello.message }; }; ```. #### Running Cromwell and AWS . Provided all of the files are within the same directory, we can run our workflow with the following command:. > **Note**: You might have a different Cromwell version number here. ```bash; java -Dconfig.file=aws.conf -jar cromwell-36.jar run hello.wdl; ```. This will:; 1. Start Cromwell in `run` mode,; 2. Prepare `hello.wdl` as a job and submit this to your AWS Batch queue. You can monitor the job within your [AWS Batch dashboard](https://console.aws.amazon.com/batch/home).; 3. Run the job, write execution files back to S3, and report progress back to Cromwell. #### Outputs. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on AWS!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials and guides interesting to tackle next:. * [Server Mode](/tutorials/ServerMode); * [AWS Batch Backend](/backends/AWSBatch); * [Persisting Data Between Restarts](/tutorials/PersistentServer); ",MatchSource.DOCS,docs/tutorials/AwsBatch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/AwsBatch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:724,Availability,down,downloads,724,"## Getting started on Google Cloud with Batch. ## Batch. ### Basic Information. Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. ; Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. ### Setting up Batch. #### Permissions:. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:2075,Availability,echo,echo,2075,"On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:5097,Availability,down,downloading,5097,"adthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will stil",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:2753,Deployability,configurat,configuration,2753," `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:3942,Deployability,configurat,configuration,3942,"loud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite upload",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:4222,Deployability,pipeline,pipeline,4222,"gine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:5332,Deployability,configurat,configuration,5332,"This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should r",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:5399,Deployability,configurat,configuration,5399,"This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should r",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:140,Energy Efficiency,schedul,schedule,140,"## Getting started on Google Cloud with Batch. ## Batch. ### Basic Information. Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. ; Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. ### Setting up Batch. #### Permissions:. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:2167,Integrability,message,message,2167,"I Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; na",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:2294,Integrability,message,message,2294,"Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""applicatio",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:2516,Integrability,protocol,protocols,2516,"et to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow exec",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:6525,Integrability,message,message,6525,"ntral1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:1514,Modifiability,config,config,1514,"oals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:2753,Modifiability,config,configuration,2753," `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:3502,Modifiability,config,config,3502,"ty/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:3942,Modifiability,config,configuration,3942,"loud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite upload",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:5332,Modifiability,config,configuration,5332,"This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should r",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:5399,Modifiability,config,configuration,5399,"This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should r",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:150,Performance,queue,queue,150,"## Getting started on Google Cloud with Batch. ## Batch. ### Basic Information. Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. ; Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. ### Setting up Batch. #### Permissions:. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:5758,Performance,cache,cache,5758," location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode]",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:5841,Performance,cache,cached,5841," location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode]",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:6159,Performance,cache,cache,6159,"ntral1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:4231,Safety,timeout,timeout,4231,"gine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:4283,Safety,timeout,timeout,4283,"gine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:3854,Security,access,access,3854,"ect id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the mi",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:3968,Security,secur,security,3968,"loud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = batch. providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; # Google project; project = ""my-cromwell-workflows"". # Base bucket for workflow executions; root = ""gs://my-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite upload",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:4595,Security,authoriz,authorization,4595,"y-cromwell-workflows-bucket"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Batch Jobs and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that service account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Location to submit jobs to Batch and store job metadata.; location = ""us-central1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; au",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:6115,Security,access,accessible,6115,"ntral1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:1015,Testability,log,login,1015,"## Getting started on Google Cloud with Batch. ## Batch. ### Basic Information. Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. ; Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. ### Setting up Batch. #### Permissions:. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:1356,Testability,log,login,1356,"etting up Batch. #### Permissions:. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {;",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:1423,Testability,log,login,1423,"leting the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over t",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:1471,Testability,log,login,1471,"leting the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Batch API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>.; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:. * Google Compute Engine API; * Cloud Storage; * Google Cloud Batch API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>`. **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar.; This workflow takes a string value as specified in the inputs file and writes it to stdout. ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over t",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md:6369,Testability,log,logs,6369,"ntral1"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""google-billing-project"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/Batch101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Batch101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:218,Deployability,configurat,configuration,218,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:866,Deployability,configurat,configuration,866,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:907,Deployability,configurat,configuration,907,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:982,Deployability,configurat,configuration,982,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1011,Deployability,configurat,configuration,1011,"on Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a ne",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1084,Deployability,configurat,configuration,1084,"vious tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the syste",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1133,Deployability,configurat,configuration,1133,"vious tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the syste",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1194,Deployability,configurat,configuration,1194," configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell shou",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1447,Deployability,configurat,configuration,1447,"large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configura",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1492,Deployability,configurat,configuration,1492,"omwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1554,Deployability,configurat,configuration,1554,"s on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `808",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1736,Deployability,configurat,configuration,1736,"r even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be l",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1916,Deployability,configurat,configuration,1916,"e many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configurati",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1968,Deployability,configurat,configuration,1968,"ify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of e",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2016,Deployability,configurat,configuration,2016,"or more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-co",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2358,Deployability,configurat,configuration,2358,"ed-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2393,Deployability,configurat,configuration,2393," one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendInt",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2449,Deployability,configurat,configuration,2449," one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendInt",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2711,Deployability,update,updated,2711,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2798,Deployability,configurat,configuration,2798,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2850,Deployability,configurat,configuration,2850,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2890,Deployability,configurat,configuration,2890,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2968,Deployability,configurat,configuration,2968,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:218,Modifiability,config,configuration,218,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:866,Modifiability,config,configuration,866,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:907,Modifiability,config,configuration,907,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:982,Modifiability,config,configuration,982,"## Configuration Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1011,Modifiability,config,configuration,1011,"on Files. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a ne",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1084,Modifiability,config,configuration,1084,"vious tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the syste",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1133,Modifiability,config,configuration,1133,"vious tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have set up a configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the syste",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1194,Modifiability,config,configuration,1194," configuration file for Cromwell and used it to modify Cromwell's behavior. ### Let's get started. #### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell shou",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1313,Modifiability,config,config,1313,"### Customizing Cromwell with Configuration Files. When Cromwell runs, it contains a large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1363,Modifiability,config,config-object-notation,1363,"large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configura",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1447,Modifiability,config,configuration,1447,"large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configura",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1492,Modifiability,config,configuration,1492,"omwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1554,Modifiability,config,configuration,1554,"s on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `808",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1736,Modifiability,config,configuration,1736,"r even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be l",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1916,Modifiability,config,configuration,1916,"e many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configurati",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1968,Modifiability,config,configuration,1968,"ify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of e",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2016,Modifiability,config,configuration,2016,"or more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-co",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2102,Modifiability,config,config,2102,"or more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-co",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2358,Modifiability,config,configuration,2358,"ed-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2393,Modifiability,config,configuration,2393," one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendInt",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2449,Modifiability,config,configuration,2449," one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendInt",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2575,Modifiability,config,config,2575,"new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.ex",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2719,Modifiability,config,config,2719,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2798,Modifiability,config,configuration,2798,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2850,Modifiability,config,configuration,2850,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2890,Modifiability,config,configuration,2890,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:2968,Modifiability,config,configuration,2968,"``. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configuration value, you can specify new values in your configuration file. For example, say you want to change the default port that cromwell listens from `8000` to `8080`. In your config file you can set:. ```hocon; # below the include line from before; webservice {; port = 8080; }; ```. When you then run Cromwell updated config file, cromwell will now be listening on 8080 or 8000. #### Finding more configuration properties. In addition to the common configuration properties listed on the [configuration](../Configuring) page, there are also a large number of example configuration stanzas commented in [cromwell.examples.conf][cromwell-examples-conf], and; backend provider examples in [cromwell.example.backends][cromwell-examples-folder]. [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf). ### Next Steps. After completing this tutorial you might find the following pages interesting:. * [Configuring the Local Backend](LocalBackendIntro); * [Server Mode](ServerMode.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md:1353,Performance,optimiz,optimized-config-object-notation,1353,"large number of default options useful for getting started. For example, by default Cromwell doesn't require an external database while running all workflow jobs on your local machine. Soon you may want to start storing the results of your Cromwell runs in an external MySQL database. Or, you may want to run jobs on your organizations compute farm, or even run jobs in the cloud via the Pipelines API. All of these changes to the defaults will be done by setting configuration values. When you have many configuration settings you would like to set, you specify them in a custom configuration file. See the [configuration](../Configuring) page for more specific information on the configuration file, and for links to the example configuration file. #### Configuration file syntax. Cromwell configuration files are written in a syntax called HOCON. See the [HOCON documentation](https://github.com/typesafehub/config/blob/master/HOCON.md#hocon-human-optimized-config-object-notation) for more information on all the ways one can create a valid configuration file. #### Creating your first configuration file. To get started customizing Cromwell via a configuration file, create a new empty text file, say `your.conf`. Then add this include at the top:. ```hocon; include required(classpath(""application"")); ```. The default Cromwell configuration values are set via Cromwell's `application.conf`. To ensure that you always have the defaults from the `application.conf`, you must include it at the top of your new configuration file. #### Running Cromwell with your configuration file. Once you have created a new configuration file, you can pass the path to Cromwell by setting the system property `config.file`:. ```bash; java -Dconfig.file=/path/to/your.conf -jar cromwell-[VERSION].jar server; ```. Cromwell should start up as normal. As you haven't actually overridden any values yet, Cromwell should be running with the same settings. #### Setting a configuration value. To override a configura",MatchSource.DOCS,docs/tutorials/ConfigurationFiles.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ConfigurationFiles.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:2806,Availability,echo,echo,2806,"er has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwell to use these independently, or with job schedulers. ### Specifying Containers in your Workflow. Containers are specified on a per-task level, this can be achieved in WDL by specifying a [`docker`](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#docker) tag in the `runtime` section. For example, the following script should run in the `ubuntu:latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docke",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9006,Availability,avail,available,9006,"ingularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessar",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10577,Availability,echo,echo,10577,"des!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10734,Availability,echo,echo,10734,"his may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DI",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:11998,Availability,echo,echo,11998,"M + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12155,Availability,echo,echo,12155,"fs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCK",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12572,Availability,alive,alive,12572," is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18310,Availability,avail,available,18310," such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18596,Availability,avail,available,18596,"ng tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20481,Availability,alive,alive,20481,"tu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20591,Availability,error,error,20591,"650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (and ensure the same results) in a completely different environment:. - [Getting started with AWS Batch](AwsBatch101.md); - [Getting started on Google Pipe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20677,Availability,error,error,20677,"650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (and ensure the same results) in a completely different environment:. - [Getting started with AWS Batch](AwsBatch101.md); - [Getting started on Google Pipe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1068,Deployability,install,installation,1068,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1367,Deployability,install,installation-,1367,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1403,Deployability,configurat,configuration,1403,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1471,Deployability,configurat,configuration-in-detail,1471,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3160,Deployability,configurat,configuration,3160,"es.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwell to use these independently, or with job schedulers. ### Specifying Containers in your Workflow. Containers are specified on a per-task level, this can be achieved in WDL by specifying a [`docker`](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#docker) tag in the `runtime` section. For example, the following script should run in the `ubuntu:latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3227,Deployability,install,installed,3227,"es.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwell to use these independently, or with job schedulers. ### Specifying Containers in your Workflow. Containers are specified on a per-task level, this can be achieved in WDL by specifying a [`docker`](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#docker) tag in the `runtime` section. For example, the following script should run in the `ubuntu:latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3246,Deployability,install,install,3246,"orial, you'll become familiar with container technologies and how to configure Cromwell to use these independently, or with job schedulers. ### Specifying Containers in your Workflow. Containers are specified on a per-task level, this can be achieved in WDL by specifying a [`docker`](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#docker) tag in the `runtime` section. For example, the following script should run in the `ubuntu:latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular,",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4444,Deployability,install,install,4444,"ide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandb",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4559,Deployability,install,installation,4559,"n alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Si",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4690,Deployability,install,installed,4690,"an allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell co",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4916,Deployability,install,install,4916,"recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5167,Deployability,install,installation,5167,"er. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell t",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5274,Deployability,install,installed,5274,"suring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docke",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5330,Deployability,install,install,5330,"suring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docke",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5398,Deployability,configurat,configuration,5398,"m, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5577,Deployability,install,installed,5577,"o the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5673,Deployability,configurat,configuration,5673,"ded that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bin",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5939,Deployability,configurat,configuration,5939,"ualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; ```. As the `Singularity exec` command does not emit a job-id, we must include the `run-in-background` tag within the the provider section in addition to the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:6074,Deployability,configurat,configurations,6074,"ualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; ```. As the `Singularity exec` command does not emit a job-id, we must include the `run-in-background` tag within the the provider section in addition to the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:7869,Deployability,configurat,configuration,7869,"o the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has the caveat that we require the Singularity container to successfully complete, otherwise the workflow might hang indefinitely. To ensure reproducibility and an isolated environment inside the container, ; `--containall` is an **important** function. By default, Singularity will mount; the user's home directory and import the user's environment as well as some ; other things that make Singularity easier to use in an interactive shell. ; Unfortunately settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is in",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8046,Deployability,configurat,configuration,8046,"he caveat that we require the Singularity container to successfully complete, otherwise the workflow might hang indefinitely. To ensure reproducibility and an isolated environment inside the container, ; `--containall` is an **important** function. By default, Singularity will mount; the user's home directory and import the user's environment as well as some ; other things that make Singularity easier to use in an interactive shell. ; Unfortunately settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module avail",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8577,Deployability,configurat,configuration,8577," means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8890,Deployability,install,installed,8890,"ocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:14023,Deployability,update,updated,14023," directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's conf",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:14456,Deployability,install,installed,14456,"gularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network acc",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:14523,Deployability,install,installation,14523,"ges with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o $",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15012,Deployability,configurat,configuration,15012,"tps://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15053,Deployability,update,update,15053,"tps://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16274,Deployability,configurat,configuration,16274,"sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18288,Deployability,configurat,configuration,18288," such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18427,Deployability,configurat,configuration,18427,"e following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl;",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:19292,Deployability,pipeline,pipeline,19292,"the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20254,Deployability,configurat,configurations,20254,"line stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-backgroun",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1216,Energy Efficiency,schedul,schedulers,1216,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1233,Energy Efficiency,schedul,schedulers,1233,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:2405,Energy Efficiency,schedul,schedulers,2405,"r](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwell to use these independently, or with job schedulers. ### Specifying Containers in your Workflow. Containers are specified on a per-task level, this can be achieved in WDL by specifying a [`docker`](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#docker) tag in the `runtime` section. For example, the following script should run in the `ubuntu:latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:6037,Energy Efficiency,schedul,scheduler,6037,"ualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; ```. As the `Singularity exec` command does not emit a job-id, we must include the `run-in-background` tag within the the provider section in addition to the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8393,Energy Efficiency,schedul,schedulers,8393,"ake Singularity easier to use in an interactive shell. ; Unfortunately settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should on",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8433,Energy Efficiency,schedul,scheduler,8433,"settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR`",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8494,Energy Efficiency,schedul,scheduler,8494,"settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR`",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20761,Energy Efficiency,allocate,allocated,20761,"has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (and ensure the same results) in a completely different environment:. - [Getting started with AWS Batch](AwsBatch101.md); - [Getting started on Google Pipelines API](PipelinesApi101.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.e",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8509,Integrability,wrap,wrapped,8509,"settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR`",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10822,Integrability,wrap,wrap,10822,"o be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DI",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:11492,Integrability,wrap,wrap,11492,"mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12430,Integrability,wrap,wrap,12430,"d} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```.",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13257,Integrability,wrap,wrap,13257,"${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implem",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:14267,Integrability,interface,interface,14267,"cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15282,Integrability,wrap,wrap,15282,"r`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), conta",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15689,Integrability,wrap,wrap,15689,"hub.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:335,Modifiability,portab,portability,335,"## Containers. Containers are encapsulated environments that include an operating system, libraries, and software. For example, if you have a host machine running Centos, you can run an isolated container with Ubuntu 18.04. At a high level, it's useful to think of a container as a program or binary.; ; To promote reproducibility and portability, it's considered best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ##",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:678,Modifiability,config,configured,678,"## Containers. Containers are encapsulated environments that include an operating system, libraries, and software. For example, if you have a host machine running Centos, you can run an isolated container with Ubuntu 18.04. At a high level, it's useful to think of a container as a program or binary.; ; To promote reproducibility and portability, it's considered best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ##",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1125,Modifiability,config,configuring-cromwell-for-singularity,1125,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1403,Modifiability,config,configuration,1403,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1471,Modifiability,config,configuration-in-detail,1471,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1670,Modifiability,config,config-block,1670,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:2346,Modifiability,config,configure,2346,"r](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwell to use these independently, or with job schedulers. ### Specifying Containers in your Workflow. Containers are specified on a per-task level, this can be achieved in WDL by specifying a [`docker`](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#docker) tag in the `runtime` section. For example, the following script should run in the `ubuntu:latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3160,Modifiability,config,configuration,3160,"es.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwell to use these independently, or with job schedulers. ### Specifying Containers in your Workflow. Containers are specified on a per-task level, this can be achieved in WDL by specifying a [`docker`](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#docker) tag in the `runtime` section. For example, the following script should run in the `ubuntu:latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4388,Modifiability,config,configure,4388,"ide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandb",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5398,Modifiability,config,configuration,5398,"m, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5424,Modifiability,sandbox,sandbox,5424,"m, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5615,Modifiability,config,config,5615,"o the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5673,Modifiability,config,configuration,5673,"ded that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bin",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5939,Modifiability,config,configuration,5939,"ualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; ```. As the `Singularity exec` command does not emit a job-id, we must include the `run-in-background` tag within the the provider section in addition to the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:6074,Modifiability,config,configurations,6074,"ualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; ```. As the `Singularity exec` command does not emit a job-id, we must include the `run-in-background` tag within the the provider section in addition to the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:6147,Modifiability,config,configure,6147,"ingularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, for that reason we'll bind in the current working directory as `${docker_cwd}`, and we'll use the container-specific script path `${docker_script}`. . An example submit script for Singularity is:; ```bash; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; ```. As the `Singularity exec` command does not emit a job-id, we must include the `run-in-background` tag within the the provider section in addition to the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has the caveat that we require the Singularity container to successfully complete, otherwise the workflow might hang indefinitely. To ensure reproducibility and an isolated environ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:7869,Modifiability,config,configuration,7869,"o the docker-submit script. As Cromwell watches for the existence of the `rc` file, the `run-in-background` option has the caveat that we require the Singularity container to successfully complete, otherwise the workflow might hang indefinitely. To ensure reproducibility and an isolated environment inside the container, ; `--containall` is an **important** function. By default, Singularity will mount; the user's home directory and import the user's environment as well as some ; other things that make Singularity easier to use in an interactive shell. ; Unfortunately settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is in",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8046,Modifiability,config,configuration,8046,"he caveat that we require the Singularity container to successfully complete, otherwise the workflow might hang indefinitely. To ensure reproducibility and an isolated environment inside the container, ; `--containall` is an **important** function. By default, Singularity will mount; the user's home directory and import the user's environment as well as some ; other things that make Singularity easier to use in an interactive shell. ; Unfortunately settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module avail",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8105,Modifiability,config,config,8105,"low might hang indefinitely. To ensure reproducibility and an isolated environment inside the container, ; `--containall` is an **important** function. By default, Singularity will mount; the user's home directory and import the user's environment as well as some ; other things that make Singularity easier to use in an interactive shell. ; Unfortunately settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singulari",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8149,Modifiability,config,config,8149,"fault, Singularity will mount; the user's home directory and import the user's environment as well as some ; other things that make Singularity easier to use in an interactive shell. ; Unfortunately settings in the home directory and the user's environment may ; affect the outcome of the tools that are used. This means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the co",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8577,Modifiability,config,configuration,8577," means different users may; get different results. Therefore, to ensure reproducibility while using ; Singularity, the `--containall` flag should be used. This will make sure the ; environment is cleaned and the HOME directory is not mounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9058,Modifiability,variab,variable,9058," ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** f",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9131,Modifiability,config,config,9131," ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** f",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9480,Modifiability,variab,variable,9480,"e the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for de",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10162,Modifiability,variab,variable,10162,"; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBac",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10993,Modifiability,config,config,10993,"ecessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for de",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:11122,Modifiability,config,config,11122,"# Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --cont",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:11167,Modifiability,config,config,11167,"sers home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:11583,Modifiability,variab,variable,11583,"ging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12813,Modifiability,config,config,12813,"i; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; expo",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12849,Modifiability,sandbox,sandbox,12849,"i; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; expo",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12997,Modifiability,sandbox,sandbox,12997,"up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13148,Modifiability,sandbox,sandbox,13148,"all docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool de",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13683,Modifiability,variab,variable,13683,"id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:14932,Modifiability,config,configure,14932,"tps://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15012,Modifiability,config,configuration,15012,"tps://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16167,Modifiability,config,config,16167," run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images usi",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16274,Modifiability,config,configuration,16274,"sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16370,Modifiability,variab,variable,16370,"ocker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16689,Modifiability,variab,variables,16689,"v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, wh",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:17465,Modifiability,config,config,17465,"etail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [exam",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:17877,Modifiability,config,config,17877,"a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:17921,Modifiability,config,config,17921,"r image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from t",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18288,Modifiability,config,configuration,18288," such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18340,Modifiability,config,config,18340," such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18427,Modifiability,config,configuration,18427,"e following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl;",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20254,Modifiability,config,configurations,20254,"line stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-backgroun",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:1318,Performance,cache,cache,1318,"ed best practice to define containers for a WDL task to run in - this ensures that running the same task on a different system will run the exact same software. . Docker images are the most common container format, but it is not advisable for certain systems to run Docker itself, and for this reason Cromwell can be configured to support a number of alternatives. * [Prerequisites](#prerequisites); * [Goals](#goals); * [Specifying Containers in your Workflow](#specifying-containers-in-your-workflow); * [Docker](#docker); * [Docker on a Local Backend](#docker-on-a-local-backend); * [Docker on Cloud](#docker-on-cloud); * [Docker on HPC](#docker-on-hpc); * [Singularity](#singularity); * [Installation](#installation); * [Configuring Cromwell for Singularity](#configuring-cromwell-for-singularity); * [Local environments](#local-environments); * [Job schedulers](#job-schedulers); * [Without Setuid](#without-setuid); * [Singularity Cache](#singularity-cache); * [udocker](#udocker); * [Installation](#installation-1); * [Configuration](#configuration); * [Caching](#caching); * [Configuration in Detail](#configuration-in-detail); * [Enforcing container requirements](#enforcing-container-requirements); * [Docker Digests](#docker-digests); * [Docker Root](#docker-root); * [Docker Config Block](#docker-config-block); * [Best Practices](#best-practices); * [Image Versions](#image-versions); * [Notes](#notes); * [How does Cromwell know when a job or container has completed?](#how-does-cromwell-know-when-a-job-or-container-has-completed); * [Cromwell: Run-in-background](#cromwell-run-in-background); * [Next Steps](#next-steps). ### Prerequisites; This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md); * [Configuration Files](ConfigurationFiles.md); * Recommended: [Getting started on HPC clusters](HPCIntro.md). ### Goals. At the end of this tutorial, you'll become familiar with container technologies and how to configure Cromwe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8853,Performance,load,loaded,8853,"ounted. Putting this together, we have an example base configuration for a local environment:; ```hocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image befo",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:8935,Performance,load,load,8935,"ocon; include required(classpath(""application"")). backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9358,Performance,cache,cache,9358," }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9582,Performance,cache,cache,9582,"e the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for de",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9673,Performance,cache,cache,9673,"e the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for de",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9719,Performance,cache,cache,9719," a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10298,Performance,cache,cache,10298," be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${run",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10359,Performance,cache,cache,10359," be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${run",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:11719,Performance,cache,cache,11719,"p ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config furthe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:11780,Performance,cache,cache,11780,"p ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config furthe",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13419,Performance,cache,cache,13419,"mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Sin",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13806,Performance,cache,cache,13806," you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up dock",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13986,Performance,cache,cache-folders,13986," directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's conf",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15247,Performance,queue,queue,15247,"r`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), conta",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15805,Performance,cache,caches,15805," have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number o",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16044,Performance,cache,cache,16044," run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images usi",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16132,Performance,cache,cache,16132," run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images usi",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:16336,Performance,cache,cache,16336,"sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18759,Performance,cache,cache,18759," edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell kno",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18797,Performance,cache,cache-entry-ttl,18797," edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell kno",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18876,Performance,cache,cache,18876,"well.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18946,Performance,cache,cache,18946,"g {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18954,Performance,cache,cache-size,18954,"g {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:10656,Safety,timeout,timeout,10656,"his may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; [...]; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. Putting this all together, a complete SLURM + Singularity config might look like this: . ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DI",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12077,Safety,timeout,timeout,12077,"fs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/.singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCK",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20535,Safety,safe,safety,20535,"tu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20893,Safety,abort,abort,20893,"s `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (and ensure the same results) in a completely different environment:. - [Getting started with AWS Batch](AwsBatch101.md); - [Getting started on Google Pipelines API](PipelinesApi101.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:20959,Safety,abort,abort,20959,"s `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (and ensure the same results) in a completely different environment:. - [Getting started with AWS Batch](AwsBatch101.md); - [Getting started on Google Pipelines API](PipelinesApi101.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3754,Security,attack,attack,3754,"latest` container:. ```wdl; task hello_world {; String name = ""World""; command {; echo 'Hello, ${name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3801,Security,secur,security,3801,"{name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.h",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3810,Security,secur,security,3810,"{name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.h",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:3834,Security,attack,attack-surface,3834,"{name}'; }; output {; File out = stdout(); }; runtime {; docker: 'ubuntu:latest'; }; }. workflow hello {; call hello_world; }; ```. ### Docker. [Docker](https://www.docker.com) is a popular container technology that is natively supported by Cromwell and WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.h",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4317,Security,secur,security,4317,"Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4592,Security,access,access,4592,"an allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell co",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4810,Security,secur,security,4810,"ty/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9201,Security,access,access,9201,"""""""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9235,Security,access,access,9235,"""""""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** for ; reproducibility. ```; submit-docker = """"""; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:14845,Security,hash,hash-lookup,14845,"/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Cac",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:15459,Security,access,access,15459,"hub.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}; """"""; ```. With a job queue like SLURM, you just need to wrap this script in an `sbatch` submission like we did with Singularity:. ```; submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; udocker pull ${docker}; ; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """"""; ```. #### Caching; udocker caches images in a single directory, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), meaning that caching is done on a per-user basis. ; However, like Singularity, if you want to share a cache with other users in your project,you you can override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:17116,Security,hash,hash,17116," override the location of the udocker cache directory either using:; * A config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), containing a line such as `topdir = ""/path/to/cache""`.; * Using the environment variable `$UDOCKER_DIR`. ___. ### Configuration in Detail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:17410,Security,hash,hash-based,17410,"etail; The behaviour of Cromwell with containers can be modified using a few other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [exam",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:17493,Security,hash,hash-lookup,17493,"other options. #### Enforcing container requirements; You can enforce the use of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend prov",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:17544,Security,hash,hash-lookup,17544,"se of a container by not including the `submit` block in the provider section. However note that some interpolated variables (`${stdout}`, `${stderr}`) are different between these two blocks. #### Docker Digests. Each Docker repository has a number of tags that can be used to refer to the latest image of a particular type. ; For instance, when you run a normal Docker image with `docker run image`, it will actually run `image:latest`, the `latest` tag of that image. However, by default Cromwell requests and runs images using their `sha` hash, rather than using tags.; This strategy is actually preferable, because it ensures every execution of the task or workflow will use the exact same version of the image, but some engines such as `udocker` don't support this feature. If you are using `udocker` or want to disable the use of hash-based image references, you can set the following config option:; ```; docker.hash-lookup.enabled = false; ```. Nb: By disabling hash-lookup, call caching will not work for any container using a floating tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Se",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18556,Security,hash,hash-lookup,18556,"ng tag. #### Docker Root; If you want to change the root directory inside your containers, where the task places input and output files, you can edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18752,Security,hash,hashes,18752," edit the following option:. ```; backend {; providers {; LocalExample {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell kno",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:18992,Security,hash,hashes,18992,"container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution direct",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:19073,Security,hash,hashes,19073,"or the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the contain",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:19141,Security,hash,hashes,19141,"or the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions""; }; }; }; }; ```. #### Docker Config Block; Further docker configuration options available to be put into your config file are as follows. ; For the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the contain",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:19349,Security,hash,hash,19349,"the latest list of parameters, refer to the [example configuration file][cromwell-examples-conf],; and [specific backend provider examples][cromwell-examples-folder]. ```; docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; }; }; ```. ### Best Practices. #### Image Versions. When choosing the image version for your pipeline stages, it is highly recommended that you use a hash rather than a tag, for the sake of reproducibility; For example, in WDL, you could do this:; ```wdl; runtime {; docker: 'ubuntu:latest'; }; ```. But what you should do is this:; ```wdl; runtime {; docker: 'ubuntu@sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210'; }; ```. You can find the `sha256` of an image using `docker images --digests`; ; ; ### Notes. #### How does Cromwell know when a job or container has completed?; Cromwell uses the presence of the `rc` (returncode) file to determine whether a task has succeeded or failed. This `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5424,Testability,sandbox,sandbox,5424,"m, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell to use a different ; `submit-docker` script that would start Singularity instead of docker. ; Singularity requires docker images to be prefixed with the prefix `docker://`. Using containers isolates the filesystem that the script is allowed to interact with, ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12849,Testability,sandbox,sandbox,12849,"i; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR ; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for ; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; expo",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:12997,Testability,sandbox,sandbox,12997,"up in `stdout.submit`.; flock --verbose --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13148,Testability,sandbox,sandbox,13148,"all docker://${docker} \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. #### Without Setuid; In addition, if you or your sysadmins were not able to give `setuid` permissions to `singularity`, you'll have to modify the config further to ensure the use of sandbox images:. ```; submit-docker = """"""; [...]. # Build the Docker image into a singularity image; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool de",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:21458,Testability,test,test,21458,"s `rc` file is generated as part of the `script` within the execution directory, where the script is assembled at runtime. This is important as if the script executes successfully but the container doesn't terminate, Cromwell will continue the execution of the workflow and the container will persist hogging system resources. Within the configurations above:; - `singularity`: The exec mode does not run a container on the background. #### Cromwell: Run-in-background. By enabling Cromwell's run-in-background mode, you remove the necessity for the `kill`, `check-alive` and `job-id-regex` blocks, which disables some safety checks when running workflows:. - If there is an error starting the container or executing the script, Cromwell may not recognise this error and hang. For example, this may occur if the container attempts to exceed its allocated resources (runs out of memory); the container daemon may terminate the container without completing the script.; - If you abort the workflow (by attempting to close Cromwell or issuing an abort command), Cromwell does not have a reference to the container execution and will not be able to terminate the container. This is only necessary in local environments where there is no job manager to control this, however if your container technology can emit an identifier to stdout, then you are able to remove the run-in-background flag. . ### Next Steps. Congratulations for improving the reproducibility of your workflows! You might find the following cloud-based tutorials interesting to test your workflows (and ensure the same results) in a completely different environment:. - [Getting started with AWS Batch](AwsBatch101.md); - [Getting started on Google Pipelines API](PipelinesApi101.md). [cromwell-examples-conf]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4085,Usability,simpl,simplicity,4085,"WDL. #### Docker on a Local Backend. On a single machine (laptop or server), no extra configuration is needed to allow docker to run, provided Docker is installed. You can install Docker for Linux, Mac or Windows from [Docker Hub](https://hub.docker.com/search/?type=edition&offering=community). #### Docker on Cloud. It is strongly advised that you provide a Docker image to tasks that will run on Cloud backends, and in fact most Cloud providers require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the ca",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4514,Usability,guid,guides,4514,"ders require it. It might be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . ###",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4531,Usability,guid,guide,4531,"ight be possible to use an alternative container engine, but this is not recommended if Docker is supported. #### Docker on HPC. Docker can allow running users to gain superuser privileges, called the [Docker daemon attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4787,Usability,guid,guides,4787,"com/engine/security/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is r",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:4804,Usability,guid,guide,4804,"ty/security/#docker-daemon-attack-surface). In HPC and multi-user environments, Docker recommends that ""only trusted users should be allowed to control your Docker Daemon"". For this reason, this tutorial will also explore other technologies that support the reproducibility and simplicity of running a workflow that use docker containers; Singularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job ",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5145,Usability,guid,guides,5145,"ngularity and udocker. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to c",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:5161,Usability,guid,guide,5161,"er. ___. ### Singularity. Singularity is a container technology designed for use on HPC systems in particular, while ensuring an appropriate level of security that Docker cannot provide. #### Installation; Before you can configure Cromwell on your HPC system, you will have to install Singularity, which is documented [here](https://www.sylabs.io/guides/3.0/admin-guide/admin_quickstart.html#installation).; In order to gain access to the full set of features in Singularity, it is strongly recommended that Singularity is installed by root, with the `setuid` bit enabled, as is ([documented here](https://www.sylabs.io/guides/2.6/admin-guide/security.html#how-does-singularity-do-it)).; This likely means that you will have to ask your sysadmin to install it for you.; Because `singularity` ideally needs `setuid`, your admins may have some qualms about giving Singularity this privilege.; If that is the case, you might consider forwarding [this letter](https://www.sylabs.io/guides/3.0/user-guide/installation.html#singularity-on-a-shared-resource) to your admins. If you are not able to get Singularity installed with these privileges, you can attempt a user install.; If this is the case, you will have to alter your Cromwell configuration to work in ""sandbox"" mode, which is explained in [this part](#without-setuid) of the documentation. . #### Configuring Cromwell for Singularity. Once Singularity is installed, you'll need to modify the `config` block inside `backend.providers` in your Cromwell configuration. In particular, this block contains a key called `submit-docker`, which will contain a script that is run whenever a job needs to run that uses a Docker image. If the job does not specify a Docker image, the regular `submit` block will be used. As the configuration will require more knowledge about your execution environment, see the local and job scheduler sections below for example configurations. ##### Local environments. On local backends, you have to configure Cromwell t",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:9079,Usability,simpl,simply,9079," ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; runtime-attributes = """"""; String? docker; """"""; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """"""; }; }; }; }; ```. ##### Job schedulers. To run Singularity on a job scheduler, the singularity command needs to be passed to the scheduler as a wrapped command. For example, in SLURM, we can use the normal SLURM configuration as explained in the [SLURM documentation](../backends/SLURM), however we'll add a `submit-docker` block to execute when a task is tagged with a docker container. . When constructing this block, there are a few things to keep in mind:; - Make sure Singularity is loaded (and in PATH). If `module` is installed for ; example you can call `module load Singularity`. If the cluster admin has made; a Singularity module available. Alternatively you can alter the `PATH` ; variable directly or simply use `/path/to/singularity` ; directly in the config.; - We should treat worker nodes as if they do not have stable access to the ; internet or build access, so we will pull the container before the task is ; submit to the cluster.; - It's a good idea to use a Singularity cache so that same images should only; have to be pulled once. Make sure you set the `SINGULARITY_CACHEDIR` ; environment variable to a location on the filesystem that is reachable by the; worker nodes!; - If we are using a cache we need to ensure that submit processes started by; Cromwell do not pull to the same cache at the same time. This may corrupt the; cache. We can prevent this by implementing a filelock with `flock` and ; pulling the image before the job is submitted. The flock and pull command ; needs to be placed *before* the submit command so all pull commands are ; executed on the same node. This is necessary for the filelock to work.; - As mentioned above the `--containall` flag is **important** f",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13941,Usability,guid,guides,13941,"mage; # We don't add the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `u",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:13957,Usability,guid,guide,13957,"the .sif file extension because sandbox images are directories, not files; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME; singularity build --sandbox $IMAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a l",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md:14148,Usability,simpl,simple,14148,"MAGE docker://${docker}. # Now submit the job; # Note the use of --userns here; sbatch \; [...]; --wrap ""singularity exec --userns --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}""; """"""; ```. #### Singularity Cache; By default, Singularity will cache the Docker images you pull in `~/.singularity`, your home directory. However, if you are sharing your Docker images with other users or have limited space in your user directory, you can redirect this caching location by exporting the `SINGULARITY_CACHEDIR` variable in your `.bashrc` or at the start of the `submit-docker` block.; ```; export SINGULARITY_CACHEDIR=/path/to/shared/cache; ```. For further information on the Singularity Cache, refer to the [Singularity 2 caching documentation](https://www.sylabs.io/guides/2.6/user-guide/build_environment.html#cache-folders) (this hasn't yet been updated for Singularity 3). ___. ### udocker. [udocker](https://github.com/indigo-dc/udocker) is a tool designed to ""execute simple docker containers in user space without requiring root privileges"". In essence, udocker provides a command line interface that mimics `docker`, and implements the commands using one of four different container backends:. * PRoot; * Fakechroot; * runC; * Singularity. #### Installation; udocker can be installed without any kind of root permissions. Refer to udocker's installation documentation [here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md) for more information. #### Configuration. (As of [2019-02-18](https://github.com/indigo-dc/udocker/issues/112)) udocker does not support looking up docker container by digests, hence you'll have to make ensure `hash-lookup` is disabled. Refer to [this section](#docker-digests) for more detail. To configure `udocker` to work in a local environment, you must tag the provider's configuration to `run-in-background` and update the `submit-docker` to use udocker:; ```; run-in-background = true; submit-docker = """"""; udocker r",MatchSource.DOCS,docs/tutorials/Containers.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/Containers.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:1074,Availability,down,download,1074,"Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save ",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:1488,Availability,down,download,1488,"he time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:1540,Availability,down,downloaded,1540,"le to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task to run `echo ""hello world""`, and then return the output as a String. If you'd like to learn",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:2262,Availability,echo,echo,2262,"orry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task to run `echo ""hello world""`, and then return the output as a String. If you'd like to learn more about how to author WDL, you can find all the WDL resources you could ever want [here](https://github.com/openwdl/wdl). ### Step 3: Running the workflow. Ok, we have Cromwell, we have a workflow, let's put it all together! . Make sure you're in the cromwell directory with the `.jar` file and the `.wdl` file. Now type in:; ```sh; java -jar cromwell-XY.jar run myWorkflow.wdl; ```. Cromwell will print out a fair old chunk of logging information, which can be configured (once you've completed this tutorial and [Configuration Files](ConfigurationFiles), you might want to investigate the [Logging](../Logging) page). Ultimately, the workflow should succeed and you'll end up with the following ou",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:2503,Availability,echo,echo,2503,"ownloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task to run `echo ""hello world""`, and then return the output as a String. If you'd like to learn more about how to author WDL, you can find all the WDL resources you could ever want [here](https://github.com/openwdl/wdl). ### Step 3: Running the workflow. Ok, we have Cromwell, we have a workflow, let's put it all together! . Make sure you're in the cromwell directory with the `.jar` file and the `.wdl` file. Now type in:; ```sh; java -jar cromwell-XY.jar run myWorkflow.wdl; ```. Cromwell will print out a fair old chunk of logging information, which can be configured (once you've completed this tutorial and [Configuration Files](ConfigurationFiles), you might want to investigate the [Logging](../Logging) page). Ultimately, the workflow should succeed and you'll end up with the following output printed out when Cromwell finishes:; ```json; {; 	""myWorkflow.myTask.out"": ""hello world""; }; ```. Ok, you can stop your timer! You just installed and ran your first workflow in Cromwell, congratulations!. ### Next ",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:245,Deployability,install,installing,245,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:354,Deployability,install,install,354,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:366,Deployability,install,install,366,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:436,Deployability,release,releases,436,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:466,Deployability,install,install,466,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:538,Deployability,update,update,538,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:611,Deployability,install,install,611,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:863,Deployability,release,releases,863,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:976,Deployability,release,releases,976,"# Five minute Introduction to Cromwell. ### Prerequisites:. * A Unix-based operating system (yes, that includes Mac!); * A Java 17 runtime environment ; * You can see what you have by running `$ java -version` on a terminal.; * If not, consider installing via conda or brew [as explained here](../Releases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite edit",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:3430,Deployability,install,installed,3430,"mwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task to run `echo ""hello world""`, and then return the output as a String. If you'd like to learn more about how to author WDL, you can find all the WDL resources you could ever want [here](https://github.com/openwdl/wdl). ### Step 3: Running the workflow. Ok, we have Cromwell, we have a workflow, let's put it all together! . Make sure you're in the cromwell directory with the `.jar` file and the `.wdl` file. Now type in:; ```sh; java -jar cromwell-XY.jar run myWorkflow.wdl; ```. Cromwell will print out a fair old chunk of logging information, which can be configured (once you've completed this tutorial and [Configuration Files](ConfigurationFiles), you might want to investigate the [Logging](../Logging) page). Ultimately, the workflow should succeed and you'll end up with the following output printed out when Cromwell finishes:; ```json; {; 	""myWorkflow.myTask.out"": ""hello world""; }; ```. Ok, you can stop your timer! You just installed and ran your first workflow in Cromwell, congratulations!. ### Next Steps. Pat yourself on the back for completing this tutorial, bravo! Then continue on to one of the follow pages:. * [Server Mode](ServerMode); * [Configuration Files](ConfigurationFiles); ",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:3052,Modifiability,config,configured,3052,"mwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task to run `echo ""hello world""`, and then return the output as a String. If you'd like to learn more about how to author WDL, you can find all the WDL resources you could ever want [here](https://github.com/openwdl/wdl). ### Step 3: Running the workflow. Ok, we have Cromwell, we have a workflow, let's put it all together! . Make sure you're in the cromwell directory with the `.jar` file and the `.wdl` file. Now type in:; ```sh; java -jar cromwell-XY.jar run myWorkflow.wdl; ```. Cromwell will print out a fair old chunk of logging information, which can be configured (once you've completed this tutorial and [Configuration Files](ConfigurationFiles), you might want to investigate the [Logging](../Logging) page). Ultimately, the workflow should succeed and you'll end up with the following output printed out when Cromwell finishes:; ```json; {; 	""myWorkflow.myTask.out"": ""hello world""; }; ```. Ok, you can stop your timer! You just installed and ran your first workflow in Cromwell, congratulations!. ### Next Steps. Pat yourself on the back for completing this tutorial, bravo! Then continue on to one of the follow pages:. * [Server Mode](ServerMode); * [Configuration Files](ConfigurationFiles); ",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:1219,Security,secur,security,1219,"ases.md).; * We recommend [SDKMAN](https://sdkman.io/install) to install the latest 17 build of [Temurin](https://adoptium.net/temurin/releases/?version=17); * `sdk install 17.0.9-tem` as of the time of this writing; * You might need to update the `export JAVA_HOME` in your bash profile to point to your JAVA install location.; * A sense of adventure!. ### Goals. At the end of this five minute introduction you will have:. - Downloaded Cromwell!; - Written your first workflow; - Run it through Cromwell. ### Step 1: Downloading Cromwell. We host our Cromwell releases on GitHub! You can find the latest version on our [Releases](https://github.com/broadinstitute/cromwell/releases/latest) page. * Look for the latest version at the top of the page, and find the link to download the jar. It'll have a name like `cromwell-XY.jar` where `XY` is the version. Download the jar file.; * WARNING! If you're on a Mac, the security settings might try to stop you from running Cromwell! Don't worry, if this happens just go to `System Preferences > Security & Privacy > General` and find the `cromwell` jar listed on the page. Click `Open anyway`. The `cromwell-XY.jar` will now automatically download to your `Downloads` directory.; * Put your downloaded Cromwell somewhere you can find it later, like in a Cromwell directory in your home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		S",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:3018,Testability,log,logging,3018,"mwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task to run `echo ""hello world""`, and then return the output as a String. If you'd like to learn more about how to author WDL, you can find all the WDL resources you could ever want [here](https://github.com/openwdl/wdl). ### Step 3: Running the workflow. Ok, we have Cromwell, we have a workflow, let's put it all together! . Make sure you're in the cromwell directory with the `.jar` file and the `.wdl` file. Now type in:; ```sh; java -jar cromwell-XY.jar run myWorkflow.wdl; ```. Cromwell will print out a fair old chunk of logging information, which can be configured (once you've completed this tutorial and [Configuration Files](ConfigurationFiles), you might want to investigate the [Logging](../Logging) page). Ultimately, the workflow should succeed and you'll end up with the following output printed out when Cromwell finishes:; ```json; {; 	""myWorkflow.myTask.out"": ""hello world""; }; ```. Ok, you can stop your timer! You just installed and ran your first workflow in Cromwell, congratulations!. ### Next Steps. Pat yourself on the back for completing this tutorial, bravo! Then continue on to one of the follow pages:. * [Server Mode](ServerMode); * [Configuration Files](ConfigurationFiles); ",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md:2581,Usability,learn,learn,2581,"r home directory. For example, in a terminal:; ```sh; cd ~; mkdir cromwell; cp ~/Downloads/cromwell-XY.jar cromwell/; cd cromwell/; ```; _(if you're not using a Mac, the final command might be different for you)_. ### Step 2: Writing your first workflow description. This bit is easy, you're just going to copy and paste something from the internet. Open your favorite editor. Paste in the following content and save it as `myWorkflow.wdl` in your new `cromwell` directory:. ```wdl; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; 	call myTask; }. task myTask {; 	command <<<; 		echo ""hello world""; 	>>>; 	output {; 		String out = read_string(stdout()); 	}; }; ```. Don't worry, **you don't need to understand too much about the workflow contents to continue for now**. In brief, it tells Cromwell to run a task to run `echo ""hello world""`, and then return the output as a String. If you'd like to learn more about how to author WDL, you can find all the WDL resources you could ever want [here](https://github.com/openwdl/wdl). ### Step 3: Running the workflow. Ok, we have Cromwell, we have a workflow, let's put it all together! . Make sure you're in the cromwell directory with the `.jar` file and the `.wdl` file. Now type in:; ```sh; java -jar cromwell-XY.jar run myWorkflow.wdl; ```. Cromwell will print out a fair old chunk of logging information, which can be configured (once you've completed this tutorial and [Configuration Files](ConfigurationFiles), you might want to investigate the [Logging](../Logging) page). Ultimately, the workflow should succeed and you'll end up with the following output printed out when Cromwell finishes:; ```json; {; 	""myWorkflow.myTask.out"": ""hello world""; }; ```. Ok, you can stop your timer! You just installed and ran your first workflow in Cromwell, congratulations!. ### Next Steps. Pat yourself on the back for completing this tutorial, bravo! Then continue on to one of the follow pages:. * [Ser",MatchSource.DOCS,docs/tutorials/FiveMinuteIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FiveMinuteIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FrequentErrors.md:239,Availability,error,error,239,"**Frequent Errors**. <!--; ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntroduction). ### Goals. At the end of this tutorial you'll have seen some a few common error cases in Cromwell, and learnt how easy ways to address them. ### Let's get started. ### Next Steps. TBD (you're already looking pretty good...!). -->. _Drop us a line in the [Forum](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) if you have a question._. \*\*\* **UNDER CONSTRUCTION** \*\*\* ; [![Pennywell pig in red wellies - Richard Austin Images](http://www.richardaustinimages.com/wp-content/uploads/2015/04/fluffyAustin_Pigets_Wellies-500x395.jpg)](http://www.richardaustinimages.com/product/pennywell-pigs-under-umbrella-2/); ",MatchSource.DOCS,docs/tutorials/FrequentErrors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FrequentErrors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FrequentErrors.md:268,Usability,learn,learnt,268,"**Frequent Errors**. <!--; ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntroduction). ### Goals. At the end of this tutorial you'll have seen some a few common error cases in Cromwell, and learnt how easy ways to address them. ### Let's get started. ### Next Steps. TBD (you're already looking pretty good...!). -->. _Drop us a line in the [Forum](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) if you have a question._. \*\*\* **UNDER CONSTRUCTION** \*\*\* ; [![Pennywell pig in red wellies - Richard Austin Images](http://www.richardaustinimages.com/wp-content/uploads/2015/04/fluffyAustin_Pigets_Wellies-500x395.jpg)](http://www.richardaustinimages.com/product/pennywell-pigs-under-umbrella-2/); ",MatchSource.DOCS,docs/tutorials/FrequentErrors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/FrequentErrors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/GeneralDebuggingTips.md:205,Usability,learn,learnt,205,**General Debugging Tips**. <!--; ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [FrequentErrors](FrequentErrors). ### Goals. At the end of this tutorial you'll have learnt some general methods to debug problems that you see in your Cromwell running in server mode. ### Let's get started. ### Next Steps. TBD (you're already looking pretty good...!); -->. _Drop us a line in the [Forum](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) if you have a question._. \*\*\* **UNDER CONSTRUCTION** \*\*\* ; [![Pennywell pig in red wellies - Richard Austin Images](http://www.richardaustinimages.com/wp-content/uploads/2015/04/fluffyAustin_Pigets_Wellies-500x395.jpg)](http://www.richardaustinimages.com/product/pennywell-pigs-under-umbrella-2/); ,MatchSource.DOCS,docs/tutorials/GeneralDebuggingTips.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/GeneralDebuggingTips.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:2445,Availability,echo,echo,2445,"i-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """""";",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:3212,Availability,echo,echo,3212," When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:6985,Availability,alive,alive,6985,"\; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; co",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7130,Availability,avail,available,7130,"``. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -w",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7213,Availability,alive,alive,7213,"``. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -w",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7324,Availability,alive,alive,7324," submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${scrip",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:8400,Availability,alive,alive,8400,"ble `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); * If you'd like to configure Cromwell to use a local scratch device, see instructions here [HPCSlurmWithLocalScratch.md](HPCSlurmWithLocalScratch.md); ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:448,Deployability,configurat,configuration,448,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:985,Deployability,configurat,configuration,985,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:3595,Deployability,pipeline,pipeline,3595,"gabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attribu",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:8481,Deployability,configurat,configuration,8481,"ble `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); * If you'd like to configure Cromwell to use a local scratch device, see instructions here [HPCSlurmWithLocalScratch.md](HPCSlurmWithLocalScratch.md); ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:5238,Integrability,depend,depending,5238,"when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, s",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:448,Modifiability,config,configuration,448,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:665,Modifiability,config,config,665,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:709,Modifiability,config,config,709,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:824,Modifiability,config,config,824,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:985,Modifiability,config,configuration,985,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:1111,Modifiability,config,config,1111,"gurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:1272,Modifiability,config,configured,1272," LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. F",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:1576,Modifiability,config,config,1576," = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory unit",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:1753,Modifiability,variab,variables,1753,"`actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.pro",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:2142,Modifiability,config,config,2142,"rent [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a t",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:2770,Modifiability,config,config,2770,"ueue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Str",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:3309,Modifiability,config,config,3309,"r example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:3419,Modifiability,config,config,3419,"o hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; St",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:3760,Modifiability,config,config,3760,"butes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4382,Modifiability,config,config,4382," runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4771,Modifiability,inherit,inherit,4771,"; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attrib",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:5895,Modifiability,config,config,5895,"cker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:6656,Modifiability,config,config,6656,", the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting t",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:6848,Modifiability,variab,variable,6848,"ires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC bac",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:6900,Modifiability,config,config,6900,"ub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.Con",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7228,Modifiability,config,configure,7228," that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7308,Modifiability,config,config,7308," submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${scrip",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7489,Modifiability,config,configure,7489,"-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### N",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7617,Modifiability,config,config,7617,"nd.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7680,Modifiability,config,config,7680,"d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode]",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7911,Modifiability,config,config,7911,"ble `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); * If you'd like to configure Cromwell to use a local scratch device, see instructions here [HPCSlurmWithLocalScratch.md](HPCSlurmWithLocalScratch.md); ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7955,Modifiability,config,config,7955,"ble `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); * If you'd like to configure Cromwell to use a local scratch device, see instructions here [HPCSlurmWithLocalScratch.md](HPCSlurmWithLocalScratch.md); ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:8481,Modifiability,config,configuration,8481,"ble `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); * If you'd like to configure Cromwell to use a local scratch device, see instructions here [HPCSlurmWithLocalScratch.md](HPCSlurmWithLocalScratch.md); ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:8721,Modifiability,config,configure,8721,"ble `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); * If you'd like to configure Cromwell to use a local scratch device, see instructions here [HPCSlurmWithLocalScratch.md](HPCSlurmWithLocalScratch.md); ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:3156,Performance,queue,queue,3156,"viders.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. For example, suppose you would like to allow the WDL to specify an sge queue in a task, like:. ```wdl; task hello {; command { echo hello }; runtime { sqe_queue: ""short"" }; }; ```. You declare your runtime attribute in your config by adding any other custom value to the `runtime-attributes` section:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String? sge_queue; # ...; """"""; }; ```. In this case, we've stated that the `sge_queue` is optional. This allows us to reuse WDLs from other pipeline authors who may not have set an `sge_queue`. Alternatively, you can also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attribut",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4685,Performance,cache,cache-consideration,4685,"also set a default for the declared runtime attributes. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4921,Performance,cache,cached,4921,"idered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:5732,Performance,queue,queue,5732,"docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.provid",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7534,Performance,concurren,concurrent-job-limit,7534,"-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### N",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7627,Performance,concurren,concurrent-job-limit,7627,"nd.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:7965,Performance,concurren,concurrent-job-limit,7965,"ble `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC backend. ```hocon; backend {; default = SGE; ; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 100; ; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; ; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". job-id-regex = ""(\\d+)"". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; }; }; }; }; ```. Running Cromwell with this in our configuration file will now submit jobs to SGE!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); * If you'd like to configure Cromwell to use a local scratch device, see instructions here [HPCSlurmWithLocalScratch.md](HPCSlurmWithLocalScratch.md); ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:877,Safety,abort,abort,877,"## Getting started on HPC clusters. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have set up Cromwell to run against your HPC cluster. We'll use SGE as an example but this applies equally to LSF and others. ### Let's get started!. ####. #### Telling Cromwell the type of backend. Start by defining your new backend configuration under the section `backend`. For now, we'll give your backend the name `SGE`, but you can use any name you would like. ```hocon; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # to be filled in; }; }; }; }; ```. The `actor-factory` above tells cromwell that you will be using the `config` section to tell cromwell how to submit jobs, abort jobs, etc. You'll likely also want to change the default backend to your new backend, by setting this configuration value:. ```hocon; backend.default = SGE; ```. #### Specifying the runtime attributes for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:6260,Safety,abort,abort,6260," will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:6724,Safety,abort,abort,6724,"on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the ab",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:6747,Safety,abort,aborting,6747,"ires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """"""; }; ```. When the job finishes submitting, Cromwell will need to retrieve the job id, so that it can abort the job if necessary. This job should be written to the stdout after submission, where Cromwell will then read the job id. Because the job id may be surrounded by other text, a custom regular expression should capture the actual job id. Because the submit above uses `-terse`, the job id will be the entire contents of the stdout, but should be all digits:. ```hocon; backend.providers.SGE.config {; job-id-regex = ""(\\d+)""; }; ```. #### How Cromwell should abort an HPC job. When aborting an HPC job, Cromwell will run a command confifured under the key `kill`, passing in the WDL variable `job_id`:. ```hocon; backend.providers.SGE.config {; kill = ""qdel ${job_id}""; }; ```. #### How Cromwell checks if an HPC job is alive. Whenever Cromwell restarts it checks to see if a job has completed by searching for return code in a file called `rc`. If this file isn't available, in this case Cromwell runs an extra check to make sure the job is still alive. You can configure the command used for this check via:. ```hocon; backend.providers.SGE.config {; check-alive = ""qstat -j ${job_id}""; }; ```. #### Other backend settings. On some systems, the administrators may limit the number of HPC jobs a user may run at a time. To configure this limit, you can use the value `concurrent-job-limit` to limit the number of jobs. ```hocon; backend.providers.SGE.config {; concurrent-job-limit = 100; }; ```. #### Putting the config section all together. With the above sections, we can combine them all together to create a completly working HPC bac",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:2079,Security,validat,validated,2079,"for your HPC tasks. In the config section for your backend, you can define the different [runtime attributes](../RuntimeAttributes) that your HPC tasks will support. Any runtime attribute configured here will be read from the WDL tasks, and then passed into the command line used to submit jobs to the HPC cluster. All runtime attributes must be defined in a single multi-line block. The syntax of this block is the same as defining the inputs for a WDL task. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. In the example above, we have defined four different WDL variables defined, `cpu`, `memory_gb`, `sge_queue`, and `sge_project`. Below you will find more information on `cpu` and `memory`, and the ability to add custom runtime attributes like the `sge_queue` and `sge_project`. **cpu**. When you declare a runtime attribute with the name `cpu`, it must be an `Int`. This integer will validated to always be `>= 1`. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; # ...; """"""; }; ```. **memory**. When running a workflow, the memory runtime attribute in the task will specify the units of memory. For example, this jobs specifies that it only needs 512 megabytes of memory when running. ```wdl; task hello {; command { echo hello }; runtime { memory: ""512 MB"" }; }; ```. However, it's possible that when submitting jobs to your HPC cluster you want to specify the units in gigabytes. To specify the memory units that the submit command should use, append the units to the memory runtime attribute. For example:. ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; Float? memory_gb; # ...; """"""; }; ```. Now, no matter what unit of memory is used within the task, the value will be converted into gigabytes before it is passed to your submit command. **custom attributes**. You can also declare other runtime attributes that a WDL task may use. ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4779,Security,validat,validation,4779,"; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attrib",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4794,Security,hash,hash-lookup,4794,"; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attrib",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4949,Security,hash,hashes,4949,"idered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attributes requires defining `submit` as one would a WDL task `command`:. ```hocon; backend.providers.SGE.config {; submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} ",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md:4846,Usability,simpl,simple,4846,"; String sge_queue = ""short""; # ...; """"""; }; ```. ##### Call Caching based on runtime attributes. The rules for call caching in HPC backends are:; * `docker`: Will be considered when call caching.; * Memory options: Will *not* be considered when call caching.; * CPU options: Will *not* be considered when call caching.; * Custom Attributes: Will *not* be considered when call caching (by default).; ; Although custom attributes will not be considered when call caching by default, you can override this in a `runtime-attributes-for-caching` section. Eg:; ```hocon; backend.providers.SGE.config {; runtime-attributes = """"""; String sge_queue = ""short""; String singularity_image; # ...; """"""; runtime-attributes-for-caching {; sge_queue: false; singularity_image: true; }; }; ```. * Note: Only *custom* attributes can be altered like this. Memory, CPU and docker will always have their default cache-consideration behavior.; * Note: Unlike memory, cpu and docker attributes which inherit validation and hash-lookup behavior, any custom attributes will be simple primitive comparisons.; * For example, a `docker` attribute will be cached by looking up docker hashes against a docker repository, but a custom `singularity` attribute would be a primitive string match. #### How Cromwell should start an HPC job. When Cromwell runs a task, it will fill in a template for the job using the declared runtime attributes. This specific template will vary depending on the requirements of your HPC cluster. For example, say you normally submit jobs to SGE using:. ```bash; qsub -terse -V -b y -N my_job_name \; -wd /path/to/working_directory \; -o /path/to/stdout.qsub \; -e /path/to/stderr.qsub \; -pe smp 1 -l mem_free=0.5g -q short \; /usr/bin/env bash myScript.bash; ```. For this particular SGE cluster, the above sets the working directory, stdout and stderr paths, the number of cpus to 1, the memory to half a gigabyte, and runs on the short queue. Converting this into a template using our runtime attrib",MatchSource.DOCS,docs/tutorials/HPCIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:693,Availability,down,download,693,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:4134,Availability,down,download,4134,"yncExecutionActor.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp direct",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5684,Availability,alive,alive,5684,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:143,Deployability,update,updated,143,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:190,Deployability,install,install,190,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:325,Deployability,install,installations,325,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:401,Deployability,install,install,401,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:464,Deployability,install,install,464,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:657,Deployability,install,install,657,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:779,Deployability,install,install,779,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:3511,Deployability,patch,patched,3511,"r/tmp.$$$$""; |export TMPDIR=""$$tmpDir/tmp.$$$$; ```. ##### i. Save and exit the file.You can also execute the following commands:. ```hocon; sed -i ""s/^\(import\ java.util.UUID\)/\/\/\1/"" \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala; sed -i '23,27d' \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala; sed -i '23i \ \ \ \ \ \ def tempPath: String = \""/genomics_local\""' \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*\)\""/\ \ \ \ \ \ \ \ |mkdir -p \$\$tmpDir\/tmp\.\$\$\$\$\n\1\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should re",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:3910,Deployability,configurat,configuration,3910,"in/scala/cromwell/backend/RuntimeEnvironment.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*\)\""/\ \ \ \ \ \ \ \ |mkdir -p \$\$tmpDir\/tmp\.\$\$\$\$\n\1\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 47",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:4181,Deployability,configurat,configuration,4181,"export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-d",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:4089,Energy Efficiency,schedul,schedule,4089,"-p \$\$tmpDir\/tmp\.\$\$\$\$\n\1\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5617,Integrability,wrap,wrap,5617,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:242,Modifiability,config,configure,242,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:536,Modifiability,config,config-manager,536,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:1451,Modifiability,config,configure,1451," now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; def tempPath: String = ""/genomics_local""; ```. ##### e. Save and exit the file. ##### f. Open the file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` for editing. ##### g. Go to line number 380. You will see the following content:. ```hocon; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; ```. ##### h. Replace those two line with the following text:. ```hocon; |mkdir -p $$tmpDir/tmp.$$$$; |export _JAVA_O",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:3841,Modifiability,config,configure,3841,"ed -i '23i \ \ \ \ \ \ def tempPath: String = \""/genomics_local\""' \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*\)\""/\ \ \ \ \ \ \ \ |mkdir -p \$\$tmpDir\/tmp\.\$\$\$\$\n\1\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:3910,Modifiability,config,configuration,3910,"in/scala/cromwell/backend/RuntimeEnvironment.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*\)\""/\ \ \ \ \ \ \ \ |mkdir -p \$\$tmpDir\/tmp\.\$\$\$\$\n\1\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 47",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:4181,Modifiability,config,configuration,4181,"export _JAVA_OPTIONS.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; sed 's/\(\s*|export TMPDIR=.*tmpDir\)\""/\1\/tmp\.\$\$\$\$\""/' \; backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala; ```. ### 4. Now we can build the patched Cromwell. ```hocon; sbt clean; sbt assembly; ```. ### 5. When the build was successful, we can move the new jar file into the cromwell directory. ```hocon; cp server/target/scala-2.13/cromwell-52-*-SNAP.jar \; cromwell/cromwell-52-fix.jar; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-d",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:4754,Modifiability,config,config,4754,"; ```. ## Configure the Execution Environment for Cromwell. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:4798,Modifiability,config,config,4798,"l. In this section we will configure the Cromwell execution environment.We will use the default configuration file as a starting point and change it to reflect our needs.; * Configure MariaDB as the database Cromwell should use; * Add the `SLURM backends`, so Cromwell can schedule job using `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5265,Modifiability,config,config,5265,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5309,Modifiability,config,config,5309,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5869,Modifiability,config,config,5869,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5913,Modifiability,config,config,5913,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5444,Performance,queue,queue,5444,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:5573,Performance,queue,queue,5573,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:6048,Performance,queue,queue,6048,"sing `SLURM`. ### 1. First, we download the default Cromwell `reference.conf` configuration file. ```hocon; wget https://raw.githubusercontent.com/broadinstitute/cromwell/52_hotfix/core/\; src/main/resources/reference.conf cromwell/; ```. ### 2. We will now add `SLURM` as the backend Cromwell should use. ##### a. Open the `cromwell/reference.conf` file for editing. ##### b. Goto line 479 which should read:. ```hocon; default = ""Local""; ```. ##### c. Change the `""Local""` in that line to `""SLURM""`. ##### d. Remove the following 5 lines, i.e. lines number 480 through 484. ```hocon; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); ```. ##### e. Now add the following text after line 479, i.e. after the line reading `default =""SLURM""`. Ensure that the lines that show line-breaks in this document are, in fact, single lines in `reference.conf`. ```hocon; providers {; SLURM {; #Modifying temp directory to write to local disk; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""all""; String? docker; """"""; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; SLURM-BWA {; temporary-directory = ""$(/genomics_local/)""; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; root = ""cromwell-slurm-exec""; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 2; Int memory_mb = 1024; String queue = ""bwa""; String? docker; """"""; submit = """"""; ```; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:953,Security,access,access,953,"####. # Installing the Cromwell To Use Local Scratch Device; #### These instructions are a community contribution. ### In the process of being updated as of 2024-02. In this section we will install the Cromwell Workflow Management system and configure it, so it can use the local scratch device on the compute nodes.; (these installations are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; ",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:1312,Security,validat,validated,1312,"ons are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; def tempPath: String = ""/genomics_local""; ```. ##### e. Save and exit the file. ##### f. Open the file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` for editing. ##### g. Go to line number 380. You will see the following content:. ```hocon; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |exp",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:1844,Security,hash,hash,1844,"e set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; def tempPath: String = ""/genomics_local""; ```. ##### e. Save and exit the file. ##### f. Open the file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` for editing. ##### g. Go to line number 380. You will see the following content:. ```hocon; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; ```. ##### h. Replace those two line with the following text:. ```hocon; |mkdir -p $$tmpDir/tmp.$$$$; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir/tmp.$$$$""; |export TMPDIR=""$$tmpDir/tmp.$$$$; ```. ##### i. Save and exit the file.You can also execute the following commands:. ```hocon; sed -i ""s/^\(import\ java.util.UUID\)/\/\/\1/"" \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala; sed -i '23,27d' \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:1913,Security,hash,hash,1913,"ge the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; def tempPath: String = ""/genomics_local""; ```. ##### e. Save and exit the file. ##### f. Open the file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` for editing. ##### g. Go to line number 380. You will see the following content:. ```hocon; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; ```. ##### h. Replace those two line with the following text:. ```hocon; |mkdir -p $$tmpDir/tmp.$$$$; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir/tmp.$$$$""; |export TMPDIR=""$$tmpDir/tmp.$$$$; ```. ##### i. Save and exit the file.You can also execute the following commands:. ```hocon; sed -i ""s/^\(import\ java.util.UUID\)/\/\/\1/"" \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala; sed -i '23,27d' \; backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala; sed -i '23i \ \ \ \ \ \ def tempPath: String = \""/genomics_local\""",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:1301,Testability,test,tested,1301,"ons are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; def tempPath: String = ""/genomics_local""; ```. ##### e. Save and exit the file. ##### f. Open the file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` for editing. ##### g. Go to line number 380. You will see the following content:. ```hocon; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |exp",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md:1291,Usability,guid,guide,1291,"ons are done in a ```centos 8``` enviornment). ### 1. In order to install Cromwell, the `sbt` build tool is required, so we will install this now. ##### a. Add the sbt online repository. ```hocon; dnf config-manager --add-repo https://www.scala-sbt.org/sbt-rpm.repo; ```. ##### b. Install SBT build tool. ```hocon; dnf -y install sbt; ```. ### 2. Now we can download Cromwell from git repository. ##### a. Create a directory into which we will install Cromwell. ```hocon; mkdir cromwell; ```. ##### b. Now, we set the owner of that directory to the cromwell user and group and change the permissions in order to allow access to all users. ```hocon; chmod 775 -R cromwell; chown -R cromwell:cromwell cromwell; ```. ##### c. Login to the cromwell user account. ```hocon; su - cromwell; ```. ##### d. We will now use git to clone the Cromwell git repository. ```hocon; cd cromwell; git clone https://github.com/broadinstitute/cromwell.git; ```. ##### e. This guide was tested and validated with `version 52` of cromwell, so we will checkout that version. ```hocon; cd cromwell; git checkout 52; ```. ### 3. We will now configure Cromwell to use the local NVMe disk as scratch space. ##### a. Open the `backend/src/main/scala/cromwell/backend/RuntimeEnvironment.scala` file for editing. ##### b. Comment out line 3, so it reads:. ```hocon; //import java.util.UUID; ```. ##### c. Remove lines 23 through 27, i.e. the following lines:. ```hocon; val tempPath: String = {; val uuid = UUID.randomUUID().toString; val hash = uuid.substring(0, uuid.indexOf('-')); callRoot.resolve(s""tmp.$hash"").pathAsString; }; ```. ##### d. Add the following text in line 23 now:. ```hocon; def tempPath: String = ""/genomics_local""; ```. ##### e. Save and exit the file. ##### f. Open the file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` for editing. ##### g. Go to line number 380. You will see the following content:. ```hocon; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |exp",MatchSource.DOCS,docs/tutorials/HPCSlurmWithLocalScratch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/HPCSlurmWithLocalScratch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md:244,Deployability,configurat,configuration,244,"**Configuration the Local Backend**. <!--; ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have seen how to add a configuration section for the Local backend to your configuration file, and seen what changing some of the various options does. ### Let's get started!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); -->. _Drop us a line in the [Forum](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) if you have a question._. \*\*\* **UNDER CONSTRUCTION** \*\*\* ; [![Pennywell pig in red wellies - Richard Austin Images](http://www.richardaustinimages.com/wp-content/uploads/2015/04/fluffyAustin_Pigets_Wellies-500x395.jpg)](http://www.richardaustinimages.com/product/pennywell-pigs-under-umbrella-2/); ",MatchSource.DOCS,docs/tutorials/LocalBackendIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md:296,Deployability,configurat,configuration,296,"**Configuration the Local Backend**. <!--; ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have seen how to add a configuration section for the Local backend to your configuration file, and seen what changing some of the various options does. ### Let's get started!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); -->. _Drop us a line in the [Forum](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) if you have a question._. \*\*\* **UNDER CONSTRUCTION** \*\*\* ; [![Pennywell pig in red wellies - Richard Austin Images](http://www.richardaustinimages.com/wp-content/uploads/2015/04/fluffyAustin_Pigets_Wellies-500x395.jpg)](http://www.richardaustinimages.com/product/pennywell-pigs-under-umbrella-2/); ",MatchSource.DOCS,docs/tutorials/LocalBackendIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md:244,Modifiability,config,configuration,244,"**Configuration the Local Backend**. <!--; ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have seen how to add a configuration section for the Local backend to your configuration file, and seen what changing some of the various options does. ### Let's get started!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); -->. _Drop us a line in the [Forum](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) if you have a question._. \*\*\* **UNDER CONSTRUCTION** \*\*\* ; [![Pennywell pig in red wellies - Richard Austin Images](http://www.richardaustinimages.com/wp-content/uploads/2015/04/fluffyAustin_Pigets_Wellies-500x395.jpg)](http://www.richardaustinimages.com/product/pennywell-pigs-under-umbrella-2/); ",MatchSource.DOCS,docs/tutorials/LocalBackendIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md:296,Modifiability,config,configuration,296,"**Configuration the Local Backend**. <!--; ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md). ### Goals. At the end of this tutorial you'll have seen how to add a configuration section for the Local backend to your configuration file, and seen what changing some of the various options does. ### Let's get started!. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); -->. _Drop us a line in the [Forum](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) if you have a question._. \*\*\* **UNDER CONSTRUCTION** \*\*\* ; [![Pennywell pig in red wellies - Richard Austin Images](http://www.richardaustinimages.com/wp-content/uploads/2015/04/fluffyAustin_Pigets_Wellies-500x395.jpg)](http://www.richardaustinimages.com/product/pennywell-pigs-under-umbrella-2/); ",MatchSource.DOCS,docs/tutorials/LocalBackendIntro.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/LocalBackendIntro.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md:211,Deployability,install,installation,211,"## Persisting Data Between Restarts. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md); * [Docker](https://docs.docker.com/engine/installation/). ### Goals. Cromwell remembers everything it knows!. ### Let's get started!. - Start the MySQL docker container with the following line:. ```bash; docker run -p 3306:3306 --name NameOfTheContainer -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql/mysql-server:5.5; ```. - Update your `application.conf` file. ```hocon; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/DatabaseName?rewriteBatchedStatements=true&useSSL=false""; user = ""ChooseAName""; password = ""YourOtherPassword""; connectionTimeout = 5000; }; }; ```; Add the line above, below the all other lines in your `application.conf`. Replace `""DatabaseName""`, `""ChooseAName""` and `""YourOtherPassword""` with the values you choose in step 2, preserving the double quotes. Test it by running your server with the updated `application.conf`:; ```bash; java -Dconfig.file=/path/to/application.conf/ -jar cromwell-[version].jar ...; ```. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Server Mode](ServerMode); ",MatchSource.DOCS,docs/tutorials/PersistentServer.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md:1155,Deployability,update,updated,1155,"## Persisting Data Between Restarts. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md); * [Docker](https://docs.docker.com/engine/installation/). ### Goals. Cromwell remembers everything it knows!. ### Let's get started!. - Start the MySQL docker container with the following line:. ```bash; docker run -p 3306:3306 --name NameOfTheContainer -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql/mysql-server:5.5; ```. - Update your `application.conf` file. ```hocon; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/DatabaseName?rewriteBatchedStatements=true&useSSL=false""; user = ""ChooseAName""; password = ""YourOtherPassword""; connectionTimeout = 5000; }; }; ```; Add the line above, below the all other lines in your `application.conf`. Replace `""DatabaseName""`, `""ChooseAName""` and `""YourOtherPassword""` with the values you choose in step 2, preserving the double quotes. Test it by running your server with the updated `application.conf`:; ```bash; java -Dconfig.file=/path/to/application.conf/ -jar cromwell-[version].jar ...; ```. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Server Mode](ServerMode); ",MatchSource.DOCS,docs/tutorials/PersistentServer.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md:769,Modifiability,rewrite,rewriteBatchedStatements,769,"## Persisting Data Between Restarts. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md); * [Docker](https://docs.docker.com/engine/installation/). ### Goals. Cromwell remembers everything it knows!. ### Let's get started!. - Start the MySQL docker container with the following line:. ```bash; docker run -p 3306:3306 --name NameOfTheContainer -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql/mysql-server:5.5; ```. - Update your `application.conf` file. ```hocon; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/DatabaseName?rewriteBatchedStatements=true&useSSL=false""; user = ""ChooseAName""; password = ""YourOtherPassword""; connectionTimeout = 5000; }; }; ```; Add the line above, below the all other lines in your `application.conf`. Replace `""DatabaseName""`, `""ChooseAName""` and `""YourOtherPassword""` with the values you choose in step 2, preserving the double quotes. Test it by running your server with the updated `application.conf`:; ```bash; java -Dconfig.file=/path/to/application.conf/ -jar cromwell-[version].jar ...; ```. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Server Mode](ServerMode); ",MatchSource.DOCS,docs/tutorials/PersistentServer.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md:836,Security,password,password,836,"## Persisting Data Between Restarts. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Configuration Files](ConfigurationFiles.md); * [Docker](https://docs.docker.com/engine/installation/). ### Goals. Cromwell remembers everything it knows!. ### Let's get started!. - Start the MySQL docker container with the following line:. ```bash; docker run -p 3306:3306 --name NameOfTheContainer -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql/mysql-server:5.5; ```. - Update your `application.conf` file. ```hocon; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/DatabaseName?rewriteBatchedStatements=true&useSSL=false""; user = ""ChooseAName""; password = ""YourOtherPassword""; connectionTimeout = 5000; }; }; ```; Add the line above, below the all other lines in your `application.conf`. Replace `""DatabaseName""`, `""ChooseAName""` and `""YourOtherPassword""` with the values you choose in step 2, preserving the double quotes. Test it by running your server with the updated `application.conf`:; ```bash; java -Dconfig.file=/path/to/application.conf/ -jar cromwell-[version].jar ...; ```. ### Next steps. You might find the following tutorials interesting to tackle next:. * [Server Mode](ServerMode); ",MatchSource.DOCS,docs/tutorials/PersistentServer.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PersistentServer.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:671,Availability,echo,echo,671,"## Getting started on Google Cloud with the Genomics Pipelines API. ## Pipelines API v2. ### Setting up PAPIv2. For now the easiest way to try PAPIv2 is to start with the sample configuration in; [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf); and adjust it to fit your needs. #### Permissions:. Google recommends using a service account to authenticate to GCP. . You may create a service account using the `gcloud` command, consider running the following script and replace MY-GOOGLE-PROJECT:. ```; #!/bin/bash; export LC_ALL=C ; RANDOM_BUCKET_NAME=$(head /dev/urandom | tr -dc a-z | head -c 32 ; echo ''). #Create a new service account called ""my-service-account"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create my-service-account --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'). # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer lifesciences.workflowsRunner lifesciences.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done. # create a bucket to keep the execution directory; gsutil mb gs://""$RANDOM_BUCKET_NAME"". # give the service account write access to the new bucket; gsutil acl ch -u ""$EMAIL"":W gs://""$RANDOM_BUCKET_NAME"". # create a file that represents your service account. KEEP THIS A SECRET.; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https:",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:2024,Availability,down,downloads,2024,"account; for i in storage.objectCreator storage.objectViewer lifesciences.workflowsRunner lifesciences.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done. # create a bucket to keep the execution directory; gsutil mb gs://""$RANDOM_BUCKET_NAME"". # give the service account write access to the new bucket; gsutil acl ch -u ""$EMAIL"":W gs://""$RANDOM_BUCKET_NAME"". # create a file that represents your service account. KEEP THIS A SECRET.; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>. ; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. . On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://googl",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:3392,Availability,echo,echo,3392," project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:6282,Availability,error,error,6282,"docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; 		 ; // Cloud Life Sciences API is limited to certain locations. See https://cloud.google.com/life-sciences/docs/concepts/locations; // and note that changing the location also requires changing the endpoint-url.; location = ""us-central1""	. // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. . ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success! . ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:6337,Availability,failure,failure,6337,"docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; 		 ; // Cloud Life Sciences API is limited to certain locations. See https://cloud.google.com/life-sciences/docs/concepts/locations; // and note that changing the location also requires changing the endpoint-url.; location = ""us-central1""	. // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. . ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success! . ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:178,Deployability,configurat,configuration,178,"## Getting started on Google Cloud with the Genomics Pipelines API. ## Pipelines API v2. ### Setting up PAPIv2. For now the easiest way to try PAPIv2 is to start with the sample configuration in; [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf); and adjust it to fit your needs. #### Permissions:. Google recommends using a service account to authenticate to GCP. . You may create a service account using the `gcloud` command, consider running the following script and replace MY-GOOGLE-PROJECT:. ```; #!/bin/bash; export LC_ALL=C ; RANDOM_BUCKET_NAME=$(head /dev/urandom | tr -dc a-z | head -c 32 ; echo ''). #Create a new service account called ""my-service-account"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create my-service-account --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'). # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer lifesciences.workflowsRunner lifesciences.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done. # create a bucket to keep the execution directory; gsutil mb gs://""$RANDOM_BUCKET_NAME"". # give the service account write access to the new bucket; gsutil acl ch -u ""$EMAIL"":W gs://""$RANDOM_BUCKET_NAME"". # create a file that represents your service account. KEEP THIS A SECRET.; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https:",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:4070,Deployability,configurat,configuration,4070,"util mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; // Google project; project = ""<google-project-id>"". // Base bucket for workflow executions; root = ""gs://<google-bucket-name>/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // Th",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:4778,Deployability,pipeline,pipelines,4778,"g <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; // Google project; project = ""<google-project-id>"". // Base bucket for workflow executions; root = ""gs://<google-bucket-name>/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:3484,Integrability,message,message,3484,"and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; na",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:3611,Integrability,message,message,3611,"enticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""applicatio",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:3833,Integrability,protocol,protocols,3833,"o hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; // Google project; project = ""<google-project-id>"". // Base bucket for w",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:6902,Integrability,message,message,6902,"docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; 		 ; // Cloud Life Sciences API is limited to certain locations. See https://cloud.google.com/life-sciences/docs/concepts/locations; // and note that changing the location also requires changing the endpoint-url.; location = ""us-central1""	. // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. . ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success! . ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:178,Modifiability,config,configuration,178,"## Getting started on Google Cloud with the Genomics Pipelines API. ## Pipelines API v2. ### Setting up PAPIv2. For now the easiest way to try PAPIv2 is to start with the sample configuration in; [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf); and adjust it to fit your needs. #### Permissions:. Google recommends using a service account to authenticate to GCP. . You may create a service account using the `gcloud` command, consider running the following script and replace MY-GOOGLE-PROJECT:. ```; #!/bin/bash; export LC_ALL=C ; RANDOM_BUCKET_NAME=$(head /dev/urandom | tr -dc a-z | head -c 32 ; echo ''). #Create a new service account called ""my-service-account"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create my-service-account --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'). # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer lifesciences.workflowsRunner lifesciences.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done. # create a bucket to keep the execution directory; gsutil mb gs://""$RANDOM_BUCKET_NAME"". # give the service account write access to the new bucket; gsutil acl ch -u ""$EMAIL"":W gs://""$RANDOM_BUCKET_NAME"". # create a file that represents your service account. KEEP THIS A SECRET.; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https:",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:2827,Modifiability,config,config,2827,"of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>. ; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. . On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/ident",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:4070,Modifiability,config,configuration,4070,"util mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy over the sample `google.conf` file utilizing <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; // Google project; project = ""<google-project-id>"". // Base bucket for workflow executions; root = ""gs://<google-bucket-name>/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // Th",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:4831,Modifiability,config,config,4831,"application-default-credentials"" target=""_blank"">Application Default credentials</a> to the same directory that contains your sample WDL, inputs and Cromwell jar.; Replace `<google-project-id>` and `<google-bucket-name>`in the configuration file with the project id and bucket name. Replace `<google-billing-project-id>` with the project id that has to be billed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; // Google project; project = ""<google-project-id>"". // Base bucket for workflow executions; root = ""gs://<google-bucket-name>/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service account to launch jobs, by default uses default service acco",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:414,Security,authenticat,authenticate,414,"## Getting started on Google Cloud with the Genomics Pipelines API. ## Pipelines API v2. ### Setting up PAPIv2. For now the easiest way to try PAPIv2 is to start with the sample configuration in; [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf); and adjust it to fit your needs. #### Permissions:. Google recommends using a service account to authenticate to GCP. . You may create a service account using the `gcloud` command, consider running the following script and replace MY-GOOGLE-PROJECT:. ```; #!/bin/bash; export LC_ALL=C ; RANDOM_BUCKET_NAME=$(head /dev/urandom | tr -dc a-z | head -c 32 ; echo ''). #Create a new service account called ""my-service-account"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create my-service-account --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'). # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer lifesciences.workflowsRunner lifesciences.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done. # create a bucket to keep the execution directory; gsutil mb gs://""$RANDOM_BUCKET_NAME"". # give the service account write access to the new bucket; gsutil acl ch -u ""$EMAIL"":W gs://""$RANDOM_BUCKET_NAME"". # create a file that represents your service account. KEEP THIS A SECRET.; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https:",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:1439,Security,access,access,1439,"account using the `gcloud` command, consider running the following script and replace MY-GOOGLE-PROJECT:. ```; #!/bin/bash; export LC_ALL=C ; RANDOM_BUCKET_NAME=$(head /dev/urandom | tr -dc a-z | head -c 32 ; echo ''). #Create a new service account called ""my-service-account"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create my-service-account --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'). # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer lifesciences.workflowsRunner lifesciences.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done. # create a bucket to keep the execution directory; gsutil mb gs://""$RANDOM_BUCKET_NAME"". # give the service account write access to the new bucket; gsutil acl ch -u ""$EMAIL"":W gs://""$RANDOM_BUCKET_NAME"". # create a file that represents your service account. KEEP THIS A SECRET.; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>. ; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. . On your Google project, open up the <a href=""https://console.developers.google.com/apis/li",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:5197,Security,access,access,5197,"ed for the request (more information for Requester Pays can be found at:; <a href=""https://cloud.google.com/storage/docs/requester-pays"" target=""_blank"">Requester Pays</a>). ***google.conf***; ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }. backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; // Google project; project = ""<google-project-id>"". // Base bucket for workflow executions; root = ""gs://<google-bucket-name>/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; 		 ; // Cloud Life Sciences API is limited to certain locations. See https://cloud.google.com/life-sciences/docs/concepts/locations; // and note that changing the location also requires changing the endpoint-url.; location = ""us-central1""	. // Pipelines v2 only: specify the number of times localization and delocaliza",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:2316,Testability,log,login,2316,"IL"" --role roles/$i; done. # create a bucket to keep the execution directory; gsutil mb gs://""$RANDOM_BUCKET_NAME"". # give the service account write access to the new bucket; gsutil acl ch -u ""$EMAIL"":W gs://""$RANDOM_BUCKET_NAME"". # create a file that represents your service account. KEEP THIS A SECRET.; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>. ; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. . On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inpu",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:2669,Testability,log,login,2669,"unt ""$EMAIL""; ```. ### Prerequisites. This tutorial page relies on completing the previous tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>. ; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. . On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:2736,Testability,log,login,2736,"us tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>. ; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. . On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy ov",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:2784,Testability,log,login,2784,"us tutorial:. * [Downloading Prerequisites](FiveMinuteIntro.md). ### Goals. At the end of this tutorial you'll have run your first workflow against the Google Pipelines API. ### Let's get started!. **Configuring a Google Project**. Install the <a href=""https://cloud.google.com/sdk/downloads"" target=""_blank"">Google Cloud SDK</a>. ; Create a <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects"" target=""_blank"">Google Cloud Project</a> and give it a project id (e.g. sample-project). Well refer to this as `<google-project-id>` and your user login (e.g. username@gmail.com) as `<google-user-id>`. . On your Google project, open up the <a href=""https://console.developers.google.com/apis/library"" target=""_blank"">API Manager</a> and enable the following APIs:; ; * Google Compute Engine API; * Cloud Storage; * Google Cloud Life Sciences API. Authenticate to Google Cloud Platform ; `gcloud auth login <google-user-id>`. Set your default account (will require to login again) ; `gcloud auth application-default login`. Set your default project ; `gcloud config set project <google-project-id>`. Create a Google Cloud Storage (GCS) bucket to hold Cromwell execution directories.; We will refer to this bucket as `google-bucket-name`, and the full identifier as `gs://google-bucket-name`. ; `gsutil mb gs://<google-bucket-name>` . **Workflow Source Files**. Copy over the sample `hello.wdl` and `hello.inputs` files to the same directory as the Cromwell jar. ; This workflow takes a string value as specified in the inputs file and writes it to stdout. . ***hello.wdl***; ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. ***hello.inputs***; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. **Google Configuration File**. Copy ov",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:6256,Testability,log,logic,6256,"docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; 		 ; // Cloud Life Sciences API is limited to certain locations. See https://cloud.google.com/life-sciences/docs/concepts/locations; // and note that changing the location also requires changing the endpoint-url.; location = ""us-central1""	. // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. . ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success! . ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md:6744,Testability,log,logs,6744,"docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // Endpoint for APIs, which defaults to us-central1. To run with a location different from us-central1,; // change the endpoint-url to start with the location, such as https://europe-west2-lifesciences.googleapis.com/; endpoint-url = ""https://lifesciences.googleapis.com/""; ; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; 		 ; // Cloud Life Sciences API is limited to certain locations. See https://cloud.google.com/life-sciences/docs/concepts/locations; // and note that changing the location also requires changing the endpoint-url.; location = ""us-central1""	. // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; }; }; }; ```. **Run Workflow**. `java -Dconfig.file=google.conf -jar cromwell-67.jar run hello.wdl -i hello.inputs`. **Outputs**. The end of your workflow logs should report the workflow outputs. . ```; [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf_hello.hello.message"": ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; },; ""id"": ""08213b40-bcf5-470d-b8b7-1d1a9dccb10e""; }; ```. Success! . ### Next steps. You might find the following tutorials interesting to tackle next:. * [Persisting Data Between Restarts](PersistentServer); * [Server Mode](ServerMode.md); ",MatchSource.DOCS,docs/tutorials/PipelinesApi101.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/PipelinesApi101.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md:558,Availability,echo,echo,558,"# Server Mode. Cromwell is best experienced in ""server"" mode, as discussed in the [Modes section of the docs](../Modes). # Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). # Goals. At the end of this tutorial you'll have run Cromwell in Server Mode, allowing you to submit more than one workflow in parallel, and query workflows even after they have completed. # Prepare Files. Paste the following into a file called `hello.wdl`:; ```wdl; task hello {; String name. command {; echo 'Hello ${name}!'; }; output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. Paste the following into a file called `inputs.json`:; ```json; {; ""test.hello.name"": ""World""; }; ```. # Run the server. 1. Run `java -jar cromwell-[version].jar server` (replace [version] with actual version). Note that there is a `server` argument, this is the special sauce!; 2. Visit <a href=""http://localhost:8000"">localhost:8000</a> in your browser. # Start the job. 1. Navigate to Workflows section and click ""Show/Hide"" ; ![](workflows.png); 2. Navigate to `/workflows/{version}` which has a green ""POST"" on the left. ; ![](submit.png); 3. Find workflowSource file, ""Choose File"" and navigate to `hello.wdl`. ; ![](workflowSource.png) ; 4. Find inputs file and navigate to `inputs.json`. ; ![](inputs.png) ; 5. Navigate to the bottom of this section and click ""Try it out!"" ; ![](try.png); 6. Observe output from the server process. # What happened?. * Did it work? Check the `/status` endpoint.; * How can I just see my outputs? `/outputs` endpoint.; * Check out metadata related to your workflow with `/metadata`. ### Next Steps. Nice job! Now that you have cromwell running in server mode you've reached the upper echilons of Cromwell prowess! After reaching these dizzy heights, you might also find the following pages interesting:. * [Viewing Metadata](MetadataEndpoint); * [Timing Diagrams](TimingDiagrams); * [Configuration",MatchSource.DOCS,docs/tutorials/ServerMode.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md:1163,Energy Efficiency,green,green,1163,"t experienced in ""server"" mode, as discussed in the [Modes section of the docs](../Modes). # Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). # Goals. At the end of this tutorial you'll have run Cromwell in Server Mode, allowing you to submit more than one workflow in parallel, and query workflows even after they have completed. # Prepare Files. Paste the following into a file called `hello.wdl`:; ```wdl; task hello {; String name. command {; echo 'Hello ${name}!'; }; output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. Paste the following into a file called `inputs.json`:; ```json; {; ""test.hello.name"": ""World""; }; ```. # Run the server. 1. Run `java -jar cromwell-[version].jar server` (replace [version] with actual version). Note that there is a `server` argument, this is the special sauce!; 2. Visit <a href=""http://localhost:8000"">localhost:8000</a> in your browser. # Start the job. 1. Navigate to Workflows section and click ""Show/Hide"" ; ![](workflows.png); 2. Navigate to `/workflows/{version}` which has a green ""POST"" on the left. ; ![](submit.png); 3. Find workflowSource file, ""Choose File"" and navigate to `hello.wdl`. ; ![](workflowSource.png) ; 4. Find inputs file and navigate to `inputs.json`. ; ![](inputs.png) ; 5. Navigate to the bottom of this section and click ""Try it out!"" ; ![](try.png); 6. Observe output from the server process. # What happened?. * Did it work? Check the `/status` endpoint.; * How can I just see my outputs? `/outputs` endpoint.; * Check out metadata related to your workflow with `/metadata`. ### Next Steps. Nice job! Now that you have cromwell running in server mode you've reached the upper echilons of Cromwell prowess! After reaching these dizzy heights, you might also find the following pages interesting:. * [Viewing Metadata](MetadataEndpoint); * [Timing Diagrams](TimingDiagrams); * [Configuration Files](ConfigurationFiles); ",MatchSource.DOCS,docs/tutorials/ServerMode.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md:635,Testability,test,test,635,"# Server Mode. Cromwell is best experienced in ""server"" mode, as discussed in the [Modes section of the docs](../Modes). # Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). # Goals. At the end of this tutorial you'll have run Cromwell in Server Mode, allowing you to submit more than one workflow in parallel, and query workflows even after they have completed. # Prepare Files. Paste the following into a file called `hello.wdl`:; ```wdl; task hello {; String name. command {; echo 'Hello ${name}!'; }; output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. Paste the following into a file called `inputs.json`:; ```json; {; ""test.hello.name"": ""World""; }; ```. # Run the server. 1. Run `java -jar cromwell-[version].jar server` (replace [version] with actual version). Note that there is a `server` argument, this is the special sauce!; 2. Visit <a href=""http://localhost:8000"">localhost:8000</a> in your browser. # Start the job. 1. Navigate to Workflows section and click ""Show/Hide"" ; ![](workflows.png); 2. Navigate to `/workflows/{version}` which has a green ""POST"" on the left. ; ![](submit.png); 3. Find workflowSource file, ""Choose File"" and navigate to `hello.wdl`. ; ![](workflowSource.png) ; 4. Find inputs file and navigate to `inputs.json`. ; ![](inputs.png) ; 5. Navigate to the bottom of this section and click ""Try it out!"" ; ![](try.png); 6. Observe output from the server process. # What happened?. * Did it work? Check the `/status` endpoint.; * How can I just see my outputs? `/outputs` endpoint.; * Check out metadata related to your workflow with `/metadata`. ### Next Steps. Nice job! Now that you have cromwell running in server mode you've reached the upper echilons of Cromwell prowess! After reaching these dizzy heights, you might also find the following pages interesting:. * [Viewing Metadata](MetadataEndpoint); * [Timing Diagrams](TimingDiagrams); * [Configuration",MatchSource.DOCS,docs/tutorials/ServerMode.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md:731,Testability,test,test,731,"# Server Mode. Cromwell is best experienced in ""server"" mode, as discussed in the [Modes section of the docs](../Modes). # Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Five Minute Introduction](FiveMinuteIntro.md). # Goals. At the end of this tutorial you'll have run Cromwell in Server Mode, allowing you to submit more than one workflow in parallel, and query workflows even after they have completed. # Prepare Files. Paste the following into a file called `hello.wdl`:; ```wdl; task hello {; String name. command {; echo 'Hello ${name}!'; }; output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. Paste the following into a file called `inputs.json`:; ```json; {; ""test.hello.name"": ""World""; }; ```. # Run the server. 1. Run `java -jar cromwell-[version].jar server` (replace [version] with actual version). Note that there is a `server` argument, this is the special sauce!; 2. Visit <a href=""http://localhost:8000"">localhost:8000</a> in your browser. # Start the job. 1. Navigate to Workflows section and click ""Show/Hide"" ; ![](workflows.png); 2. Navigate to `/workflows/{version}` which has a green ""POST"" on the left. ; ![](submit.png); 3. Find workflowSource file, ""Choose File"" and navigate to `hello.wdl`. ; ![](workflowSource.png) ; 4. Find inputs file and navigate to `inputs.json`. ; ![](inputs.png) ; 5. Navigate to the bottom of this section and click ""Try it out!"" ; ![](try.png); 6. Observe output from the server process. # What happened?. * Did it work? Check the `/status` endpoint.; * How can I just see my outputs? `/outputs` endpoint.; * Check out metadata related to your workflow with `/metadata`. ### Next Steps. Nice job! Now that you have cromwell running in server mode you've reached the upper echilons of Cromwell prowess! After reaching these dizzy heights, you might also find the following pages interesting:. * [Viewing Metadata](MetadataEndpoint); * [Timing Diagrams](TimingDiagrams); * [Configuration",MatchSource.DOCS,docs/tutorials/ServerMode.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/ServerMode.md
https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/TimingDiagrams.md:952,Availability,echo,echo,952,"## Timing Diagrams. ### Prerequisites. This tutorial page relies on completing the previous tutorials:. * [Server Mode](ServerMode). ### Goals. At the end of this tutorial you'll have seen how to view timing diagram for a workflow as it's running, and interpreting the information on the timing diagram for a completed workflow. ### Let's get started. Want to see the tasks that ran in your workflow laid out in a handy timing diagram? Good news! That's exactly what's about to happen to you!. #### Preparing files. To get a good example of these timing diagrams, we're going to make a workflow that scatters a task across a few indices, and see how that gets represented. Open your favorite text editor, copy the following text in and save it in your cromwell directory as `timingWorkflow.wdl`:; ```wdl; workflow timingWorkflow {; 	scatter(i in range(15)) {; 		call sleep { input: sleep_time = i }; 	}; }. task sleep {; 	Int sleep_time; 	command {; 		echo ""I slept for ${sleep_time}""; 		sleep ${sleep_time}; 	}; 	output {; 		String out = read_string(stdout()); 	}; }; ```. In brief, this workflow will scatter 15 tasks, each one will sleep for a time proportional to their scatter index. Is that hard to imagine? Never mind, we'll see it in diagramatic form soon!. #### Submit to cromwell:. If it's not running already, start the cromwell server:; ```sh; java -jar cromwell-29.jar server; ```. Submit the workflow:; ```sh; curl -X POST --header ""Accept: application/json""\; 	-v ""localhost:8000/api/workflows/v1"" \; 	-F workflowSource=@timingWorkflow.wdl; ```. Amongst the curl output you should see the workflow ID come back, eg:; ```; [...] Workflow 8d18b845-7143-4f35-9543-1977383b7d2f submitted.; ```. I can now enter the following address into my web browser (i.e. Chrome) and see the timing diagram for the workflow. You'll need to swap out my workflow ID for the one that you received (they're all randomly generated) ; ```; http://localhost:8000/api/workflows/v1/8d18b845-7143-4f35-9543-197738",MatchSource.DOCS,docs/tutorials/TimingDiagrams.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/tutorials/TimingDiagrams.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:3778,Deployability,configurat,configuration,3778,"his script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |; | `delete_intermediate_output_files` | `boolean` | **Experimental:** Any `File` variables referenced in call `output` sections that are not found in the workflow `output` section will be considered an intermediate `File`. When the workflow finishes and this option is set to `true`, all intermediate `File` objects will be deleted from GCS. Cromwell must be run with the configuration value `system.delete-workflow-files` set to `true`. The default for both values is `false`. NOTE: The behavior of this option on other backends is unspecified. |; | `enable_fuse` | `boolean` | Specifies if workflow tasks should be submitted to Google Pipelines with an additional `ENABLE_FUSE` flag. It causes container to be executed with `CAP_SYS_ADMIN`. Use it only for trusted containers. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |. <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. # Example; ```json; {; ""jes_gcs_root"": ""gs://my-bucket/workflows"",; ""google_project"": ""my_google_project"",; ""google_compute_service_account"": ""my-new-svcacct@my-google-project.iam.gserviceaccount.com"",; ""auth_bucket"": ""gs://my-auth-bucket/private"",; ""monitoring_script"": ""gs://bucket/monitoring_script.sh"",; ""monitoring_image"": ""quay.io/broadinstitute/cromwell-monitor"",; ""monitoring",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2119,Energy Efficiency,monitor,monitoring,2119,"d `gcp_batch_gcs_root` on the GCPBatch backend.) |; | `google_compute_service_account` | `string` | Alternate service account to use on the compute instance (e.g. `my-new-svcacct@my-google-project.iam.gserviceaccount.com`). If this is not set, this defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must con",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2149,Energy Efficiency,monitor,monitoring,2149,"the GCPBatch backend.) |; | `google_compute_service_account` | `string` | Alternate service account to use on the compute instance (e.g. `my-new-svcacct@my-google-project.iam.gserviceaccount.com`). If this is not set, this defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2271,Energy Efficiency,monitor,monitor,2271,"t to use on the compute instance (e.g. `my-new-svcacct@my-google-project.iam.gserviceaccount.com`). If this is not set, this defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2513,Energy Efficiency,monitor,monitor,2513,"oject` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |; | `delete_intermediate_output_files` | `boolean` | **Experimental:** Any `File` variables referenced in call `output` sections that are no",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2615,Energy Efficiency,monitor,monitoring,2615,"th_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |; | `delete_intermediate_output_files` | `boolean` | **Experimental:** Any `File` variables referenced in call `output` sections that are not found in the workflow `output` section will be considered an intermediate",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:4755,Energy Efficiency,monitor,monitor,4755," string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |; | `delete_intermediate_output_files` | `boolean` | **Experimental:** Any `File` variables referenced in call `output` sections that are not found in the workflow `output` section will be considered an intermediate `File`. When the workflow finishes and this option is set to `true`, all intermediate `File` objects will be deleted from GCS. Cromwell must be run with the configuration value `system.delete-workflow-files` set to `true`. The default for both values is `false`. NOTE: The behavior of this option on other backends is unspecified. |; | `enable_fuse` | `boolean` | Specifies if workflow tasks should be submitted to Google Pipelines with an additional `ENABLE_FUSE` flag. It causes container to be executed with `CAP_SYS_ADMIN`. Use it only for trusted containers. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |. <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. # Example; ```json; {; ""jes_gcs_root"": ""gs://my-bucket/workflows"",; ""google_project"": ""my_google_project"",; ""google_compute_service_account"": ""my-new-svcacct@my-google-project.iam.gserviceaccount.com"",; ""auth_bucket"": ""gs://my-auth-bucket/private"",; ""monitoring_script"": ""gs://bucket/monitoring_script.sh"",; ""monitoring_image"": ""quay.io/broadinstitute/cromwell-monitor"",; ""monitoring_image_script"": ""gs://bucket/monitoring_image_script.sh"",; ""enable_ssh_access"": false,; ""google_labels"": {; ""custom-label"": ""custom-value""; },; ""delete_intermediate_output_files"": false,; ""enable_fuse"": false; }; ```; ",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:1050,Modifiability,config,config,1050,"ow options provide Google-specific information for workflows running tasks on the Google PAPI backend. <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. | Keys | Possible Values | Description |; |------------------------------------|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | `jes_gcs_root` | `string` | Where outputs of the workflow will be written. Expects this to be a GCS URL (e.g. `gs://my-bucket/workflows`). If this is not set, this defaults to the value within `backend.jes.config.root` in the [Configuration](../Configuring). (Note this is called `gcp_batch_gcs_root` on the GCPBatch backend.) |; | `google_compute_service_account` | `string` | Alternate service account to use on the compute instance (e.g. `my-new-svcacct@my-google-project.iam.gserviceaccount.com`). If this is not set, this defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:1413,Modifiability,config,config,1413,"---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | `jes_gcs_root` | `string` | Where outputs of the workflow will be written. Expects this to be a GCS URL (e.g. `gs://my-bucket/workflows`). If this is not set, this defaults to the value within `backend.jes.config.root` in the [Configuration](../Configuring). (Note this is called `gcp_batch_gcs_root` on the GCPBatch backend.) |; | `google_compute_service_account` | `string` | Alternate service account to use on the compute instance (e.g. `my-new-svcacct@my-google-project.iam.gserviceaccount.com`). If this is not set, this defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latt",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:3487,Modifiability,variab,variables,3487,"nitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |; | `delete_intermediate_output_files` | `boolean` | **Experimental:** Any `File` variables referenced in call `output` sections that are not found in the workflow `output` section will be considered an intermediate `File`. When the workflow finishes and this option is set to `true`, all intermediate `File` objects will be deleted from GCS. Cromwell must be run with the configuration value `system.delete-workflow-files` set to `true`. The default for both values is `false`. NOTE: The behavior of this option on other backends is unspecified. |; | `enable_fuse` | `boolean` | Specifies if workflow tasks should be submitted to Google Pipelines with an additional `ENABLE_FUSE` flag. It causes container to be executed with `CAP_SYS_ADMIN`. Use it only for trusted containers. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |. <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. # Example; ```json; {; ""jes_gcs_root"": ""gs://my-bucket/workflows"",; ""google_project"": ""my_google_project"",; ""google_comput",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:3778,Modifiability,config,configuration,3778,"his script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |; | `delete_intermediate_output_files` | `boolean` | **Experimental:** Any `File` variables referenced in call `output` sections that are not found in the workflow `output` section will be considered an intermediate `File`. When the workflow finishes and this option is set to `true`, all intermediate `File` objects will be deleted from GCS. Cromwell must be run with the configuration value `system.delete-workflow-files` set to `true`. The default for both values is `false`. NOTE: The behavior of this option on other backends is unspecified. |; | `enable_fuse` | `boolean` | Specifies if workflow tasks should be submitted to Google Pipelines with an additional `ENABLE_FUSE` flag. It causes container to be executed with `CAP_SYS_ADMIN`. Use it only for trusted containers. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |. <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. # Example; ```json; {; ""jes_gcs_root"": ""gs://my-bucket/workflows"",; ""google_project"": ""my_google_project"",; ""google_compute_service_account"": ""my-new-svcacct@my-google-project.iam.gserviceaccount.com"",; ""auth_bucket"": ""gs://my-auth-bucket/private"",; ""monitoring_script"": ""gs://bucket/monitoring_script.sh"",; ""monitoring_image"": ""quay.io/broadinstitute/cromwell-monitor"",; ""monitoring",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2309,Performance,concurren,concurrently,2309,"defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported b",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:3241,Security,access,access,3241,"` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_ssh_access` | `boolean` | If set to true, will enable SSH access to the Google Genomics worker machines. Please note that this is a community contribution and is not officially supported by the Cromwell development team. |; | `delete_intermediate_output_files` | `boolean` | **Experimental:** Any `File` variables referenced in call `output` sections that are not found in the workflow `output` section will be considered an intermediate `File`. When the workflow finishes and this option is set to `true`, all intermediate `File` objects will be deleted from GCS. Cromwell must be run with the configuration value `system.delete-workflow-files` set to `true`. The default for both values is `false`. NOTE: The behavior of this option on other backends is unspecified. |; | `enable_fuse` | `boolean` | Specifies if workflow tasks should be submitted to Google Pipelines with an additional `ENABLE_FUSE` flag. It causes container to be executed with `CAP_SYS_ADMIN`. Use it only for trusted containers. Please note that this is a community contri",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2130,Testability,log,log,2130,"h_gcs_root` on the GCPBatch backend.) |; | `google_compute_service_account` | `string` | Alternate service account to use on the compute instance (e.g. `my-new-svcacct@my-google-project.iam.gserviceaccount.com`). If this is not set, this defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the ",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md:2160,Testability,log,log,2160,"e_compute_service_account` | `string` | Alternate service account to use on the compute instance (e.g. `my-new-svcacct@my-google-project.iam.gserviceaccount.com`). If this is not set, this defaults to the value within `backend.jes.config.genomics.compute-service-account` in the [Configuration](../Configuring) if specified or `default` otherwise. |; | `google_project` | `string` | Google project used to execute this workflow. |; | `auth_bucket` | `string` | A GCS URL that only Cromwell can write to. The Cromwell account is determined by the `google.authScheme` (and the corresponding `google.userAuth` and `google.serviceAuth`). Defaults to the the value in [jes_gcs_root](#jes_gcs_root). |; | `monitoring_script` | `string` | Specifies a GCS URL to a script that will be invoked prior to the user command being run. For example, if the value for monitoring_script is `""gs://bucket/script.sh""`, it will be invoked as `./script.sh > monitoring.log &`. The value `monitoring.log` file will be automatically de-localized. |; | `monitoring_image` | `string` | Specifies a Docker image to monitor the task. This image will run concurrently with the task container, and provides an alternative mechanism to `monitoring_script` (the latter runs *inside* the task container). For example, one can use `quay.io/broadinstitute/cromwell-monitor`, which reports cpu/memory/disk utilization metrics to [Stackdriver](https://cloud.google.com/monitoring/). |; | `monitoring_image_script` | `string` | Specifies a GCS URL to a script that will be invoked on the container running the `monitoring_image`. This script will be invoked instead of the ENTRYPOINT defined in the `monitoring_image`. Unlike the `monitoring_script` no files are automatically de-localized. |; | `google_labels` | `object` | An object containing only string values. Represent custom labels to send with PAPI job requests. Per the PAPI specification, each key and value must conform to the regex `[a-z]([-a-z0-9]*[a-z0-9])?`. |; | `enable_",MatchSource.DOCS,docs/wf_options/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:2098,Availability,avail,available,2098,"for any [Runtime Attributes](../RuntimeAttributes.md) by adding a `default_runtime_attributes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `Local` backend unless they provide a value explicitly in their `runtime { ... }` block. In addition, the `continueOnReturnCode` value for all tasks is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are ",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:2830,Availability,failure,failure-mode,2830,"## Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `Local` backend unless they provide a value explicitly in their `runtime { ... }` block. In addition, the `continueOnReturnCode` value for all tasks is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are started but existing calls are allowed to finish.|. Example `options.json`:; ```json; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```. ## Output Copying; | Option | Value | Description |; |--------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | `final_workflow_outputs_dir` | A directory available to Cromwell | Specifies a path where final workflow outputs will be written. If this is not specified, wo",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:3726,Availability,avail,available,3726,"s is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are started but existing calls are allowed to finish.|. Example `options.json`:; ```json; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```. ## Output Copying; | Option | Value | Description |; |--------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | `final_workflow_outputs_dir` | A directory available to Cromwell | Specifies a path where final workflow outputs will be written. If this is not specified, workflow outputs will not be copied out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metadata`| `""source""` or `""destination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it wil",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:4588,Availability,avail,available,4588,"------------------------------------------------------------------------|; | `final_workflow_outputs_dir` | A directory available to Cromwell | Specifies a path where final workflow outputs will be written. If this is not specified, workflow outputs will not be copied out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metadata`| `""source""` or `""destination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it will default to `false`. |; | `final_workflow_log_dir` | A directory available to Cromwell | Specifies a path where per-workflow logs will be written. If this is not specified, per-workflow logs will not be copied out of the Cromwell workflow log temporary directory/path before they are deleted. |; | `final_call_logs_dir` | A directory available to Cromwell | Specifies a path where final call logs will be written. If this is not specified, call logs will not be copied out of the Cromwell workflow execution directory/path. |. Note that these directories should be using the same filesystem as the workflow. Eg if you run on Google's PAPI, you should provide `gs://...` paths. Example `options.json`:; ```json; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs w",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:4857,Availability,avail,available,4857,"out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metadata`| `""source""` or `""destination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it will default to `false`. |; | `final_workflow_log_dir` | A directory available to Cromwell | Specifies a path where per-workflow logs will be written. If this is not specified, per-workflow logs will not be copied out of the Cromwell workflow log temporary directory/path before they are deleted. |; | `final_call_logs_dir` | A directory available to Cromwell | Specifies a path where final call logs will be written. If this is not specified, call logs will not be copied out of the Cromwell workflow execution directory/path. |. Note that these directories should be using the same filesystem as the workflow. Eg if you run on Google's PAPI, you should provide `gs://...` paths. Example `options.json`:; ```json; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_picture.jpg; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_other_task/execution/created_subdir/sub",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:6241,Availability,error,error,6241,"; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_picture.jpg; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_other_task/execution/created_subdir/submarine.txt; ```. The above result will look like this when `""use_relative_output_paths"": true`:; ```; final_workflow_outputs_dir/my_output_picture.jpg; final_workflow_outputs_dir/created_subdir/submarine.txt; ```. This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCac",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:7559,Availability,error,error,7559,". This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|. Example `options.json`:; ```json; {; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. ## Retry with More Memory Multiplier. The `memory_retry_multiplier` workflow option sets the factor by which the memory should be multiplied while retrying ; when Cromwell encounters one of the error keys (specified in Cromwell config using `system.memory-retry-error-keys`) in ; the `stderr` file. The factor should be in the range `1.0  multipler  99.0` (note: if set to `1.0` the task will retry ; with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not change the ; memory amount. See the [Retry with More Memory](../cromwell_features/RetryWithMoreMemory.md) section for ; more details. Example `options.json`:; ```json; {; ""memory_retry_multiplier"" : 1.1; }; ```; ",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:7627,Availability,error,error-keys,7627,". This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|. Example `options.json`:; ```json; {; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. ## Retry with More Memory Multiplier. The `memory_retry_multiplier` workflow option sets the factor by which the memory should be multiplied while retrying ; when Cromwell encounters one of the error keys (specified in Cromwell config using `system.memory-retry-error-keys`) in ; the `stderr` file. The factor should be in the range `1.0  multipler  99.0` (note: if set to `1.0` the task will retry ; with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not change the ; memory amount. See the [Retry with More Memory](../cromwell_features/RetryWithMoreMemory.md) section for ; more details. Example `options.json`:; ```json; {; ""memory_retry_multiplier"" : 1.1; }; ```; ",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:117,Deployability,configurat,configuration,117,"# Workflow Options Overview. Workflow options can affect the execution of a single workflow without having to change configuration options or restart Cromwell. . You provide workflow options to Cromwell in a JSON format. This can be supplied at workflow-submit time either via the [CLI](../CommandLine.md) or the Submit endpoint:. ```json; {; 	""option_name_1"": ""option value 1"",; 	""option_name_2"": ""option value 2""; }; ```. Unless otherwise specified you can expect workflow options to override any hard-coded defaults in Cromwell or defaults provided in the [configuration file](../Configuring.md), but to be overridden by any values provided in the workflow definition file itself (WDL). Some workflow options apply only to tasks running on the [Google Pipelines API backend](Google). # Global Workflow Options . The following workflow options apply to all workflows and their calls regardless of the backend being used. ## Runtime Attributes. Some options allow you to override or set defaults for runtime attributes. ### Setting Default Runtime Attributes. You can supply a default for any [Runtime Attributes](../RuntimeAttributes.md) by adding a `default_runtime_attributes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `co",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:560,Deployability,configurat,configuration,560,"# Workflow Options Overview. Workflow options can affect the execution of a single workflow without having to change configuration options or restart Cromwell. . You provide workflow options to Cromwell in a JSON format. This can be supplied at workflow-submit time either via the [CLI](../CommandLine.md) or the Submit endpoint:. ```json; {; 	""option_name_1"": ""option value 1"",; 	""option_name_2"": ""option value 2""; }; ```. Unless otherwise specified you can expect workflow options to override any hard-coded defaults in Cromwell or defaults provided in the [configuration file](../Configuring.md), but to be overridden by any values provided in the workflow definition file itself (WDL). Some workflow options apply only to tasks running on the [Google Pipelines API backend](Google). # Global Workflow Options . The following workflow options apply to all workflows and their calls regardless of the backend being used. ## Runtime Attributes. Some options allow you to override or set defaults for runtime attributes. ### Setting Default Runtime Attributes. You can supply a default for any [Runtime Attributes](../RuntimeAttributes.md) by adding a `default_runtime_attributes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `co",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:1349,Deployability,configurat,configuration,1349,",; 	""option_name_2"": ""option value 2""; }; ```. Unless otherwise specified you can expect workflow options to override any hard-coded defaults in Cromwell or defaults provided in the [configuration file](../Configuring.md), but to be overridden by any values provided in the workflow definition file itself (WDL). Some workflow options apply only to tasks running on the [Google Pipelines API backend](Google). # Global Workflow Options . The following workflow options apply to all workflows and their calls regardless of the backend being used. ## Runtime Attributes. Some options allow you to override or set defaults for runtime attributes. ### Setting Default Runtime Attributes. You can supply a default for any [Runtime Attributes](../RuntimeAttributes.md) by adding a `default_runtime_attributes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `L",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:2190,Deployability,configurat,configuration,2190,"utes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `Local` backend unless they provide a value explicitly in their `runtime { ... }` block. In addition, the `continueOnReturnCode` value for all tasks is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are started but existing calls are allowed to finish.|. Example `options.json`:; ```json; {; """,MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:2845,Deployability,configurat,configuration,2845,"## Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `Local` backend unless they provide a value explicitly in their `runtime { ... }` block. In addition, the `continueOnReturnCode` value for all tasks is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are started but existing calls are allowed to finish.|. Example `options.json`:; ```json; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```. ## Output Copying; | Option | Value | Description |; |--------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | `final_workflow_outputs_dir` | A directory available to Cromwell | Specifies a path where final workflow outputs will be written. If this is not specified, wo",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:3023,Integrability,depend,depend,3023," array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `Local` backend unless they provide a value explicitly in their `runtime { ... }` block. In addition, the `continueOnReturnCode` value for all tasks is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are started but existing calls are allowed to finish.|. Example `options.json`:; ```json; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```. ## Output Copying; | Option | Value | Description |; |--------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | `final_workflow_outputs_dir` | A directory available to Cromwell | Specifies a path where final workflow outputs will be written. If this is not specified, workflow outputs will not be copied out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metada",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:117,Modifiability,config,configuration,117,"# Workflow Options Overview. Workflow options can affect the execution of a single workflow without having to change configuration options or restart Cromwell. . You provide workflow options to Cromwell in a JSON format. This can be supplied at workflow-submit time either via the [CLI](../CommandLine.md) or the Submit endpoint:. ```json; {; 	""option_name_1"": ""option value 1"",; 	""option_name_2"": ""option value 2""; }; ```. Unless otherwise specified you can expect workflow options to override any hard-coded defaults in Cromwell or defaults provided in the [configuration file](../Configuring.md), but to be overridden by any values provided in the workflow definition file itself (WDL). Some workflow options apply only to tasks running on the [Google Pipelines API backend](Google). # Global Workflow Options . The following workflow options apply to all workflows and their calls regardless of the backend being used. ## Runtime Attributes. Some options allow you to override or set defaults for runtime attributes. ### Setting Default Runtime Attributes. You can supply a default for any [Runtime Attributes](../RuntimeAttributes.md) by adding a `default_runtime_attributes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `co",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:560,Modifiability,config,configuration,560,"# Workflow Options Overview. Workflow options can affect the execution of a single workflow without having to change configuration options or restart Cromwell. . You provide workflow options to Cromwell in a JSON format. This can be supplied at workflow-submit time either via the [CLI](../CommandLine.md) or the Submit endpoint:. ```json; {; 	""option_name_1"": ""option value 1"",; 	""option_name_2"": ""option value 2""; }; ```. Unless otherwise specified you can expect workflow options to override any hard-coded defaults in Cromwell or defaults provided in the [configuration file](../Configuring.md), but to be overridden by any values provided in the workflow definition file itself (WDL). Some workflow options apply only to tasks running on the [Google Pipelines API backend](Google). # Global Workflow Options . The following workflow options apply to all workflows and their calls regardless of the backend being used. ## Runtime Attributes. Some options allow you to override or set defaults for runtime attributes. ### Setting Default Runtime Attributes. You can supply a default for any [Runtime Attributes](../RuntimeAttributes.md) by adding a `default_runtime_attributes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `co",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:1349,Modifiability,config,configuration,1349,",; 	""option_name_2"": ""option value 2""; }; ```. Unless otherwise specified you can expect workflow options to override any hard-coded defaults in Cromwell or defaults provided in the [configuration file](../Configuring.md), but to be overridden by any values provided in the workflow definition file itself (WDL). Some workflow options apply only to tasks running on the [Google Pipelines API backend](Google). # Global Workflow Options . The following workflow options apply to all workflows and their calls regardless of the backend being used. ## Runtime Attributes. Some options allow you to override or set defaults for runtime attributes. ### Setting Default Runtime Attributes. You can supply a default for any [Runtime Attributes](../RuntimeAttributes.md) by adding a `default_runtime_attributes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `L",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:2190,Modifiability,config,configuration,2190,"utes` map to your workflow options file. Use the key to provide the attribute name and the value to supply the default. . These defaults replace any defaults in the Cromwell configuration file but are themselves replaced by any values explicitly provided by the task in the WDL file. Example `options.json`:; ```json; {; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""continueOnReturnCode"": [4, 8, 15, 16, 23, 42]; }; }; ```. In this example, if a task in a workflow specifies a `docker:` attribute, the task will get what it specifies. However if any task does not provide a value then it will be treated as though it had specified `ubuntu:latest`. ### Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `Local` backend unless they provide a value explicitly in their `runtime { ... }` block. In addition, the `continueOnReturnCode` value for all tasks is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are started but existing calls are allowed to finish.|. Example `options.json`:; ```json; {; """,MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:2845,Modifiability,config,configuration,2845,"## Specific Runtime Attributes. |Option|Value|Description|; |---|---|---|; |`continueOnReturnCode`|`true` or `false` or integer array|Globally overrides the `continueOnReturnCode` [runtime attribute](../RuntimeAttributes.md) for all tasks| ; |`backend`|An [available](../Configuring.md) backend|Set the **default** backend specified in the Cromwell configuration for this workflow only.|. Example `options.json`:; ```json; {; ""continueOnReturnCode"": false,; ""backend"": ""Local""; }; ```. In this example, all tasks will be given to the `Local` backend unless they provide a value explicitly in their `runtime { ... }` block. In addition, the `continueOnReturnCode` value for all tasks is hard-coded to `false`, regardless of what the tasks put in their `runtime` block. **TODO or is just a default ala `default_runtime_attributes`?**. ## Workflow Failure. The `workflow_failure_mode` option can be given the following values. This overrides any default set by the `workflow-options.workflow-failure-mode` [configuration](../Configuring.md) options. |Value|Description|; |---|---|; |`ContinueWhilePossible`|Continues to start and process calls in the workflow, as long as they did not depend on the failing call.|; |`NoNewCalls`|No *new* calls are started but existing calls are allowed to finish.|. Example `options.json`:; ```json; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```. ## Output Copying; | Option | Value | Description |; |--------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | `final_workflow_outputs_dir` | A directory available to Cromwell | Specifies a path where final workflow outputs will be written. If this is not specified, wo",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:6345,Modifiability,config,configured,6345,"ce"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_picture.jpg; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_other_task/execution/created_subdir/submarine.txt; ```. The above result will look like this when `""use_relative_output_paths"": true`:; ```; final_workflow_outputs_dir/my_output_picture.jpg; final_workflow_outputs_dir/created_subdir/submarine.txt; ```. This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|. Example `options.json`:; ```json; {; ""write_to_cache"": true,; ""read_from_cache"": tr",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:7593,Modifiability,config,config,7593,". This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|. Example `options.json`:; ```json; {; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. ## Retry with More Memory Multiplier. The `memory_retry_multiplier` workflow option sets the factor by which the memory should be multiplied while retrying ; when Cromwell encounters one of the error keys (specified in Cromwell config using `system.memory-retry-error-keys`) in ; the `stderr` file. The factor should be in the range `1.0  multipler  99.0` (note: if set to `1.0` the task will retry ; with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not change the ; memory amount. See the [Retry with More Memory](../cromwell_features/RetryWithMoreMemory.md) section for ; more details. Example `options.json`:; ```json; {; ""memory_retry_multiplier"" : 1.1; }; ```; ",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:6930,Performance,cache,cache,6930,"ion/created_subdir/submarine.txt; ```. The above result will look like this when `""use_relative_output_paths"": true`:; ```; final_workflow_outputs_dir/my_output_picture.jpg; final_workflow_outputs_dir/created_subdir/submarine.txt; ```. This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|. Example `options.json`:; ```json; {; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. ## Retry with More Memory Multiplier. The `memory_retry_multiplier` workflow option sets the factor by which the memory should be multiplied while retrying ; when Cromwell encounters one of the error keys (specified in Cromwell config using `system.memory-retry-error-keys`) in ; the `stderr` file. The factor should be in the range `1.0  multipler  99.0` (note: if set to `1.0` the task will retry ; with same amount of memory). If this is not specified, Cromwell will retry the task (if",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:7103,Performance,cache,cache,7103,"ne.txt; ```. This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|. Example `options.json`:; ```json; {; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. ## Retry with More Memory Multiplier. The `memory_retry_multiplier` workflow option sets the factor by which the memory should be multiplied while retrying ; when Cromwell encounters one of the error keys (specified in Cromwell config using `system.memory-retry-error-keys`) in ; the `stderr` file. The factor should be in the range `1.0  multipler  99.0` (note: if set to `1.0` the task will retry ; with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not change the ; memory amount. See the [Retry with More Memory](../cromwell_features/RetryWithMoreMemory.md) section for ; more details. Example `options.json`:; ```json; {; ""memory_retry_multiplier"" : 1.1",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:6200,Safety,detect,detects,6200,"; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_picture.jpg; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_other_task/execution/created_subdir/submarine.txt; ```. The above result will look like this when `""use_relative_output_paths"": true`:; ```; final_workflow_outputs_dir/my_output_picture.jpg; final_workflow_outputs_dir/created_subdir/submarine.txt; ```. This will create file collisions in `final_workflow_outputs_dir` when a workflow is run twice. When Cromwell; detects file collisions it will throw an error and report the workflow as failed. ## Call Caching Options. These options can override Cromwell's configured call caching behavior for a single workflow. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details and how to set defaults. The call caching section will also explain how these options interact when, for example, one is set `true` and the other is `false`. **Note:** If call caching is disabled, these options will be ignored and the options will be treated as though they were `false`. |Option|Values|Description|; |---|---|---|; |`write_to_cache`|`true` or `false`|If `false`, the completed calls from this workflow will not be added to the cache. See the [Call Caching](../cromwell_features/CallCaching.md) section for more details.|; |`read_from_cache`|`true` or `false`|If `false`, Cromwell will not search the cache when invoking a call (i.e. every call will be executed unconditionally). See the [Call Caching](../cromwell_features/CallCac",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:4648,Testability,log,logs,4648,"------------------------------------------------------------------------|; | `final_workflow_outputs_dir` | A directory available to Cromwell | Specifies a path where final workflow outputs will be written. If this is not specified, workflow outputs will not be copied out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metadata`| `""source""` or `""destination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it will default to `false`. |; | `final_workflow_log_dir` | A directory available to Cromwell | Specifies a path where per-workflow logs will be written. If this is not specified, per-workflow logs will not be copied out of the Cromwell workflow log temporary directory/path before they are deleted. |; | `final_call_logs_dir` | A directory available to Cromwell | Specifies a path where final call logs will be written. If this is not specified, call logs will not be copied out of the Cromwell workflow execution directory/path. |. Note that these directories should be using the same filesystem as the workflow. Eg if you run on Google's PAPI, you should provide `gs://...` paths. Example `options.json`:; ```json; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs w",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:4709,Testability,log,logs,4709,"mwell | Specifies a path where final workflow outputs will be written. If this is not specified, workflow outputs will not be copied out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metadata`| `""source""` or `""destination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it will default to `false`. |; | `final_workflow_log_dir` | A directory available to Cromwell | Specifies a path where per-workflow logs will be written. If this is not specified, per-workflow logs will not be copied out of the Cromwell workflow log temporary directory/path before they are deleted. |; | `final_call_logs_dir` | A directory available to Cromwell | Specifies a path where final call logs will be written. If this is not specified, call logs will not be copied out of the Cromwell workflow execution directory/path. |. Note that these directories should be using the same filesystem as the workflow. Eg if you run on Google's PAPI, you should provide `gs://...` paths. Example `options.json`:; ```json; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:4762,Testability,log,log,4762,"mwell | Specifies a path where final workflow outputs will be written. If this is not specified, workflow outputs will not be copied out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metadata`| `""source""` or `""destination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it will default to `false`. |; | `final_workflow_log_dir` | A directory available to Cromwell | Specifies a path where per-workflow logs will be written. If this is not specified, per-workflow logs will not be copied out of the Cromwell workflow log temporary directory/path before they are deleted. |; | `final_call_logs_dir` | A directory available to Cromwell | Specifies a path where final call logs will be written. If this is not specified, call logs will not be copied out of the Cromwell workflow execution directory/path. |. Note that these directories should be using the same filesystem as the workflow. Eg if you run on Google's PAPI, you should provide `gs://...` paths. Example `options.json`:; ```json; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:4915,Testability,log,logs,4915,"out of the Cromwell workflow execution directory/path. |; | `final_workflow_outputs_dir_metadata`| `""source""` or `""destination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it will default to `false`. |; | `final_workflow_log_dir` | A directory available to Cromwell | Specifies a path where per-workflow logs will be written. If this is not specified, per-workflow logs will not be copied out of the Cromwell workflow log temporary directory/path before they are deleted. |; | `final_call_logs_dir` | A directory available to Cromwell | Specifies a path where final call logs will be written. If this is not specified, call logs will not be copied out of the Cromwell workflow execution directory/path. |. Note that these directories should be using the same filesystem as the workflow. Eg if you run on Google's PAPI, you should provide `gs://...` paths. Example `options.json`:; ```json; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_picture.jpg; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_other_task/execution/created_subdir/sub",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md:4968,Testability,log,logs,4968,"estination""` | Specifies whether the `/outputs` endpoint returns the source or destination of the copy. `""source""` is the original behavior, and the default when the parameter is not supplied.; | `use_relative_output_paths` | A boolean | When set to `true` this will copy all the outputs relative to their execution directory. my_final_workflow_outputs_dir/~~MyWorkflow/af76876d8-6e8768fa/call-MyTask/execution/~~output_of_interest . Cromwell will throw an exception when this leads to collisions. When the option is not set it will default to `false`. |; | `final_workflow_log_dir` | A directory available to Cromwell | Specifies a path where per-workflow logs will be written. If this is not specified, per-workflow logs will not be copied out of the Cromwell workflow log temporary directory/path before they are deleted. |; | `final_call_logs_dir` | A directory available to Cromwell | Specifies a path where final call logs will be written. If this is not specified, call logs will not be copied out of the Cromwell workflow execution directory/path. |. Note that these directories should be using the same filesystem as the workflow. Eg if you run on Google's PAPI, you should provide `gs://...` paths. Example `options.json`:; ```json; {; ""final_workflow_outputs_dir"": ""/Users/michael_scott/cromwell/outputs"",; ""final_workflow_outputs_dir_metadata"": ""source"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""/Users/michael_scott/cromwell/wf_logs"",; ""final_call_logs_dir"": ""/Users/michael_scott/cromwell/call_logs""; }; ```. With `""use_relative_output_paths"": false` (the default) the outputs will look like this. ```; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_one_task/execution/my_output_picture.jpg; final_workflow_outputs_dir/my_workflow/ade68a6d876e8d-8a98d7e9-ad98e9ae8d/call-my_other_task/execution/created_subdir/submarine.txt; ```. The above result will look like this when `""use_relative_output_paths"": true`:; ```; final_workflo",MatchSource.DOCS,docs/wf_options/Overview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/wf_options/Overview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:172,Integrability,message,messages,172,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:220,Integrability,rout,routes,220,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:300,Integrability,message,message,300,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:361,Integrability,message,message,361,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:392,Integrability,message,message,392,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:631,Integrability,message,messages,631,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:704,Integrability,message,messages,704,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:993,Integrability,message,message,993,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:1070,Integrability,message,message,1070,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:1129,Integrability,message,messages,1129,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:549,Modifiability,config,configurable,549,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:1292,Modifiability,config,configure,1292,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:640,Performance,queue,queue,640,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:679,Testability,log,logic,679,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:890,Testability,log,logic,890,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md:1478,Testability,log,logic,1478,"# IoActor: basic concepts. * **Word count:** 225. ## Actor Hierarchy. IO subsystem consists of the following actors. ![IoActor hierarchy](IoActor_basic_hierarchy.png). All messages are sent to `IoActorProxy`, which then routes them either to `IoActor` or `IoPromiseProxyActor` based on ; whether the message contains a Promise. `IoPromiseProxyActor` receives a message with Promise, forwards message to the ; `IoActor`, and once response from `IoActor` comes back, completes the promise. `IoActor` has back-pressure (always enabled) and throttling (configurable) features implemented in order to prevent ; overflowing the incoming messages queue. ## Message types and processing logic. `IoActor` accepts messages of base type `IoCommand[T]` with or without clientContext (which is a Promise, which will be ; passed through, back to the `IoPromiseProxyActor` in the end). Message processing logic is implemented using Akka Stream API and can be represented with the following graph:; ![IoActor message processing graph](IoActor_message_processing_graph.png). There are 2 message processing flows: ; 1. ""GCS batch flow"" to process messages, which are subtypes of `GcsBatchIoCommand` ; 1. ""Default flow"" for all other subtypes of `IoCommand`; ; Both ""GCS batch flow"" and ""Default flow"" allow to configure desired parallelism level, which can be done during ; `IoActor's` creation using `gcsParallelism` and `nioParallelism` parameters respectively. ; The particular implementation logic for each base IO command is encapsulated inside `ParallelGcsBatchFlow` and ; `NioFlow` classes. The final `IoResult` is sent both to the ""reply sink"" which is responsible for sending result to the original sender, and ; ""instrumentation sink"", which is responsible for sending `InstrumentationServiceMessage` to the `ServiceRegistryActor` ; ",MatchSource.DOCS,docs/developers/bitesize/IoActor/IoActor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/IoActor/IoActor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md:32,Testability,log,logic,32,"# Workflow jobs: retry decision logic. * **Word count:** 132. ## Key concepts. * Workflow may consist of multiple jobs.; * Job ""retry"" actually means creating a new job.; * Job may have attributes which are being stored in the `JOB_KEY_VALUE_ENTRY` table in the database, identified by a ; `ScopedKey`, which comprises workflow id, call fully qualified name, job index, job attempt number, and attribute name.; * There are 2 types of retries:; * backend-specific retries (e.g., VM preemption in PAPI); * general retries ; * Backend-specific and general retries have separate retry counters, which are being stored in `JOB_KEY_VALUE_ENTRY` in; the end of the job execution attempt, and pre-fetched from the table in the beginning of the next attempt. The retry logic is shown on the sequence diagram below with the example of PAPIv2 backend and VM preemption as an example ; of backend-specific retry reason. ![Job retry logic (example with PAPIv2 and VM preemption)](Workflow_job_retry_logic_(example_with_PAPIv2_and_VM_preemption).png); ",MatchSource.DOCS,docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md:760,Testability,log,logic,760,"# Workflow jobs: retry decision logic. * **Word count:** 132. ## Key concepts. * Workflow may consist of multiple jobs.; * Job ""retry"" actually means creating a new job.; * Job may have attributes which are being stored in the `JOB_KEY_VALUE_ENTRY` table in the database, identified by a ; `ScopedKey`, which comprises workflow id, call fully qualified name, job index, job attempt number, and attribute name.; * There are 2 types of retries:; * backend-specific retries (e.g., VM preemption in PAPI); * general retries ; * Backend-specific and general retries have separate retry counters, which are being stored in `JOB_KEY_VALUE_ENTRY` in; the end of the job execution attempt, and pre-fetched from the table in the beginning of the next attempt. The retry logic is shown on the sequence diagram below with the example of PAPIv2 backend and VM preemption as an example ; of backend-specific retry reason. ![Job retry logic (example with PAPIv2 and VM preemption)](Workflow_job_retry_logic_(example_with_PAPIv2_and_VM_preemption).png); ",MatchSource.DOCS,docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md:920,Testability,log,logic,920,"# Workflow jobs: retry decision logic. * **Word count:** 132. ## Key concepts. * Workflow may consist of multiple jobs.; * Job ""retry"" actually means creating a new job.; * Job may have attributes which are being stored in the `JOB_KEY_VALUE_ENTRY` table in the database, identified by a ; `ScopedKey`, which comprises workflow id, call fully qualified name, job index, job attempt number, and attribute name.; * There are 2 types of retries:; * backend-specific retries (e.g., VM preemption in PAPI); * general retries ; * Backend-specific and general retries have separate retry counters, which are being stored in `JOB_KEY_VALUE_ENTRY` in; the end of the job execution attempt, and pre-fetched from the table in the beginning of the next attempt. The retry logic is shown on the sequence diagram below with the example of PAPIv2 backend and VM preemption as an example ; of backend-specific retry reason. ![Job retry logic (example with PAPIv2 and VM preemption)](Workflow_job_retry_logic_(example_with_PAPIv2_and_VM_preemption).png); ",MatchSource.DOCS,docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/jobRetryLogic/jobRetryLogic.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:572,Availability,echo,echo,572,"# Workflow Execution: Execution Store and Value Store Examples. ## Introduction. This page provides run-throughs to give insight into how the; [Execution Store](executionStore.md) and [Value Store](valueStore.md) work in; practice in some example situations. . ## Handling a single task call. To begin, consider this simple workflow. It has a single task call whose; result is exposed as an output String:. ```wdl; version 1.0. workflow single_task_workflow {; call single_task; ; output {; String string_out = single_task.string_out; }; }. task single_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. The **Execution Store** will keep track of statuses as the workflow runs:. | | `single_task` | `string_out` |; |---|---|---|; |1|`NotStarted`|`NotStarted`|; |2|`QueuedInCromwell`|`NotStarted`|; |3|`Starting`|`NotStarted`|; |4|`Running`|`NotStarted`|; |5|`Done`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Executi",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:1579,Availability,avail,available,1579,"hello; }; output {; String string_out = ""hello""; }; }; ```. The **Execution Store** will keep track of statuses as the workflow runs:. | | `single_task` | `string_out` |; |---|---|---|; |1|`NotStarted`|`NotStarted`|; |2|`QueuedInCromwell`|`NotStarted`|; |3|`Starting`|`NotStarted`|; |4|`Running`|`NotStarted`|; |5|`Done`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Execution Store cannot tell ahead of time how many; `JobKey`s it will need to represent all of the shards in the scatter. It can get around; this problem by putting a placeholder `JobKey` for the scatter node in the Execution Store. When; the scatter key is evaluated, it expands the Execution Store to include new `JobKey`s representing; every shard in the scatter. As with the single task example, the Value Store starts empty, and is updated with the results of each; shard only as and when they are generated. To see that in action, Consider this workflow:. ```wdl; version 1.0. w",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:2769,Availability,echo,echo,2769,"; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Execution Store cannot tell ahead of time how many; `JobKey`s it will need to represent all of the shards in the scatter. It can get around; this problem by putting a placeholder `JobKey` for the scatter node in the Execution Store. When; the scatter key is evaluated, it expands the Execution Store to include new `JobKey`s representing; every shard in the scatter. As with the single task example, the Value Store starts empty, and is updated with the results of each; shard only as and when they are generated. To see that in action, Consider this workflow:. ```wdl; version 1.0. workflow scattered_task_workflow {; scatter (x in range(2)) {; call scattered_task; }; output {; Int results_count = length(scattered_task.string_out); }; }. task scattered_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. #### Scatter Expansion. As the workflow starts, the execution store has three entries. An `x` represents the array-input for the scatter,; a`ScatterNode` represents the placeholder for expanding the scatter, and a `results_count` represents the workflow output. . The start of workflow execution looks like this:. | | `x` | `ScatterNode` | `results_count` |; |---|---|---|---|; |1|`NotStarted`|`NotStarted`| `NotStarted` |; |2|`Running`|`NotStarted`| `NotStarted` | ; |3|`Done`|`NotStarted`| `NotStarted` | . Once `x` is evaluated the value store gains an entry:; ```json; {; ""x"": ""[0, 1]""; }; ```. The scatter node now becomes runnable because its upstream dependency (`x`) is `Done` in the Execution Store. The evaluation of `ScatterNode` updates the execution store in a number of ways:. * One call key for each index of `scattered_task` is added.; * The `scattered_task` gets an un-indexed key too. This key is used to mark when all of the shards",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:1239,Deployability,update,updated,1239,"in, consider this simple workflow. It has a single task call whose; result is exposed as an output String:. ```wdl; version 1.0. workflow single_task_workflow {; call single_task; ; output {; String string_out = single_task.string_out; }; }. task single_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. The **Execution Store** will keep track of statuses as the workflow runs:. | | `single_task` | `string_out` |; |---|---|---|; |1|`NotStarted`|`NotStarted`|; |2|`QueuedInCromwell`|`NotStarted`|; |3|`Starting`|`NotStarted`|; |4|`Running`|`NotStarted`|; |5|`Done`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Execution Store cannot tell ahead of time how many; `JobKey`s it will need to represent all of the shards in the scatter. It can get around; this problem by putting a placeholder `JobKey` for the scatter node in the Execution Store. When; the scatter key is evaluated, it expands the Execution Store to in",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:1310,Deployability,update,updated,1310,"in, consider this simple workflow. It has a single task call whose; result is exposed as an output String:. ```wdl; version 1.0. workflow single_task_workflow {; call single_task; ; output {; String string_out = single_task.string_out; }; }. task single_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. The **Execution Store** will keep track of statuses as the workflow runs:. | | `single_task` | `string_out` |; |---|---|---|; |1|`NotStarted`|`NotStarted`|; |2|`QueuedInCromwell`|`NotStarted`|; |3|`Starting`|`NotStarted`|; |4|`Running`|`NotStarted`|; |5|`Done`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Execution Store cannot tell ahead of time how many; `JobKey`s it will need to represent all of the shards in the scatter. It can get around; this problem by putting a placeholder `JobKey` for the scatter node in the Execution Store. When; the scatter key is evaluated, it expands the Execution Store to in",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:1708,Deployability,update,updated,1708," |; |---|---|---|; |1|`NotStarted`|`NotStarted`|; |2|`QueuedInCromwell`|`NotStarted`|; |3|`Starting`|`NotStarted`|; |4|`Running`|`NotStarted`|; |5|`Done`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Execution Store cannot tell ahead of time how many; `JobKey`s it will need to represent all of the shards in the scatter. It can get around; this problem by putting a placeholder `JobKey` for the scatter node in the Execution Store. When; the scatter key is evaluated, it expands the Execution Store to include new `JobKey`s representing; every shard in the scatter. As with the single task example, the Value Store starts empty, and is updated with the results of each; shard only as and when they are generated. To see that in action, Consider this workflow:. ```wdl; version 1.0. workflow scattered_task_workflow {; scatter (x in range(2)) {; call scattered_task; }; output {; Int results_count = length(scattered_task.string_out); }; }. task scatt",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:2431,Deployability,update,updated,2431,"ll can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Execution Store cannot tell ahead of time how many; `JobKey`s it will need to represent all of the shards in the scatter. It can get around; this problem by putting a placeholder `JobKey` for the scatter node in the Execution Store. When; the scatter key is evaluated, it expands the Execution Store to include new `JobKey`s representing; every shard in the scatter. As with the single task example, the Value Store starts empty, and is updated with the results of each; shard only as and when they are generated. To see that in action, Consider this workflow:. ```wdl; version 1.0. workflow scattered_task_workflow {; scatter (x in range(2)) {; call scattered_task; }; output {; Int results_count = length(scattered_task.string_out); }; }. task scattered_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. #### Scatter Expansion. As the workflow starts, the execution store has three entries. An `x` represents the array-input for the scatter,; a`ScatterNode` represents the placeholder for expanding the scatter, and a `results_count` represents the workflow output. . The start of workflow execution looks like this:. | | `x` | `ScatterNode` | `results_count` |; |---|---|---|---|; |1|`NotStarted`|`NotStarted`| `NotStarted` |; |2|`Running`|`NotStarted`| `NotStarted` | ; |3|`Done`|`NotStarted`| `NotStarted` | . Once `x` is evaluated the value store gains an entry:; ```json; {; ""x"": ""[0, 1]""; }; ```. T",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:3575,Deployability,update,updates,3575,"ow scattered_task_workflow {; scatter (x in range(2)) {; call scattered_task; }; output {; Int results_count = length(scattered_task.string_out); }; }. task scattered_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. #### Scatter Expansion. As the workflow starts, the execution store has three entries. An `x` represents the array-input for the scatter,; a`ScatterNode` represents the placeholder for expanding the scatter, and a `results_count` represents the workflow output. . The start of workflow execution looks like this:. | | `x` | `ScatterNode` | `results_count` |; |---|---|---|---|; |1|`NotStarted`|`NotStarted`| `NotStarted` |; |2|`Running`|`NotStarted`| `NotStarted` | ; |3|`Done`|`NotStarted`| `NotStarted` | . Once `x` is evaluated the value store gains an entry:; ```json; {; ""x"": ""[0, 1]""; }; ```. The scatter node now becomes runnable because its upstream dependency (`x`) is `Done` in the Execution Store. The evaluation of `ScatterNode` updates the execution store in a number of ways:. * One call key for each index of `scattered_task` is added.; * The `scattered_task` gets an un-indexed key too. This key is used to mark when all of the shards of the call are complete.; * The gathered value `scattered_task.string_out` represents the ""gathered"" results of the task's output. It only runs ; once the un-indexed `scattered_task` key is Done and gathers output values into an array.; This gather key also acts as the upstream dependency of the `results_count` output expression.; * The `ScatterNode` is marked as `Done` so that it doesn't get triggered to run again. Following the scatter-expansion evaluation of `ScatterNode`, the Execution Store looks like this:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`. The Value Store is not c",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:4824,Deployability,update,updates,4824,"he gathered value `scattered_task.string_out` represents the ""gathered"" results of the task's output. It only runs ; once the un-indexed `scattered_task` key is Done and gathers output values into an array.; This gather key also acts as the upstream dependency of the `results_count` output expression.; * The `ScatterNode` is marked as `Done` so that it doesn't get triggered to run again. Following the scatter-expansion evaluation of `ScatterNode`, the Execution Store looks like this:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`. The Value Store is not changed at this time because no new values have been generated. #### Parallel Shard Execution. The two scattered shards are now immediately runnable because they have no upsteam dependencies.; As the two jobs are run, the Execution Store map updates to track their statuses:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`; |5|`Done`|`Done`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`; |6|`Done`|`Done`|`QueuedInCromwell`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`; |7|`Done`|`Done`|`Starting`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`; |8|`Done`|`Done`|`Starting`|`Starting`|`NotStarted`|`NotStarted`|`NotStarted`; |9|`Done`|`Done`|`Running`|`Starting`|`NotStarted`|`NotStarted`|`NotStarted`; |10|`Done`|`Done`|`Running`|`Running`|`NotStarted`|`NotStarted`|`NotStarted`; |11|`Done`|`Done`|`Running`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`; |12|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`. As the results for each shard come in, the value s",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:5828,Deployability,update,updated,5828,"ution Store map updates to track their statuses:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`; |5|`Done`|`Done`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`; |6|`Done`|`Done`|`QueuedInCromwell`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`; |7|`Done`|`Done`|`Starting`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`; |8|`Done`|`Done`|`Starting`|`Starting`|`NotStarted`|`NotStarted`|`NotStarted`; |9|`Done`|`Done`|`Running`|`Starting`|`NotStarted`|`NotStarted`|`NotStarted`; |10|`Done`|`Done`|`Running`|`Running`|`NotStarted`|`NotStarted`|`NotStarted`; |11|`Done`|`Done`|`Running`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`; |12|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`. As the results for each shard come in, the value store is also updated to include them:. At step 11 (shard 1 has finished but shard 0 has not):; ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:1"": ""hello""; }; ```. At step 12:; ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello""; }; ```. #### Scatter Completion and Gathering. Once all of the sharded keys for `scattered_task` are complete, the un-indexed marker key for that call becomes; runnable. And once the marker is complete, the gather key for the output also becomes runnable. The progression in the Execution Store goes like:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |12|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`; |13|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`; |14|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`. As the gather node comple",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:6849,Deployability,update,updated,6849," At step 11 (shard 1 has finished but shard 0 has not):; ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:1"": ""hello""; }; ```. At step 12:; ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello""; }; ```. #### Scatter Completion and Gathering. Once all of the sharded keys for `scattered_task` are complete, the un-indexed marker key for that call becomes; runnable. And once the marker is complete, the gather key for the output also becomes runnable. The progression in the Execution Store goes like:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |12|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`; |13|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`; |14|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`. As the gather node completes in step 14, the value store is also updated to contain the unindexed, gathered result of ; the `scattered_task.string_out` output:. ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello"",; ""scattered_task.string_out"": [""hello"", ""hello""]; }; ```. When the `scattered_task.string_out` gather node completes, the upstream dependencies of the `results_count` output are; finally satisfied and it becomes runnable too. It runs to produce the workflow outputs:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |14|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`; |15|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`Running`; |16|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`. For step 16, completion of the output evaluation creates an entry in the Value Store which can be exposed as a workflow output as the ; workflow completes:. ```json; {; ""x"": ""[0, 1]"",; ""scat",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:3492,Integrability,depend,dependency,3492," they are generated. To see that in action, Consider this workflow:. ```wdl; version 1.0. workflow scattered_task_workflow {; scatter (x in range(2)) {; call scattered_task; }; output {; Int results_count = length(scattered_task.string_out); }; }. task scattered_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. #### Scatter Expansion. As the workflow starts, the execution store has three entries. An `x` represents the array-input for the scatter,; a`ScatterNode` represents the placeholder for expanding the scatter, and a `results_count` represents the workflow output. . The start of workflow execution looks like this:. | | `x` | `ScatterNode` | `results_count` |; |---|---|---|---|; |1|`NotStarted`|`NotStarted`| `NotStarted` |; |2|`Running`|`NotStarted`| `NotStarted` | ; |3|`Done`|`NotStarted`| `NotStarted` | . Once `x` is evaluated the value store gains an entry:; ```json; {; ""x"": ""[0, 1]""; }; ```. The scatter node now becomes runnable because its upstream dependency (`x`) is `Done` in the Execution Store. The evaluation of `ScatterNode` updates the execution store in a number of ways:. * One call key for each index of `scattered_task` is added.; * The `scattered_task` gets an un-indexed key too. This key is used to mark when all of the shards of the call are complete.; * The gathered value `scattered_task.string_out` represents the ""gathered"" results of the task's output. It only runs ; once the un-indexed `scattered_task` key is Done and gathers output values into an array.; This gather key also acts as the upstream dependency of the `results_count` output expression.; * The `ScatterNode` is marked as `Done` so that it doesn't get triggered to run again. Following the scatter-expansion evaluation of `ScatterNode`, the Execution Store looks like this:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:4065,Integrability,depend,dependency,4065,"sents the workflow output. . The start of workflow execution looks like this:. | | `x` | `ScatterNode` | `results_count` |; |---|---|---|---|; |1|`NotStarted`|`NotStarted`| `NotStarted` |; |2|`Running`|`NotStarted`| `NotStarted` | ; |3|`Done`|`NotStarted`| `NotStarted` | . Once `x` is evaluated the value store gains an entry:; ```json; {; ""x"": ""[0, 1]""; }; ```. The scatter node now becomes runnable because its upstream dependency (`x`) is `Done` in the Execution Store. The evaluation of `ScatterNode` updates the execution store in a number of ways:. * One call key for each index of `scattered_task` is added.; * The `scattered_task` gets an un-indexed key too. This key is used to mark when all of the shards of the call are complete.; * The gathered value `scattered_task.string_out` represents the ""gathered"" results of the task's output. It only runs ; once the un-indexed `scattered_task` key is Done and gathers output values into an array.; This gather key also acts as the upstream dependency of the `results_count` output expression.; * The `ScatterNode` is marked as `Done` so that it doesn't get triggered to run again. Following the scatter-expansion evaluation of `ScatterNode`, the Execution Store looks like this:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`. The Value Store is not changed at this time because no new values have been generated. #### Parallel Shard Execution. The two scattered shards are now immediately runnable because they have no upsteam dependencies.; As the two jobs are run, the Execution Store map updates to track their statuses:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarte",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:4760,Integrability,depend,dependencies,4760,"xed key too. This key is used to mark when all of the shards of the call are complete.; * The gathered value `scattered_task.string_out` represents the ""gathered"" results of the task's output. It only runs ; once the un-indexed `scattered_task` key is Done and gathers output values into an array.; This gather key also acts as the upstream dependency of the `results_count` output expression.; * The `ScatterNode` is marked as `Done` so that it doesn't get triggered to run again. Following the scatter-expansion evaluation of `ScatterNode`, the Execution Store looks like this:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`. The Value Store is not changed at this time because no new values have been generated. #### Parallel Shard Execution. The two scattered shards are now immediately runnable because they have no upsteam dependencies.; As the two jobs are run, the Execution Store map updates to track their statuses:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |4|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`; |5|`Done`|`Done`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`|`NotStarted`; |6|`Done`|`Done`|`QueuedInCromwell`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`; |7|`Done`|`Done`|`Starting`|`QueuedInCromwell`|`NotStarted`|`NotStarted`|`NotStarted`; |8|`Done`|`Done`|`Starting`|`Starting`|`NotStarted`|`NotStarted`|`NotStarted`; |9|`Done`|`Done`|`Running`|`Starting`|`NotStarted`|`NotStarted`|`NotStarted`; |10|`Done`|`Done`|`Running`|`Running`|`NotStarted`|`NotStarted`|`NotStarted`; |11|`Done`|`Done`|`Running`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`; |12|`Done`|`Done`|`Done`|`Done`|",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:7185,Integrability,depend,dependencies,7185," {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello""; }; ```. #### Scatter Completion and Gathering. Once all of the sharded keys for `scattered_task` are complete, the un-indexed marker key for that call becomes; runnable. And once the marker is complete, the gather key for the output also becomes runnable. The progression in the Execution Store goes like:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |12|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`; |13|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`; |14|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`. As the gather node completes in step 14, the value store is also updated to contain the unindexed, gathered result of ; the `scattered_task.string_out` output:. ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello"",; ""scattered_task.string_out"": [""hello"", ""hello""]; }; ```. When the `scattered_task.string_out` gather node completes, the upstream dependencies of the `results_count` output are; finally satisfied and it becomes runnable too. It runs to produce the workflow outputs:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |14|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`; |15|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`Running`; |16|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`. For step 16, completion of the output evaluation creates an entry in the Value Store which can be exposed as a workflow output as the ; workflow completes:. ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello"",; ""scattered_task.string_out"": [""hello"", ""hello""],; ""results_count"": 2; }; ```",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:377,Security,expose,exposed,377,"# Workflow Execution: Execution Store and Value Store Examples. ## Introduction. This page provides run-throughs to give insight into how the; [Execution Store](executionStore.md) and [Value Store](valueStore.md) work in; practice in some example situations. . ## Handling a single task call. To begin, consider this simple workflow. It has a single task call whose; result is exposed as an output String:. ```wdl; version 1.0. workflow single_task_workflow {; call single_task; ; output {; String string_out = single_task.string_out; }; }. task single_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. The **Execution Store** will keep track of statuses as the workflow runs:. | | `single_task` | `string_out` |; |---|---|---|; |1|`NotStarted`|`NotStarted`|; |2|`QueuedInCromwell`|`NotStarted`|; |3|`Starting`|`NotStarted`|; |4|`Running`|`NotStarted`|; |5|`Done`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Executi",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:7762,Security,expose,exposed,7762," {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello""; }; ```. #### Scatter Completion and Gathering. Once all of the sharded keys for `scattered_task` are complete, the un-indexed marker key for that call becomes; runnable. And once the marker is complete, the gather key for the output also becomes runnable. The progression in the Execution Store goes like:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |12|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`|`NotStarted`; |13|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`|`NotStarted`; |14|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`. As the gather node completes in step 14, the value store is also updated to contain the unindexed, gathered result of ; the `scattered_task.string_out` output:. ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello"",; ""scattered_task.string_out"": [""hello"", ""hello""]; }; ```. When the `scattered_task.string_out` gather node completes, the upstream dependencies of the `results_count` output are; finally satisfied and it becomes runnable too. It runs to produce the workflow outputs:. | | `x` | `ScatterNode` | `scattered_task:0` | `scattered_task:1` | `scattered_task` | `scattered_task.string_out` | `results_count` |; |---|---|---|---|---|---|---|---|; |14|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`NotStarted`; |15|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`Running`; |16|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`|`Done`. For step 16, completion of the output evaluation creates an entry in the Value Store which can be exposed as a workflow output as the ; workflow completes:. ```json; {; ""x"": ""[0, 1]"",; ""scattered_task.string_out:0"": ""hello"",; ""scattered_task.string_out:1"": ""hello"",; ""scattered_task.string_out"": [""hello"", ""hello""],; ""results_count"": 2; }; ```",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:1925,Testability,log,logic,1925,"one`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Execution Store cannot tell ahead of time how many; `JobKey`s it will need to represent all of the shards in the scatter. It can get around; this problem by putting a placeholder `JobKey` for the scatter node in the Execution Store. When; the scatter key is evaluated, it expands the Execution Store to include new `JobKey`s representing; every shard in the scatter. As with the single task example, the Value Store starts empty, and is updated with the results of each; shard only as and when they are generated. To see that in action, Consider this workflow:. ```wdl; version 1.0. workflow scattered_task_workflow {; scatter (x in range(2)) {; call scattered_task; }; output {; Int results_count = length(scattered_task.string_out); }; }. task scattered_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. #### Scatter Expansion. As the workflow starts, the executi",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md:317,Usability,simpl,simple,317,"# Workflow Execution: Execution Store and Value Store Examples. ## Introduction. This page provides run-throughs to give insight into how the; [Execution Store](executionStore.md) and [Value Store](valueStore.md) work in; practice in some example situations. . ## Handling a single task call. To begin, consider this simple workflow. It has a single task call whose; result is exposed as an output String:. ```wdl; version 1.0. workflow single_task_workflow {; call single_task; ; output {; String string_out = single_task.string_out; }; }. task single_task {; command {; echo hello; }; output {; String string_out = ""hello""; }; }; ```. The **Execution Store** will keep track of statuses as the workflow runs:. | | `single_task` | `string_out` |; |---|---|---|; |1|`NotStarted`|`NotStarted`|; |2|`QueuedInCromwell`|`NotStarted`|; |3|`Starting`|`NotStarted`|; |4|`Running`|`NotStarted`|; |5|`Done`|`NotStarted`|; |6|`Done`|`Running`|; |7|`Done`|`Done`|. In step 1, the workflow has just started and the ExecutionStore is created in its initial; state. The Value Store doesn't track statuses and so begins empty: `{ }`. In steps 2-4, the Execution Store tracks the `single_task` job as the engine is executing it. As the Execution Store is updated to indicate task completion is step 5, the Value Store is also updated to; include the output value of the task:; ```json; {; ""single_task.string_out"": ""hello""; }; ```. By step 6, Cromwell can use the fact that the task is complete to decide that the output node is ready to be; evaluated. And the input to the output expression is available for lookup in the Value Store. In step 7, all workflow nodes have run and the workflow is complete. The Value Store is updated once again to; additionally contain the output node value:; ```json; {; ""single_task.string_out"": ""hello"",; ""string_out"": ""hello""; }; ``` . Cromwell can use this information to trigger the ""workflow complete"" logic. ## Handling scatters. When Cromwell runs scattered tasks, the Executi",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionStore.md:652,Integrability,depend,dependencies,652,"# Workflow Execution: The Execution Store. ## Purpose. The Execution Store is a data structure owned and maintained inside each ; `WorkflowExecutionActor` (see [Major Actors](majorActors.md)). Its purpose is to hold the current _status_ of what the workflow is doing, ; what it has done, and what it has left to do. If the WOM graph holds a static; representation of the workflow which doesn't change as the workflow is run, the; Execution Store holds the dynamic status of each node in the graph at any given moment; in time. The Execution Store is also used to answer questions like ""which nodes in the; workflow graph are ready to run because their dependencies are satisfied?"". **Note:** The Execution Store does **not** hold the values which are generated by; running the various nodes in the WOM graph. That is the domain of the [Value Store](valueStore.md). . ## Data Structure. The Execution Store is a mapping from WOM nodes (and a shard index, if necessary); to the execution status of those nodes in the actual workflow. ## Examples. Some worked through examples of how the Execution Store and Value Store change as workflows progress; are given on the [Execution and Value Store Examples](executionAndValueStoreExamples.md) page.",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/executionStore.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/executionStore.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md:378,Modifiability,config,configured,378,"# Workflow Execution: Major Actors. * **Word Count:** 245. ## Major Actor Hierarchy. At the highest level, these are the main actors involved in workflow execution. ![high level overview diagram](WorkflowExecutionHighLevelOverview.png). ## Actors and their Purposes. ### WorkflowManagerActor. The `WorkflowManagerActor` is responsible for:. * Polling the `WorkflowStore` at pre-configured intervals.; * Starting new workflows; * Tracking, supervising and aborting running workflows; * Parent actor for all `WorkflowActor`s. ### WorkflowActor(s). The `WorkflowActor` is responsible for:; ; * Co-ordinating the stages of a workflow lifecycle from parsing through to finalization.; * Parent actor of the `WorkflowExecutionActor` which runs the workflow's jobs. ### WorkflowExecutionActor(s). The `WorkflowExecutionActor` is responsible for:. * Starting jobs and sub-workflows as soon as they are able to run.; * Based on values in the (in-memory) ValueStore and ExecutionStore objects.; * Parent actor for all `EngineJobExecutionActor`s and `SubWorkflowExecutionActor`s. ### EngineJobExecutionActor(s). Each `EngineJobExecutionActor` (EJEA) is responsible for:. * Running a single job.; * A ""job"" is a command line instruction to run on a backend.; * Multiple shards for a single call each get their own EJEA.; * Multiple attempts to run the same job operate within the same EJEA; * Respects hog-limiting; * Checks the call cache and job store to avoid running the job if it doesn't have to.; * Triggers job initialization, execution and finalization at appropriate times. ### SubWorkflowExecutionActor(s). Each `SubWorkflowExecutionActor` is responsible for:. * Running a single sub-workflow.; * Parent actor for a new `WorkflowExecutionActor` (see above) created to run the sub-workflow. ## Major Actor Hierarchy (in context). The above diagram omitted a lot of details. This diagram attempts to show a little more of the; context:. ![high level overview in context diagram](WorkflowExecutionHighLevelO",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/majorActors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md:1421,Performance,cache,cache,1421,"e the main actors involved in workflow execution. ![high level overview diagram](WorkflowExecutionHighLevelOverview.png). ## Actors and their Purposes. ### WorkflowManagerActor. The `WorkflowManagerActor` is responsible for:. * Polling the `WorkflowStore` at pre-configured intervals.; * Starting new workflows; * Tracking, supervising and aborting running workflows; * Parent actor for all `WorkflowActor`s. ### WorkflowActor(s). The `WorkflowActor` is responsible for:; ; * Co-ordinating the stages of a workflow lifecycle from parsing through to finalization.; * Parent actor of the `WorkflowExecutionActor` which runs the workflow's jobs. ### WorkflowExecutionActor(s). The `WorkflowExecutionActor` is responsible for:. * Starting jobs and sub-workflows as soon as they are able to run.; * Based on values in the (in-memory) ValueStore and ExecutionStore objects.; * Parent actor for all `EngineJobExecutionActor`s and `SubWorkflowExecutionActor`s. ### EngineJobExecutionActor(s). Each `EngineJobExecutionActor` (EJEA) is responsible for:. * Running a single job.; * A ""job"" is a command line instruction to run on a backend.; * Multiple shards for a single call each get their own EJEA.; * Multiple attempts to run the same job operate within the same EJEA; * Respects hog-limiting; * Checks the call cache and job store to avoid running the job if it doesn't have to.; * Triggers job initialization, execution and finalization at appropriate times. ### SubWorkflowExecutionActor(s). Each `SubWorkflowExecutionActor` is responsible for:. * Running a single sub-workflow.; * Parent actor for a new `WorkflowExecutionActor` (see above) created to run the sub-workflow. ## Major Actor Hierarchy (in context). The above diagram omitted a lot of details. This diagram attempts to show a little more of the; context:. ![high level overview in context diagram](WorkflowExecutionHighLevelOverviewInContext.png). ## See Also . * EngineJobExecutionActor (**TODO**); * Backend Execution Actors (**TODO**); ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/majorActors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md:455,Safety,abort,aborting,455,"# Workflow Execution: Major Actors. * **Word Count:** 245. ## Major Actor Hierarchy. At the highest level, these are the main actors involved in workflow execution. ![high level overview diagram](WorkflowExecutionHighLevelOverview.png). ## Actors and their Purposes. ### WorkflowManagerActor. The `WorkflowManagerActor` is responsible for:. * Polling the `WorkflowStore` at pre-configured intervals.; * Starting new workflows; * Tracking, supervising and aborting running workflows; * Parent actor for all `WorkflowActor`s. ### WorkflowActor(s). The `WorkflowActor` is responsible for:; ; * Co-ordinating the stages of a workflow lifecycle from parsing through to finalization.; * Parent actor of the `WorkflowExecutionActor` which runs the workflow's jobs. ### WorkflowExecutionActor(s). The `WorkflowExecutionActor` is responsible for:. * Starting jobs and sub-workflows as soon as they are able to run.; * Based on values in the (in-memory) ValueStore and ExecutionStore objects.; * Parent actor for all `EngineJobExecutionActor`s and `SubWorkflowExecutionActor`s. ### EngineJobExecutionActor(s). Each `EngineJobExecutionActor` (EJEA) is responsible for:. * Running a single job.; * A ""job"" is a command line instruction to run on a backend.; * Multiple shards for a single call each get their own EJEA.; * Multiple attempts to run the same job operate within the same EJEA; * Respects hog-limiting; * Checks the call cache and job store to avoid running the job if it doesn't have to.; * Triggers job initialization, execution and finalization at appropriate times. ### SubWorkflowExecutionActor(s). Each `SubWorkflowExecutionActor` is responsible for:. * Running a single sub-workflow.; * Parent actor for a new `WorkflowExecutionActor` (see above) created to run the sub-workflow. ## Major Actor Hierarchy (in context). The above diagram omitted a lot of details. This diagram attempts to show a little more of the; context:. ![high level overview in context diagram](WorkflowExecutionHighLevelO",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/majorActors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md:1444,Safety,avoid,avoid,1444,"e the main actors involved in workflow execution. ![high level overview diagram](WorkflowExecutionHighLevelOverview.png). ## Actors and their Purposes. ### WorkflowManagerActor. The `WorkflowManagerActor` is responsible for:. * Polling the `WorkflowStore` at pre-configured intervals.; * Starting new workflows; * Tracking, supervising and aborting running workflows; * Parent actor for all `WorkflowActor`s. ### WorkflowActor(s). The `WorkflowActor` is responsible for:; ; * Co-ordinating the stages of a workflow lifecycle from parsing through to finalization.; * Parent actor of the `WorkflowExecutionActor` which runs the workflow's jobs. ### WorkflowExecutionActor(s). The `WorkflowExecutionActor` is responsible for:. * Starting jobs and sub-workflows as soon as they are able to run.; * Based on values in the (in-memory) ValueStore and ExecutionStore objects.; * Parent actor for all `EngineJobExecutionActor`s and `SubWorkflowExecutionActor`s. ### EngineJobExecutionActor(s). Each `EngineJobExecutionActor` (EJEA) is responsible for:. * Running a single job.; * A ""job"" is a command line instruction to run on a backend.; * Multiple shards for a single call each get their own EJEA.; * Multiple attempts to run the same job operate within the same EJEA; * Respects hog-limiting; * Checks the call cache and job store to avoid running the job if it doesn't have to.; * Triggers job initialization, execution and finalization at appropriate times. ### SubWorkflowExecutionActor(s). Each `SubWorkflowExecutionActor` is responsible for:. * Running a single sub-workflow.; * Parent actor for a new `WorkflowExecutionActor` (see above) created to run the sub-workflow. ## Major Actor Hierarchy (in context). The above diagram omitted a lot of details. This diagram attempts to show a little more of the; context:. ![high level overview in context diagram](WorkflowExecutionHighLevelOverviewInContext.png). ## See Also . * EngineJobExecutionActor (**TODO**); * Backend Execution Actors (**TODO**); ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/majorActors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/majorActors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/valueStore.md:636,Availability,down,downstream,636,"# Workflow Execution: The Value Store. ## Purpose. The Value Store is a data structure owned and maintained inside each ; `WorkflowExecutionActor` (see [Major Actors](majorActors.md)). Its purpose is to hold the set of _values_ produced by the workflow so far. If; the WOM graph holds a static representation of the workflow which doesn't change; as the workflow is run, the Value Store records the values assigned to every ; task output and value definition evaluated so far during workflow execution. **Note:** The Value Store does **not** hold the execution status of the various ; nodes in the WOM graph. Nor does it determine when downstream nodes are ready; to run. That is the domain of the [Execution Store](executionStore.md). . ## Data Structure. The Value Store data structure is a mapping from output ports on WOM `GraphNode`s ; (and shard index if necessary) to the appropriate `WomValue`. . ## Examples. Some worked through examples of how the Execution Store and Value Store change as workflows progress; are given on the [Execution and Value Store Examples](executionAndValueStoreExamples.md) page.",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/valueStore.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/valueStore.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md:227,Availability,avail,available,227,"### WDL Expression Evaluation. #### Expressions in WOM. Expressions in WOM expose the following methods:. * List the names of inputs which the expression will need in order to evaluate.; * List the files which would need to be available in order to evaluate.; * Evaluate the type of value which the expression will evaluate to.; * Evaluate the expression. . #### How WDL expressions become WomExpressions. Relating back to the [WDL parsing](../workflowParsing/wdlParsingOverview.md) process:. * WDL 1.0 engine functions become contextless WDLOM `ExpressionElement`s during the transliteration phase; * Reference resolution between WDLOM elements occurs during the linking phase. ; * WDLOM `ExpressionElement`s are wrapped into `WdlomWomExpression`s during the graph building phase. During the graph construction phase static expression elements are mixed together with evaluation functions; to produce the final WOM expressions, and any differences between language versions are baked in. . #### Where evaluation functions are coded. The four evaluators types described in ""Expressions in WOM"" are coded in four package objects for each version. They live in:. * WDL `version 1.0`: [`wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression); * WDL `version development`: [`wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/biscayne/src/test/scala/wdl/transforms/biscayne/linking/expression). These evaluators are really just long pattern matches from various WDLOM element types to the relevant evaluator for that element. Because the evaluations are largely the same for now between WDL versions, you'll notice that the imported evaluators mostly come from files ; in [`wdl.transforms.base.linking.expression`](https://github.com/broadin",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md:714,Integrability,wrap,wrapped,714,"### WDL Expression Evaluation. #### Expressions in WOM. Expressions in WOM expose the following methods:. * List the names of inputs which the expression will need in order to evaluate.; * List the files which would need to be available in order to evaluate.; * Evaluate the type of value which the expression will evaluate to.; * Evaluate the expression. . #### How WDL expressions become WomExpressions. Relating back to the [WDL parsing](../workflowParsing/wdlParsingOverview.md) process:. * WDL 1.0 engine functions become contextless WDLOM `ExpressionElement`s during the transliteration phase; * Reference resolution between WDLOM elements occurs during the linking phase. ; * WDLOM `ExpressionElement`s are wrapped into `WdlomWomExpression`s during the graph building phase. During the graph construction phase static expression elements are mixed together with evaluation functions; to produce the final WOM expressions, and any differences between language versions are baked in. . #### Where evaluation functions are coded. The four evaluators types described in ""Expressions in WOM"" are coded in four package objects for each version. They live in:. * WDL `version 1.0`: [`wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression); * WDL `version development`: [`wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/biscayne/src/test/scala/wdl/transforms/biscayne/linking/expression). These evaluators are really just long pattern matches from various WDLOM element types to the relevant evaluator for that element. Because the evaluations are largely the same for now between WDL versions, you'll notice that the imported evaluators mostly come from files ; in [`wdl.transforms.base.linking.expression`](https://github.com/broadin",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md:75,Security,expose,expose,75,"### WDL Expression Evaluation. #### Expressions in WOM. Expressions in WOM expose the following methods:. * List the names of inputs which the expression will need in order to evaluate.; * List the files which would need to be available in order to evaluate.; * Evaluate the type of value which the expression will evaluate to.; * Evaluate the expression. . #### How WDL expressions become WomExpressions. Relating back to the [WDL parsing](../workflowParsing/wdlParsingOverview.md) process:. * WDL 1.0 engine functions become contextless WDLOM `ExpressionElement`s during the transliteration phase; * Reference resolution between WDLOM elements occurs during the linking phase. ; * WDLOM `ExpressionElement`s are wrapped into `WdlomWomExpression`s during the graph building phase. During the graph construction phase static expression elements are mixed together with evaluation functions; to produce the final WOM expressions, and any differences between language versions are baked in. . #### Where evaluation functions are coded. The four evaluators types described in ""Expressions in WOM"" are coded in four package objects for each version. They live in:. * WDL `version 1.0`: [`wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression); * WDL `version development`: [`wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/biscayne/src/test/scala/wdl/transforms/biscayne/linking/expression). These evaluators are really just long pattern matches from various WDLOM element types to the relevant evaluator for that element. Because the evaluations are largely the same for now between WDL versions, you'll notice that the imported evaluators mostly come from files ; in [`wdl.transforms.base.linking.expression`](https://github.com/broadin",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md:1599,Testability,test,test,1599," phase; * Reference resolution between WDLOM elements occurs during the linking phase. ; * WDLOM `ExpressionElement`s are wrapped into `WdlomWomExpression`s during the graph building phase. During the graph construction phase static expression elements are mixed together with evaluation functions; to produce the final WOM expressions, and any differences between language versions are baked in. . #### Where evaluation functions are coded. The four evaluators types described in ""Expressions in WOM"" are coded in four package objects for each version. They live in:. * WDL `version 1.0`: [`wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/linking/expression); * WDL `version development`: [`wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/biscayne/src/test/scala/wdl/transforms/biscayne/linking/expression). These evaluators are really just long pattern matches from various WDLOM element types to the relevant evaluator for that element. Because the evaluations are largely the same for now between WDL versions, you'll notice that the imported evaluators mostly come from files ; in [`wdl.transforms.base.linking.expression`](https://github.com/broadinstitute/cromwell/tree/develop/wdl/transforms/new-base/src/main/scala/wdl/transforms/base/linking/expression). However, in WDL development, there are a number of imported functions from biscayne-specific directories.; If future WDL versions diverge more starkly from the 1.0 base, it is likely that the imports into these pattern match evaluators will come from a; wider variety of origins. #### How Evaluation Functions Build WOM Expressions. The top-level Graph construction functions for WDL 1.0 and the development version are:. * WDL `version 1.0`: [`wdl.draft3.transforms.wdlom2wom.work",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/wdlExpressionEvaluation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md:969,Availability,recover,recovering,969,"# Workflow Execution: Database Tables: Workflow, Subworkflow, and Job Stores. ## Database Tables: Workflow, Subworkflow, and Job Stores. Cromwell uses the workflow, subworkflow and job store tables to hold data related to submitted or running workflows.; Once a workflow reaches a terminal state all data for that workflow should be deleted from these tables. ### Workflow Store / `WORKFLOW_STORE_ENTRY`. `WORKFLOW_STORE_ENTRY` holds data received in a workflow submission (workflow sources, inputs, options etc.); and workflow-scoped execution data (e.g. submission time, status, fields to support; running [Horizontal Cromwell](../horicromtal.md) etc). ### Job Store / `JOB_STORE_ENTRY`. `JOB_STORE_ENTRY` holds data for *completed* jobs within a workflow. Jobs that are still running or have not yet been; started will not have rows in this table. The main purpose of the job store table is to support resuming execution of; a workflow when Cromwell is restarted by recovering the outputs of completed jobs. This table is closely related to; `JOB_STORE_SIMPLETON_ENTRY` which holds the [simpleton](../general/simpletons.md) values comprising a job's outputs,; and loosely related to the [job key/value store (`JOB_KEY_VALUE_ENTRY`)](jobKeyValueStore.md) which holds other; job-scoped data important in recovering jobs on Cromwell restart. ### Subworkflow Store / `SUB_WORKFLOW_STORE_ENTRY`. `SUB_WORKFLOW_STORE_ENTRY` holds data for subworkflows that have begun execution. The rows in this table persist the fact; that particular subworkflows corresponding to a call FQN and index were started and assigned a workflow ID.; The completed jobs within these subworkflows will be recorded in the job store described above, linking to the; subworkflows in this table by the subworkflow's ID.; ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md:1305,Availability,recover,recovering,1305,"# Workflow Execution: Database Tables: Workflow, Subworkflow, and Job Stores. ## Database Tables: Workflow, Subworkflow, and Job Stores. Cromwell uses the workflow, subworkflow and job store tables to hold data related to submitted or running workflows.; Once a workflow reaches a terminal state all data for that workflow should be deleted from these tables. ### Workflow Store / `WORKFLOW_STORE_ENTRY`. `WORKFLOW_STORE_ENTRY` holds data received in a workflow submission (workflow sources, inputs, options etc.); and workflow-scoped execution data (e.g. submission time, status, fields to support; running [Horizontal Cromwell](../horicromtal.md) etc). ### Job Store / `JOB_STORE_ENTRY`. `JOB_STORE_ENTRY` holds data for *completed* jobs within a workflow. Jobs that are still running or have not yet been; started will not have rows in this table. The main purpose of the job store table is to support resuming execution of; a workflow when Cromwell is restarted by recovering the outputs of completed jobs. This table is closely related to; `JOB_STORE_SIMPLETON_ENTRY` which holds the [simpleton](../general/simpletons.md) values comprising a job's outputs,; and loosely related to the [job key/value store (`JOB_KEY_VALUE_ENTRY`)](jobKeyValueStore.md) which holds other; job-scoped data important in recovering jobs on Cromwell restart. ### Subworkflow Store / `SUB_WORKFLOW_STORE_ENTRY`. `SUB_WORKFLOW_STORE_ENTRY` holds data for subworkflows that have begun execution. The rows in this table persist the fact; that particular subworkflows corresponding to a call FQN and index were started and assigned a workflow ID.; The completed jobs within these subworkflows will be recorded in the job store described above, linking to the; subworkflows in this table by the subworkflow's ID.; ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md:969,Safety,recover,recovering,969,"# Workflow Execution: Database Tables: Workflow, Subworkflow, and Job Stores. ## Database Tables: Workflow, Subworkflow, and Job Stores. Cromwell uses the workflow, subworkflow and job store tables to hold data related to submitted or running workflows.; Once a workflow reaches a terminal state all data for that workflow should be deleted from these tables. ### Workflow Store / `WORKFLOW_STORE_ENTRY`. `WORKFLOW_STORE_ENTRY` holds data received in a workflow submission (workflow sources, inputs, options etc.); and workflow-scoped execution data (e.g. submission time, status, fields to support; running [Horizontal Cromwell](../horicromtal.md) etc). ### Job Store / `JOB_STORE_ENTRY`. `JOB_STORE_ENTRY` holds data for *completed* jobs within a workflow. Jobs that are still running or have not yet been; started will not have rows in this table. The main purpose of the job store table is to support resuming execution of; a workflow when Cromwell is restarted by recovering the outputs of completed jobs. This table is closely related to; `JOB_STORE_SIMPLETON_ENTRY` which holds the [simpleton](../general/simpletons.md) values comprising a job's outputs,; and loosely related to the [job key/value store (`JOB_KEY_VALUE_ENTRY`)](jobKeyValueStore.md) which holds other; job-scoped data important in recovering jobs on Cromwell restart. ### Subworkflow Store / `SUB_WORKFLOW_STORE_ENTRY`. `SUB_WORKFLOW_STORE_ENTRY` holds data for subworkflows that have begun execution. The rows in this table persist the fact; that particular subworkflows corresponding to a call FQN and index were started and assigned a workflow ID.; The completed jobs within these subworkflows will be recorded in the job store described above, linking to the; subworkflows in this table by the subworkflow's ID.; ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md:1305,Safety,recover,recovering,1305,"# Workflow Execution: Database Tables: Workflow, Subworkflow, and Job Stores. ## Database Tables: Workflow, Subworkflow, and Job Stores. Cromwell uses the workflow, subworkflow and job store tables to hold data related to submitted or running workflows.; Once a workflow reaches a terminal state all data for that workflow should be deleted from these tables. ### Workflow Store / `WORKFLOW_STORE_ENTRY`. `WORKFLOW_STORE_ENTRY` holds data received in a workflow submission (workflow sources, inputs, options etc.); and workflow-scoped execution data (e.g. submission time, status, fields to support; running [Horizontal Cromwell](../horicromtal.md) etc). ### Job Store / `JOB_STORE_ENTRY`. `JOB_STORE_ENTRY` holds data for *completed* jobs within a workflow. Jobs that are still running or have not yet been; started will not have rows in this table. The main purpose of the job store table is to support resuming execution of; a workflow when Cromwell is restarted by recovering the outputs of completed jobs. This table is closely related to; `JOB_STORE_SIMPLETON_ENTRY` which holds the [simpleton](../general/simpletons.md) values comprising a job's outputs,; and loosely related to the [job key/value store (`JOB_KEY_VALUE_ENTRY`)](jobKeyValueStore.md) which holds other; job-scoped data important in recovering jobs on Cromwell restart. ### Subworkflow Store / `SUB_WORKFLOW_STORE_ENTRY`. `SUB_WORKFLOW_STORE_ENTRY` holds data for subworkflows that have begun execution. The rows in this table persist the fact; that particular subworkflows corresponding to a call FQN and index were started and assigned a workflow ID.; The completed jobs within these subworkflows will be recorded in the job store described above, linking to the; subworkflows in this table by the subworkflow's ID.; ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md:1090,Usability,simpl,simpleton,1090,"# Workflow Execution: Database Tables: Workflow, Subworkflow, and Job Stores. ## Database Tables: Workflow, Subworkflow, and Job Stores. Cromwell uses the workflow, subworkflow and job store tables to hold data related to submitted or running workflows.; Once a workflow reaches a terminal state all data for that workflow should be deleted from these tables. ### Workflow Store / `WORKFLOW_STORE_ENTRY`. `WORKFLOW_STORE_ENTRY` holds data received in a workflow submission (workflow sources, inputs, options etc.); and workflow-scoped execution data (e.g. submission time, status, fields to support; running [Horizontal Cromwell](../horicromtal.md) etc). ### Job Store / `JOB_STORE_ENTRY`. `JOB_STORE_ENTRY` holds data for *completed* jobs within a workflow. Jobs that are still running or have not yet been; started will not have rows in this table. The main purpose of the job store table is to support resuming execution of; a workflow when Cromwell is restarted by recovering the outputs of completed jobs. This table is closely related to; `JOB_STORE_SIMPLETON_ENTRY` which holds the [simpleton](../general/simpletons.md) values comprising a job's outputs,; and loosely related to the [job key/value store (`JOB_KEY_VALUE_ENTRY`)](jobKeyValueStore.md) which holds other; job-scoped data important in recovering jobs on Cromwell restart. ### Subworkflow Store / `SUB_WORKFLOW_STORE_ENTRY`. `SUB_WORKFLOW_STORE_ENTRY` holds data for subworkflows that have begun execution. The rows in this table persist the fact; that particular subworkflows corresponding to a call FQN and index were started and assigned a workflow ID.; The completed jobs within these subworkflows will be recorded in the job store described above, linking to the; subworkflows in this table by the subworkflow's ID.; ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md:1112,Usability,simpl,simpletons,1112,"# Workflow Execution: Database Tables: Workflow, Subworkflow, and Job Stores. ## Database Tables: Workflow, Subworkflow, and Job Stores. Cromwell uses the workflow, subworkflow and job store tables to hold data related to submitted or running workflows.; Once a workflow reaches a terminal state all data for that workflow should be deleted from these tables. ### Workflow Store / `WORKFLOW_STORE_ENTRY`. `WORKFLOW_STORE_ENTRY` holds data received in a workflow submission (workflow sources, inputs, options etc.); and workflow-scoped execution data (e.g. submission time, status, fields to support; running [Horizontal Cromwell](../horicromtal.md) etc). ### Job Store / `JOB_STORE_ENTRY`. `JOB_STORE_ENTRY` holds data for *completed* jobs within a workflow. Jobs that are still running or have not yet been; started will not have rows in this table. The main purpose of the job store table is to support resuming execution of; a workflow when Cromwell is restarted by recovering the outputs of completed jobs. This table is closely related to; `JOB_STORE_SIMPLETON_ENTRY` which holds the [simpleton](../general/simpletons.md) values comprising a job's outputs,; and loosely related to the [job key/value store (`JOB_KEY_VALUE_ENTRY`)](jobKeyValueStore.md) which holds other; job-scoped data important in recovering jobs on Cromwell restart. ### Subworkflow Store / `SUB_WORKFLOW_STORE_ENTRY`. `SUB_WORKFLOW_STORE_ENTRY` holds data for subworkflows that have begun execution. The rows in this table persist the fact; that particular subworkflows corresponding to a call FQN and index were started and assigned a workflow ID.; The completed jobs within these subworkflows will be recorded in the job store described above, linking to the; subworkflows in this table by the subworkflow's ID.; ",MatchSource.DOCS,docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlParsingOverview.md:1175,Modifiability,variab,variable,1175,"### WDL Source to WOM Conversion. #### Parsing Flowchart. For the various versions of WDL supported in Cromwell, the conversion to WOM follows; these paths: . ![Parsing Flowchart](wdlmap.svg). #### Process Description. You can think of WDL parsing in Cromwell in terms of the following major steps:. 1. Lexing: Converting the raw WDL string into a one dimensional stream of ""tokens"".; 2. Parsing: Converting the stream of tokens into an abstract syntax tree (AST).; 3. Transliteration: Transforming the language-specific AST into a standard set of Scala objects; 4. Import Resolution: Recursively processing any import statements into WOM bundles.; 5. Linking: Discovering, resolving and recording all references within the AST and imports.; 6. WOM Building: Creating a set of WOM objects; 7. Input Validation: Link any provided inputs to inputs on the WOM objects. #### Intermediate Data Formats. * **WDL Object Model (WDLOM)**:; * A Scala case class representation of WDL grammar ASTs.; * **Linked inputs**:; * The original WDL source's WDLOM representation; * And WOM bundles imported; * Links from any references to their definitions; * Including custom type references, variable references, task calls, subworkflow calls ; * **WOM Bundle**:; * A set of tasks, workflows and custom types, and the fully qualified names by which they can be referenced.; * In Cromwell's WOM format (the WOM format is the ultimate destination for _all_ languages, including WDL); * **Validated WOM Namespace**:; * The conjunction of a WOM bundle with an input set.; * The entry point workflow is known.; ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/wdlParsingOverview.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlParsingOverview.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md:3724,Availability,avail,available,3724,"ment.scala):. * The conversion from AST to WDLOM relies on two other conversions, `astNodeToTypeElement` and `astNodeToExpressionElement`, ; to validate the attributes on the AST it is given, and construct WDLOM from Hermes ASTs.; * The resulting function can itself be used as a building block to construct higher-level WDLOM types. . #### How AST-to-WDLOM building blocks are chained together. To see how these building blocks are pieced together we can look at WDL 1.0's [`ast2wdlom`](https://github.com/broadinstitute/cromwell/blob/master/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/ast2wdlom/ast2wdlom.scala) package object. Notice how - and where - the `astToInputDeclarationElement` value is declared:. * It follows the `implicit val`s `astNodeToTypeElement` and `astNodeToExpressionElement` and so can use those in its processing.; * It is, as an `implicit val`, used later on by `astNodeToInputsSectionElement`, which is then used by `astNodeToWorkflowBodyElement` and `astNodeToTaskSectionElement`, and so on. . #### How different WDL versions convert to WDLOM differently. These package objects ultimately control how different WDL versions are parsed differently. These differences can be seen most easily by comparing package objects for different WDL versions:. ```bash; diff \; wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/ast2wdlom/ast2wdlom.scala \; wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/ast2wdlom/ast2wdlom.scala ; ``` . In an extreme case, these could contain fundamentally different conversion logic from AST to WDLOM for different WDL versions. Luckily, between; WDL 1.0 and the WDL `development` version, there are only a few minor changes, amongst which are:. * Importing a new Hermes parser.; * Specifying that additional functions are available when constructing the `astNodeToExpressionElement` conversion.; * Specifying that a new `Directory` type is available when constructing the `astNodeToTypeElement` conversion.; ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/wdlToWdlom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md:3842,Availability,avail,available,3842,"ment.scala):. * The conversion from AST to WDLOM relies on two other conversions, `astNodeToTypeElement` and `astNodeToExpressionElement`, ; to validate the attributes on the AST it is given, and construct WDLOM from Hermes ASTs.; * The resulting function can itself be used as a building block to construct higher-level WDLOM types. . #### How AST-to-WDLOM building blocks are chained together. To see how these building blocks are pieced together we can look at WDL 1.0's [`ast2wdlom`](https://github.com/broadinstitute/cromwell/blob/master/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/ast2wdlom/ast2wdlom.scala) package object. Notice how - and where - the `astToInputDeclarationElement` value is declared:. * It follows the `implicit val`s `astNodeToTypeElement` and `astNodeToExpressionElement` and so can use those in its processing.; * It is, as an `implicit val`, used later on by `astNodeToInputsSectionElement`, which is then used by `astNodeToWorkflowBodyElement` and `astNodeToTaskSectionElement`, and so on. . #### How different WDL versions convert to WDLOM differently. These package objects ultimately control how different WDL versions are parsed differently. These differences can be seen most easily by comparing package objects for different WDL versions:. ```bash; diff \; wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/ast2wdlom/ast2wdlom.scala \; wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/ast2wdlom/ast2wdlom.scala ; ``` . In an extreme case, these could contain fundamentally different conversion logic from AST to WDLOM for different WDL versions. Luckily, between; WDL 1.0 and the WDL `development` version, there are only a few minor changes, amongst which are:. * Importing a new Hermes parser.; * Specifying that additional functions are available when constructing the `astNodeToExpressionElement` conversion.; * Specifying that a new `Directory` type is available when constructing the `astNodeToTypeElement` conversion.; ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/wdlToWdlom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md:797,Security,hash,hashes,797,"### Converting WDL into WDLOM. To provide a concrete example, we will see how Cromwell parses the following line of WDL into WDLOM:. ```wdl; Int foo = min(100, max(1,2)); ``` . #### How Hermes interprets the WDL. Hermes can be asked to show its parse tree for a valid WDL file by running:. ```bash; $ hermes analyze grammar.hgr <WDL FILE>; ```. This allows us to see how Hermes interprets our line of WDL:. ```; (Declaration:; type=<string:5:5 type ""SW50"">,; name=<string:5:9 identifier ""Zl9yb3VuZA=="">,; expression=(FunctionCall:; name=<string:5:19 identifier ""bWlu"">,; params=[; <string:5:23 integer ""MTAw"">,; (FunctionCall:; name=<string:5:28 identifier ""bWF4"">,; params=[; <string:5:32 integer ""MQ=="">,; <string:5:34 integer ""Mg=="">; ]; ); ]; ); ); ```. In graphical form this is (with string hashes replaced by values, for convenience):. ![Hermes AST Graph](wdlToWdlom_hermes.svg). #### How WDLOM represents WDL. WDLOM tries to be a programmer-friendlier, WDL version agnostic data model to hold WDL syntax trees. It would use the following data structure to represent this declaration:. ```scala; InputDeclarationElement(; typeElement = PrimitiveTypeElement(WomIntegerType),; name = ""foo"",; expression = Min(; arg1 = PrimitiveLiteralExpressionElement(WomInteger(100)),; arg2 = Max(; arg1 = PrimitiveLiteralExpressionElement(WomInteger(1)),; arg2 = PrimitiveLiteralExpressionElement(WomInteger(2)); ); ); ); ```. Again, attempting to show this graphically:. ![WDLOM AST Graph](wdlToWdlom_wdlom.svg). #### Transliteration functions from Hermes ASTs to WDLOM. The various classes in the `wdl.transforms.base.ast2wdlom` package implement conversions from various types of AST element into various types of WDLOM. Let's look at [`AstToInputDeclarationElement`](https://github.com/broadinstitute/cromwell/blob/master/wdl/transforms/new-base/src/main/scala/wdl/transforms/base/ast2wdlom/AstToInputDeclarationElement.scala):. * The conversion from AST to WDLOM relies on two other conversions, `astNodeT",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/wdlToWdlom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md:2054,Security,validat,validate,2054," the following data structure to represent this declaration:. ```scala; InputDeclarationElement(; typeElement = PrimitiveTypeElement(WomIntegerType),; name = ""foo"",; expression = Min(; arg1 = PrimitiveLiteralExpressionElement(WomInteger(100)),; arg2 = Max(; arg1 = PrimitiveLiteralExpressionElement(WomInteger(1)),; arg2 = PrimitiveLiteralExpressionElement(WomInteger(2)); ); ); ); ```. Again, attempting to show this graphically:. ![WDLOM AST Graph](wdlToWdlom_wdlom.svg). #### Transliteration functions from Hermes ASTs to WDLOM. The various classes in the `wdl.transforms.base.ast2wdlom` package implement conversions from various types of AST element into various types of WDLOM. Let's look at [`AstToInputDeclarationElement`](https://github.com/broadinstitute/cromwell/blob/master/wdl/transforms/new-base/src/main/scala/wdl/transforms/base/ast2wdlom/AstToInputDeclarationElement.scala):. * The conversion from AST to WDLOM relies on two other conversions, `astNodeToTypeElement` and `astNodeToExpressionElement`, ; to validate the attributes on the AST it is given, and construct WDLOM from Hermes ASTs.; * The resulting function can itself be used as a building block to construct higher-level WDLOM types. . #### How AST-to-WDLOM building blocks are chained together. To see how these building blocks are pieced together we can look at WDL 1.0's [`ast2wdlom`](https://github.com/broadinstitute/cromwell/blob/master/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/ast2wdlom/ast2wdlom.scala) package object. Notice how - and where - the `astToInputDeclarationElement` value is declared:. * It follows the `implicit val`s `astNodeToTypeElement` and `astNodeToExpressionElement` and so can use those in its processing.; * It is, as an `implicit val`, used later on by `astNodeToInputsSectionElement`, which is then used by `astNodeToWorkflowBodyElement` and `astNodeToTaskSectionElement`, and so on. . #### How different WDL versions convert to WDLOM differently. These package objects ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/wdlToWdlom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md:3478,Testability,log,logic,3478,"ment.scala):. * The conversion from AST to WDLOM relies on two other conversions, `astNodeToTypeElement` and `astNodeToExpressionElement`, ; to validate the attributes on the AST it is given, and construct WDLOM from Hermes ASTs.; * The resulting function can itself be used as a building block to construct higher-level WDLOM types. . #### How AST-to-WDLOM building blocks are chained together. To see how these building blocks are pieced together we can look at WDL 1.0's [`ast2wdlom`](https://github.com/broadinstitute/cromwell/blob/master/wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/ast2wdlom/ast2wdlom.scala) package object. Notice how - and where - the `astToInputDeclarationElement` value is declared:. * It follows the `implicit val`s `astNodeToTypeElement` and `astNodeToExpressionElement` and so can use those in its processing.; * It is, as an `implicit val`, used later on by `astNodeToInputsSectionElement`, which is then used by `astNodeToWorkflowBodyElement` and `astNodeToTaskSectionElement`, and so on. . #### How different WDL versions convert to WDLOM differently. These package objects ultimately control how different WDL versions are parsed differently. These differences can be seen most easily by comparing package objects for different WDL versions:. ```bash; diff \; wdl/transforms/draft3/src/main/scala/wdl/draft3/transforms/ast2wdlom/ast2wdlom.scala \; wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/ast2wdlom/ast2wdlom.scala ; ``` . In an extreme case, these could contain fundamentally different conversion logic from AST to WDLOM for different WDL versions. Luckily, between; WDL 1.0 and the WDL `development` version, there are only a few minor changes, amongst which are:. * Importing a new Hermes parser.; * Specifying that additional functions are available when constructing the `astNodeToExpressionElement` conversion.; * Specifying that a new `Directory` type is available when constructing the `astNodeToTypeElement` conversion.; ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/wdlToWdlom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/wdlToWdlom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md:928,Availability,echo,echo,928,"### Intro to Workflow Object Model. ""WOM"" is the acronym for the Workflow Object Model, living in the `wom` package. WOM is a directed acyclic graph that captures workflow inputs, outputs, calls, and the dependencies between them. Examine the workflow below, making note of how the outputs of early calls become inputs for later calls. The first example is the `numbers` output of the `mkFile` call serving as the `in_file` input to the `grep` call. ; ```; version 1.0. ##; # Checks a simple branch and join operation.; # We start with a task, branch into two parallel executions, and then rejoin to calculate the result.; ##. workflow forkjoin {; call mkFile. call grep { input: in_file = mkFile.numbers }; call wc { input: in_file=mkFile.numbers }. call join { input: wcCount = wc.count, grepCount = grep.count }. output {; Int joined_proportion = join.proportion; }; }. task mkFile {; command <<<; for i in `seq 1 1000`; do; echo $i; done; >>>; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; input {; String pattern; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && grep '~{pattern}' ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; input {; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && cat ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task join {; input {; Int grepCount; Int wcCount; }; command <<<; expr ~{wcCount} / ~{grepCount}; >>>; output {; Int proportion = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }; ``` . Now, compare the workflow source to its WOM graph (generated with the `womtool womgraph` command). ![Graph of forkjoin](forkjoin_graph.svg). Input values are ovals, outputs are hexagons. You can see that a hexagon in one node (call) becomes an oval in the next. . The `grep` call is special because one of its two inputs, `pattern`, is not specified by any previous call within ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/whatIsWom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md:2430,Energy Efficiency,green,green,2430,"test""}; }. task grep {; input {; String pattern; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && grep '~{pattern}' ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; input {; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && cat ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task join {; input {; Int grepCount; Int wcCount; }; command <<<; expr ~{wcCount} / ~{grepCount}; >>>; output {; Int proportion = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }; ``` . Now, compare the workflow source to its WOM graph (generated with the `womtool womgraph` command). ![Graph of forkjoin](forkjoin_graph.svg). Input values are ovals, outputs are hexagons. You can see that a hexagon in one node (call) becomes an oval in the next. . The `grep` call is special because one of its two inputs, `pattern`, is not specified by any previous call within the bounds of the graph. This causes Cromwell to generate a blue ""external graph input node"" (`wom.graph.ExternalGraphInputNode` if you're looking at the code). Its value must be specified by the user in the inputs file of the workflow under key `grep.pattern`, and Cromwell will pass it into the `grep` call. . Finally, the `proportion` output of the `join` call is piped out of the bounds of the workflow graph by becoming the green `joined_proportion` graph output node (`wom.graph.ExpressionBasedGraphOutputNode` in the code). . Graph nodes inherit from trait `wom.graph.GraphNode`. The graph is constructed in class `wdl.transforms.base.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition`. The `convertGraphElements` function is especially interesting. It accepts a `Set` of `WorkflowGraphElement` objects, which represent individual pieces of the parsed workflow, and converts them to WOM nodes. Then it links the WOM nodes together with edges and emits the finished graph as a `wom.graph.Graph`.; ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/whatIsWom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md:204,Integrability,depend,dependencies,204,"### Intro to Workflow Object Model. ""WOM"" is the acronym for the Workflow Object Model, living in the `wom` package. WOM is a directed acyclic graph that captures workflow inputs, outputs, calls, and the dependencies between them. Examine the workflow below, making note of how the outputs of early calls become inputs for later calls. The first example is the `numbers` output of the `mkFile` call serving as the `in_file` input to the `grep` call. ; ```; version 1.0. ##; # Checks a simple branch and join operation.; # We start with a task, branch into two parallel executions, and then rejoin to calculate the result.; ##. workflow forkjoin {; call mkFile. call grep { input: in_file = mkFile.numbers }; call wc { input: in_file=mkFile.numbers }. call join { input: wcCount = wc.count, grepCount = grep.count }. output {; Int joined_proportion = join.proportion; }; }. task mkFile {; command <<<; for i in `seq 1 1000`; do; echo $i; done; >>>; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; input {; String pattern; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && grep '~{pattern}' ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; input {; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && cat ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task join {; input {; Int grepCount; Int wcCount; }; command <<<; expr ~{wcCount} / ~{grepCount}; >>>; output {; Int proportion = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }; ``` . Now, compare the workflow source to its WOM graph (generated with the `womtool womgraph` command). ![Graph of forkjoin](forkjoin_graph.svg). Input values are ovals, outputs are hexagons. You can see that a hexagon in one node (call) becomes an oval in the next. . The `grep` call is special because one of its two inputs, `pattern`, is not specified by any previous call within ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/whatIsWom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md:2546,Modifiability,inherit,inherit,2546,"test""}; }. task grep {; input {; String pattern; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && grep '~{pattern}' ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; input {; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && cat ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task join {; input {; Int grepCount; Int wcCount; }; command <<<; expr ~{wcCount} / ~{grepCount}; >>>; output {; Int proportion = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }; ``` . Now, compare the workflow source to its WOM graph (generated with the `womtool womgraph` command). ![Graph of forkjoin](forkjoin_graph.svg). Input values are ovals, outputs are hexagons. You can see that a hexagon in one node (call) becomes an oval in the next. . The `grep` call is special because one of its two inputs, `pattern`, is not specified by any previous call within the bounds of the graph. This causes Cromwell to generate a blue ""external graph input node"" (`wom.graph.ExternalGraphInputNode` if you're looking at the code). Its value must be specified by the user in the inputs file of the workflow under key `grep.pattern`, and Cromwell will pass it into the `grep` call. . Finally, the `proportion` output of the `join` call is piped out of the bounds of the workflow graph by becoming the green `joined_proportion` graph output node (`wom.graph.ExpressionBasedGraphOutputNode` in the code). . Graph nodes inherit from trait `wom.graph.GraphNode`. The graph is constructed in class `wdl.transforms.base.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition`. The `convertGraphElements` function is especially interesting. It accepts a `Set` of `WorkflowGraphElement` objects, which represent individual pieces of the parsed workflow, and converts them to WOM nodes. Then it links the WOM nodes together with edges and emits the finished graph as a `wom.graph.Graph`.; ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/whatIsWom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md:485,Usability,simpl,simple,485,"### Intro to Workflow Object Model. ""WOM"" is the acronym for the Workflow Object Model, living in the `wom` package. WOM is a directed acyclic graph that captures workflow inputs, outputs, calls, and the dependencies between them. Examine the workflow below, making note of how the outputs of early calls become inputs for later calls. The first example is the `numbers` output of the `mkFile` call serving as the `in_file` input to the `grep` call. ; ```; version 1.0. ##; # Checks a simple branch and join operation.; # We start with a task, branch into two parallel executions, and then rejoin to calculate the result.; ##. workflow forkjoin {; call mkFile. call grep { input: in_file = mkFile.numbers }; call wc { input: in_file=mkFile.numbers }. call join { input: wcCount = wc.count, grepCount = grep.count }. output {; Int joined_proportion = join.proportion; }; }. task mkFile {; command <<<; for i in `seq 1 1000`; do; echo $i; done; >>>; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; input {; String pattern; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && grep '~{pattern}' ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; input {; File in_file; }; command <<<; [ -f ""~{in_file}"" ] && cat ~{in_file} | wc -l; >>>; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task join {; input {; Int grepCount; Int wcCount; }; command <<<; expr ~{wcCount} / ~{grepCount}; >>>; output {; Int proportion = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }; ``` . Now, compare the workflow source to its WOM graph (generated with the `womtool womgraph` command). ![Graph of forkjoin](forkjoin_graph.svg). Input values are ovals, outputs are hexagons. You can see that a hexagon in one node (call) becomes an oval in the next. . The `grep` call is special because one of its two inputs, `pattern`, is not specified by any previous call within ",MatchSource.DOCS,docs/developers/bitesize/workflowParsing/whatIsWom.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/bitesize/workflowParsing/whatIsWom.md
https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md:115,Deployability,release,released,115,"An **Amazon AWS S3** FileSystem Provider **JSR-203** for Java 8 (NIO2) using the AWS SDK v2. This version uses the released version of the AWS SDK v2 and is forked from; [Emil Lerch's S3 FileSystem](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/tree/e8283b5).; That version uses a preview version of the AWS SDK v2 and is forked from; [Upplication's S3 FileSystem](https://github.com/upplication/Amazon-S3-FileSystem-NIO2). The previous fork did not update the tests for AWS SDK v2, so the tests targeting AWS SDK v1 have been left out of this; fork.; ",MatchSource.DOCS,filesystems/s3/src/main/java/org/lerch/s3fs/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md
https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md:456,Deployability,update,update,456,"An **Amazon AWS S3** FileSystem Provider **JSR-203** for Java 8 (NIO2) using the AWS SDK v2. This version uses the released version of the AWS SDK v2 and is forked from; [Emil Lerch's S3 FileSystem](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/tree/e8283b5).; That version uses a preview version of the AWS SDK v2 and is forked from; [Upplication's S3 FileSystem](https://github.com/upplication/Amazon-S3-FileSystem-NIO2). The previous fork did not update the tests for AWS SDK v2, so the tests targeting AWS SDK v1 have been left out of this; fork.; ",MatchSource.DOCS,filesystems/s3/src/main/java/org/lerch/s3fs/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md
https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md:467,Testability,test,tests,467,"An **Amazon AWS S3** FileSystem Provider **JSR-203** for Java 8 (NIO2) using the AWS SDK v2. This version uses the released version of the AWS SDK v2 and is forked from; [Emil Lerch's S3 FileSystem](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/tree/e8283b5).; That version uses a preview version of the AWS SDK v2 and is forked from; [Upplication's S3 FileSystem](https://github.com/upplication/Amazon-S3-FileSystem-NIO2). The previous fork did not update the tests for AWS SDK v2, so the tests targeting AWS SDK v1 have been left out of this; fork.; ",MatchSource.DOCS,filesystems/s3/src/main/java/org/lerch/s3fs/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md
https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md:496,Testability,test,tests,496,"An **Amazon AWS S3** FileSystem Provider **JSR-203** for Java 8 (NIO2) using the AWS SDK v2. This version uses the released version of the AWS SDK v2 and is forked from; [Emil Lerch's S3 FileSystem](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/tree/e8283b5).; That version uses a preview version of the AWS SDK v2 and is forked from; [Upplication's S3 FileSystem](https://github.com/upplication/Amazon-S3-FileSystem-NIO2). The previous fork did not update the tests for AWS SDK v2, so the tests targeting AWS SDK v1 have been left out of this; fork.; ",MatchSource.DOCS,filesystems/s3/src/main/java/org/lerch/s3fs/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/filesystems/s3/src/main/java/org/lerch/s3fs/README.md
https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD:1679,Modifiability,config,config,1679,"eat them with a little more; flexibility and with the assumption that the contributor probably knows better than us what they want the feature to do. **Officially Supported:** If the PR changes core/supported features in Cromwell, review them as thoroughly as you would PRs from within the team. Remember; that one day you might need to support this code!. ### Reviewing the Concept. Ask the questions:; ; - Will Cromwell be a better product with this change adopted. ; - Will it be better enough to warrant the time necessary to review the PR; - Note: The answer to this is almost always a yes if the first answer was yes; - However, overly long, opaque, or ""risky"" changes might benefit from requests to break the PR up and merge/review things in stages. ; ; ### Review the changes in the PR. - For PRs changing ""supported"" features, treat it like any other PR coming from within the team.; - Remember: we will have to support these changes in the future. Possibly forever!; - For PRs only making changes to features we don't officially support - be generous. But make sure:; - That any new optional functionality is opt-in rather than becoming the default.; - That any community features are flagged in documentation and config examples as originating from the community (and thus may not be supported by the team if bugs are found).; ; ### Run CI against the PR. - Opening a PR triggers CI to run automatically. However, if the PR was not opened by a trusted contributor, it will wait in a Github Actions queue until a trusted contributor explicitly allows CI tests run. ### Cycle through Review and CI. - If the community contributor makes changes following your reviews or the CI results:; - Glance at the changes to make sure they still seem reasonable.; - Make any additional comments; - Rerun CI.; ; ### Merge the PR. - Once the tests have completed successfully and the PR has two approvals, it can be merged.; - Remember to delete your branch clone PR (and the cloned branch itself too!). ",MatchSource.DOCS,processes/external-contributions/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD:1964,Performance,queue,queue,1964,"eat them with a little more; flexibility and with the assumption that the contributor probably knows better than us what they want the feature to do. **Officially Supported:** If the PR changes core/supported features in Cromwell, review them as thoroughly as you would PRs from within the team. Remember; that one day you might need to support this code!. ### Reviewing the Concept. Ask the questions:; ; - Will Cromwell be a better product with this change adopted. ; - Will it be better enough to warrant the time necessary to review the PR; - Note: The answer to this is almost always a yes if the first answer was yes; - However, overly long, opaque, or ""risky"" changes might benefit from requests to break the PR up and merge/review things in stages. ; ; ### Review the changes in the PR. - For PRs changing ""supported"" features, treat it like any other PR coming from within the team.; - Remember: we will have to support these changes in the future. Possibly forever!; - For PRs only making changes to features we don't officially support - be generous. But make sure:; - That any new optional functionality is opt-in rather than becoming the default.; - That any community features are flagged in documentation and config examples as originating from the community (and thus may not be supported by the team if bugs are found).; ; ### Run CI against the PR. - Opening a PR triggers CI to run automatically. However, if the PR was not opened by a trusted contributor, it will wait in a Github Actions queue until a trusted contributor explicitly allows CI tests run. ### Cycle through Review and CI. - If the community contributor makes changes following your reviews or the CI results:; - Glance at the changes to make sure they still seem reasonable.; - Make any additional comments; - Rerun CI.; ; ### Merge the PR. - Once the tests have completed successfully and the PR has two approvals, it can be merged.; - Remember to delete your branch clone PR (and the cloned branch itself too!). ",MatchSource.DOCS,processes/external-contributions/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD:1115,Safety,risk,risky,1115,"her the PR is adding community features or affects ""supported"" functionality.; - Review the concept; - Review the changes in the PR; - Run CI against the PR; - Cycle through Review and CI until satisfied; - Merge the PR. ## Process. ### Decide ""Community"" or ""Supported"". **Community Supported:** If the PR only changes parts of Cromwell which are not part of the supported feature set, treat them with a little more; flexibility and with the assumption that the contributor probably knows better than us what they want the feature to do. **Officially Supported:** If the PR changes core/supported features in Cromwell, review them as thoroughly as you would PRs from within the team. Remember; that one day you might need to support this code!. ### Reviewing the Concept. Ask the questions:; ; - Will Cromwell be a better product with this change adopted. ; - Will it be better enough to warrant the time necessary to review the PR; - Note: The answer to this is almost always a yes if the first answer was yes; - However, overly long, opaque, or ""risky"" changes might benefit from requests to break the PR up and merge/review things in stages. ; ; ### Review the changes in the PR. - For PRs changing ""supported"" features, treat it like any other PR coming from within the team.; - Remember: we will have to support these changes in the future. Possibly forever!; - For PRs only making changes to features we don't officially support - be generous. But make sure:; - That any new optional functionality is opt-in rather than becoming the default.; - That any community features are flagged in documentation and config examples as originating from the community (and thus may not be supported by the team if bugs are found).; ; ### Run CI against the PR. - Opening a PR triggers CI to run automatically. However, if the PR was not opened by a trusted contributor, it will wait in a Github Actions queue until a trusted contributor explicitly allows CI tests run. ### Cycle through Review and CI. - If",MatchSource.DOCS,processes/external-contributions/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD:2019,Testability,test,tests,2019,"eat them with a little more; flexibility and with the assumption that the contributor probably knows better than us what they want the feature to do. **Officially Supported:** If the PR changes core/supported features in Cromwell, review them as thoroughly as you would PRs from within the team. Remember; that one day you might need to support this code!. ### Reviewing the Concept. Ask the questions:; ; - Will Cromwell be a better product with this change adopted. ; - Will it be better enough to warrant the time necessary to review the PR; - Note: The answer to this is almost always a yes if the first answer was yes; - However, overly long, opaque, or ""risky"" changes might benefit from requests to break the PR up and merge/review things in stages. ; ; ### Review the changes in the PR. - For PRs changing ""supported"" features, treat it like any other PR coming from within the team.; - Remember: we will have to support these changes in the future. Possibly forever!; - For PRs only making changes to features we don't officially support - be generous. But make sure:; - That any new optional functionality is opt-in rather than becoming the default.; - That any community features are flagged in documentation and config examples as originating from the community (and thus may not be supported by the team if bugs are found).; ; ### Run CI against the PR. - Opening a PR triggers CI to run automatically. However, if the PR was not opened by a trusted contributor, it will wait in a Github Actions queue until a trusted contributor explicitly allows CI tests run. ### Cycle through Review and CI. - If the community contributor makes changes following your reviews or the CI results:; - Glance at the changes to make sure they still seem reasonable.; - Make any additional comments; - Rerun CI.; ; ### Merge the PR. - Once the tests have completed successfully and the PR has two approvals, it can be merged.; - Remember to delete your branch clone PR (and the cloned branch itself too!). ",MatchSource.DOCS,processes/external-contributions/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD:2293,Testability,test,tests,2293,"eat them with a little more; flexibility and with the assumption that the contributor probably knows better than us what they want the feature to do. **Officially Supported:** If the PR changes core/supported features in Cromwell, review them as thoroughly as you would PRs from within the team. Remember; that one day you might need to support this code!. ### Reviewing the Concept. Ask the questions:; ; - Will Cromwell be a better product with this change adopted. ; - Will it be better enough to warrant the time necessary to review the PR; - Note: The answer to this is almost always a yes if the first answer was yes; - However, overly long, opaque, or ""risky"" changes might benefit from requests to break the PR up and merge/review things in stages. ; ; ### Review the changes in the PR. - For PRs changing ""supported"" features, treat it like any other PR coming from within the team.; - Remember: we will have to support these changes in the future. Possibly forever!; - For PRs only making changes to features we don't officially support - be generous. But make sure:; - That any new optional functionality is opt-in rather than becoming the default.; - That any community features are flagged in documentation and config examples as originating from the community (and thus may not be supported by the team if bugs are found).; ; ### Run CI against the PR. - Opening a PR triggers CI to run automatically. However, if the PR was not opened by a trusted contributor, it will wait in a Github Actions queue until a trusted contributor explicitly allows CI tests run. ### Cycle through Review and CI. - If the community contributor makes changes following your reviews or the CI results:; - Glance at the changes to make sure they still seem reasonable.; - Make any additional comments; - Rerun CI.; ; ### Merge the PR. - Once the tests have completed successfully and the PR has two approvals, it can be merged.; - Remember to delete your branch clone PR (and the cloned branch itself too!). ",MatchSource.DOCS,processes/external-contributions/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/external-contributions/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:3660,Availability,down,down,3660,"thubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workfl",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:31,Deployability,update,update,31,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:88,Deployability,deploy,deployment,88,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:295,Deployability,update,update,295,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:423,Deployability,update,updates-in-Terra,423,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:470,Deployability,release,release,470,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:679,Deployability,release,release,679,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:705,Deployability,release,release,705,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:781,Deployability,integrat,integration,781,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:1070,Deployability,deploy,deploy-cromwell-in-caas-staging-and-caas-prod,1070,"ve a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:1436,Deployability,release,release,1436,"ric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing y",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:2304,Deployability,release,release,2304,"PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing your token. * A copy of the workflow file to run (https://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl); * An inputs json like this one. ```json; {; ""publish_workflow.githubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workfl",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:2375,Deployability,release,release,2375,"orthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing your token. * A copy of the workflow file to run (https://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl); * An inputs json like this one. ```json; {; ""publish_workflow.githubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using D",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:3156,Deployability,release,release,3156,"d save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing your token. * A copy of the workflow file to run (https://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl); * An inputs json like this one. ```json; {; ""publish_workflow.githubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private`",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:3236,Deployability,release,release,3236,"d save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing your token. * A copy of the workflow file to run (https://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl); * An inputs json like this one. ```json; {; ""publish_workflow.githubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private`",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:4070,Deployability,release,release,4070," know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workflow went well, it's time to also publish Docker images for this release. ; * `git checkout` the Cromwell hash that was just published (i.e. the one directly BEFORE the ""Update Cromwell version from x to x+1"" commit that the publish WDL makes). It's important that the image being built uses the exact same code as the .jar files published to github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isR",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:4135,Deployability,release,releases,4135,"p-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workflow went well, it's time to also publish Docker images for this release. ; * `git checkout` the Cromwell hash that was just published (i.e. the one directly BEFORE the ""Update Cromwell version from x to x+1"" commit that the publish WDL makes). It's important that the image being built uses the exact same code as the .jar files published to github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush` from your local Cromwell d",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:4241,Deployability,release,release,4241,".wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workflow went well, it's time to also publish Docker images for this release. ; * `git checkout` the Cromwell hash that was just published (i.e. the one directly BEFORE the ""Update Cromwell version from x to x+1"" commit that the publish WDL makes). It's important that the image being built uses the exact same code as the .jar files published to github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush` from your local Cromwell directory. ; * Grab a cup of coffee, and verify that all of the new images were pushed successfully. For example, you should now be able to do `docker pull broadins",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:4653,Deployability,release,release,4653,"ce intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workflow went well, it's time to also publish Docker images for this release. ; * `git checkout` the Cromwell hash that was just published (i.e. the one directly BEFORE the ""Update Cromwell version from x to x+1"" commit that the publish WDL makes). It's important that the image being built uses the exact same code as the .jar files published to github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush` from your local Cromwell directory. ; * Grab a cup of coffee, and verify that all of the new images were pushed successfully. For example, you should now be able to do `docker pull broadinstitute/cromwell:{new version #}`; * The list of images is `cromwell`, `cromiam`, `cromwell-drs-localizer`, and `womtool`; * Docker Hub links: [Cromwell](https://hub.docker.com/repository/docker/broadinstitute/cromwell/general), [CromIAM](https://hub.docker.com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromw",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:4730,Deployability,release,release,4730,"ce intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workflow went well, it's time to also publish Docker images for this release. ; * `git checkout` the Cromwell hash that was just published (i.e. the one directly BEFORE the ""Update Cromwell version from x to x+1"" commit that the publish WDL makes). It's important that the image being built uses the exact same code as the .jar files published to github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush` from your local Cromwell directory. ; * Grab a cup of coffee, and verify that all of the new images were pushed successfully. For example, you should now be able to do `docker pull broadinstitute/cromwell:{new version #}`; * The list of images is `cromwell`, `cromiam`, `cromwell-drs-localizer`, and `womtool`; * Docker Hub links: [Cromwell](https://hub.docker.com/repository/docker/broadinstitute/cromwell/general), [CromIAM](https://hub.docker.com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromw",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:6018,Deployability,upgrade,upgrade,6018,"o github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush` from your local Cromwell directory. ; * Grab a cup of coffee, and verify that all of the new images were pushed successfully. For example, you should now be able to do `docker pull broadinstitute/cromwell:{new version #}`; * The list of images is `cromwell`, `cromiam`, `cromwell-drs-localizer`, and `womtool`; * Docker Hub links: [Cromwell](https://hub.docker.com/repository/docker/broadinstitute/cromwell/general), [CromIAM](https://hub.docker.com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromwell-drs-localizer/general), [Womtool](https://hub.docker.com/repository/docker/broadinstitute/womtool/general). ### How to Deploy Cromwell in CaaS staging and CaaS prod. CaaS is ""Cromwell as a Service"". It is only used by Epigenomics, and will eventually be retired in favor of Terra. **Note:** If the Cromwell CHANGELOG indicates that the upgrade might take some time (e.g., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the ge",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:6147,Deployability,upgrade,upgrade,6147,"well directory. ; * Grab a cup of coffee, and verify that all of the new images were pushed successfully. For example, you should now be able to do `docker pull broadinstitute/cromwell:{new version #}`; * The list of images is `cromwell`, `cromiam`, `cromwell-drs-localizer`, and `womtool`; * Docker Hub links: [Cromwell](https://hub.docker.com/repository/docker/broadinstitute/cromwell/general), [CromIAM](https://hub.docker.com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromwell-drs-localizer/general), [Womtool](https://hub.docker.com/repository/docker/broadinstitute/womtool/general). ### How to Deploy Cromwell in CaaS staging and CaaS prod. CaaS is ""Cromwell as a Service"". It is only used by Epigenomics, and will eventually be retired in favor of Terra. **Note:** If the Cromwell CHANGELOG indicates that the upgrade might take some time (e.g., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 fai",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:6454,Deployability,update,updated,6454,"ps://hub.docker.com/repository/docker/broadinstitute/cromwell/general), [CromIAM](https://hub.docker.com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromwell-drs-localizer/general), [Womtool](https://hub.docker.com/repository/docker/broadinstitute/womtool/general). ### How to Deploy Cromwell in CaaS staging and CaaS prod. CaaS is ""Cromwell as a Service"". It is only used by Epigenomics, and will eventually be retired in favor of Terra. **Note:** If the Cromwell CHANGELOG indicates that the upgrade might take some time (e.g., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/crom",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:6542,Deployability,release,release,6542,"com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromwell-drs-localizer/general), [Womtool](https://hub.docker.com/repository/docker/broadinstitute/womtool/general). ### How to Deploy Cromwell in CaaS staging and CaaS prod. CaaS is ""Cromwell as a Service"". It is only used by Epigenomics, and will eventually be retired in favor of Terra. **Note:** If the Cromwell CHANGELOG indicates that the upgrade might take some time (e.g., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/cromwell-artifactory`. Then run. ```; export ARTIFACTORY_USERNAME=<the-username>; export ARTIFACTORY_PAS",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:6827,Deployability,update,updated,6827,"y Cromwell in CaaS staging and CaaS prod. CaaS is ""Cromwell as a Service"". It is only used by Epigenomics, and will eventually be retired in favor of Terra. **Note:** If the Cromwell CHANGELOG indicates that the upgrade might take some time (e.g., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/cromwell-artifactory`. Then run. ```; export ARTIFACTORY_USERNAME=<the-username>; export ARTIFACTORY_PASSWORD=<the-password>; ./scripts/publish-client.sh; ```. A SNAP version of the client library will be published in jFrog at [this path](https://broadinstitute.jfrog.io/ui/repos/tree/General/libs-release-local/org/broadinstitute/cromwell). ## Cromwell setup for ",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:7740,Deployability,release,release-local,7740,"., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/cromwell-artifactory`. Then run. ```; export ARTIFACTORY_USERNAME=<the-username>; export ARTIFACTORY_PASSWORD=<the-password>; ./scripts/publish-client.sh; ```. A SNAP version of the client library will be published in jFrog at [this path](https://broadinstitute.jfrog.io/ui/repos/tree/General/libs-release-local/org/broadinstitute/cromwell). ## Cromwell setup for publishing. One can run a publishing-friendly Cromwell using a containerized MySQL server and config files; from Cromwell's CI. See the `start_publish_mysql_docker.sh` and `start_publish_cromwell.sh`; scripts under `release_processes/scripts`.; ",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:781,Integrability,integrat,integration,781,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:1199,Integrability,message,message,1199,"ate automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a tempor",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:1322,Integrability,wrap,wrapped,1322,"ate automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a tempor",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:3111,Integrability,message,message,3111,"d save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing your token. * A copy of the workflow file to run (https://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl); * An inputs json like this one. ```json; {; ""publish_workflow.githubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private`",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:3528,Modifiability,config,config,3528,"s://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl); * An inputs json like this one. ```json; {; ""publish_workflow.githubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation b",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:7900,Modifiability,config,config,7900,"., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/cromwell-artifactory`. Then run. ```; export ARTIFACTORY_USERNAME=<the-username>; export ARTIFACTORY_PASSWORD=<the-password>; ./scripts/publish-client.sh; ```. A SNAP version of the client library will be published in jFrog at [this path](https://broadinstitute.jfrog.io/ui/repos/tree/General/libs-release-local/org/broadinstitute/cromwell). ## Cromwell setup for publishing. One can run a publishing-friendly Cromwell using a containerized MySQL server and config files; from Cromwell's CI. See the `start_publish_mysql_docker.sh` and `start_publish_cromwell.sh`; scripts under `release_processes/scripts`.; ",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:1471,Performance,perform,perform,1471,"ric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing y",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:6569,Performance,perform,performed,6569,"com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromwell-drs-localizer/general), [Womtool](https://hub.docker.com/repository/docker/broadinstitute/womtool/general). ### How to Deploy Cromwell in CaaS staging and CaaS prod. CaaS is ""Cromwell as a Service"". It is only used by Epigenomics, and will eventually be retired in favor of Terra. **Note:** If the Cromwell CHANGELOG indicates that the upgrade might take some time (e.g., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/cromwell-artifactory`. Then run. ```; export ARTIFACTORY_USERNAME=<the-username>; export ARTIFACTORY_PAS",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:3718,Safety,avoid,avoid,3718,"thubToken"": ""<<GITHUB TOKEN VALUE>>"",; ""publish_workflow.publishDocker"": ""broadinstitute/cromwell-publish:latest"",; ""publish_workflow.organization"": ""broadinstitute""; }; ```. #### Make sure Docker will have enough memory. Follow the instructions [here](https://docs.docker.com/docker-for-mac/#resources) to increase Docker memory.; Ensure you have at least 8GB; 4GB is not sufficient. #### Let people know the publish is underway. Post another message in `#dsp-workflows-private` that the release is underway, asking everyone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workfl",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:1569,Security,authenticat,authenticating-to-github,1569,"e, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing your token. * A copy of the workflow file to run (https://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl);",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:1614,Security,access,access-token-for-the-command-line,1614,"e, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewing the token in the [list-view of tokens](https://github.com/settings/tokens), its scopes string should read exactly `read:org, repo, user:email, workflow`; * Copy the token and save it to a file on your local machine, for example `~/.github-token`. You'll use it in a second. #### Prepare a temporary `release` directory. Make or copy the following files into a temporary `release/` directory outside the Cromwell repository. This removes any chance of committing your token. * A copy of the workflow file to run (https://github.com/broadinstitute/cromwell/blob/develop/publish/publish_workflow.wdl);",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:4771,Security,hash,hash,4771," to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workflow went well, it's time to also publish Docker images for this release. ; * `git checkout` the Cromwell hash that was just published (i.e. the one directly BEFORE the ""Update Cromwell version from x to x+1"" commit that the publish WDL makes). It's important that the image being built uses the exact same code as the .jar files published to github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush` from your local Cromwell directory. ; * Grab a cup of coffee, and verify that all of the new images were pushed successfully. For example, you should now be able to do `docker pull broadinstitute/cromwell:{new version #}`; * The list of images is `cromwell`, `cromiam`, `cromwell-drs-localizer`, and `womtool`; * Docker Hub links: [Cromwell](https://hub.docker.com/repository/docker/broadinstitute/cromwell/general), [CromIAM](https://hub.docker.com/repository/docker/broadinstitute/cromiam/general), [DRS Localizer](https://hub.docker.com/repository/docker/broadinstitute/cromwell-drs-localizer/general), [Womtool](https://hub.docker.com/repository/docker/broadinstitute",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:7557,Security,password,password,7557,"., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/cromwell-artifactory`. Then run. ```; export ARTIFACTORY_USERNAME=<the-username>; export ARTIFACTORY_PASSWORD=<the-password>; ./scripts/publish-client.sh; ```. A SNAP version of the client library will be published in jFrog at [this path](https://broadinstitute.jfrog.io/ui/repos/tree/General/libs-release-local/org/broadinstitute/cromwell). ## Cromwell setup for publishing. One can run a publishing-friendly Cromwell using a containerized MySQL server and config files; from Cromwell's CI. See the `start_publish_mysql_docker.sh` and `start_publish_cromwell.sh`; scripts under `release_processes/scripts`.; ",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:869,Testability,test,testing,869,"# Release Processes. ## How to update these processes. Have a better idea about how the deployment processes should work?; See our ""updating the process"" [process](../README.MD)!. ## Core Process: How to Publish and Release Cromwell. This process is unrelated to Terra. Both GCP and Azure Terra update automatically. For more information see [here](https://support.terra.bio/hc/en-us/articles/9512163608731-Faster-Cromwell-updates-in-Terra-). Manually cutting a numeric release is expected to continue being done on a cadence of every ~6 months. If you're the lucky ""Release Rotation Engineer"" this time, you should do the following four things:. 1. Create a Jira ticket for the release (look at previous release tickets if you're not sure how).; 1. Check that Cromwell's [nightly integration](https://github.com/broadinstitute/cromwell/actions?query=event%3Aschedule) testing is passing.; 1. [Run the publish script to create a new version of Cromwell](#how-to-publish-a-new-cromwell-version); 1. [Run through the ""How to Deploy Cromwell in CaaS prod"" process](#how-to-deploy-cromwell-in-caas-staging-and-caas-prod). ### How to publish a new Cromwell version. #### Announce your intentions. Post a message in `#dsp-workflows-private` letting people know that a publish is imminent in case there are PRs they want to get; wrapped up and merged to `develop` to go out in the forthcoming version of Cromwell. #### Get a Github token. The release WDL uses a github token to perform actions on your behalf.; * Follow the directions [here](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).; When asked give the token these exact permissions ([example screenshot](github_token_scopes.png)):; * the full set of `repo` permissions; * the `workflow` permission; * only the `read:org` permission (do not let this token administer your organizations!); * only the `user:email` permission (do not let this token administer your user!); * When viewi",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:7014,Testability,test,tests,7014,"NGELOG indicates that the upgrade might take some time (e.g., because of a database migration), checking in with the CaaS users; to let them know that the upgrade is about to happen is a good idea. Deploying to CaaS is detailed in the [Quick CaaS Deployment Guide](https://docs.google.com/document/d/1s0YC-oohJ7o-OGcgnH_-YBtIEKmLIPTRpG36yvWxUpE). ### How to be Sure You're Done. Confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) was updated. ## Bonus Processes. The Swagger client library is not part of our core publish/release process but can be performed from time to time, as required. ### How to Generate and Publish Swagger Client Library. **Note:** This part of publishing may or may not work for you until; [BT-38](https://broadworkbench.atlassian.net/browse/BT-38) is finished and this section is updated. The first step is to generate the client library. From the root of the repo run. ```; ./scripts/gen_java_client.sh; ```. This generates the client library and runs the generated tests as well. A successful run should end with something similar to. ```; [debug] Test run finished: 0 failed, 0 ignored, 7 total, 0.007s; [info] Passed: Total 103, Failed 0, Errors 0, Passed 100, Skipped 3; [success] Total time: 4 s, completed Jun 26, 2019 3:01:19 PM; ```. To publish to artifactory, first obtain the artifactory username and credentials. Credentials are located in Vault at path `secret/dsde/cromwell/common/cromwell-artifactory`. Then run. ```; export ARTIFACTORY_USERNAME=<the-username>; export ARTIFACTORY_PASSWORD=<the-password>; ./scripts/publish-client.sh; ```. A SNAP version of the client library will be published in jFrog at [this path](https://broadinstitute.jfrog.io/ui/repos/tree/General/libs-release-local/org/broadinstitute/cromwell). ## Cromwell setup for publishing. One can run a publishing-friendly Cromwell using a containerized MySQL server and config files; from Cromwell's CI. See the `start_publish_mysql_docker.sh` and `start_publish",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD:4201,Usability,resume,resume,4201,"eryone to hold off merges to `develop` until; the release is published. #### Run the `publish_workflow.wdl` Workflow. Run Cromwell in server mode with a persistent backing database, using Docker containers. This allows call caching to happen if you need to restart for any reason.; See instructions for using a Dockerized MySQL server and CI config [here](#cromwell-setup-for-publishing). Note that the publish workflow is quite resource intensive; it's a good idea to shut down other resource intensive apps before launching it to avoid painfully slow or failed executions. Make sure to plug in your laptop. Using the Swagger API at `http://localhost:8000`, submit the workflow to Cromwell along with the inputs file. The workflow outputs its status to the console. #### Make sure it all went swimmingly. * Check that the workflow succeeded.; * Check that there's now a new Cromwell release listed [here](https://github.com/broadinstitute/cromwell/releases).; * Let `#dsp-workflows-private` know that it's okay to resume merges to `develop`.; * Announce release in `#dsp-workflows`, with context that it's for standalone Cromwells (the code is already in Terra).; * **One business day later,** confirm that [the Homebrew package](https://formulae.brew.sh/formula/cromwell) has the latest version. If it doesn't, start investigation by looking at [Homebrew PR's](https://github.com/Homebrew/homebrew-core/pulls?q=is%3Apr+cromwell). ### Publish Docker Image; * If the release workflow went well, it's time to also publish Docker images for this release. ; * `git checkout` the Cromwell hash that was just published (i.e. the one directly BEFORE the ""Update Cromwell version from x to x+1"" commit that the publish WDL makes). It's important that the image being built uses the exact same code as the .jar files published to github.; * Run `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush` from your local Cromwell directory. ; * Grab a cup of coffee, and verify that all of th",MatchSource.DOCS,processes/release_processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/release_processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/troubleshooting/README.MD:819,Availability,recover,recoverable,819,"# Production Troubleshooting Processes. **Note:** These processes contain shorthand descriptions for various tasks.; If you aren't sure how to achieve any of these steps, look for the details in; the [Cromwell playbook](https://docs.google.com/document/d/1_iRESDzuCgPTOPJnTYxTncIqJU8B1IFWarypDe3gbCY). ## General Purpose Fallback Process. * Have you run through the end of the playbook suggestions and not found anything which fixes the issue?; * Do you just want the problem to go away so that you can get back to sleep as quickly as possible?. This is a (near-) foolproof series of steps to bring Cromwell back into a good state as quickly as ; possible if something weird is happening in Cromwell and you don't know why. It also leaves any offending; workflows from a problem-causing submission in the database in a recoverable state for when the issue is resolved. . ![all-purpose-mess-remover](all-purpose-mess-remover.dot.png) ; ; ## How to update these processes. Have a better idea about how the troubleshooting processes should work? ; See our ""updating the process"" [process](../README.MD)!; ",MatchSource.DOCS,processes/troubleshooting/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/troubleshooting/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/troubleshooting/README.MD:947,Deployability,update,update,947,"# Production Troubleshooting Processes. **Note:** These processes contain shorthand descriptions for various tasks.; If you aren't sure how to achieve any of these steps, look for the details in; the [Cromwell playbook](https://docs.google.com/document/d/1_iRESDzuCgPTOPJnTYxTncIqJU8B1IFWarypDe3gbCY). ## General Purpose Fallback Process. * Have you run through the end of the playbook suggestions and not found anything which fixes the issue?; * Do you just want the problem to go away so that you can get back to sleep as quickly as possible?. This is a (near-) foolproof series of steps to bring Cromwell back into a good state as quickly as ; possible if something weird is happening in Cromwell and you don't know why. It also leaves any offending; workflows from a problem-causing submission in the database in a recoverable state for when the issue is resolved. . ![all-purpose-mess-remover](all-purpose-mess-remover.dot.png) ; ; ## How to update these processes. Have a better idea about how the troubleshooting processes should work? ; See our ""updating the process"" [process](../README.MD)!; ",MatchSource.DOCS,processes/troubleshooting/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/troubleshooting/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/troubleshooting/README.MD:819,Safety,recover,recoverable,819,"# Production Troubleshooting Processes. **Note:** These processes contain shorthand descriptions for various tasks.; If you aren't sure how to achieve any of these steps, look for the details in; the [Cromwell playbook](https://docs.google.com/document/d/1_iRESDzuCgPTOPJnTYxTncIqJU8B1IFWarypDe3gbCY). ## General Purpose Fallback Process. * Have you run through the end of the playbook suggestions and not found anything which fixes the issue?; * Do you just want the problem to go away so that you can get back to sleep as quickly as possible?. This is a (near-) foolproof series of steps to bring Cromwell back into a good state as quickly as ; possible if something weird is happening in Cromwell and you don't know why. It also leaves any offending; workflows from a problem-causing submission in the database in a recoverable state for when the issue is resolved. . ![all-purpose-mess-remover](all-purpose-mess-remover.dot.png) ; ; ## How to update these processes. Have a better idea about how the troubleshooting processes should work? ; See our ""updating the process"" [process](../README.MD)!; ",MatchSource.DOCS,processes/troubleshooting/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/troubleshooting/README.MD
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:519,Deployability,install,install,519,"# Backpressure Report. This `backpressure_report` Python project allows for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for indivi",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:731,Integrability,message,messages,731,"# Backpressure Report. This `backpressure_report` Python project allows for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for indivi",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:872,Integrability,message,message,872,"# Backpressure Report. This `backpressure_report` Python project allows for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for indivi",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:923,Integrability,message,message,923,"# Backpressure Report. This `backpressure_report` Python project allows for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for indivi",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:629,Testability,log,logs,629,"# Backpressure Report. This `backpressure_report` Python project allows for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for indivi",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:696,Testability,log,logs,696,"# Backpressure Report. This `backpressure_report` Python project allows for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for indivi",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:1020,Testability,log,logging,1020,"s for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for individual pods during the interval in seconds. ### Questions. - Q: Why not ",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:1117,Testability,log,log,1117,"s for measuring the amount of time Cromwell runner instances spend in a; high I/O state which triggers internal backpressure. While in this high I/O state the Cromwell runners will not hand out; job execution or restart check tokens, so job starts and restarts will be slowed until I/O returns to normal levels. ## Running. Installation:. Probably best done inside a [virtual environment](https://docs.python.org/3/library/venv.html). ```shell; pip install .; ```. Usage:. ```shell; python -m backpressure_report.main <files with JSON formatted Logs Explorer logs>; ```. The program parses JSON-formatted Google Logs Explorer logs JSON looking for backpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for individual pods during the interval in seconds. ### Questions. - Q: Why not ",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:2284,Testability,test,tests,2284,"ckpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for individual pods during the interval in seconds. ### Questions. - Q: Why not run the scripts directly, eg `python main.py`?; - A: Running Python from this outer directory allows it to discover the `backpressure_report` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the Python unit tests from the top-level `backpressure_report` directory ; (ie the one containing this README.MD file), run:; ```sh; python -m unittest discover -v; ```. This will:; - Find the `backpressure_report` project in that subdirectory.; - And make it importable to other scripts.; - Run the Python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:2314,Testability,test,tests,2314,"ckpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for individual pods during the interval in seconds. ### Questions. - Q: Why not run the scripts directly, eg `python main.py`?; - A: Running Python from this outer directory allows it to discover the `backpressure_report` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the Python unit tests from the top-level `backpressure_report` directory ; (ie the one containing this README.MD file), run:; ```sh; python -m unittest discover -v; ```. This will:; - Find the `backpressure_report` project in that subdirectory.; - And make it importable to other scripts.; - Run the Python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:2659,Testability,test,tests,2659,"ckpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for individual pods during the interval in seconds. ### Questions. - Q: Why not run the scripts directly, eg `python main.py`?; - A: Running Python from this outer directory allows it to discover the `backpressure_report` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the Python unit tests from the top-level `backpressure_report` directory ; (ie the one containing this README.MD file), run:; ```sh; python -m unittest discover -v; ```. This will:; - Find the `backpressure_report` project in that subdirectory.; - And make it importable to other scripts.; - Run the Python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md:2681,Testability,test,test,2681,"ckpressure messages.; The Logs Explorer query should look like the following:. ```; resource.labels.container_name=""cromwell1-runner-app""; (jsonPayload.message=~""IoActor backpressure off"" OR jsonPayload.message=~""Beginning IoActor backpressure""); ```. Multiple input files may be required to capture logging output from an entire interval of interest since Google imposes; limits on the number of log entries that can be exported from a single query. Output is a CSV file like:. ```; Interval (1 hour),All pods,Pod 47z68,Pod 4hgd4,Pod 7svrs,Pod 9l2ld,Pod 9p9j4,Pod bj4vh,Pod d85vc,Pod gdp8x,Pod gth4r,Pod jkpbj,Pod jrgsx,Pod ltmvs,Pod mkdjt,Pod qt2bq,Pod th2p8,Pod thwz9,Pod xvcrk,Pod z7jfk; 2022-01-01 05:00:00+00:00,62,20,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 06:00:00+00:00,40,0,0,0,0,0,0,0,40,0,0,0,0,0,0,0,0,0,0; 2022-01-01 07:00:00+00:00,20,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 08:00:00+00:00,40,20,0,0,0,20,0,0,0,0,0,0,0,0,0,0,0,0,0; 2022-01-01 09:00:00+00:00,110,0,0,0,0,70,0,0,40,0,0,0,0,0,0,0,0,0,0; ...; ```. The first column is the timestamp for the interval start, the second column is the sum of all backpressure durations from all runner; pods during that interval in seconds, and all subsequent columns are the backpressure durations for individual pods during the interval in seconds. ### Questions. - Q: Why not run the scripts directly, eg `python main.py`?; - A: Running Python from this outer directory allows it to discover the `backpressure_report` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the Python unit tests from the top-level `backpressure_report` directory ; (ie the one containing this README.MD file), run:; ```sh; python -m unittest discover -v; ```. This will:; - Find the `backpressure_report` project in that subdirectory.; - And make it importable to other scripts.; - Run the Python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/backpressure_report/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/backpressure_report/README.md
https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD:177,Performance,perform,performance,177,"# Metadata Comparison Scripts. This `metadata_comparison` python project provides tools to compare workflows run; in different Cromwell environments to compare overall cost and performance. . ## Running a script. Choose a script to run. For this example we will use the `extractor`. From this top-level directory `metadata_comparison` directory (ie the one ; containing this README.MD file), run:. ```sh; # python3 -m metadata_comparison.extractor <ARGS>; ```. ### Questions. - Q: Why not run the scripts directly, eg `python3 extractor.py`?; - A: Running python from this outer directory allows it to discover the `metadata_comparison` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the python unit tests from the top-level `metadata_comparison` directory ; (ie the one containing this README.MD file), run:; ```sh; # python3 -m unittest discover -v; ```. This will:; - Find the `metadata_comparison` project in that subdirectory.; - And make it importable to other scripts.; - Run the python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/metadata_comparison/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD
https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD:708,Testability,test,tests,708,"# Metadata Comparison Scripts. This `metadata_comparison` python project provides tools to compare workflows run; in different Cromwell environments to compare overall cost and performance. . ## Running a script. Choose a script to run. For this example we will use the `extractor`. From this top-level directory `metadata_comparison` directory (ie the one ; containing this README.MD file), run:. ```sh; # python3 -m metadata_comparison.extractor <ARGS>; ```. ### Questions. - Q: Why not run the scripts directly, eg `python3 extractor.py`?; - A: Running python from this outer directory allows it to discover the `metadata_comparison` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the python unit tests from the top-level `metadata_comparison` directory ; (ie the one containing this README.MD file), run:; ```sh; # python3 -m unittest discover -v; ```. This will:; - Find the `metadata_comparison` project in that subdirectory.; - And make it importable to other scripts.; - Run the python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/metadata_comparison/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD
https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD:738,Testability,test,tests,738,"# Metadata Comparison Scripts. This `metadata_comparison` python project provides tools to compare workflows run; in different Cromwell environments to compare overall cost and performance. . ## Running a script. Choose a script to run. For this example we will use the `extractor`. From this top-level directory `metadata_comparison` directory (ie the one ; containing this README.MD file), run:. ```sh; # python3 -m metadata_comparison.extractor <ARGS>; ```. ### Questions. - Q: Why not run the scripts directly, eg `python3 extractor.py`?; - A: Running python from this outer directory allows it to discover the `metadata_comparison` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the python unit tests from the top-level `metadata_comparison` directory ; (ie the one containing this README.MD file), run:; ```sh; # python3 -m unittest discover -v; ```. This will:; - Find the `metadata_comparison` project in that subdirectory.; - And make it importable to other scripts.; - Run the python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/metadata_comparison/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD
https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD:1086,Testability,test,tests,1086,"# Metadata Comparison Scripts. This `metadata_comparison` python project provides tools to compare workflows run; in different Cromwell environments to compare overall cost and performance. . ## Running a script. Choose a script to run. For this example we will use the `extractor`. From this top-level directory `metadata_comparison` directory (ie the one ; containing this README.MD file), run:. ```sh; # python3 -m metadata_comparison.extractor <ARGS>; ```. ### Questions. - Q: Why not run the scripts directly, eg `python3 extractor.py`?; - A: Running python from this outer directory allows it to discover the `metadata_comparison` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the python unit tests from the top-level `metadata_comparison` directory ; (ie the one containing this README.MD file), run:; ```sh; # python3 -m unittest discover -v; ```. This will:; - Find the `metadata_comparison` project in that subdirectory.; - And make it importable to other scripts.; - Run the python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/metadata_comparison/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD
https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD:1108,Testability,test,test,1108,"# Metadata Comparison Scripts. This `metadata_comparison` python project provides tools to compare workflows run; in different Cromwell environments to compare overall cost and performance. . ## Running a script. Choose a script to run. For this example we will use the `extractor`. From this top-level directory `metadata_comparison` directory (ie the one ; containing this README.MD file), run:. ```sh; # python3 -m metadata_comparison.extractor <ARGS>; ```. ### Questions. - Q: Why not run the scripts directly, eg `python3 extractor.py`?; - A: Running python from this outer directory allows it to discover the `metadata_comparison` ; project, and thus allows imports across and between scripts. ## Unit tests. To run the python unit tests from the top-level `metadata_comparison` directory ; (ie the one containing this README.MD file), run:; ```sh; # python3 -m unittest discover -v; ```. This will:; - Find the `metadata_comparison` project in that subdirectory.; - And make it importable to other scripts.; - Run the python built-in unittest script, which will:; - Discover the tests project in the `test` directory; - Run them, verbosely.",MatchSource.DOCS,scripts/metadata_comparison/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/scripts/metadata_comparison/README.MD
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:3594,Availability,down,download,3594,"ick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html). The ECS workers used by the AWS Batch backend can be any instance type and should; be based on an AMI running the ECS agent and docker. An ECS optimized AMI is recommended.; An EC2 LaunchTemplate is used to provide some additional ""on first boot"" configuration that:; 1. Installs AWS CLI v2,; 1. Installs a script to mount an EBS as a `btrfs` file system that will auto-expand,; 1. Configures docker to use that file system so that the ""filesystem"" of the container; will auto-expand,; 1. Installs a `fetch_and_run.sh` script that allows the container to download ; generated shell scripts from S3 that contain the instructions of the workflow; task . ```text; +-------------+; | |; | AWS Batch |; | |; +------+------+; |; |; |; |; |; +----------------v------------------+; | |; | Elastic Container Service (ECS) |; | |; +----------------+------------------+; |; |; |; |; |; +------------------------v-------------------------+; | |; | AutoScaling Group |; | |; | +---------------------------------+ |; | | | |; | | EC2 Instance | |; | | | |; | | +--------------------+ | |; | | | | | |; | | | Docker Container | | |; | | | | | |; | | +--------------------+ ... | |; | | | |; | +---------------------------------+ ... |; | |; +--------------------------------------------------+. ```. Cromwell AWS Batch Backend; --------------------------. There are several scala classes as part of the AWS Batch Backend, but; the primary classes involved in running the backend are shown below. The; arrows represent the flow of job submission. ```text; +----------------------------------------+;",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:8968,Availability,down,down,8968,"ad size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required fo",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:14822,Availability,avail,available,14822,"ference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We sh",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15467,Availability,down,down,15467,"more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time.",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16187,Availability,avail,available,16187,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:845,Deployability,configurat,configuration,845,"AWS Batch Backend Architecture; ==============================. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or m",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:892,Deployability,configurat,configuration,892,"AWS Batch Backend Architecture; ==============================. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or m",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1077,Deployability,configurat,configuration,1077,"=========. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed thro",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1374,Deployability,configurat,configuration,1374,"s, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a co",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1444,Deployability,configurat,configuration,1444,"s, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a co",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:2656,Deployability,update,updates,2656,"l free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html). The ECS workers used by the AWS Batch backend can be any instance type and should; be based on an AMI running the ECS agent and docker. An ECS optimized AMI is recommended.; An EC2 LaunchTemplate is used to provide some additional ""on first boot"" configuration that:; 1. Installs AWS CLI v2,; 1. Installs a script to mount an EBS as a `btrfs` file system that will auto-expand,; 1. Configures docker to use that file system so that the ""filesystem"" of the container; will auto-expand,; 1. Installs a `fetch_and_run.sh` script that allows the container to download ; generated shell scripts from S3 that contain the inst",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:3286,Deployability,configurat,configuration,3286," that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html). The ECS workers used by the AWS Batch backend can be any instance type and should; be based on an AMI running the ECS agent and docker. An ECS optimized AMI is recommended.; An EC2 LaunchTemplate is used to provide some additional ""on first boot"" configuration that:; 1. Installs AWS CLI v2,; 1. Installs a script to mount an EBS as a `btrfs` file system that will auto-expand,; 1. Configures docker to use that file system so that the ""filesystem"" of the container; will auto-expand,; 1. Installs a `fetch_and_run.sh` script that allows the container to download ; generated shell scripts from S3 that contain the instructions of the workflow; task . ```text; +-------------+; | |; | AWS Batch |; | |; +------+------+; |; |; |; |; |; +----------------v------------------+; | |; | Elastic Container Service (ECS) |; | |; +----------------+------------------+; |; |; |; |; |; +------------------------v-------------------------+; | |; | AutoScaling Group |; | |; | +---------------------------------+ |; | | | |; | | EC2 Instance | |; | | | |; | | +--------------------+ | |; | | | | | |; | | | Docker Container | | |; | | | | | |; | | +--------------------+ ... | |; | | | |; | +---------------------------------+ ... |; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9503,Deployability,integrat,integrated,9503,"de to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 I",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9532,Deployability,configurat,configuration,9532,"de to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 I",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9693,Deployability,configurat,configuration,9693," are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9778,Deployability,configurat,configuration,9778,"nt amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within A",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:10708,Deployability,configurat,configuration,10708,"tion for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within AWS, everything must be authorized. This is a consistent rule, and as; such, AWS Services themselves are not immune to the rule. Therefore, customers; of AWS are responsible for granting services access to APIs within their account.; The flow described below represents the permissions needed by each stage, from ; Cromwell server through the task running. This includes the permissions needed for; the AWS Services involved in the processing of the work. ```text; +----------------------------+; | | s3:GetObject on bucket for workflow and script bucket; | | s3:ListObjects on script bucket; | | s3:PutObject on script bucket; | Cromwell | batch:RegisterTaskDefinition; | | batch:SubmitJob; | | batch:DescribeJobs; | | batch:DescribeJobDefinitions; +-------------+--------------+; |; |; |; +-------------v--------------+; | | AWSBatchServiceRole managed policy - described at:; | AWS Batch |; |",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:13365,Deployability,update,updates,13365,"2ContainerServiceforEC2Role managed policy, described at:; | ECS Agent (running on EC2) | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html (EC2); | | OR; | | AmazonECSTaskExecutionRolePolicy managed policy, described at: ; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html (Fargate); +-------------+--------------+ ; |; |; |; +-------------v--------------+; | | Task Role permissions. These are user defined, but ecs-tasks.amazon.com must have sts:AssumeRole trust relationship defined. Documentation:; | Task Container |; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html; | | s3:GetObject, s3:PutObject, s3:ListObjects; +----------------------------+; ```. 1. ECS has several sets of permissions for various items. AWS Batch, however,; does not take advantage of certain features of ECS, most importantly; ECS Services are out of scope of AWS Batch. ECS services require things; like load balancing registration and DNS updates. While there is; documentation regarding roles related to ECS services, these are; irrelevant to the Cromwell use case.; 2. Other than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ----------------",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:13897,Deployability,configurat,configuration,13897,"d. Documentation:; | Task Container |; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html; | | s3:GetObject, s3:PutObject, s3:ListObjects; +----------------------------+; ```. 1. ECS has several sets of permissions for various items. AWS Batch, however,; does not take advantage of certain features of ECS, most importantly; ECS Services are out of scope of AWS Batch. ECS services require things; like load balancing registration and DNS updates. While there is; documentation regarding roles related to ECS services, these are; irrelevant to the Cromwell use case.; 2. Other than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding th",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:14381,Deployability,configurat,configuration,14381,"er than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of look",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:14465,Deployability,configurat,configuration,14465,"er than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of look",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15873,Deployability,configurat,configuration,15873,"e.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a res",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15942,Deployability,configurat,configuration,15942,"e.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a res",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16003,Deployability,integrat,integration,16003,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1809,Energy Efficiency,efficient,efficiently,1809,"t unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1978,Energy Efficiency,schedul,scheduled,1978,"is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16101,Energy Efficiency,reduce,reduced,16101,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:5471,Integrability,interface,interface,5471,"ses involved in running the backend are shown below. The; arrows represent the flow of job submission. ```text; +----------------------------------------+; | |; | AwsBatchBackendLifecycleActorFactory |; | |; +------------------+---------------------+; |; |; |; |; |; +------------------v----------------------+; | |; | AwsBatchAsyncBackendJobExecutionActor |; | |; +------------------+----------------------+; |; |; |; |; |; +-------v-------+ +-------------------------+; | | | |; | AwsBatchJob +-----------------> AwsBatchJobDefinition |; | | | |; +---------------+ +-------------------------+; ```. 1. The `AwsBatchBackendLifecycleActorFactory` class is configured by the user; as the Cromwell backend. This factory provides an object from the; `AwsBatchAsyncBackendJobExecutionActor` class to create and manage the job.; 2. The `AwsBatchAsyncBackendJobExecutionActor` creates and manages the job.; The job itself is encapsulated by the functionality in `AwsBatchJob`.; 3. `AwsBatchJob` is the primary interface to AWS Batch. It creates the; necessary `AwsBatchJobDefinition`, then submits the job using the SubmitJob; API.; 4. `AwsBatchJobDefinition` is responsible for the creation of the job definition.; In AWS Batch, every job must have a definition. Note that the job definition; can be overridden by the `SubmitJob`, so the `JobDefinition` contains core information such; as the docker image type while the `SubmitJob` contains details that are more related to; the actual task. AWS Batch Job Instantiation; ---------------------------; ```text; +--------------------+; | |; | Cromwell Backend |; | |; +---------+----------+; |; |; SubmitJob; |; |; +------v------+; | |; | AWS Batch |; | |; +------^------+; |; |; Polls; |; |; +------+------+; | |; | ECS Agent |; | |; +------+------+; |; Creates, Launches and Monitors; |; +--------v---------+ ; | |; | Task Container |; | |; +------------------+. ```. When a Cromwell task begins, the Cromwell backend will call the SubmitJob; API of AWS Ba",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:8577,Integrability,depend,dependencies,8577,"ations such as the MergeVCFs; task of the Haplotype caller test, that the container overrides length limit; of 8k was being exceeded. There are several limits described on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Jav",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:8664,Integrability,depend,dependencies,8664,"ations such as the MergeVCFs; task of the Haplotype caller test, that the container overrides length limit; of 8k was being exceeded. There are several limits described on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Jav",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:8834,Integrability,depend,dependencies,8834,"d on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possibl",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:8950,Integrability,depend,dependencies,8950,"ad size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required fo",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9503,Integrability,integrat,integrated,9503,"de to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 I",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:13674,Integrability,interface,interfaces,13674,"//docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html (Fargate); +-------------+--------------+ ; |; |; |; +-------------v--------------+; | | Task Role permissions. These are user defined, but ecs-tasks.amazon.com must have sts:AssumeRole trust relationship defined. Documentation:; | Task Container |; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html; | | s3:GetObject, s3:PutObject, s3:ListObjects; +----------------------------+; ```. 1. ECS has several sets of permissions for various items. AWS Batch, however,; does not take advantage of certain features of ECS, most importantly; ECS Services are out of scope of AWS Batch. ECS services require things; like load balancing registration and DNS updates. While there is; documentation regarding roles related to ECS services, these are; irrelevant to the Cromwell use case.; 2. Other than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attribut",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15581,Integrability,depend,dependency,15581,"lt-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context obj",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16003,Integrability,integrat,integration,16003,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16688,Integrability,depend,dependent,16688,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16905,Integrability,depend,dependencies,16905,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:845,Modifiability,config,configuration,845,"AWS Batch Backend Architecture; ==============================. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or m",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:892,Modifiability,config,configuration,892,"AWS Batch Backend Architecture; ==============================. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or m",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1077,Modifiability,config,configuration,1077,"=========. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed thro",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1374,Modifiability,config,configuration,1374,"s, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a co",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1444,Modifiability,config,configuration,1444,"s, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a co",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:2392,Modifiability,config,configured,2392,"guration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html). The ECS workers used by the AWS Batch backend can be any instance type and should; be based on an AMI running the ECS agent and docker. An ECS optimized AMI is recommended.; An EC2 LaunchTemplate is used to provide some additional ""on first boot"" configuration that:; 1. Installs AWS CLI v2,; 1. Installs a script to mount an EBS as a `btrf",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:3286,Modifiability,config,configuration,3286," that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html). The ECS workers used by the AWS Batch backend can be any instance type and should; be based on an AMI running the ECS agent and docker. An ECS optimized AMI is recommended.; An EC2 LaunchTemplate is used to provide some additional ""on first boot"" configuration that:; 1. Installs AWS CLI v2,; 1. Installs a script to mount an EBS as a `btrfs` file system that will auto-expand,; 1. Configures docker to use that file system so that the ""filesystem"" of the container; will auto-expand,; 1. Installs a `fetch_and_run.sh` script that allows the container to download ; generated shell scripts from S3 that contain the instructions of the workflow; task . ```text; +-------------+; | |; | AWS Batch |; | |; +------+------+; |; |; |; |; |; +----------------v------------------+; | |; | Elastic Container Service (ECS) |; | |; +----------------+------------------+; |; |; |; |; |; +------------------------v-------------------------+; | |; | AutoScaling Group |; | |; | +---------------------------------+ |; | | | |; | | EC2 Instance | |; | | | |; | | +--------------------+ | |; | | | | | |; | | | Docker Container | | |; | | | | | |; | | +--------------------+ ... | |; | | | |; | +---------------------------------+ ... |; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:5123,Modifiability,config,configured,5123,"| | |; | | | Docker Container | | |; | | | | | |; | | +--------------------+ ... | |; | | | |; | +---------------------------------+ ... |; | |; +--------------------------------------------------+. ```. Cromwell AWS Batch Backend; --------------------------. There are several scala classes as part of the AWS Batch Backend, but; the primary classes involved in running the backend are shown below. The; arrows represent the flow of job submission. ```text; +----------------------------------------+; | |; | AwsBatchBackendLifecycleActorFactory |; | |; +------------------+---------------------+; |; |; |; |; |; +------------------v----------------------+; | |; | AwsBatchAsyncBackendJobExecutionActor |; | |; +------------------+----------------------+; |; |; |; |; |; +-------v-------+ +-------------------------+; | | | |; | AwsBatchJob +-----------------> AwsBatchJobDefinition |; | | | |; +---------------+ +-------------------------+; ```. 1. The `AwsBatchBackendLifecycleActorFactory` class is configured by the user; as the Cromwell backend. This factory provides an object from the; `AwsBatchAsyncBackendJobExecutionActor` class to create and manage the job.; 2. The `AwsBatchAsyncBackendJobExecutionActor` creates and manages the job.; The job itself is encapsulated by the functionality in `AwsBatchJob`.; 3. `AwsBatchJob` is the primary interface to AWS Batch. It creates the; necessary `AwsBatchJobDefinition`, then submits the job using the SubmitJob; API.; 4. `AwsBatchJobDefinition` is responsible for the creation of the job definition.; In AWS Batch, every job must have a definition. Note that the job definition; can be overridden by the `SubmitJob`, so the `JobDefinition` contains core information such; as the docker image type while the `SubmitJob` contains details that are more related to; the actual task. AWS Batch Job Instantiation; ---------------------------; ```text; +--------------------+; | |; | Cromwell Backend |; | |; +---------+----------+; |; |; SubmitJob; |;",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:6967,Modifiability,variab,variables,6967,"lated to; the actual task. AWS Batch Job Instantiation; ---------------------------; ```text; +--------------------+; | |; | Cromwell Backend |; | |; +---------+----------+; |; |; SubmitJob; |; |; +------v------+; | |; | AWS Batch |; | |; +------^------+; |; |; Polls; |; |; +------+------+; | |; | ECS Agent |; | |; +------+------+; |; Creates, Launches and Monitors; |; +--------v---------+ ; | |; | Task Container |; | |; +------------------+. ```. When a Cromwell task begins, the Cromwell backend will call the SubmitJob; API of AWS Batch. From there, the backend will call the AWS Batch `DescribeJobs`; API to provide status to the Cromwell engine as requested. Once the job is Submitted in AWS Batch, one of the EC2 instances assigned; to the compute environment (a.k.a. ECS Cluster) with a running agent will; pick up the Cromwell Job/AWS Batch Job/ECS Task and run it. Importantly,; AWS Batch calls ECS' `RunTask` API when submitting the job. It uses the; task definition, and overrides both the command text and the environment; variables. Input files are read into the container from S3 and output files are copied back to; S3. Three additional files are also written to the S3 bucket using the names of these; environment variables:. * AWS_CROMWELL_RC_FILE (the return code of the task); * AWS_CROMWELL_STDOUT_FILE (STDOUT of the task); * AWS_CROMWELL_STDERR_FILE (STDERR of the task). These files are placed in the correct location in S3 after task execution. In addition; STDOUT and STDERR are fed to the tasks cloudwatch log. Input and Command Compression; -----------------------------. NOTE: All limits in this section are subject to change. In testing, specifically with large fan-in operations such as the MergeVCFs; task of the Haplotype caller test, that the container overrides length limit; of 8k was being exceeded. There are several limits described on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for Regi",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:7162,Modifiability,variab,variables,7162,"|; |; +------v------+; | |; | AWS Batch |; | |; +------^------+; |; |; Polls; |; |; +------+------+; | |; | ECS Agent |; | |; +------+------+; |; Creates, Launches and Monitors; |; +--------v---------+ ; | |; | Task Container |; | |; +------------------+. ```. When a Cromwell task begins, the Cromwell backend will call the SubmitJob; API of AWS Batch. From there, the backend will call the AWS Batch `DescribeJobs`; API to provide status to the Cromwell engine as requested. Once the job is Submitted in AWS Batch, one of the EC2 instances assigned; to the compute environment (a.k.a. ECS Cluster) with a running agent will; pick up the Cromwell Job/AWS Batch Job/ECS Task and run it. Importantly,; AWS Batch calls ECS' `RunTask` API when submitting the job. It uses the; task definition, and overrides both the command text and the environment; variables. Input files are read into the container from S3 and output files are copied back to; S3. Three additional files are also written to the S3 bucket using the names of these; environment variables:. * AWS_CROMWELL_RC_FILE (the return code of the task); * AWS_CROMWELL_STDOUT_FILE (STDOUT of the task); * AWS_CROMWELL_STDERR_FILE (STDERR of the task). These files are placed in the correct location in S3 after task execution. In addition; STDOUT and STDERR are fed to the tasks cloudwatch log. Input and Command Compression; -----------------------------. NOTE: All limits in this section are subject to change. In testing, specifically with large fan-in operations such as the MergeVCFs; task of the Haplotype caller test, that the container overrides length limit; of 8k was being exceeded. There are several limits described on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9532,Modifiability,config,configuration,9532,"de to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 I",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9693,Modifiability,config,configuration,9693," are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9758,Modifiability,variab,variables,9758," are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9778,Modifiability,config,configuration,9778,"nt amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within A",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:10451,Modifiability,config,config,10451,"he project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within AWS, everything must be authorized. This is a consistent rule, and as; such, AWS Services themselves are not immune to the rule. Therefore, customers; of AWS are responsible for granting services access to APIs within their account.; The flow described below represents the permissions needed by each stage, from ; Cromwell server through the task running. This includes the permissions needed for; the AWS Services involved in the processing of the work. ```text; +----------------------------+; | | s3:GetObject on bucket for workflow and script bucket; | | s3:ListObjects on script bucket; | | s3:PutObject on script bucket; | Cromwell",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:10708,Modifiability,config,configuration,10708,"tion for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within AWS, everything must be authorized. This is a consistent rule, and as; such, AWS Services themselves are not immune to the rule. Therefore, customers; of AWS are responsible for granting services access to APIs within their account.; The flow described below represents the permissions needed by each stage, from ; Cromwell server through the task running. This includes the permissions needed for; the AWS Services involved in the processing of the work. ```text; +----------------------------+; | | s3:GetObject on bucket for workflow and script bucket; | | s3:ListObjects on script bucket; | | s3:PutObject on script bucket; | Cromwell | batch:RegisterTaskDefinition; | | batch:SubmitJob; | | batch:DescribeJobs; | | batch:DescribeJobDefinitions; +-------------+--------------+; |; |; |; +-------------v--------------+; | | AWSBatchServiceRole managed policy - described at:; | AWS Batch |; |",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:13897,Modifiability,config,configuration,13897,"d. Documentation:; | Task Container |; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html; | | s3:GetObject, s3:PutObject, s3:ListObjects; +----------------------------+; ```. 1. ECS has several sets of permissions for various items. AWS Batch, however,; does not take advantage of certain features of ECS, most importantly; ECS Services are out of scope of AWS Batch. ECS services require things; like load balancing registration and DNS updates. While there is; documentation regarding roles related to ECS services, these are; irrelevant to the Cromwell use case.; 2. Other than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding th",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:14381,Modifiability,config,configuration,14381,"er than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of look",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:14465,Modifiability,config,configuration,14465,"er than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of look",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:14484,Modifiability,portab,portable,14484,"er than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of look",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15873,Modifiability,config,configuration,15873,"e.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a res",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15942,Modifiability,config,configuration,15942,"e.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a res",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16291,Modifiability,inherit,inherited,16291,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:16396,Modifiability,variab,variables,16396,"n conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time. Workflow root vs call root is a great example. Adding additional; comments and potentially context objects to encapsulate state would make; the codebase more approachable.; * There is a significant amount of dependent libraries (with more added by; the introduction of the S3 filesystem and the AWS SDK). Dependency management; is challenging as a result. Adding significant new functionality is relatively; painful when new dependencies are needed.; ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:544,Performance,tune,tune,544,"AWS Batch Backend Architecture; ==============================. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or m",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:1953,Performance,queue,queue,1953,"is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:3182,Performance,optimiz,optimized,3182,"CS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html). The ECS workers used by the AWS Batch backend can be any instance type and should; be based on an AMI running the ECS agent and docker. An ECS optimized AMI is recommended.; An EC2 LaunchTemplate is used to provide some additional ""on first boot"" configuration that:; 1. Installs AWS CLI v2,; 1. Installs a script to mount an EBS as a `btrfs` file system that will auto-expand,; 1. Configures docker to use that file system so that the ""filesystem"" of the container; will auto-expand,; 1. Installs a `fetch_and_run.sh` script that allows the container to download ; generated shell scripts from S3 that contain the instructions of the workflow; task . ```text; +-------------+; | |; | AWS Batch |; | |; +------+------+; |; |; |; |; |; +----------------v------------------+; | |; | Elastic Container Service (ECS) |; | |; +----------------+------------------+; |; |; |; |; |; +------------------------v-------------------------+; | |; | AutoScaling Group |; | |; | +---------------------------------+ |; | | | |; | | EC2 Instance | |; | | | |; | | +--------------------+ | |; | | | | | |; | | | Docker Container | | |; | | | | | |; | | +------------------",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9072,Performance,throughput,throughput,9072,"ask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies were also leveraged. These two are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:9728,Performance,perform,performed,9728," are:. * AWS Java SDK v2; * elerch/S3 Filesystem. The Java SDK version two carries with it a significant amount of additional; dependencies. These had a significant effect on the ability of Cromwell to; compile, but ultimately the overlapping dependencies came down to:. * com.fasterxml.jackson.core (jackson-annotations):; Jackson version was bumped to accomodate throughput; * org.slf4j (jcl-over-slf4j):; Ignored by aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:13329,Performance,load,load,13329,"2ContainerServiceforEC2Role managed policy, described at:; | ECS Agent (running on EC2) | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html (EC2); | | OR; | | AmazonECSTaskExecutionRolePolicy managed policy, described at: ; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html (Fargate); +-------------+--------------+ ; |; |; |; +-------------v--------------+; | | Task Role permissions. These are user defined, but ecs-tasks.amazon.com must have sts:AssumeRole trust relationship defined. Documentation:; | Task Container |; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html; | | s3:GetObject, s3:PutObject, s3:ListObjects; +----------------------------+; ```. 1. ECS has several sets of permissions for various items. AWS Batch, however,; does not take advantage of certain features of ECS, most importantly; ECS Services are out of scope of AWS Batch. ECS services require things; like load balancing registration and DNS updates. While there is; documentation regarding roles related to ECS services, these are; irrelevant to the Cromwell use case.; 2. Other than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ----------------",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:14554,Performance,queue,queueArn,14554,"er than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of look",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:470,Security,expose,exposes,470,"AWS Batch Backend Architecture; ==============================. Overview; --------. The architecture of the code base follows very closely to the Google version.; Probably a little too closely, and lots of code was lifted from the Google; backend originally, then modified to work with AWS. Fundamentally, Google Pipelines API (a.k.a. PAPI) works pretty differently from; AWS Batch. In Pipelines, all the infrastructure is completely managed by Google,; while AWS Batch exposes that infrastructure to a large degree so that customers; can fine tune it as necessary. An implementation that uses Fargate might be an ; alternative that is closer or an implementation that uses Step Functions although; that would be a separate backend. From a Cromwell perspective, this means that unlike Pipelines, where; infrastructure details are defined in the configuration or the WDL, in AWS; Batch, these configuration details are handled outside. All the AWS Batch; backend needs to know is ""what is the ARN for the job Queue""?. A good example of the difference can be seen in the 'disks' configuration. In; Pipelines, you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or m",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:10117,Security,access,access,10117," aws sdk in favor of the version bundled in Cromwell; * nettyHandler:; Version bumped to accomodate AWS SDK; * sttpV:; Version bumped to accomodate AWS SDK. While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within AWS, everything must be authorized. This is a consistent rule, and as; such, AWS Services themselves are not immune to the rule. Therefore, customers; of AWS are responsible for granting services access to APIs within their account.; The flow described below represents the permissions needed by each stage, from ; Cr",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:10832,Security,authoriz,authorized,10832,"isjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within AWS, everything must be authorized. This is a consistent rule, and as; such, AWS Services themselves are not immune to the rule. Therefore, customers; of AWS are responsible for granting services access to APIs within their account.; The flow described below represents the permissions needed by each stage, from ; Cromwell server through the task running. This includes the permissions needed for; the AWS Services involved in the processing of the work. ```text; +----------------------------+; | | s3:GetObject on bucket for workflow and script bucket; | | s3:ListObjects on script bucket; | | s3:PutObject on script bucket; | Cromwell | batch:RegisterTaskDefinition; | | batch:SubmitJob; | | batch:DescribeJobs; | | batch:DescribeJobDefinitions; +-------------+--------------+; |; |; |; +-------------v--------------+; | | AWSBatchServiceRole managed policy - described at:; | AWS Batch |; | | https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html; +-------------+--------------+; |; |; |; +",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:11004,Security,access,access,11004," in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within AWS, everything must be authorized. This is a consistent rule, and as; such, AWS Services themselves are not immune to the rule. Therefore, customers; of AWS are responsible for granting services access to APIs within their account.; The flow described below represents the permissions needed by each stage, from ; Cromwell server through the task running. This includes the permissions needed for; the AWS Services involved in the processing of the work. ```text; +----------------------------+; | | s3:GetObject on bucket for workflow and script bucket; | | s3:ListObjects on script bucket; | | s3:PutObject on script bucket; | Cromwell | batch:RegisterTaskDefinition; | | batch:SubmitJob; | | batch:DescribeJobs; | | batch:DescribeJobDefinitions; +-------------+--------------+; |; |; |; +-------------v--------------+; | | AWSBatchServiceRole managed policy - described at:; | AWS Batch |; | | https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html; +-------------+--------------+; |; |; |; +-------------v--------------+; | | AWSServiceRoleForECS Service-linked role, documented at:; | |; | Elastic Container Service | https://docs.aws.amazon.com/AmazonECS/",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:13508,Security,access,access,13508,"//docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html (Fargate); +-------------+--------------+ ; |; |; |; +-------------v--------------+; | | Task Role permissions. These are user defined, but ecs-tasks.amazon.com must have sts:AssumeRole trust relationship defined. Documentation:; | Task Container |; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html; | | s3:GetObject, s3:PutObject, s3:ListObjects; +----------------------------+; ```. 1. ECS has several sets of permissions for various items. AWS Batch, however,; does not take advantage of certain features of ECS, most importantly; ECS Services are out of scope of AWS Batch. ECS services require things; like load balancing registration and DNS updates. While there is; documentation regarding roles related to ECS services, these are; irrelevant to the Cromwell use case.; 2. Other than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attribut",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:13722,Security,access,access,13722,"-------------+; | | Task Role permissions. These are user defined, but ecs-tasks.amazon.com must have sts:AssumeRole trust relationship defined. Documentation:; | Task Container |; | | https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html; | | s3:GetObject, s3:PutObject, s3:ListObjects; +----------------------------+; ```. 1. ECS has several sets of permissions for various items. AWS Batch, however,; does not take advantage of certain features of ECS, most importantly; ECS Services are out of scope of AWS Batch. ECS services require things; like load balancing registration and DNS updates. While there is; documentation regarding roles related to ECS services, these are; irrelevant to the Cromwell use case.; 2. Other than access to the main Cromwell bucket, the task container itself ; does not need additional permissions unless; the task in the WDL has been defined with a command that interfaces; with AWS directly. This may include access to additional s3 buckets containing; things like reference genome files. Task container permissions are currently supported through ECS and AWS; Batch, but there is no configuration currently wired for the Cromwell; AWS Backend to pass these settings to AWS. As such, the task container; permissions must be managed by attaching a role to the EC2 Instance; with permissions necessary for both the ECS Agent and the task container. NOTE: ECS Agent permissions currently must use the permissions as outlined; in the AmazonEC2ContainerServiceForEC2Role managed policy. Future considerations; ---------------------. AWS Batch Backend. * Should the 'disks' configuration be renamed to mount points or maybe; ignored? This might make the wdl configuration more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15478,Security,hash,hashed,15478,"more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time.",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:15512,Security,hash,hash,15512,"more portable between backends; * The intent is to be able to override the queueArn between the; default-runtime-attributes and the runtime attributes in the WDL. This; appears broken at the moment.; * Job caching appears to be broken at the moment. Identical tasks need not be repeated; if the results of a previous run of the task are still available.; * Retrying failed jobs is not currently attempted. Adding this would be beneficial ; especially in conjunction with result caching; * Some S3 FS stuff can be removed: It looks like theres a bunch of leftover; unused code here from before having the nio implementation; [here](https://github.com/broadinstitute/cromwell/tree/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch); Also, I recommend adding another case for S3; [here](https://github.com/broadinstitute/cromwell/blob/7a830a7fceaab9e7eaaa6802e58fe3cfd5d411a8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L95); otherwise files will be pulled down to be hashed instead of looking up the; hash from S3 metadata. You might need to add the s3 Cromwell fs as a dependency; to the engine to be able to do that.; * S3 Filesystem should be an official AWS thing (e.g. upplication or awslabs account); * 8k container overrides limit should be bigger (ECS); * We should understand why AWS Batch is putting everything in container overrides; * Authentication configuration consistency between S3FS and AWS Batch backend; * Full configuration of jobs in AWS Batch. Cromwell. * The style of integration with backends requires a lot of boilerplate code; that I believe can be significantly reduced via heavier use of traits and; supporting libraries; * There is a lot of data available within the backend if you know where; to look, but if you don't, there is a lot of looking at inherited classes; etc. to try to find them. Often, lack of code comments, etc and very similarly; named variables can be confusing for folks confronting the code base for the; first time.",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:7464,Testability,log,log,7464," call the SubmitJob; API of AWS Batch. From there, the backend will call the AWS Batch `DescribeJobs`; API to provide status to the Cromwell engine as requested. Once the job is Submitted in AWS Batch, one of the EC2 instances assigned; to the compute environment (a.k.a. ECS Cluster) with a running agent will; pick up the Cromwell Job/AWS Batch Job/ECS Task and run it. Importantly,; AWS Batch calls ECS' `RunTask` API when submitting the job. It uses the; task definition, and overrides both the command text and the environment; variables. Input files are read into the container from S3 and output files are copied back to; S3. Three additional files are also written to the S3 bucket using the names of these; environment variables:. * AWS_CROMWELL_RC_FILE (the return code of the task); * AWS_CROMWELL_STDOUT_FILE (STDOUT of the task); * AWS_CROMWELL_STDERR_FILE (STDERR of the task). These files are placed in the correct location in S3 after task execution. In addition; STDOUT and STDERR are fed to the tasks cloudwatch log. Input and Command Compression; -----------------------------. NOTE: All limits in this section are subject to change. In testing, specifically with large fan-in operations such as the MergeVCFs; task of the Haplotype caller test, that the container overrides length limit; of 8k was being exceeded. There are several limits described on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and S",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:7590,Testability,test,testing,7590,"e compute environment (a.k.a. ECS Cluster) with a running agent will; pick up the Cromwell Job/AWS Batch Job/ECS Task and run it. Importantly,; AWS Batch calls ECS' `RunTask` API when submitting the job. It uses the; task definition, and overrides both the command text and the environment; variables. Input files are read into the container from S3 and output files are copied back to; S3. Three additional files are also written to the S3 bucket using the names of these; environment variables:. * AWS_CROMWELL_RC_FILE (the return code of the task); * AWS_CROMWELL_STDOUT_FILE (STDOUT of the task); * AWS_CROMWELL_STDERR_FILE (STDERR of the task). These files are placed in the correct location in S3 after task execution. In addition; STDOUT and STDERR are fed to the tasks cloudwatch log. Input and Command Compression; -----------------------------. NOTE: All limits in this section are subject to change. In testing, specifically with large fan-in operations such as the MergeVCFs; task of the Haplotype caller test, that the container overrides length limit; of 8k was being exceeded. There are several limits described on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:7693,Testability,test,test,7693,"e compute environment (a.k.a. ECS Cluster) with a running agent will; pick up the Cromwell Job/AWS Batch Job/ECS Task and run it. Importantly,; AWS Batch calls ECS' `RunTask` API when submitting the job. It uses the; task definition, and overrides both the command text and the environment; variables. Input files are read into the container from S3 and output files are copied back to; S3. Three additional files are also written to the S3 bucket using the names of these; environment variables:. * AWS_CROMWELL_RC_FILE (the return code of the task); * AWS_CROMWELL_STDOUT_FILE (STDOUT of the task); * AWS_CROMWELL_STDERR_FILE (STDERR of the task). These files are placed in the correct location in S3 after task execution. In addition; STDOUT and STDERR are fed to the tasks cloudwatch log. Input and Command Compression; -----------------------------. NOTE: All limits in this section are subject to change. In testing, specifically with large fan-in operations such as the MergeVCFs; task of the Haplotype caller test, that the container overrides length limit; of 8k was being exceeded. There are several limits described on AWS Batch,; and a limit for container overrides on ECS, all of which should be considered. * Maximum payload size for RegisterJobDefinition calls: 24KiB; * Maximum payload size for SubmitJob calls: 30KiB; * Maximum JSON payload for ECS RunTask containerOverrides values: 8KiB. Effective limits, however, are much, much smaller. While both AWS Batch and; ECS have command and environment as part of their Job/Task definitions; respectively, AWS Batch passes both command and environment through to ECS; based solely on RunTask. While a lot of effort was initially placed on; balancing payloads between RegisterJobDefinition and SubmitJob, because; everything is passed as an override to RunTask, we're gated by the ECS; RunTask 8KiB limit. Dependencies; ------------. Two dependencies were added to Cromwell as part of this project, though; existing Cromwell dependencies ",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:2125,Usability,simpl,simply,2125,"you need to specify the type of disk and size. In AWS, this will; be defined instead when you setup your environment (more on that later), so; all the AWS backend really needs to know is what mount points you need; defined. This infrastructure and all the associated configuration still exists; however,; it is moved out of the Cromwell configuration. AWS Batch; ---------. Because AWS Batch is so different from PAPI, those familiar only with PAPI; would be best off with an overview of AWS Batch. If you are familiar with; the workings of Batch, feel free to skip this section, and move on. [AWS Batch](https://aws.amazon.com/batch/) fundamentally is a service to allow batch jobs to run easily and; efficiently. To use it effectively, however, you need to understand its own; technical stack. To create a job, you need a ""Job Queue"". That job queue allows; jobs to be scheduled onto one or more ""Compute Environments"". This can; be managed through AWS Batch, but when AWS Batch sets up a compute environment,; it's simply setting up an Elastic Container Service (ECS) Cluster. The ECS; cluster, in turn is just a few managed CloudFormation templates, that is; controlling an AutoScaling group of EC2 instances. What really makes an ECS instance an ECS instance is the presence of a configured; [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent). This agent polls; the ECS service to determine if there are any tasks to run. An AWS Batch Job; will be turned into an ECS task. From there, the agent will pick it up and; manage it, sending updates back to ECS (and from there AWS Batch) on a regular; basis. There are some limits that will impact the design of the AWS Batch backend, and; will be discussed later. These are:. * [AWS Batch Limits](https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html); * [8k container overrides limit.](https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html). The ECS workers used by the AWS Batch backend can be any instance ty",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md:10286,Usability,guid,guide,10286,". While the AWS SDK v2 has nio capabilities, it does not include; a FileSystemProvider to allow Cromwell to operate seamlessly with AWS S3.; This was found later in the project, and as such, the filesystem provider; is not integrated with the Cromwell configuration system. The S3 FS was; forked from Udacity's provider, which was based on AWS SDK for Java version 1.; Of particular note is the fact that all API configuration for the provider; is performed through environment variables. As such, configuration is possible,; but currently disjoint from Cromwell proper. Authentication; --------------. Authentication is required for both the Cromwell AWS Backend and the S3; Filesystem provider. By default, in both cases, the default credential provider; chain is followed. AWS tools all follow the same prioritized set of; checks for access key/secret key (and optionally token), with the exception; of #2 below. [Default credential provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html); 1. Configured permissions (environment); 2. (NOTE BELOW) Java properties; 3. Access key/secret key as defined in $HOME/.aws/credentials and config; 4. Container role; 5. EC2 Instance role. NOTE: The Java properties check is specific and unique to the Java SDK. This; does not apply to other SDKs or tools (e.g. the CLI). Normally customers will be using an EC2 Instance role (recommended) or file configuration; as described in #3 (not recommended in production). Permissions; -----------. Within AWS, everything must be authorized. This is a consistent rule, and as; such, AWS Services themselves are not immune to the rule. Therefore, customers; of AWS are responsible for granting services access to APIs within their account.; The flow described below represents the permissions needed by each stage, from ; Cromwell server through the task running. This includes the permissions needed for; the AWS Services involved in the processing of the work. ```text; +------",MatchSource.DOCS,supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:426,Deployability,continuous,continuously,426,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:1104,Deployability,configurat,configuration,1104,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:19,Energy Efficiency,monitor,monitor,19,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:59,Energy Efficiency,monitor,monitoring,59,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:166,Energy Efficiency,monitor,monitoring,166,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:180,Energy Efficiency,monitor,monitor,180,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:192,Energy Efficiency,monitor,monitor,192,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:272,Energy Efficiency,monitor,monitoring,272,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:1001,Energy Efficiency,power,powerful,1001,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:1104,Modifiability,config,configuration,1104,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md:1065,Performance,optimiz,optimization,1065,"# Stackdriver task monitor*. This folder contains code for monitoring resource utilization in PAPIv2 tasks; through [Stackdriver Monitoring](https://cloud.google.com/monitoring). [monitor.py](monitor.py) script; is intended to be used as a Docker image, via a background ""monitoring action"" in PAPIv2.; The image can be specified through `monitoring_image` workflow option. It uses [psutil](https://psutil.readthedocs.io) to; continuously measure CPU, memory and disk space utilization; and disk IOPS, and periodically report them; as distinct metrics to Stackdriver Monitoring API. The labels for each time point contain; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. This approach enables:. 1) Users to easily plot real-time resource usage statistics across all tasks in; a workflow, or for a single task call across many workflow runs,; etc. This can be very powerful to quickly determine the outlier tasks; that could use optimization, without the need for any configuration; or code. 2) Scripts to easily get aggregate statistics; on resource utilization and to produce suggestions; based on those. [*] Detailed discussion: [PR 4510](https://github.com/broadinstitute/cromwell/pull/4510).; ",MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/README.md
https://github.com/broadinstitute/cromwell/tree/87/CromwellRefdiskManifestCreator/src/test/resources/test-directory/nested-directory/test-file-nested.txt:7,Testability,test,test,7,Nested test file; ,MatchSource.DOCS,CromwellRefdiskManifestCreator/src/test/resources/test-directory/nested-directory/test-file-nested.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/CromwellRefdiskManifestCreator/src/test/resources/test-directory/nested-directory/test-file-nested.txt
https://github.com/broadinstitute/cromwell/tree/87/services/src/test/resources/describe/wdl_1_0/relative_url_imports/workflow_url.txt:93,Security,validat,validate,93,https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/relative_imports.wdl; ,MatchSource.DOCS,services/src/test/resources/describe/wdl_1_0/relative_url_imports/workflow_url.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/services/src/test/resources/describe/wdl_1_0/relative_url_imports/workflow_url.txt
https://github.com/broadinstitute/cromwell/tree/87/services/src/test/resources/describe/wdl_1_0/relative_url_imports/workflow_url.txt:78,Testability,test,test,78,https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/relative_imports.wdl; ,MatchSource.DOCS,services/src/test/resources/describe/wdl_1_0/relative_url_imports/workflow_url.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/services/src/test/resources/describe/wdl_1_0/relative_url_imports/workflow_url.txt
https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/requirements.txt:13,Energy Efficiency,monitor,monitoring,13,google-cloud-monitoring; psutil; requests; ,MatchSource.DOCS,supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/requirements.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/supportedBackends/google/pipelines/v2beta/src/main/resources/cromwell-monitor/requirements.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/after_circle/error.txt:102,Integrability,depend,dependency,102,"Failed to process workflow definition 'after_circle' (reason 1 of 1): This workflow contains a cyclic dependency:; ""Call ""foo as foo1 after foo2"""" -> ""Call ""foo as foo2 after foo1""""; ""Call ""foo as foo2 after foo1"""" -> ""Call ""foo as foo1 after foo2""""; ",MatchSource.DOCS,womtool/src/test/resources/validate/biscayne/invalid/after_circle/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/after_circle/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/object_type/error.txt:112,Availability,avail,available,112,Failed to process workflow definition 'object_type' (reason 1 of 2): No struct definition for 'Object' found in available structs: []; Failed to process workflow definition 'object_type' (reason 2 of 2): No struct definition for 'Object' found in available structs: []; ,MatchSource.DOCS,womtool/src/test/resources/validate/biscayne/invalid/object_type/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/object_type/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/object_type/error.txt:247,Availability,avail,available,247,Failed to process workflow definition 'object_type' (reason 1 of 2): No struct definition for 'Object' found in available structs: []; Failed to process workflow definition 'object_type' (reason 2 of 2): No struct definition for 'Object' found in available structs: []; ,MatchSource.DOCS,womtool/src/test/resources/validate/biscayne/invalid/object_type/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/object_type/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt:114,Availability,avail,available,114,Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_object is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_objects is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_object is no longer available in this WDL version. Consider using read_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_objects is no longer available in this WDL version. Consider using read_json instead; ,MatchSource.DOCS,womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt:295,Availability,avail,available,295,Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_object is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_objects is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_object is no longer available in this WDL version. Consider using read_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_objects is no longer available in this WDL version. Consider using read_json instead; ,MatchSource.DOCS,womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt:474,Availability,avail,available,474,Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_object is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_objects is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_object is no longer available in this WDL version. Consider using read_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_objects is no longer available in this WDL version. Consider using read_json instead; ,MatchSource.DOCS,womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt:653,Availability,avail,available,653,Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_object is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): write_objects is no longer available in this WDL version. Consider using write_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_object is no longer available in this WDL version. Consider using read_json instead; Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): read_objects is no longer available in this WDL version. Consider using read_json instead; ,MatchSource.DOCS,womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/biscayne/invalid/read_and_write_object/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft2/invalid/cycle/error.txt:30,Integrability,depend,dependencies,30,"This workflow contains cyclic dependencies containing these edges: [Call cycle.m1], [Call cycle.m2], [Call cycle.m3]; ",MatchSource.DOCS,womtool/src/test/resources/validate/wdl_draft2/invalid/cycle/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft2/invalid/cycle/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft2/invalid/unexpected_call_input_failure/error.txt:170,Availability,error,error,170,"ERROR: Call supplied an unexpected input: The 'hello' task doesn't have an input called 'greeting':. greeting = ""bonjour"" # But this is an unexpected input! Should be an error!; ^. Options:; - Add the input 'greeting' to the 'hello' task (defined on line 13).; - Remove 'greeting = ...' from hello's inputs (on line 5).; ",MatchSource.DOCS,womtool/src/test/resources/validate/wdl_draft2/invalid/unexpected_call_input_failure/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft2/invalid/unexpected_call_input_failure/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft2/invalid/unexpected_subworkflow_call_input_failure/error.txt:252,Integrability,depend,depend,252,"ERROR: Call supplied an unexpected input: The 'subwf' task doesn't have an input called 'j':. call subwf.subwf { input: i = 10, j = 20 }; ^. Options:; - Add the input 'j' to the 'subwf' task (defined on line 1).; - When calling a workflow, values that depend on previous values are considered intermediate values rather than overridable inputs.; - You can allow overriding intermediate values by having an optional override input and a select_first, eg:; # This is an optional input to the workflow:; Int? override_x. # This is a value based on some upstream task or declaration:; Int some_previous_result = ... # This allows us to override an upstream result with override_x, or just use the previous result otherwise:; Int x = select_first(override_x, some_previous_result). - Remove 'j = ...' from subwf's inputs (on line 4).; ",MatchSource.DOCS,womtool/src/test/resources/validate/wdl_draft2/invalid/unexpected_subworkflow_call_input_failure/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft2/invalid/unexpected_subworkflow_call_input_failure/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft3/invalid/cycle/error.txt:95,Integrability,depend,dependency,95,"Failed to process workflow definition 'cycle' (reason 1 of 1): This workflow contains a cyclic dependency:; ""Call ""mirror as m1"""" -> ""Call ""mirror as m2""""; ""Call ""mirror as m2"""" -> ""Call ""mirror as m3""""; ""Call ""mirror as m3"""" -> ""Call ""mirror as m1""""; ",MatchSource.DOCS,womtool/src/test/resources/validate/wdl_draft3/invalid/cycle/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft3/invalid/cycle/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft3/invalid/directory_type/error.txt:114,Availability,avail,available,114,Failed to process task definition 'make_directory' (reason 1 of 1): No struct definition for 'Directory' found in available structs: []; Failed to process task definition 'read_from_directory' (reason 1 of 1): No struct definition for 'Directory' found in available structs: []; ,MatchSource.DOCS,womtool/src/test/resources/validate/wdl_draft3/invalid/directory_type/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft3/invalid/directory_type/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft3/invalid/directory_type/error.txt:256,Availability,avail,available,256,Failed to process task definition 'make_directory' (reason 1 of 1): No struct definition for 'Directory' found in available structs: []; Failed to process task definition 'read_from_directory' (reason 1 of 1): No struct definition for 'Directory' found in available structs: []; ,MatchSource.DOCS,womtool/src/test/resources/validate/wdl_draft3/invalid/directory_type/error.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate/wdl_draft3/invalid/directory_type/error.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/afters_and_scatters/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; None; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/afters_and_scatters/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/afters_and_scatters/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt:138,Security,validat,validate,138,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt:295,Security,validat,validate,295,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt:453,Security,validat,validate,453,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt:123,Testability,test,test,123,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt:280,Testability,test,test,280,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt:438,Testability,test,test,438,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; https://raw.githubusercontent.com/broadinstitute/cromwell/develop/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/http_relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt:97,Security,validat,validate,97,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt:213,Security,validat,validate,213,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt:330,Security,validat,validate,330,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt:82,Testability,test,test,82,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt:198,Testability,test,test,198,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt:315,Testability,test,test,315,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/structs/my_struct.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/tasks/add5.wdl; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/biscayne/valid/relative_imports/sub_wfs/foo.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/biscayne/relative_imports/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/5e0197d1c016d4c802ef3c2890f0ca4e0ca542c1/womtool/src/test/resources/validate/wdl_draft2/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt:171,Security,validat,validate,171,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/5e0197d1c016d4c802ef3c2890f0ca4e0ca542c1/womtool/src/test/resources/validate/wdl_draft2/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt:156,Testability,test,test,156,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/5e0197d1c016d4c802ef3c2890f0ca4e0ca542c1/womtool/src/test/resources/validate/wdl_draft2/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/http_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft2/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt:97,Security,validat,validate,97,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft2/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt:82,Testability,test,test,82,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft2/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/relative_local_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft2/valid/subworkflow_input/subworkflow.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt:97,Security,validat,validate,97,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft2/valid/subworkflow_input/subworkflow.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt:82,Testability,test,test,82,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft2/valid/subworkflow_input/subworkflow.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/subworkflow_input/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/task_only/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; None; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft2/task_only/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft2/task_only/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/5e0197d1c016d4c802ef3c2890f0ca4e0ca542c1/womtool/src/test/resources/validate/wdl_draft3/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt:171,Security,validat,validate,171,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/5e0197d1c016d4c802ef3c2890f0ca4e0ca542c1/womtool/src/test/resources/validate/wdl_draft3/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt:156,Testability,test,test,156,Success!; List of Workflow dependencies is:; https://raw.githubusercontent.com/broadinstitute/cromwell/5e0197d1c016d4c802ef3c2890f0ca4e0ca542c1/womtool/src/test/resources/validate/wdl_draft3/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/http_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft3/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt:97,Security,validat,validate,97,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft3/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt:82,Testability,test,test,82,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft3/valid/task_only/task_only.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/relative_local_import/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft3/valid/subworkflow_input/subworkflow.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt:97,Security,validat,validate,97,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft3/valid/subworkflow_input/subworkflow.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt:82,Testability,test,test,82,Success!; List of Workflow dependencies is:; {REPLACE_WITH_ROOT_PATH}/womtool/src/test/resources/validate/wdl_draft3/valid/subworkflow_input/subworkflow.wdl; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/subworkflow_input/expected_imports.txt
https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/task_only/expected_imports.txt:27,Integrability,depend,dependencies,27,Success!; List of Workflow dependencies is:; None; ,MatchSource.DOCS,womtool/src/test/resources/validate-with-imports/wdl_draft3/task_only/expected_imports.txt,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/womtool/src/test/resources/validate-with-imports/wdl_draft3/task_only/expected_imports.txt
