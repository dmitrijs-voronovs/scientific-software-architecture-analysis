id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/8469:48152,Integrability,wrap,wrapper,48152,"38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Traceback (most recent call last):; File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 199, in run; new_deps = self._run_get_new_deps(); File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 141, in _run_get_new_deps; task_gen = self.task.run(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 54, in run; self.read_vcf_write_mt(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 84, in read_vcf_write_mt; mt.write(self.output().path, stage_locally=True, overwrite=True); File ""<decorator-gen-1092>"", line 2, in write; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:49403,Integrability,Wrap,WrappedArray,49403,"ail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:77); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:49424,Integrability,Wrap,WrappedArray,49424,"nd.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:77); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:84334,Integrability,Wrap,WrappedMatrixWriter,84334,anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:989); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:936); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:586); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:85890,Integrability,Wrap,WrappedArray,85890,ss.scala:50); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:77); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:85911,Integrability,Wrap,WrappedArray,85911, is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); 	at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:77); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:21981,Modifiability,plugin,plugin,21981,"27745>; ##contig=<ID=chrUn_KI270753v1,length=62944>; ##contig=<ID=chrUn_KI270754v1,length=40191>; ##contig=<ID=chrUn_KI270755v1,length=36723>; ##contig=<ID=chrUn_KI270756v1,length=79590>; ##contig=<ID=chrUn_KI270757v1,length=71251>; ##contig=<ID=chrUn_GL000214v1,length=137718>; ##contig=<ID=chrUn_KI270742v1,length=186739>; ##contig=<ID=chrUn_GL000216v2,length=176608>; ##contig=<ID=chrUn_GL000218v1,length=161147>; ##contig=<ID=chrEBV,length=171823>; ##contig=<ID=hs38d1,length=10560522>; ##bcftools_pluginVersion=1.9+htslib-1.9; ##bcftools_pluginCommand=plugin fill-AN-AC; Date=Sat Dec 29 14:52:44 2018; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:84691,Modifiability,rewrite,rewrite,84691,runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:989); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:936); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:586); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:2226,Performance,optimiz,optimizations,2226, --gvcf-gq-bands 17 --gvcf-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --gvcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision fal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:24705,Performance,cache,cache,24705,"-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false"",Version=4.0.1.2,Date=""March 22, 2018 1:12:03 AM EDT"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##VEP=""v91"" time=""2018-03-22 04:15:25"" cache=""/media/SE5/.vep/homo_sapiens/91_GRCh38"" db=""homo_sapiens_core_91_38@ensembldb.ensembl.org"" ensembl-variation=91.c78d8b4 ensembl-funcgen=91.4681d69 ensembl-io=91.923d668 ensembl=91.18ee742 1000genomes=""phase3"" COSMIC=""82"" ClinVar=""201710"" ESP=""V2-SSA137"" HGMD-PUBLIC=""20172"" assembly=""GRCh38.p10"" dbSNP=""150"" gencode=""GENCODE 27"" genebuild=""2014-07"" gnomAD=""170228"" polyphen=""2.2.2"" regbuild=""16"" sift=""sift5.2.2""; ##INFO=<ID=CSQ,Number=.,Type=String,Description=""Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|gnomAD_AF|gnomAD_AFR_AF|gnomAD_AMR_AF|gnomAD_ASJ_AF|gnomAD_EAS_AF|gnomAD_FIN_AF|gnomAD_NFE_AF|gn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:36112,Performance,load,loading-cluster-m,36112,"1_70_65,GAAGTCTCATCCCATC-1_45;AACGTCAGTGTTTCAG-1_74:1:19:255	0/1:2,4:6:60:101,0,60:.:.:.:.; chr1	12198	rs62635282	G	C	69.6	.	BaseQRankSum=0;ClippingRankSum=0;DB;ExcessHet=3.0103;FS=4.771;MQ=42;MQRankSum=-0.967;QD=23.2;ReadPosRankSum=0.967;SOR=2.225;CSQ=C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000450305|transcribed_unprocessed_pseudogene|2/6||||68|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000456328|processed_transcript|1/3||||330|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|downstream_gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinva",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:36767,Performance,load,loading-cluster-m,36767,"gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:46930,Performance,load,loading-cluster-m,46930,"s: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'xpos': int64; 'xstart': int64; 'xstop': int64; ----------------------------------------; Entry fields:; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'PL': array<int32>; 'BX': array<str>; 'PS': int32; 'PQ': int32; 'JQ': int32; 'MIN_DP': int32; 'PGT': call; 'PID': str; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 4:===================================================> (480 + 20) / 500]2020-04-05 14:09:48 Hail: INFO: Coerced almost-sorted dataset; [Stage 5:======================================================>(498 + 2) / 500]2020-04-05 14:09:50 Hail: INFO: Coerced almost-sorted dataset; [Stage 7:> (0 + 108) / 500]ERROR: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) failed SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:48470,Performance,load,loads,48470,"ent call last):; File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 199, in run; new_deps = self._run_get_new_deps(); File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 141, in _run_get_new_deps; task_gen = self.task.run(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 54, in run; self.read_vcf_write_mt(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 84, in read_vcf_write_mt; mt.write(self.output().path, stage_locally=True, overwrite=True); File ""<decorator-gen-1092>"", line 2, in write; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowerin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:51226,Performance,load,loading-cluster-sw-,51226," 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:81881,Performance,concurren,concurrent,81881,ollection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:81966,Performance,concurren,concurrent,81966,tion.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:118134,Performance,concurren,concurrent,118134,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:118219,Performance,concurren,concurrent,118219,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:2605,Safety,recover,recover-dangling-heads,2605,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:2643,Safety,recover,recover-dangling-branches,2643,36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:8090,Safety,detect,detect,8090," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:9311,Safety,detect,detect,9311,"D=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symmetric Odds Ratio of 2x2 contingency table to detect strand bias"">; ##INFO=<ID=TYPE,Number=A,Type=String,Description=""The type of allele, either snp, mnp, ins, del, or complex."">; ##INFO=<ID=LEN,Number=A,Type=Integer,Description=""allele length"">; ##INFO=<ID=VCFALLELICPRIMITIVE,Number=0,Type=Flag,Description=""The allele was parsed using vcfallelicprimitives."">; ##INFO=<ID=TENX,Number=0,Type=Flag,Description=""called by 10X"">; ##INFO=<ID=POSTHPC,Number=.,Type=Integer,Description=""Postvariant homopolymer count"">; ##INFO=<ID=POSTHPB,Number=.,Type=Character,Description=""Postvariant homopolymer base"">; ##INFO=<ID=MUMAP_REF,Number=1,Type=Float,Description=""Mean mapping score of ref allele"">; ##INFO=<ID=MUMAP_ALT,Number=.,Type=Float,Description=""Mean mapping scores of alt alleles"">; ##INFO=<ID=AO,Number=.,Type=Integer,Description=""Alternate allele observed count"">; ##INFO=<ID=RO,Number=1,Type=Integer,Description=""Reference allele observed count"">; ##INFO=<ID=MMD,Number=.,Type=Float,Description=""Mean molecule divergence from reference per read",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:22877,Safety,detect,detect,22877,"27745>; ##contig=<ID=chrUn_KI270753v1,length=62944>; ##contig=<ID=chrUn_KI270754v1,length=40191>; ##contig=<ID=chrUn_KI270755v1,length=36723>; ##contig=<ID=chrUn_KI270756v1,length=79590>; ##contig=<ID=chrUn_KI270757v1,length=71251>; ##contig=<ID=chrUn_GL000214v1,length=137718>; ##contig=<ID=chrUn_KI270742v1,length=186739>; ##contig=<ID=chrUn_GL000216v2,length=176608>; ##contig=<ID=chrUn_GL000218v1,length=161147>; ##contig=<ID=chrEBV,length=171823>; ##contig=<ID=hs38d1,length=10560522>; ##bcftools_pluginVersion=1.9+htslib-1.9; ##bcftools_pluginCommand=plugin fill-AN-AC; Date=Sat Dec 29 14:52:44 2018; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:51092,Safety,abort,aborted,51092,"at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82292,Safety,abort,abortStage,82292,fun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82390,Safety,abort,abortStage,82390,5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:82635,Safety,abort,abortStage,82635,spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1892); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2113); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2062); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2051); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:4273,Security,validat,validation-stringency,4273,"false --debug false --use-filtered-reads-for-annotations false --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --capture-assembly-failure-bam false --error-correct-reads false --do-not-run-physical-phasing false --min-base-quality-score 10 --smith-waterman JAVA --use-new-qual-calculator false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genotype-filtered-alleles false --contamination-fraction-to-filter 0.0 --output-mode EMIT_VARIANTS_ONLY --all-site-pls false --min-assembly-region-size 50 --max-assembly-region-size 300 --assembly-region-padding 100 --max-reads-per-alignment-start 50 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotations false"",Version=4.0.7.0,Date=""December 21, 2018 6:32:37 PM EST"">; ##source=HaplotypeCaller; ##source=10X/pipelines/stages/snpindels/attach_bcs_snpindels 2.2.2; ##source=10X/pipelines/stages/snpindels/phase_snpindel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:4372,Security,validat,validation,4372,"in-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genotype-filtered-alleles false --contamination-fraction-to-filter 0.0 --output-mode EMIT_VARIANTS_ONLY --all-site-pls false --min-assembly-region-size 50 --max-assembly-region-size 300 --assembly-region-padding 100 --max-reads-per-alignment-start 50 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false --minimum-mapping-quality 20 --disable-tool-default-annotations false --enable-all-annotations false"",Version=4.0.7.0,Date=""December 21, 2018 6:32:37 PM EST"">; ##source=HaplotypeCaller; ##source=10X/pipelines/stages/snpindels/attach_bcs_snpindels 2.2.2; ##source=10X/pipelines/stages/snpindels/phase_snpindels 2.2.2; ##bcftools_filterVersion=1.1-3-g9058fce+htslib-1.1-1-g03a4427; ##bcftools_filterCommand=filter canonicalized.vcftmp.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_RESCUED_MOLECULE_HIGH_DIVERSITY -e '(((RESCUED+NOT_RESCUED) > 0 & RESCUED/(RESCUED+NOT_RESCUED) > 0.1) & (MMD == -1 | MMD >= 3.0)) ' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:23750,Security,validat,validation-stringency,23750,"ditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --disable-tool-default-annotations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false"",Version=4.0.1.2,Date=""March 22, 2018 1:12:03 AM EDT"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:23849,Security,validat,validation,23849," StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --disable-tool-default-annotations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false"",Version=4.0.1.2,Date=""March 22, 2018 1:12:03 AM EDT"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##VEP=""v91"" time=""2018-03-22 04:15:25"" cache=""/media/SE5/.vep/homo_sapiens/91_GRCh38"" db=""homo_sapiens_core_91_38@ensembldb.ensembl.org"" ensembl-variation=91.c78d8b4 ensembl-funcgen=91.4681d69 ensembl-io=91.923d668 ensembl=91.18ee742 1000genomes=""phase3"" COSMIC=""82"" ClinVar=""201710"" ESP=""V2-SSA137"" HGMD-PUBLIC=""20172"" assembly=""GRCh38.p10"" dbSNP=""150"" gencode=""GENCODE 27"" genebuild=""2014-07"" gnomAD=""1702",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:36549,Security,validat,validate,36549,"|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000456328|processed_transcript|1/3||||330|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|downstream_gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:37919,Security,validat,validate,37919,"08-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht',; 'remap_path': None,; 'sample_type': 'WGS',; 'source_paths': 'gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz',; 'subset_path': None,; 'validate': False,; 'vep_config_json_path': None,; 'vep_runner': 'VEP'},; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht',; 'remap_path': None,; 'sample_type': 'WGS',; 'scheduler_messages': None,; 'set_progress_percentage': <bound method TaskStatusReporter.update_progress_percentage of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'set_status_message': <bound method TaskStatusReporter.update_status_message of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'set_tracking_url': <bound method TaskStatusReporter.update_tracking_url of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'source_paths': ['gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz'],; 'subset_path': None,; 'task_id': 'SeqrVCFToMTTask_gs___seqr_refere_VARIANTS_gs___seqr_bw_mer_b185718e87',; 'validate': False,; 'vep_config_json_path': None,; 'vep_runner': 'VEP'}; [Stage 1:======================================================>(492 + 8) / 500]2020-04-05 14:09:30 Hail: INFO: Coerced almost-sorted data",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:38783,Security,validat,validate,38783,": None,; 'vep_runner': 'VEP'},; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht',; 'remap_path': None,; 'sample_type': 'WGS',; 'scheduler_messages': None,; 'set_progress_percentage': <bound method TaskStatusReporter.update_progress_percentage of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'set_status_message': <bound method TaskStatusReporter.update_status_message of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'set_tracking_url': <bound method TaskStatusReporter.update_tracking_url of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'source_paths': ['gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz'],; 'subset_path': None,; 'task_id': 'SeqrVCFToMTTask_gs___seqr_refere_VARIANTS_gs___seqr_bw_mer_b185718e87',; 'validate': False,; 'vep_config_json_path': None,; 'vep_runner': 'VEP'}; [Stage 1:======================================================>(492 + 8) / 500]2020-04-05 14:09:30 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:====================================================> (485 + 15) / 500]2020-04-05 14:09:34 Hail: INFO: Coerced almost-sorted dataset; [Stage 3:==================================================> (467 + 33) / 500]MT using schema class <class 'lib.model.seqr_mt_schema.SeqrVariantsAndGenotypesSchema'> already has vep annotation.; MT using schema class <class 'lib.model.seqr_mt_schema.SeqrVariantsAndGenotypesSchema'> already has filters annotation.; MT using schema class <class 'lib.model.seqr_mt_schema.SeqrVariantsAndGenotypesSchema'> already has rsid annotation.; MT using schema class <class 'lib.model.seqr_mt_schema.SeqrVariantsAndGenotypesSchema'> already has vep annotation.; ----------------------------------------; Global fields:; 'gencodeVersion': str; 'sourceFilePath': str; 'genomeVersion': str; 'sampleType': str; 'hail_version': str; ----------------------------------------; Column fields:; 's': str; -----------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:47366,Security,validat,validate,47366,"---------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 4:===================================================> (480 + 20) / 500]2020-04-05 14:09:48 Hail: INFO: Coerced almost-sorted dataset; [Stage 5:======================================================>(498 + 2) / 500]2020-04-05 14:09:50 Hail: INFO: Coerced almost-sorted dataset; [Stage 7:> (0 + 108) / 500]ERROR: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) failed SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Traceback (most recent call last):; File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 199, in run; new_deps = self._run_get_new_deps(); File ""/opt/conda/default/lib/python3.6/site-packages/luigi/worker.py"", line 141, in _run_get_new_deps; task_gen = self.task.run(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 54, in run; self.read_vcf_write_mt(); File ""/tmp/c7e0443c47b54e91b295e2bff7b554b9/seqr_loading.py"", line 84, in read_vcf_write_mt; mt.write(self.output().path, stage_locally=True, overwrite=True); File ""<decorator-gen-1092>"", line 2, in write; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/matrixtable.py"", line 2529, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:7641,Testability,test,test,7641,"& RESCUED/(RESCUED+NOT_RESCUED) > 0.1) & (MMD == -1 | MMD >= 3.0)) ' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951f096b/files/default.vcf.gztmp3.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_QUAL_FILTER -e '(%QUAL <= 15 || (AF[0] > 0.5 && %QUAL < 50))' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951f096b/files/default.vcf.gztmp2.vcf; ##bcftools_filterCommand=filter -O v --soft-filter 10X_ALLELE_FRACTION_FILTER -e '(AO[0] < 2 || AO[0]/(AO[0] + RO) < 0.15)' -m '+' /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/POPULATE_INFO_FIELDS/fork0/chnk00-u77951f096b/files/default.vcf.gztmp2.vcf; ##INFO=<ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:7960,Testability,test,test,7960," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:8082,Testability,test,test,8082," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:8863,Testability,test,test,8863," count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Description=""Phred-scaled p-value for exact test of excess heterozygosity"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:9172,Testability,test,test,9172,"iption=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RAW_MQ,Number=1,Type=Float,Description=""Raw data for RMS Mapping Quality"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SOR,Number=1,Type=Float,Description=""Symmetric Odds Ratio of 2x2 contingency table to detect strand bias"">; ##INFO=<ID=TYPE,Number=A,Type=String,Description=""The type of allele, either snp, mnp, ins, del, or complex."">; ##INFO=<ID=LEN,Number=A,Type=Integer,Description=""allele length"">; ##INFO=<ID=VCFALLELICPRIMITIVE,Number=0,Type=Flag,Description=""The allele was parsed using vcfallelicprimitives."">; ##INFO=<ID=TENX,Number=0,Type=Flag,Description=""called by 10X"">; ##INFO=<ID=POSTHPC,Number=.,Type=Integer,Description=""Postvariant homopolymer count"">; ##INFO=<ID=POSTHPB,Number=.,Type=Character,Description=""Postvariant homopolymer base"">; ##INFO=<ID=MUMAP_REF,Number=1,Type=Float,Description=""Mean mapping score of ref allele"">; ##INFO=<ID=MUMAP_ALT,Number=.,Type=Float,Description=""Mean mapping scores o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:22869,Testability,Test,Test,22869,"27745>; ##contig=<ID=chrUn_KI270753v1,length=62944>; ##contig=<ID=chrUn_KI270754v1,length=40191>; ##contig=<ID=chrUn_KI270755v1,length=36723>; ##contig=<ID=chrUn_KI270756v1,length=79590>; ##contig=<ID=chrUn_KI270757v1,length=71251>; ##contig=<ID=chrUn_GL000214v1,length=137718>; ##contig=<ID=chrUn_KI270742v1,length=186739>; ##contig=<ID=chrUn_GL000216v2,length=176608>; ##contig=<ID=chrUn_GL000218v1,length=161147>; ##contig=<ID=chrEBV,length=171823>; ##contig=<ID=hs38d1,length=10560522>; ##bcftools_pluginVersion=1.9+htslib-1.9; ##bcftools_pluginCommand=plugin fill-AN-AC; Date=Sat Dec 29 14:52:44 2018; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output 3P5CH.new.vcf --use-new-qual-calculator true --annotation-group StandardAnnotation --annotation-group StandardHCAnnotation --dbsnp /home/fgc3/dbsnp/150/GRCh38/All_20170710.vcf.gz --variant 3P5CH.new.g.vcf.gz --reference /home/fgc3/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --create-output-variant-index false --verbosity ERROR --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:24465,Testability,test,test,24465,"ygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 10.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --disable-tool-default-annotations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false"",Version=4.0.1.2,Date=""March 22, 2018 1:12:03 AM EDT"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##VEP=""v91"" time=""2018-03-22 04:15:25"" cache=""/media/SE5/.vep/homo_sapiens/91_GRCh38"" db=""homo_sapiens_core_91_38@ensembldb.ensembl.org"" ensembl-variation=91.c78d8b4 ensembl-funcgen=91.4681d69 ensembl-io=91.923d668 ensembl=91.18ee742 1000genomes=""phase3"" COSMIC=""82"" ClinVar=""201710"" ESP=""V2-SSA137"" HGMD-PUBLIC=""20172"" assembly=""GRCh38.p10"" dbSNP=""150"" gencode=""GENCODE 27"" genebuild=""2014-07"" gnomAD=""170228"" polyphen=""2.2.2"" regbuild=""16"" sift=""sift5.2.2""; ##INFO=<ID=CSQ,Number=.,Type=String,Description=""Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:36919,Testability,LOG,LOGGING,36919,"AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht',; 'remap_path': None,; 'sample_type': 'WGS',; 'source_paths': 'gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz',; 'subset_path': None,; 'validate': False,; 'vep_config_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/issues/8469:37016,Testability,log,log,37016,"er(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'decrease_running_resources': <bound method TaskStatusReporter.decrease_running_resources of <luigi.worker.TaskStatusReporter object at 0x7f0583f0f588>>,; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'param_kwargs': {'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht',; 'dataset_type': 'VARIANTS',; 'dest_path': 'gs://seqr-bw/merged_phased_3P5CH.mt',; 'genome_version': '38',; 'hgmd_ht_path': None,; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht',; 'remap_path': None,; 'sample_type': 'WGS',; 'source_paths': 'gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz',; 'subset_path': None,; 'validate': False,; 'vep_config_json_path': None,; 'vep_runner': 'VEP'},; 'reference_ht_path': 'gs://seqr-reference-data/GRCh38/all_reference_data/combine",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469
https://github.com/hail-is/hail/pull/8470:33,Availability,error,errors,33,This might avoid command to long errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8470
https://github.com/hail-is/hail/pull/8470:11,Safety,avoid,avoid,11,This might avoid command to long errors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8470
https://github.com/hail-is/hail/pull/8472:21,Testability,benchmark,benchmarks,21,WIP while I wait for benchmarks to succeed with this change,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8472
https://github.com/hail-is/hail/issues/8473:115,Availability,Error,Error,115,"exit code 0 but actually a timeout. ```; {; ""batch_id"": 27671,; ""job_id"": 65,; ""name"": ""test_pipeline"",; ""state"": ""Error"",; ""exit_code"": 0,; ""duration"": 1211266,; ""msec_mcpu"": 1200805000,; ""cost"": ""$0.0072"",; ""status"": {; ""worker"": ""batch-worker-default-i68pm"",; ""batch_id"": 27671,; ""job_id"": 65,; ""attempt_id"": ""bpqqnj"",; ""user"": ""ci"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""input"": {; ""name"": ""input"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188234696,; ""finish_time"": 1586188234698,; ""duration"": 2; },; ""creating"": {; ""start_time"": 1586188234699,; ""finish_time"": 1586188234766,; ""duration"": 67; },; ""runtime"": {; ""start_time"": 1586188234766,; ""finish_time"": 1586188245227,; ""duration"": 10461; },; ""starting"": {; ""start_time"": 1586188234767,; ""finish_time"": 1586188236190,; ""duration"": 1423; },; ""running"": {; ""start_time"": 1586188236190,; ""finish_time"": 1586188245227,; ""duration"": 9037; },; ""uploading_log"": {; ""start_time"": 1586188245231,; ""finish_time"": 1586188245276,; ""duration"": 45; },; ""deleting"": {; ""start_time"": 1586188245276,; ""finish_time"": 1586188245305,; ""duration"": 29; }; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2020-04-06T15:50:36.182009912Z"",; ""finished_at"": ""2020-04-06T15:50:44.884808909Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188245305,; ""finish_time"": 1586188245404,; ""duration"": 99; },; ""creating"": {; ""start_time"": 1586188245404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:348,Availability,error,error,348,"exit code 0 but actually a timeout. ```; {; ""batch_id"": 27671,; ""job_id"": 65,; ""name"": ""test_pipeline"",; ""state"": ""Error"",; ""exit_code"": 0,; ""duration"": 1211266,; ""msec_mcpu"": 1200805000,; ""cost"": ""$0.0072"",; ""status"": {; ""worker"": ""batch-worker-default-i68pm"",; ""batch_id"": 27671,; ""job_id"": 65,; ""attempt_id"": ""bpqqnj"",; ""user"": ""ci"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""input"": {; ""name"": ""input"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188234696,; ""finish_time"": 1586188234698,; ""duration"": 2; },; ""creating"": {; ""start_time"": 1586188234699,; ""finish_time"": 1586188234766,; ""duration"": 67; },; ""runtime"": {; ""start_time"": 1586188234766,; ""finish_time"": 1586188245227,; ""duration"": 10461; },; ""starting"": {; ""start_time"": 1586188234767,; ""finish_time"": 1586188236190,; ""duration"": 1423; },; ""running"": {; ""start_time"": 1586188236190,; ""finish_time"": 1586188245227,; ""duration"": 9037; },; ""uploading_log"": {; ""start_time"": 1586188245231,; ""finish_time"": 1586188245276,; ""duration"": 45; },; ""deleting"": {; ""start_time"": 1586188245276,; ""finish_time"": 1586188245305,; ""duration"": 29; }; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2020-04-06T15:50:36.182009912Z"",; ""finished_at"": ""2020-04-06T15:50:44.884808909Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188245305,; ""finish_time"": 1586188245404,; ""duration"": 99; },; ""creating"": {; ""start_time"": 1586188245404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:1381,Availability,error,error,1381,"245227,; ""duration"": 10461; },; ""starting"": {; ""start_time"": 1586188234767,; ""finish_time"": 1586188236190,; ""duration"": 1423; },; ""running"": {; ""start_time"": 1586188236190,; ""finish_time"": 1586188245227,; ""duration"": 9037; },; ""uploading_log"": {; ""start_time"": 1586188245231,; ""finish_time"": 1586188245276,; ""duration"": 45; },; ""deleting"": {; ""start_time"": 1586188245276,; ""finish_time"": 1586188245305,; ""duration"": 29; }; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2020-04-06T15:50:36.182009912Z"",; ""finished_at"": ""2020-04-06T15:50:44.884808909Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188245305,; ""finish_time"": 1586188245404,; ""duration"": 99; },; ""creating"": {; ""start_time"": 1586188245404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2095,Availability,error,error,2095,"245227,; ""duration"": 10461; },; ""starting"": {; ""start_time"": 1586188234767,; ""finish_time"": 1586188236190,; ""duration"": 1423; },; ""running"": {; ""start_time"": 1586188236190,; ""finish_time"": 1586188245227,; ""duration"": 9037; },; ""uploading_log"": {; ""start_time"": 1586188245231,; ""finish_time"": 1586188245276,; ""duration"": 45; },; ""deleting"": {; ""start_time"": 1586188245276,; ""finish_time"": 1586188245305,; ""duration"": 29; }; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2020-04-06T15:50:36.182009912Z"",; ""finished_at"": ""2020-04-06T15:50:44.884808909Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188245305,; ""finish_time"": 1586188245404,; ""duration"": 99; },; ""creating"": {; ""start_time"": 1586188245404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2647,Deployability,deploy,deploy-config,2647,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2661,Deployability,deploy,deploy-config,2661,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2730,Deployability,pipeline,pipeline,2730,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2994,Deployability,deploy,deploy-config,2994,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:3027,Deployability,deploy,deploy-config,3027,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:3379,Deployability,pipeline,pipeline,3379,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2654,Modifiability,config,config,2654,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2668,Modifiability,config,config,2668,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:3001,Modifiability,config,config,3001,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:3034,Modifiability,config,config,3034,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:27,Safety,timeout,timeout,27,"exit code 0 but actually a timeout. ```; {; ""batch_id"": 27671,; ""job_id"": 65,; ""name"": ""test_pipeline"",; ""state"": ""Error"",; ""exit_code"": 0,; ""duration"": 1211266,; ""msec_mcpu"": 1200805000,; ""cost"": ""$0.0072"",; ""status"": {; ""worker"": ""batch-worker-default-i68pm"",; ""batch_id"": 27671,; ""job_id"": 65,; ""attempt_id"": ""bpqqnj"",; ""user"": ""ci"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""input"": {; ""name"": ""input"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188234696,; ""finish_time"": 1586188234698,; ""duration"": 2; },; ""creating"": {; ""start_time"": 1586188234699,; ""finish_time"": 1586188234766,; ""duration"": 67; },; ""runtime"": {; ""start_time"": 1586188234766,; ""finish_time"": 1586188245227,; ""duration"": 10461; },; ""starting"": {; ""start_time"": 1586188234767,; ""finish_time"": 1586188236190,; ""duration"": 1423; },; ""running"": {; ""start_time"": 1586188236190,; ""finish_time"": 1586188245227,; ""duration"": 9037; },; ""uploading_log"": {; ""start_time"": 1586188245231,; ""finish_time"": 1586188245276,; ""duration"": 45; },; ""deleting"": {; ""start_time"": 1586188245276,; ""finish_time"": 1586188245305,; ""duration"": 29; }; },; ""container_status"": {; ""state"": ""exited"",; ""started_at"": ""2020-04-06T15:50:36.182009912Z"",; ""finished_at"": ""2020-04-06T15:50:44.884808909Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; },; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188245305,; ""finish_time"": 1586188245404,; ""duration"": 99; },; ""creating"": {; ""start_time"": 1586188245404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2271,Safety,timeout,timeout,2271,"},; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1586188245305,; ""finish_time"": 1586188245404,; ""duration"": 99; },; ""creating"": {; ""start_time"": 1586188245404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/buil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:3269,Safety,timeout,timeout,3269,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2706,Testability,test,test-,2706,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2760,Testability,log,log-cli-level,2760,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:2802,Testability,test,test,2802,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:3108,Testability,test,test-tokens,3108,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/issues/8473:3388,Testability,test,test,3388,"45404,; ""finish_time"": 1586188245457,; ""duration"": 53; },; ""runtime"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586189446263,; ""duration"": 1200805; },; ""starting"": {; ""start_time"": 1586188245458,; ""finish_time"": 1586188246261,; ""duration"": 803; },; ""running"": {; ""start_time"": 1586188246262,; ""finish_time"": 1586189446263,; ""duration"": 1200001; },; ""uploading_log"": {; ""start_time"": 1586189446266,; ""finish_time"": 1586189446350,; ""duration"": 84; },; ""deleting"": {; ""start_time"": 1586189446351,; ""finish_time"": 1586189456802,; ""duration"": 10451; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 387, in run\n raise JobTimeoutError(f'timed out after {self.timeout}s')\nJobTimeoutError: timed out after 1200s\n"",; ""container_status"": {; ""state"": ""running"",; ""started_at"": ""2020-04-06T15:50:46.250931386Z"",; ""finished_at"": ""0001-01-01T00:00:00Z"",; ""out_of_memory"": false,; ""exit_code"": 0; }; }; },; ""start_time"": 1586188245458,; ""end_time"": 1586189446263; },; ""spec"": {; ""command"": [; ""bash"",; ""-c"",; ""export HAIL_DEPLOY_CONFIG_FILE=/deploy-config/deploy-config.json\nexport SCRATCH=gs://hail-test-dmk9z/o1111h6zxn1p/pipeline\npython3 -m pytest --log-cli-level=INFO -s -vv --instafail /io/test/""; ],; ""image"": ""gcr.io/hail-vdc/ci-intermediate:q7503hc818u5"",; ""job_id"": 65,; ""mount_docker_socket"": false,; ""secrets"": [; {; ""namespace"": ""pr-8470-default-dyvil12gxzyf"",; ""name"": ""gce-deploy-config"",; ""mount_path"": ""/deploy-config""; },; {; ""namespace"": ""pr-8470-batch-pods-r3e5lmgvb8dl"",; ""name"": ""test-tokens"",; ""mount_path"": ""/user-tokens""; },; {; ""namespace"": ""batch-pods"",; ""name"": ""ci-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""timeout"": 1200,; ""input_files"": [; {; ""from"": ""gs://hail-ci-bpk3h/build/23dca3776b11f404e2d0a242697d3b5f/repo/pipeline/test"",; ""to"": ""/io/""; }; ],; ""resources"": {; ""cpu"": ""1"",; ""memory"": ""3.75G""; },; ""env"": []; },; ""attributes"": {; ""name"": ""test_pipeline""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8473
https://github.com/hail-is/hail/pull/8475:57,Availability,down,download,57,Couldn't build with sbt after googleFS changes. Couldn't download the proper LZ4 dependency.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8475
https://github.com/hail-is/hail/pull/8475:81,Integrability,depend,dependency,81,Couldn't build with sbt after googleFS changes. Couldn't download the proper LZ4 dependency.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8475
https://github.com/hail-is/hail/pull/8476:343,Integrability,Protocol,Protocols,343,"Removed id from anchor tags. Unneeded (all anchor tags should have same style, or determined by nesting level in menu, and now not used to make active links), less for Kumar to remember to do when adding pages. Also removed cpage == """" check. By RFC2616, cannot have empty pathname; if none is provided ""/"" is defaulted to. https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html. ""The most common form of Request-URI is that used to identify a resource on an origin server or gateway. In this case the absolute path of the URI MUST be transmitted (see section 3.2.1, abs_path) as the Request-URI, and the network location of the URI (authority) MUST be transmitted in a Host header field."". and . ""The absoluteURI form is REQUIRED when the request is being made to a proxy. "". Also verified in chrome/safari/edge",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8476
https://github.com/hail-is/hail/pull/8480:112,Availability,error,error,112,"CI was getting 422s from GitHub. Using a; `raise_for_status=True` ClientSession circumvented gidgethubs native; error handling logic smothering the HTTP response body where github; places critical debugging information. Aiohttp is aware that; `raise_for_status` provides no access to the response body. They addressed; this in https://github.com/aio-libs/aiohttp/pulls/3892, but that has not; been released because 4.0.0 has not yet been released. Moreover, `gidgethub` incorrectly handles the too many statuses response. I'll PR a fix into their repo. For now, I've added a bit more information the logs and fixed the main issue, the missing `['status']`. Another relevant issue: https://github.com/aio-libs/aiohttp/issues/4600.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8480
https://github.com/hail-is/hail/pull/8480:398,Deployability,release,released,398,"CI was getting 422s from GitHub. Using a; `raise_for_status=True` ClientSession circumvented gidgethubs native; error handling logic smothering the HTTP response body where github; places critical debugging information. Aiohttp is aware that; `raise_for_status` provides no access to the response body. They addressed; this in https://github.com/aio-libs/aiohttp/pulls/3892, but that has not; been released because 4.0.0 has not yet been released. Moreover, `gidgethub` incorrectly handles the too many statuses response. I'll PR a fix into their repo. For now, I've added a bit more information the logs and fixed the main issue, the missing `['status']`. Another relevant issue: https://github.com/aio-libs/aiohttp/issues/4600.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8480
https://github.com/hail-is/hail/pull/8480:438,Deployability,release,released,438,"CI was getting 422s from GitHub. Using a; `raise_for_status=True` ClientSession circumvented gidgethubs native; error handling logic smothering the HTTP response body where github; places critical debugging information. Aiohttp is aware that; `raise_for_status` provides no access to the response body. They addressed; this in https://github.com/aio-libs/aiohttp/pulls/3892, but that has not; been released because 4.0.0 has not yet been released. Moreover, `gidgethub` incorrectly handles the too many statuses response. I'll PR a fix into their repo. For now, I've added a bit more information the logs and fixed the main issue, the missing `['status']`. Another relevant issue: https://github.com/aio-libs/aiohttp/issues/4600.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8480
https://github.com/hail-is/hail/pull/8480:274,Security,access,access,274,"CI was getting 422s from GitHub. Using a; `raise_for_status=True` ClientSession circumvented gidgethubs native; error handling logic smothering the HTTP response body where github; places critical debugging information. Aiohttp is aware that; `raise_for_status` provides no access to the response body. They addressed; this in https://github.com/aio-libs/aiohttp/pulls/3892, but that has not; been released because 4.0.0 has not yet been released. Moreover, `gidgethub` incorrectly handles the too many statuses response. I'll PR a fix into their repo. For now, I've added a bit more information the logs and fixed the main issue, the missing `['status']`. Another relevant issue: https://github.com/aio-libs/aiohttp/issues/4600.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8480
https://github.com/hail-is/hail/pull/8480:127,Testability,log,logic,127,"CI was getting 422s from GitHub. Using a; `raise_for_status=True` ClientSession circumvented gidgethubs native; error handling logic smothering the HTTP response body where github; places critical debugging information. Aiohttp is aware that; `raise_for_status` provides no access to the response body. They addressed; this in https://github.com/aio-libs/aiohttp/pulls/3892, but that has not; been released because 4.0.0 has not yet been released. Moreover, `gidgethub` incorrectly handles the too many statuses response. I'll PR a fix into their repo. For now, I've added a bit more information the logs and fixed the main issue, the missing `['status']`. Another relevant issue: https://github.com/aio-libs/aiohttp/issues/4600.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8480
https://github.com/hail-is/hail/pull/8480:600,Testability,log,logs,600,"CI was getting 422s from GitHub. Using a; `raise_for_status=True` ClientSession circumvented gidgethubs native; error handling logic smothering the HTTP response body where github; places critical debugging information. Aiohttp is aware that; `raise_for_status` provides no access to the response body. They addressed; this in https://github.com/aio-libs/aiohttp/pulls/3892, but that has not; been released because 4.0.0 has not yet been released. Moreover, `gidgethub` incorrectly handles the too many statuses response. I'll PR a fix into their repo. For now, I've added a bit more information the logs and fixed the main issue, the missing `['status']`. Another relevant issue: https://github.com/aio-libs/aiohttp/issues/4600.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8480
https://github.com/hail-is/hail/pull/8484:45,Testability,log,login,45,This means tokens obtained via `hailctl auth login` won't expire (but can be explicitly revoked by `hailctl auth logout`),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8484
https://github.com/hail-is/hail/pull/8484:113,Testability,log,logout,113,This means tokens obtained via `hailctl auth login` won't expire (but can be explicitly revoked by `hailctl auth logout`),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8484
https://github.com/hail-is/hail/pull/8485:934,Testability,LOG,LOGGING,934,"@danking @tpoterba I wasn't sure if this should go to compilers or services team. I randomly picked Dan. Who ""owns"" GoogleStorageFS? ServiceBackend? Let's discuss in staff meeting this week. Summary of changes:; - ExecuteContext aded to IRParserEnvironment (maybe should be renamed IRParserContext now?); - TableReader in general will need the filesystem for construct. This breaks the json4s de/serializer model. I started expanding the serializers and writing them by hand. Just did the TableNativeReader for now. We might reconsider the mix of custom and JSON in the printed format at some point in the future.; - A bunch of functions take a hc but only need an fs. More of this to come. ```; hl.init(_backend=hl.backend.ServiceBackend()); t = hl.read_table('gs://hail-cseed-k0ox4/sample_rows.ht'); print(t.count()); ```. ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.36-75a0f869d72d; LOGGING: writing to /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log; 346; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485
https://github.com/hail-is/hail/pull/8485:1006,Testability,log,log,1006,"@danking @tpoterba I wasn't sure if this should go to compilers or services team. I randomly picked Dan. Who ""owns"" GoogleStorageFS? ServiceBackend? Let's discuss in staff meeting this week. Summary of changes:; - ExecuteContext aded to IRParserEnvironment (maybe should be renamed IRParserContext now?); - TableReader in general will need the filesystem for construct. This breaks the json4s de/serializer model. I started expanding the serializers and writing them by hand. Just did the TableNativeReader for now. We might reconsider the mix of custom and JSON in the printed format at some point in the future.; - A bunch of functions take a hc but only need an fs. More of this to come. ```; hl.init(_backend=hl.backend.ServiceBackend()); t = hl.read_table('gs://hail-cseed-k0ox4/sample_rows.ht'); print(t.count()); ```. ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.36-75a0f869d72d; LOGGING: writing to /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log; 346; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485
https://github.com/hail-is/hail/pull/8486:13,Deployability,deploy,deployed,13,"I've already deployed these changes. Without these changes, I've locked myself out of my dev namespace because my dev namespace refuses to speak HTTP now ().",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8486
https://github.com/hail-is/hail/pull/8487:319,Modifiability,variab,variable,319,"This illustrates one of the footguns in using CodeBuilder. When using; functionality like ifx, we must be sure to actually put the resultant; code on the CodeBuilder in the block we pass to ifx. Otherwise we won't; add any code and the behavior may be surprising. This is basically equivalent to storing some Code in a variable and then; never passing that variable into a returned Code, so it is never; executed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8487
https://github.com/hail-is/hail/pull/8487:357,Modifiability,variab,variable,357,"This illustrates one of the footguns in using CodeBuilder. When using; functionality like ifx, we must be sure to actually put the resultant; code on the CodeBuilder in the block we pass to ifx. Otherwise we won't; add any code and the behavior may be surprising. This is basically equivalent to storing some Code in a variable and then; never passing that variable into a returned Code, so it is never; executed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8487
https://github.com/hail-is/hail/pull/8488:61,Availability,error,error,61,This is the same issue as #8487. Not actually generating the error; throwing code.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8488
https://github.com/hail-is/hail/pull/8489:266,Performance,load,load,266,"Forgot that we were importing navbar.css. We could go back to a separate navbar.css, but I think that is less optimal because I don't think you can include the html directly in xslt, and importing it using jquery can cause the navbar to flash immediately after page load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8489
https://github.com/hail-is/hail/pull/8490:315,Deployability,deploy,deployed,315,"This change is temporary. I do not intend to keep the extra hop to `auth` on all internal-gateway requests. Once all the TLS changes go in and everything in the cluster is TLS-secured, then I can switch the internal gateway to unconditionally use HTTPS and remove the router-resolver's extra endpoint. I've already deployed this (I need it to get batch tests to pass in my namespace.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490
https://github.com/hail-is/hail/pull/8490:268,Integrability,rout,router-resolver,268,"This change is temporary. I do not intend to keep the extra hop to `auth` on all internal-gateway requests. Once all the TLS changes go in and everything in the cluster is TLS-secured, then I can switch the internal gateway to unconditionally use HTTPS and remove the router-resolver's extra endpoint. I've already deployed this (I need it to get batch tests to pass in my namespace.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490
https://github.com/hail-is/hail/pull/8490:176,Security,secur,secured,176,"This change is temporary. I do not intend to keep the extra hop to `auth` on all internal-gateway requests. Once all the TLS changes go in and everything in the cluster is TLS-secured, then I can switch the internal gateway to unconditionally use HTTPS and remove the router-resolver's extra endpoint. I've already deployed this (I need it to get batch tests to pass in my namespace.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490
https://github.com/hail-is/hail/pull/8490:353,Testability,test,tests,353,"This change is temporary. I do not intend to keep the extra hop to `auth` on all internal-gateway requests. Once all the TLS changes go in and everything in the cluster is TLS-secured, then I can switch the internal gateway to unconditionally use HTTPS and remove the router-resolver's extra endpoint. I've already deployed this (I need it to get batch tests to pass in my namespace.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490
https://github.com/hail-is/hail/pull/8492:28,Deployability,deploy,deployment,28,This change brings the site deployment naming scheme in line with; our other k8s Deployments and also adds a `wait` to build.yaml so; that we check that the site has come up successfully.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8492
https://github.com/hail-is/hail/pull/8492:81,Deployability,Deploy,Deployments,81,This change brings the site deployment naming scheme in line with; our other k8s Deployments and also adds a `wait` to build.yaml so; that we check that the site has come up successfully.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8492
https://github.com/hail-is/hail/pull/8494:4,Availability,error,error,4,"The error message you get is this:; ```; Error from server (BadRequest): a container name must be specified for pod blog-0, choose one of: [nginx blog]; ```. This option is described in `kubectl logs --help` as:; > --all-containers=false: Get all containers logs in the pod(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8494
https://github.com/hail-is/hail/pull/8494:41,Availability,Error,Error,41,"The error message you get is this:; ```; Error from server (BadRequest): a container name must be specified for pod blog-0, choose one of: [nginx blog]; ```. This option is described in `kubectl logs --help` as:; > --all-containers=false: Get all containers logs in the pod(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8494
https://github.com/hail-is/hail/pull/8494:10,Integrability,message,message,10,"The error message you get is this:; ```; Error from server (BadRequest): a container name must be specified for pod blog-0, choose one of: [nginx blog]; ```. This option is described in `kubectl logs --help` as:; > --all-containers=false: Get all containers logs in the pod(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8494
https://github.com/hail-is/hail/pull/8494:195,Testability,log,logs,195,"The error message you get is this:; ```; Error from server (BadRequest): a container name must be specified for pod blog-0, choose one of: [nginx blog]; ```. This option is described in `kubectl logs --help` as:; > --all-containers=false: Get all containers logs in the pod(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8494
https://github.com/hail-is/hail/pull/8494:258,Testability,log,logs,258,"The error message you get is this:; ```; Error from server (BadRequest): a container name must be specified for pod blog-0, choose one of: [nginx blog]; ```. This option is described in `kubectl logs --help` as:; > --all-containers=false: Get all containers logs in the pod(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8494
https://github.com/hail-is/hail/pull/8496:557,Testability,test,test,557,"It was wrong for the `elemRef` values in the emit code `NDArrayMap` and `NDArrayMap2` to use `mb` as opposed to `elemMB`. They were being emitted in a different method builder than they were being used in. In order to fix this, I had to fix `NDArrayEmitter` to take an `EmitMethodBuilder[C]` instead of `EmitMethodBuilder[_]`, so I did that. I also had to pass the arguments along from the `mb` method builder to the `innerMethod` method builder in the `NDArrayEmitter` emit method. Factored out `mb.getParamsList()` to make that easier. I also added a new test that is remedied by this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8496
https://github.com/hail-is/hail/pull/8498:100,Modifiability,config,config,100,"If you look in the body of create_database, you will see that I copy the; keys from an existing sql-config using `get`. This propagates the `None`s; rather than leaving the keys missing. This fix changes `write_user_config`; to filter out keys set to None. Such keys should not appear in normal; configs because we never use `null` in our configs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8498
https://github.com/hail-is/hail/pull/8498:296,Modifiability,config,configs,296,"If you look in the body of create_database, you will see that I copy the; keys from an existing sql-config using `get`. This propagates the `None`s; rather than leaving the keys missing. This fix changes `write_user_config`; to filter out keys set to None. Such keys should not appear in normal; configs because we never use `null` in our configs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8498
https://github.com/hail-is/hail/pull/8498:339,Modifiability,config,configs,339,"If you look in the body of create_database, you will see that I copy the; keys from an existing sql-config using `get`. This propagates the `None`s; rather than leaving the keys missing. This fix changes `write_user_config`; to filter out keys set to None. Such keys should not appear in normal; configs because we never use `null` in our configs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8498
https://github.com/hail-is/hail/pull/8500:12,Modifiability,config,config,12,"I fixed sql-config.cnf in the last change, not sql-config.json. I just changed everything to treat a key being `null` in JSON the; same as the key not existing. The situation now:; - sql-config.cnf will not have the string None (seems right, cnf is for MySQL); - sql-config.json might have `null` which is treated the same as a missing key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8500
https://github.com/hail-is/hail/pull/8500:51,Modifiability,config,config,51,"I fixed sql-config.cnf in the last change, not sql-config.json. I just changed everything to treat a key being `null` in JSON the; same as the key not existing. The situation now:; - sql-config.cnf will not have the string None (seems right, cnf is for MySQL); - sql-config.json might have `null` which is treated the same as a missing key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8500
https://github.com/hail-is/hail/pull/8500:187,Modifiability,config,config,187,"I fixed sql-config.cnf in the last change, not sql-config.json. I just changed everything to treat a key being `null` in JSON the; same as the key not existing. The situation now:; - sql-config.cnf will not have the string None (seems right, cnf is for MySQL); - sql-config.json might have `null` which is treated the same as a missing key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8500
https://github.com/hail-is/hail/pull/8500:267,Modifiability,config,config,267,"I fixed sql-config.cnf in the last change, not sql-config.json. I just changed everything to treat a key being `null` in JSON the; same as the key not existing. The situation now:; - sql-config.cnf will not have the string None (seems right, cnf is for MySQL); - sql-config.json might have `null` which is treated the same as a missing key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8500
https://github.com/hail-is/hail/pull/8504:240,Availability,error,error,240,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8504:135,Deployability,deploy,deployed,135,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8504:309,Deployability,deploy,deploying,309,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8504:374,Deployability,deploy,deployed,374,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8504:217,Energy Efficiency,reduce,reduces,217,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8504:49,Integrability,rout,router,49,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8504:185,Integrability,rout,router-resolver,185,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8504:399,Integrability,rout,router,399,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504
https://github.com/hail-is/hail/pull/8506:89,Deployability,deploy,deployment,89,The `wait` directive tells CI to wait for the service to come up before; continuing with deployment. If the service fails to come up (including; failing to respond to the readiness checks) then CI will fail the build.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8506
https://github.com/hail-is/hail/pull/8510:44,Security,attack,attacks,44,Protectes from (unlikely) man in the middle attacks on our infrastructure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8510
https://github.com/hail-is/hail/pull/8513:6460,Availability,outage,outages,6460,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:1089,Deployability,deploy,deployment,1089,". The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:2922,Deployability,configurat,configuration,2922,"re the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3256,Deployability,configurat,configuration,3256,"fused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this glob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3352,Deployability,configurat,configuration,3352,"em)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3515,Deployability,configurat,configuration,3515,"t. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3795,Deployability,configurat,configuration,3795,"e HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) serve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4269,Deployability,configurat,configuration,4269,"format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4515,Deployability,configurat,configuration,4515,"url configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4886,Deployability,configurat,configuration,4886,"ode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5010,Deployability,configurat,configuration,5010,"mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5366,Deployability,deploy,deploy,5366,"ate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5424,Deployability,deploy,deploy,5424," ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5451,Deployability,deploy,deployed,5451," ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5492,Deployability,deploy,deployed,5492," ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5551,Deployability,deploy,deploy,5551,", but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5635,Deployability,deploy,deploy,5635,", but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5739,Deployability,deploy,deployed,5739," VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5770,Deployability,Deploy,Deploy,5770,", and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5833,Deployability,deploy,deployed,5833,", and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6276,Deployability,configurat,configurations,6276,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6379,Deployability,deploy,deploy,6379,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6397,Deployability,deploy,deploy,6397,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6424,Deployability,deploy,deploy,6424,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6697,Deployability,deploy,deploy,6697,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6718,Deployability,Deploy,Deploy,6718,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6787,Deployability,deploy,deploy,6787,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:2490,Energy Efficiency,power,powerful,2490,"ck interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:207,Integrability,rout,router,207,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:1040,Integrability,depend,depends,1040,". The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:1419,Integrability,interface,interface,1419," ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:1496,Integrability,interface,interfaces,1496,"dentity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:2828,Integrability,rout,router,2828,"his, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists ever",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5752,Integrability,depend,dependency,5752," VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5812,Integrability,depend,depend,5812,", and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6431,Integrability,rout,router-resolver,6431,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6557,Integrability,rout,router,6557,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6896,Integrability,rout,router,6896,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:2890,Modifiability,config,config,2890,"re the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:2922,Modifiability,config,configuration,2922,"re the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:2947,Modifiability,config,config,2947,"re the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3235,Modifiability,config,config,3235,"questreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3256,Modifiability,config,configuration,3256,"fused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this glob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3299,Modifiability,config,config-http,3299,"ed the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3326,Modifiability,config,config-proxy,3326,"puty; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. Fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3352,Modifiability,config,configuration,3352,"em)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3378,Modifiability,config,configure,3378,"em)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3492,Modifiability,config,config,3492,"owerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3515,Modifiability,config,configuration,3515,"t. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3795,Modifiability,config,configuration,3795,"e HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) serve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4269,Modifiability,config,configuration,4269,"format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4515,Modifiability,config,configuration,4515,"url configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4886,Modifiability,config,configuration,4886,"ode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5010,Modifiability,config,configuration,5010,"mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5929,Modifiability,config,config,5929,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6276,Modifiability,config,configurations,6276,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:1102,Performance,load,load,1102,". The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4554,Performance,load,load,4554,"acy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the tru",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4633,Performance,load,load,4633,"tandard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4756,Performance,load,load,4756,"s. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4776,Performance,load,load,4776,"s. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5903,Performance,load,load,5903,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:324,Security,certificate,certificate,324,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:425,Security,secur,security,425,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:455,Security,authenticat,authenticatable,455,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:620,Security,certificate,certificate,620,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:898,Security,certificate,certificates,898,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:953,Security,certificate,certificate,953,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:1278,Security,secur,secured,1278," (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:1712,Security,certificate,certificates,1712,"cipal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reje",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:2041,Security,certificate,certificate,2041,"rts` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3096,Security,certificate,certificate,3096,"tpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3177,Security,certificate,certificates,3177,"ub.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3637,Security,certificate,certificates,3637,"to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (pr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3664,Security,certificate,certificates,3664,"eject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3702,Security,certificate,certificates,3702,"principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I requ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:3978,Security,secur,secure,3978,"fig.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4649,Security,certificate,certificates,4649,"tandard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:5560,Security,certificate,certificates,5560,", but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:6102,Security,secur,secured,6102,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4690,Testability,test,test,4690,"tandard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8513:4420,Usability,simpl,simple,4420," (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513
https://github.com/hail-is/hail/pull/8515:49,Integrability,rout,router,49,"It's already gone, this just removes it from the router.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8515
https://github.com/hail-is/hail/pull/8518:12,Testability,test,tests,12,"passes some tests, including IRSuite.testSelectFields",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8518
https://github.com/hail-is/hail/pull/8518:37,Testability,test,testSelectFields,37,"passes some tests, including IRSuite.testSelectFields",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8518
https://github.com/hail-is/hail/pull/8519:119,Usability,simpl,simpler,119,"I had to modify the way stream lengths are tracked in the emitter to be able to handle StreamTake. The new way is also simpler, and moves in the direction of the new CodeBuilder style. Now `SizedStream` is ; ```; case class SizedStream(setup: Code[Unit], stream: Stream[EmitCode], length: Option[Code[Int]]); ```; The `setup` must be emitted before either the `stream` or the `length` are used. This allows the length and stream to share setup code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8519
https://github.com/hail-is/hail/pull/8528:152,Usability,simpl,simpler,152,"In looking at your PSelectFieldsStruct PR, I got a bit angry at the number of abstract methods on PStruct. This cleanup should make your PR smaller and simpler. We'll sort out the constructible methods at some point too. All the methods I changed were ones that are always used to do type-level manipulation to return a new constructible type that is fed into an RVB. Returning a PCanonicalStruct to these usages is the correct thing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8528
https://github.com/hail-is/hail/pull/8530:42,Deployability,integrat,integration,42,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:471,Deployability,deploy,deploy,471,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:538,Deployability,deploy,deploy,538,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:567,Deployability,update,update,567,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:591,Deployability,deploy,deploy,591,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:684,Deployability,deploy,deploy,684,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:722,Deployability,update,update,722,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:42,Integrability,integrat,integration,42,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:614,Safety,safe,safety,614,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8530:485,Testability,test,tested,485,"cc: @cseed . In preparation for the terra integration prototype, I've modified build.yaml to build and publish two public images. Both are versioned by the hail_pip_version. The first, `hailgenetics/hail`, contains hail, common python libraries, and the google cloud sdk. The second, `hailgenetics/genetics`, contains a slew of genetics tools that were used by the CCG Workshop. There is no R in any of these images. That is intentional. The image push step is scoped to deploy (but I tested it in my dev environment), so it only runs on deploy. Ergo, we should only update the image during deploy. I also added a safety check that doesn't overwrite an extant tag, if, for example, a deploy partially ran. Maybe we should update the image? I'm not sure. It seems better to not change the image automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8530
https://github.com/hail-is/hail/pull/8533:79,Deployability,deploy,deploy,79,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:124,Deployability,release,release,124,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:215,Deployability,deploy,deploy,215,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:287,Deployability,deploy,deploy,287,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:333,Deployability,deploy,deploy,333,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:504,Deployability,deploy,deploys,504,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:988,Deployability,deploy,deploy,988,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:1011,Deployability,deploy,deploy,1011,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:1169,Deployability,release,release,1169,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:723,Security,password,password,723,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:785,Security,access,access,785,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:920,Security,Authoriz,Authorization,920,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:11,Testability,test,tested,11,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8533:316,Usability,simpl,simply,316,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533
https://github.com/hail-is/hail/pull/8535:5,Testability,Benchmark,Benchmark,5,```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; import_and_transform_gvcf 69.9% 128.759 90.060; import_gvcf_force_count 66.5% 109.791 73.018```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8535
https://github.com/hail-is/hail/pull/8537:332,Availability,error,error,332,"Plus:; - Boolean ldc (load constant) instructions need an int, not a boolean. JVM seems OK with it, but the asm bytecode verifier rejects it.; - In Apply codegen, the zip in function lookup was potentially truncating the arguments, selecting an incorrect function. Fix, and simplify the definition of `methods`.; - Fix/simplify asm error reporting from asm in lir Emit. The old code was crashing inside asm. I used the new code to debug some bytecode issues, it works well.; - compute max locals/stack, needed by the asm verifier (CheckClass). @konradjk This fixes your class not found issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8537
https://github.com/hail-is/hail/pull/8537:22,Performance,load,load,22,"Plus:; - Boolean ldc (load constant) instructions need an int, not a boolean. JVM seems OK with it, but the asm bytecode verifier rejects it.; - In Apply codegen, the zip in function lookup was potentially truncating the arguments, selecting an incorrect function. Fix, and simplify the definition of `methods`.; - Fix/simplify asm error reporting from asm in lir Emit. The old code was crashing inside asm. I used the new code to debug some bytecode issues, it works well.; - compute max locals/stack, needed by the asm verifier (CheckClass). @konradjk This fixes your class not found issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8537
https://github.com/hail-is/hail/pull/8537:274,Usability,simpl,simplify,274,"Plus:; - Boolean ldc (load constant) instructions need an int, not a boolean. JVM seems OK with it, but the asm bytecode verifier rejects it.; - In Apply codegen, the zip in function lookup was potentially truncating the arguments, selecting an incorrect function. Fix, and simplify the definition of `methods`.; - Fix/simplify asm error reporting from asm in lir Emit. The old code was crashing inside asm. I used the new code to debug some bytecode issues, it works well.; - compute max locals/stack, needed by the asm verifier (CheckClass). @konradjk This fixes your class not found issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8537
https://github.com/hail-is/hail/pull/8537:319,Usability,simpl,simplify,319,"Plus:; - Boolean ldc (load constant) instructions need an int, not a boolean. JVM seems OK with it, but the asm bytecode verifier rejects it.; - In Apply codegen, the zip in function lookup was potentially truncating the arguments, selecting an incorrect function. Fix, and simplify the definition of `methods`.; - Fix/simplify asm error reporting from asm in lir Emit. The old code was crashing inside asm. I used the new code to debug some bytecode issues, it works well.; - compute max locals/stack, needed by the asm verifier (CheckClass). @konradjk This fixes your class not found issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8537
https://github.com/hail-is/hail/pull/8538:54,Security,Expose,Exposed,54,"- Adds typeArgs: Array[Type] to registered functions. Exposed on Apply, ApplyIR, ApplySpecial, but currently not on ApplySeeded. - Removes munging of reference genome function names: there is no longer a global registry of ""{function}_{rg}"" functions. - Minor: (bug) unify was previously being used without asserts (in apply methods on IRFunctions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8538
https://github.com/hail-is/hail/pull/8538:307,Testability,assert,asserts,307,"- Adds typeArgs: Array[Type] to registered functions. Exposed on Apply, ApplyIR, ApplySpecial, but currently not on ApplySeeded. - Removes munging of reference genome function names: there is no longer a global registry of ""{function}_{rg}"" functions. - Minor: (bug) unify was previously being used without asserts (in apply methods on IRFunctions)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8538
https://github.com/hail-is/hail/pull/8540:40,Availability,error,error,40,"Gidgethub does not properly handle this error response. The right answer is to fix gidgethub, but, alas, too much to do and too little time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8540
https://github.com/hail-is/hail/pull/8541:115,Testability,test,testing,115,"The previous location directive redirected every URL matching that prefix to `/batch`, thus losing the suffix. I'm testing right now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8541
https://github.com/hail-is/hail/pull/8542:48,Testability,log,logs,48,I missed a few places where CI tries to get the logs of a service.; All of these can fail if the pod has multiple containers. I added; `--all-containers` to ensure it never fails for multi-container Pods.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8542
https://github.com/hail-is/hail/issues/8545:18,Availability,error,error,18,"Hi, I'm facing an error when importing a bgen file and I was wondering if you can help me. I'd made a small .bgen file using qctools to select only a few samples from the original file and now I'm trying to import this smaller file into hail, like this:. `hl.import_bgen('subsetted_chr22.bgen', entry_fields=['GT', 'GP', 'dosage'], sample_file='samples_test.sample').write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:431,Availability,error,error,431,"Hi, I'm facing an error when importing a bgen file and I was wondering if you can help me. I'd made a small .bgen file using qctools to select only a few samples from the original file and now I'm trying to import this smaller file into hail, like this:. `hl.import_bgen('subsetted_chr22.bgen', entry_fields=['GT', 'GP', 'dosage'], sample_file='samples_test.sample').write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:1728,Availability,Error,Error,1728,"odule>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:1944,Availability,error,error,1944,"ck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:4093,Availability,failure,failure,4093,"at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): is.hail.utils.HailException: Hail only supports 8-bit probabilities, found 16.; 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:222); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:206); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); 	at scala.collection.TraversableOnce$FlattenOps$$anon$1.hasNext(TraversableOnce.scala:464); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterato",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:4150,Availability,failure,failure,4150," is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): is.hail.utils.HailException: Hail only supports 8-bit probabilities, found 16.; 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:222); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:206); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); 	at scala.collection.TraversableOnce$FlattenOps$$anon$1.hasNext(TraversableOnce.scala:464); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at is.hail.io.RichContextRDDRegionValue$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:19854,Availability,Error,Error,19854,"ator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.31-6060f9c971cc; Error summary: HailException: Hail only supports 8-bit probabilities, found 16. How can I solve it? Or why is it happening?. Thank you very much!. Kind regards,; Catarina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:8697,Energy Efficiency,schedul,scheduler,8697,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:8769,Energy Efficiency,schedul,scheduler,8769,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9294,Energy Efficiency,schedul,scheduler,9294,stractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9334,Energy Efficiency,schedul,scheduler,9334,.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9433,Energy Efficiency,schedul,scheduler,9433,he.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9531,Energy Efficiency,schedul,scheduler,9531,ntext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9785,Energy Efficiency,schedul,scheduler,9785,sk.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9866,Energy Efficiency,schedul,scheduler,9866,y(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9972,Energy Efficiency,schedul,scheduler,9972,he.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:10122,Energy Efficiency,schedul,scheduler,10122,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:10211,Energy Efficiency,schedul,scheduler,10211,va.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:10309,Energy Efficiency,schedul,scheduler,10309,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:223); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:10405,Energy Efficiency,schedul,scheduler,10405,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:223); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:915); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:10570,Energy Efficiency,schedul,scheduler,10570,.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:223); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:915); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:19263,Energy Efficiency,schedul,scheduler,19263,"ator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.31-6060f9c971cc; Error summary: HailException: Hail only supports 8-bit probabilities, found 16. How can I solve it? Or why is it happening?. Thank you very much!. Kind regards,; Catarina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:19335,Energy Efficiency,schedul,scheduler,19335,"ator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.31-6060f9c971cc; Error summary: HailException: Hail only supports 8-bit probabilities, found 16. How can I solve it? Or why is it happening?. Thank you very much!. Kind regards,; Catarina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:993,Integrability,wrap,wrapper,993,"ror when importing a bgen file and I was wondering if you can help me. I'd made a small .bgen file using qctools to select only a few samples from the original file and now I'm trying to import this smaller file into hail, like this:. `hl.import_bgen('subsetted_chr22.bgen', entry_fields=['GT', 'GP', 'dosage'], sample_file='samples_test.sample').write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:2309,Integrability,Wrap,WrappedArray,2309,"kend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:2330,Integrability,Wrap,WrappedArray,2330,"9, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:11507,Integrability,Wrap,WrappedMatrixWriter,11507,anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:223); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:915); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:583); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:48); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:13063,Integrability,Wrap,WrappedArray,13063,ss.scala:48); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:11); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:11); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:43); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:13084,Integrability,Wrap,WrappedArray,13084, is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:11); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:11); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:43); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:10); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:9); 	at is.hail.utils.package$.using(package.scala:596); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.r,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:11864,Modifiability,rewrite,rewrite,11864,runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:223); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:915); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:583); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:48); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:11); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:11); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:43); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:1361,Performance,load,loads,1361,".write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9054,Performance,concurren,concurrent,9054,ollection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9139,Performance,concurren,concurrent,9139,tion.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:19620,Performance,concurren,concurrent,19620,"ator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.31-6060f9c971cc; Error summary: HailException: Hail only supports 8-bit probabilities, found 16. How can I solve it? Or why is it happening?. Thank you very much!. Kind regards,; Catarina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:19705,Performance,concurren,concurrent,19705,"ator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.31-6060f9c971cc; Error summary: HailException: Hail only supports 8-bit probabilities, found 16. How can I solve it? Or why is it happening?. Thank you very much!. Kind regards,; Catarina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:4072,Safety,abort,aborted,4072,"at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:9); 	at is.hail.backend.Backend.execute(Backend.scala:56); 	at is.hail.backend.Backend.executeJSON(Backend.scala:62); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): is.hail.utils.HailException: Hail only supports 8-bit probabilities, found 16.; 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.codegen.generated.C_bgen_rdd_decoder_13.apply(Unknown Source); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:222); 	at is.hail.io.bgen.BgenRecordIteratorWithoutFilter.next(BgenRDD.scala:206); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410); 	at scala.collection.TraversableOnce$FlattenOps$$anon$1.hasNext(TraversableOnce.scala:464); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterato",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9465,Safety,abort,abortStage,9465,fun$collect$1$$anonfun$13.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9563,Safety,abort,abortStage,9563,5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/issues/8545:9808,Safety,abort,abortStage,9808,spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545
https://github.com/hail-is/hail/pull/8546:172,Performance,load,load,172,"missed during testing: there is a brief interval where z-height of placeholder navbar is smaller than the sphinx side nav element, causing a flash from blue to white after load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8546
https://github.com/hail-is/hail/pull/8546:14,Testability,test,testing,14,"missed during testing: there is a brief interval where z-height of placeholder navbar is smaller than the sphinx side nav element, causing a flash from blue to white after load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8546
https://github.com/hail-is/hail/pull/8547:87,Integrability,depend,depends,87,"This is a simple refactor, that moves `Stream[A]` to a top-level class. Some of my wip depends on this, so it will be helpful to get it merged. This should also simplify merging `Stream` with the PType infrastructure, where `Stream` should become a `PCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8547
https://github.com/hail-is/hail/pull/8547:17,Modifiability,refactor,refactor,17,"This is a simple refactor, that moves `Stream[A]` to a top-level class. Some of my wip depends on this, so it will be helpful to get it merged. This should also simplify merging `Stream` with the PType infrastructure, where `Stream` should become a `PCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8547
https://github.com/hail-is/hail/pull/8547:10,Usability,simpl,simple,10,"This is a simple refactor, that moves `Stream[A]` to a top-level class. Some of my wip depends on this, so it will be helpful to get it merged. This should also simplify merging `Stream` with the PType infrastructure, where `Stream` should become a `PCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8547
https://github.com/hail-is/hail/pull/8547:161,Usability,simpl,simplify,161,"This is a simple refactor, that moves `Stream[A]` to a top-level class. Some of my wip depends on this, so it will be helpful to get it merged. This should also simplify merging `Stream` with the PType infrastructure, where `Stream` should become a `PCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8547
https://github.com/hail-is/hail/pull/8549:38,Availability,avail,availible,38,Documentation for this field's use is availible at the end of the linked; section. We can maybe version the docs url in order to host the docs to; the released versions. https://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8549
https://github.com/hail-is/hail/pull/8549:151,Deployability,release,released,151,Documentation for this field's use is availible at the end of the linked; section. We can maybe version the docs url in order to host the docs to; the released versions. https://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8549
https://github.com/hail-is/hail/pull/8550:60,Deployability,deploy,deploy,60,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:118,Deployability,deploy,deploy,118,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:215,Deployability,release,release,215,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:614,Deployability,deploy,deploy,614,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:706,Deployability,deploy,deploy,706,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1133,Deployability,deploy,deploy,1133,". However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1195,Deployability,deploy,deploy,1195,"ion changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gser",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1253,Deployability,deploy,deploy,1253,"teps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy us",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1272,Deployability,deploy,deploy,1272,"; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1314,Deployability,deploy,deploys,1314,"; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1345,Deployability,deploy,deploy,1345,"; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1391,Deployability,deploy,deploy,1391,"oc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1426,Deployability,deploy,deploy,1426,"oc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1452,Deployability,deploy,deploy,1452,"artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and datapr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1879,Deployability,deploy,deploy,1879,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1915,Deployability,deploy,deploy,1915,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:2224,Deployability,deploy,deploy,2224,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:243,Performance,perform,perform,243,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1630,Security,password,password,1630,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1689,Security,access,access,1689,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1824,Security,Authoriz,Authorization,1824,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:593,Testability,test,test-dataproc,593,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:719,Testability,test,test-dataproc,719,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:930,Testability,test,testing,930,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:987,Testability,test,test,987," `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1092,Testability,test,test,1092," `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1101,Testability,test,tested,1101,". However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:1187,Testability,test,tested,1187,"ion changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gser",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:2120,Testability,test,test-dataproc-service-account-key,2120,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:2158,Testability,test,test-dataproc,2158,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8550:2371,Testability,test,test-dataproc,2371,"in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. ---. I added two new credentials:; - `pypi-credentials`: `hailgenetics` PyPI credentials, and; - `test-dataproc-service-account-key`:; `test-dataproc@hail-vdc.iam.gserviceaccount.com` credentials.; Dev deploy users (@cseed, @jigold, @tpoterba, @johnc1231) will need to copy; these into their dev namespaces if they want to run `test_dataproc`. The `test-dataproc` service account has storage admin permission to hail-common and dataproc admin permissions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550
https://github.com/hail-is/hail/pull/8555:173,Usability,simpl,simplifies,173,"This PR switches the `NDArrayEmitter` to emit column major data by default. However, it still allows things like the result of `MakeNDArray` to come back as row major. This simplifies code that calls BLAS considerably, since we no longer have to manually flip to column major and then flip back. . There's still unnecessary copying going on, just less. `emitNDArrayStandardStrides` is not a very smart method, always copies the data even if it's already in the right format. At some point a smarter version of this should check if the ndarray exists in memory with the proper striding already.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8555
https://github.com/hail-is/hail/pull/8559:118,Deployability,configurat,configuration,118,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:468,Deployability,configurat,configuration,468,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:715,Deployability,configurat,configuration,715,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:816,Deployability,configurat,configuration,816,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:118,Modifiability,config,configuration,118,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:197,Modifiability,config,config,197,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:468,Modifiability,config,configuration,468,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:715,Modifiability,config,configuration,715,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:816,Modifiability,config,configuration,816,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:846,Modifiability,config,configparser,846,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:895,Modifiability,config,configparser,895,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:917,Modifiability,config,configparser,917,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:991,Modifiability,config,config,991,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:276,Testability,test,test,276,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8559:308,Testability,test,test,308,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559
https://github.com/hail-is/hail/pull/8560:52,Deployability,install,installation,52,These are no longer relevant with the advent of pip installation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8560
https://github.com/hail-is/hail/pull/8561:916,Availability,downtime,downtime,916,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6033,Availability,avail,available,6033,"yption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a ne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:10621,Availability,error,error,10621,"eputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13267,Availability,outage,outages,13267," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:900,Deployability,deploy,deploys,900,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6784,Deployability,configurat,configuration,6784,"s://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6824,Deployability,configurat,configuration,6824,"s://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7117,Deployability,configurat,configuration,7117,"of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7218,Deployability,configurat,configuration,7218,"New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8013,Deployability,Deploy,Deploy,8013,"-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8060,Deployability,deploy,deploy,8060,"-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8074,Deployability,deploy,deployed,8074,": an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change wit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8127,Deployability,deploy,deployed,8127,": an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change wit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8165,Deployability,deploy,deploy-ago,8165," configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Conf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8273,Deployability,upgrade,upgrade,8273," configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Conf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8383,Deployability,update,updated,8383,"t.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the att",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8976,Deployability,deploy,deployed,8976,"cher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:11514,Deployability,deploy,deploying,11514,"he HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13089,Deployability,configurat,configurations,13089," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13186,Deployability,deploy,deploy,13186," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13204,Deployability,deploy,deploy,13204," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13231,Deployability,deploy,deploy,13231," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13504,Deployability,deploy,deploy,13504," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13525,Deployability,Deploy,Deploy,13525," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13594,Deployability,deploy,deploy,13594," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:207,Integrability,rout,router,207,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:1532,Integrability,message,message,1532," speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:1601,Integrability,protocol,protocol,1601,"ryone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:1959,Integrability,interface,interface,1959,"and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:1989,Integrability,protocol,protocol,1989,"tation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2043,Integrability,message,messages,2043,"tation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2099,Integrability,protocol,protocol,2099,"e structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:5463,Integrability,message,messages,5463,"). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6644,Integrability,rout,router,6644," to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6923,Integrability,rout,router,6923,"e has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8709,Integrability,rout,router-resolver,8709,"e to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can nam",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:10398,Integrability,message,message,10398,"hority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12025,Integrability,message,message,12025," There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12168,Integrability,message,messages,12168,"ate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12332,Integrability,message,messages,12332,"ubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12653,Integrability,message,messages,12653," Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will compl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13238,Integrability,rout,router-resolver,13238," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13364,Integrability,rout,router,13364," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13703,Integrability,rout,router,13703," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:364,Modifiability,config,config-hail-root,364,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2516,Modifiability,enhance,enhanced,2516,"age is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The firs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6315,Modifiability,config,config-NAME,6315,"recy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6497,Modifiability,config,config,6497,"nt over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6546,Modifiability,config,config,6546,"es who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site mak",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6784,Modifiability,config,configuration,6784,"s://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6824,Modifiability,config,configuration,6824,"s://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7040,Modifiability,config,config-site,7040,"ble. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_cer",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7089,Modifiability,config,config-http,7089,"se to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7117,Modifiability,config,configuration,7117,"of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7141,Modifiability,config,configures,7141,"of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7189,Modifiability,config,config-proxy,7189,"Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7218,Modifiability,config,configuration,7218,"New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7242,Modifiability,config,configures,7242,"New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12742,Modifiability,config,config,12742," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13089,Modifiability,config,configurations,13089," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12716,Performance,load,load,12716," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:932,Safety,avoid,avoid,932,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:10053,Safety,safe,safe,10053,"nes that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:312,Security,Certificate,Certificate,312,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:750,Security,certificate,certificate,750,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:851,Security,certificate,certificate,851,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:1543,Security,encrypt,encrypted,1543," speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2029,Security,encrypt,encryption,2029,"tation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2067,Security,authenticat,authentication,2067,"tation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2116,Security,authenticat,authentication,2116,"e structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2186,Security,authenticat,authentication,2186,"ng and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both ou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2235,Security,authenticat,authenticate,2235,"is structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2307,Security,Authenticat,Authentication,2307,"## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2334,Security,Certificate,Certificates,2334," is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2363,Security,Certificate,Certificates,2363,"o implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2380,Security,authenticat,authentication,2380,"o implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2443,Security,Certificate,Certificates,2443,"nnection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2813,Security,CERTIFICATE,CERTIFICATE,2813,"nsidered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2855,Security,Certificate,Certificate,2855,"are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certifi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2981,Security,certificate,certificate,2981,"y the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostna",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:3002,Security,secur,security,3002,"on of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:3026,Security,authenticat,authenticatable,3026,"on of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:3288,Security,certificate,certificate,3288,"tication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:3801,Security,encrypt,encrypted,3801," use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:3827,Security,password,password,3827," use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:3870,Security,certificate,certificate,3870,"tity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root cert",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4038,Security,certificate,certificate,4038,"henticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4110,Security,certificate,certificate,4110,"henticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4265,Security,Certificate,Certificate,4265,"ficate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4425,Security,Certificate,Certificate,4425,"sa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared privat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4455,Security,Certificate,Certificate,4455,"explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4528,Security,certificate,certificate,4528,"explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4609,Security,certificate,certificate,4609,"explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4715,Security,certificate,certificates,4715,"`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wik",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4742,Security,Certificate,Certificate,4742,"`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wik",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4814,Security,certificate,certificate,4814," using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4842,Security,certificate,certificate,4842," using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4894,Security,certificate,certificate,4894,"for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4929,Security,certificate,certificate,4929,"ng"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:5014,Security,Encrypt,Encryption,5014,"ient will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:5039,Security,encrypt,encryption,5039,"tificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:5075,Security,encrypt,encryption,5075,"icate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure cipher",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:5443,Security,encrypt,encrypt,5443,"). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6143,Security,secur,secure,6143,"ble *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:6190,Security,Secur,Security,6190,"parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7350,Security,certificate,certificate,7350,"matically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their cert",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7397,Security,certificate,certificates,7397,"e_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7475,Security,certificate,certificates,7475,"y; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authentica",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7598,Security,certificate,certificate,7598,"`; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:7780,Security,certificate,certificate,7780,"site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8176,Security,certificate,certificates,8176," configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Conf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8348,Security,certificate,certificates,8348,"t.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the att",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8480,Security,authenticat,authenticated,8480,"tgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8592,Security,certificate,certificate,8592,"server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:8780,Security,certificate,certificates,8780,"e to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can nam",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9017,Security,certificate,certificates,9017,"cher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9265,Security,secur,security,9265,"ixed, I am unsure how to smoothly upgrade services. I probably need to notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9338,Security,attack,attacker,9338,"notify; old services to refresh their certificates after the secrets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9382,Security,attack,attacker,9382,"crets are updated. ### Incoming Trust. Mutual TLS (mTLS) refers to TLS connections wherein both sides are; authenticated. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not includ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9503,Security,attack,attacker,9503,"d. This is rare on the web. In our system, it means verifying that a; request made to you carries a certificate in the `NAME-incoming.pem` file. I; cannot enable that in this PR because the three unmanaged services,; router-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes cruc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9752,Security,expose,exposed,9752,"outer-resolver, internal-gateway, and gateway, do not currently have; certificates. As a result, all the services in the PR namespace reject the; requests from the unmangaed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; doe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9907,Security,certificate,certificate,9907,"aed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. Th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:9951,Security,certificate,certificates,9951,"aed services. In particular, batch pods cannot; communicate with batch-driver. After this PR is deployed and the unmanaged services have certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. Th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:10006,Security,certificate,certificate,10006,"ve certificates, I can; enable mutual TLS. I've marked the tow lines that need to change with `# FIXME:; mTLS`. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. In master, Batch has a confused deputy problem: it issues a callback in response; to a batch finishing. That callback is issued from within the cluster and; therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not supp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:10832,Security,certificate,certificates,10832,"f TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new na",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:11152,Security,certificate,certificate,11152,". ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:11495,Security,certificate,certificates,11495,"he HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:11574,Security,Secur,Security,11574,"the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it withou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:11596,Security,Secur,Secure,11596,"error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:11821,Security,Secur,Security,11821,"ge for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:11981,Security,encrypt,encryption,11981," the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12508,Security,secur,securely,12508," Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will compl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12915,Security,secur,secured,12915," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:13152,Security,secur,security,13152," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:3251,Testability,test,test-batch,3251,"lients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.50",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:2593,Usability,simpl,simple,2593,"ry. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:4869,Usability,simpl,simpler,4869,"This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is simpler. We have no root certificate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:5930,Usability,clear,clear,5930,"ate. Each principal has a; certificate which is given to all the principals to which it might; communicate. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/DiffieHellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8561:12087,Usability,learn,learned,12087," There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Future Work. - Require TLS 1.3 everywhere.; - Comply with Mozilla's ""Modern"" recommendations.; - [Incoming Trust](#incoming-trust).; - Refresh certificates after deploying new ones. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561
https://github.com/hail-is/hail/pull/8562:301,Availability,down,download,301,"This adds two new Dockerfiles. The first has gsutil and pip-wheel-installed; Hail. The second builds on the first adding a number of bioinformatics tools; that were included in the CCG Tutorial. Deployment to dockerhub is not trivial because, unfortunately, I need to mount; the docker socket even to download and then upload an image (never starting a; container). I'll design and implement some extension to CI that lets me deploy; images to docker hub later. For now, I used dev deploy to build; these (https://ci.hail.is/batches/33294) and then manually pulled them and; uploaded them to PyPI as hailgenetics/hail:0.2.37 and; hailgenetics/genetics:0.2.37. I particularly appreciate criticism of the names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8562
https://github.com/hail-is/hail/pull/8562:66,Deployability,install,installed,66,"This adds two new Dockerfiles. The first has gsutil and pip-wheel-installed; Hail. The second builds on the first adding a number of bioinformatics tools; that were included in the CCG Tutorial. Deployment to dockerhub is not trivial because, unfortunately, I need to mount; the docker socket even to download and then upload an image (never starting a; container). I'll design and implement some extension to CI that lets me deploy; images to docker hub later. For now, I used dev deploy to build; these (https://ci.hail.is/batches/33294) and then manually pulled them and; uploaded them to PyPI as hailgenetics/hail:0.2.37 and; hailgenetics/genetics:0.2.37. I particularly appreciate criticism of the names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8562
https://github.com/hail-is/hail/pull/8562:195,Deployability,Deploy,Deployment,195,"This adds two new Dockerfiles. The first has gsutil and pip-wheel-installed; Hail. The second builds on the first adding a number of bioinformatics tools; that were included in the CCG Tutorial. Deployment to dockerhub is not trivial because, unfortunately, I need to mount; the docker socket even to download and then upload an image (never starting a; container). I'll design and implement some extension to CI that lets me deploy; images to docker hub later. For now, I used dev deploy to build; these (https://ci.hail.is/batches/33294) and then manually pulled them and; uploaded them to PyPI as hailgenetics/hail:0.2.37 and; hailgenetics/genetics:0.2.37. I particularly appreciate criticism of the names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8562
https://github.com/hail-is/hail/pull/8562:426,Deployability,deploy,deploy,426,"This adds two new Dockerfiles. The first has gsutil and pip-wheel-installed; Hail. The second builds on the first adding a number of bioinformatics tools; that were included in the CCG Tutorial. Deployment to dockerhub is not trivial because, unfortunately, I need to mount; the docker socket even to download and then upload an image (never starting a; container). I'll design and implement some extension to CI that lets me deploy; images to docker hub later. For now, I used dev deploy to build; these (https://ci.hail.is/batches/33294) and then manually pulled them and; uploaded them to PyPI as hailgenetics/hail:0.2.37 and; hailgenetics/genetics:0.2.37. I particularly appreciate criticism of the names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8562
https://github.com/hail-is/hail/pull/8562:482,Deployability,deploy,deploy,482,"This adds two new Dockerfiles. The first has gsutil and pip-wheel-installed; Hail. The second builds on the first adding a number of bioinformatics tools; that were included in the CCG Tutorial. Deployment to dockerhub is not trivial because, unfortunately, I need to mount; the docker socket even to download and then upload an image (never starting a; container). I'll design and implement some extension to CI that lets me deploy; images to docker hub later. For now, I used dev deploy to build; these (https://ci.hail.is/batches/33294) and then manually pulled them and; uploaded them to PyPI as hailgenetics/hail:0.2.37 and; hailgenetics/genetics:0.2.37. I particularly appreciate criticism of the names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8562
https://github.com/hail-is/hail/pull/8563:146,Safety,safe,safe,146,"```; # hailctl auth copy-paste-login LXTFDIXKJ6N4Q23G2Q53VQ7GDOGUTYUUWHQI7EZZ6XHLUKDUFZGQ==== ; Logged into namespace dking as dking.; ```; It is safe to reprint this token because it is not only expired but already used. Tokens live for 5 minutes and can be used only once. You can try logging in with that token (to dking's auth) to see for yourself. The user page now includes a button to retrieve a fresh copy-paste token:. ![Screen Shot 2020-04-15 at 11 35 20 PM](https://user-images.githubusercontent.com/106194/79411894-14315c80-7f72-11ea-9c94-e769919da09f.png). This sends you to a rather ugly page that includes a base32 encoded (dashes create word breaks which makes highlighting annoying, so I preferred base32 over base64) token. That token is good for 5 minutes. You copy that token and log in as above. ![Screen Shot 2020-04-19 at 4 16 27 PM](https://user-images.githubusercontent.com/106194/79698776-2b659800-8259-11ea-8697-ea41aa3a6c23.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8563
https://github.com/hail-is/hail/pull/8563:31,Testability,log,login,31,"```; # hailctl auth copy-paste-login LXTFDIXKJ6N4Q23G2Q53VQ7GDOGUTYUUWHQI7EZZ6XHLUKDUFZGQ==== ; Logged into namespace dking as dking.; ```; It is safe to reprint this token because it is not only expired but already used. Tokens live for 5 minutes and can be used only once. You can try logging in with that token (to dking's auth) to see for yourself. The user page now includes a button to retrieve a fresh copy-paste token:. ![Screen Shot 2020-04-15 at 11 35 20 PM](https://user-images.githubusercontent.com/106194/79411894-14315c80-7f72-11ea-9c94-e769919da09f.png). This sends you to a rather ugly page that includes a base32 encoded (dashes create word breaks which makes highlighting annoying, so I preferred base32 over base64) token. That token is good for 5 minutes. You copy that token and log in as above. ![Screen Shot 2020-04-19 at 4 16 27 PM](https://user-images.githubusercontent.com/106194/79698776-2b659800-8259-11ea-8697-ea41aa3a6c23.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8563
https://github.com/hail-is/hail/pull/8563:96,Testability,Log,Logged,96,"```; # hailctl auth copy-paste-login LXTFDIXKJ6N4Q23G2Q53VQ7GDOGUTYUUWHQI7EZZ6XHLUKDUFZGQ==== ; Logged into namespace dking as dking.; ```; It is safe to reprint this token because it is not only expired but already used. Tokens live for 5 minutes and can be used only once. You can try logging in with that token (to dking's auth) to see for yourself. The user page now includes a button to retrieve a fresh copy-paste token:. ![Screen Shot 2020-04-15 at 11 35 20 PM](https://user-images.githubusercontent.com/106194/79411894-14315c80-7f72-11ea-9c94-e769919da09f.png). This sends you to a rather ugly page that includes a base32 encoded (dashes create word breaks which makes highlighting annoying, so I preferred base32 over base64) token. That token is good for 5 minutes. You copy that token and log in as above. ![Screen Shot 2020-04-19 at 4 16 27 PM](https://user-images.githubusercontent.com/106194/79698776-2b659800-8259-11ea-8697-ea41aa3a6c23.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8563
https://github.com/hail-is/hail/pull/8563:287,Testability,log,logging,287,"```; # hailctl auth copy-paste-login LXTFDIXKJ6N4Q23G2Q53VQ7GDOGUTYUUWHQI7EZZ6XHLUKDUFZGQ==== ; Logged into namespace dking as dking.; ```; It is safe to reprint this token because it is not only expired but already used. Tokens live for 5 minutes and can be used only once. You can try logging in with that token (to dking's auth) to see for yourself. The user page now includes a button to retrieve a fresh copy-paste token:. ![Screen Shot 2020-04-15 at 11 35 20 PM](https://user-images.githubusercontent.com/106194/79411894-14315c80-7f72-11ea-9c94-e769919da09f.png). This sends you to a rather ugly page that includes a base32 encoded (dashes create word breaks which makes highlighting annoying, so I preferred base32 over base64) token. That token is good for 5 minutes. You copy that token and log in as above. ![Screen Shot 2020-04-19 at 4 16 27 PM](https://user-images.githubusercontent.com/106194/79698776-2b659800-8259-11ea-8697-ea41aa3a6c23.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8563
https://github.com/hail-is/hail/pull/8563:800,Testability,log,log,800,"```; # hailctl auth copy-paste-login LXTFDIXKJ6N4Q23G2Q53VQ7GDOGUTYUUWHQI7EZZ6XHLUKDUFZGQ==== ; Logged into namespace dking as dking.; ```; It is safe to reprint this token because it is not only expired but already used. Tokens live for 5 minutes and can be used only once. You can try logging in with that token (to dking's auth) to see for yourself. The user page now includes a button to retrieve a fresh copy-paste token:. ![Screen Shot 2020-04-15 at 11 35 20 PM](https://user-images.githubusercontent.com/106194/79411894-14315c80-7f72-11ea-9c94-e769919da09f.png). This sends you to a rather ugly page that includes a base32 encoded (dashes create word breaks which makes highlighting annoying, so I preferred base32 over base64) token. That token is good for 5 minutes. You copy that token and log in as above. ![Screen Shot 2020-04-19 at 4 16 27 PM](https://user-images.githubusercontent.com/106194/79698776-2b659800-8259-11ea-8697-ea41aa3a6c23.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8563
https://github.com/hail-is/hail/pull/8564:461,Integrability,wrap,wraps,461,"This PR adds infrastructure for unrealizable `PType`s and `PCode`s. My primary goal was finding the shortest path to enabling nested streams in EmitStream. The main changes are:; * Add `PUnrealizable` and `PUnrealizableCode`. These are traits in the `PType` and `PCode` hierarchies, respectively, that provide ""implementations"" (throw exceptions) for methods not supported on unrealizable types.; * Add `PStreamCode` and `PCanonicalStreamCode`. The latter just wraps a `Stream` from EmitStream.scala, but eventually these should be unified. I also added `PCanonicalStream`, mostly for consistency with the rest of the `PType` hierarchy. If you think the added noise isn't worth the consistency, I can get rid of the abstract classes.; * I added assertions to `TypeCheck` for all cases I could think of where a node could take a child of any type, but now will only work for realizable types.; * To support nested streams in the emitter, we need to be able to bind streams in the environment, e.g. to map over stream of streams. The easiest way I could see to enable this was to keep the environment an `Env[EmitValue]`, but to allow `EmitValue`s of unrealizable types. I added `EmitUnrealizableValue` which asserts that it is only used once. This does go against the concept that ""values are things that can safely be used multiple times""; I'm open to discussion on what the right design is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8564
https://github.com/hail-is/hail/pull/8564:1308,Safety,safe,safely,1308,"This PR adds infrastructure for unrealizable `PType`s and `PCode`s. My primary goal was finding the shortest path to enabling nested streams in EmitStream. The main changes are:; * Add `PUnrealizable` and `PUnrealizableCode`. These are traits in the `PType` and `PCode` hierarchies, respectively, that provide ""implementations"" (throw exceptions) for methods not supported on unrealizable types.; * Add `PStreamCode` and `PCanonicalStreamCode`. The latter just wraps a `Stream` from EmitStream.scala, but eventually these should be unified. I also added `PCanonicalStream`, mostly for consistency with the rest of the `PType` hierarchy. If you think the added noise isn't worth the consistency, I can get rid of the abstract classes.; * I added assertions to `TypeCheck` for all cases I could think of where a node could take a child of any type, but now will only work for realizable types.; * To support nested streams in the emitter, we need to be able to bind streams in the environment, e.g. to map over stream of streams. The easiest way I could see to enable this was to keep the environment an `Env[EmitValue]`, but to allow `EmitValue`s of unrealizable types. I added `EmitUnrealizableValue` which asserts that it is only used once. This does go against the concept that ""values are things that can safely be used multiple times""; I'm open to discussion on what the right design is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8564
https://github.com/hail-is/hail/pull/8564:745,Testability,assert,assertions,745,"This PR adds infrastructure for unrealizable `PType`s and `PCode`s. My primary goal was finding the shortest path to enabling nested streams in EmitStream. The main changes are:; * Add `PUnrealizable` and `PUnrealizableCode`. These are traits in the `PType` and `PCode` hierarchies, respectively, that provide ""implementations"" (throw exceptions) for methods not supported on unrealizable types.; * Add `PStreamCode` and `PCanonicalStreamCode`. The latter just wraps a `Stream` from EmitStream.scala, but eventually these should be unified. I also added `PCanonicalStream`, mostly for consistency with the rest of the `PType` hierarchy. If you think the added noise isn't worth the consistency, I can get rid of the abstract classes.; * I added assertions to `TypeCheck` for all cases I could think of where a node could take a child of any type, but now will only work for realizable types.; * To support nested streams in the emitter, we need to be able to bind streams in the environment, e.g. to map over stream of streams. The easiest way I could see to enable this was to keep the environment an `Env[EmitValue]`, but to allow `EmitValue`s of unrealizable types. I added `EmitUnrealizableValue` which asserts that it is only used once. This does go against the concept that ""values are things that can safely be used multiple times""; I'm open to discussion on what the right design is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8564
https://github.com/hail-is/hail/pull/8564:1207,Testability,assert,asserts,1207,"This PR adds infrastructure for unrealizable `PType`s and `PCode`s. My primary goal was finding the shortest path to enabling nested streams in EmitStream. The main changes are:; * Add `PUnrealizable` and `PUnrealizableCode`. These are traits in the `PType` and `PCode` hierarchies, respectively, that provide ""implementations"" (throw exceptions) for methods not supported on unrealizable types.; * Add `PStreamCode` and `PCanonicalStreamCode`. The latter just wraps a `Stream` from EmitStream.scala, but eventually these should be unified. I also added `PCanonicalStream`, mostly for consistency with the rest of the `PType` hierarchy. If you think the added noise isn't worth the consistency, I can get rid of the abstract classes.; * I added assertions to `TypeCheck` for all cases I could think of where a node could take a child of any type, but now will only work for realizable types.; * To support nested streams in the emitter, we need to be able to bind streams in the environment, e.g. to map over stream of streams. The easiest way I could see to enable this was to keep the environment an `Env[EmitValue]`, but to allow `EmitValue`s of unrealizable types. I added `EmitUnrealizableValue` which asserts that it is only used once. This does go against the concept that ""values are things that can safely be used multiple times""; I'm open to discussion on what the right design is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8564
https://github.com/hail-is/hail/pull/8567:79,Testability,benchmark,benchmarks,79,"* push image to unique tag based on docker digest; * prevent multiple overlaid benchmarks from using incorrect image. Thought this was already in, oops...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8567
https://github.com/hail-is/hail/pull/8568:67,Integrability,interface,interface,67,"Simplified `NDArrayRef` considerably by switching to `CodeBuilder` interface, adding `EmitCode.mapN`, and adding `PNDArrayCode`. . Thanks @patrick-schultz for contributing code for `EmitCode.mapN`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8568
https://github.com/hail-is/hail/pull/8568:0,Usability,Simpl,Simplified,0,"Simplified `NDArrayRef` considerably by switching to `CodeBuilder` interface, adding `EmitCode.mapN`, and adding `PNDArrayCode`. . Thanks @patrick-schultz for contributing code for `EmitCode.mapN`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8568
https://github.com/hail-is/hail/pull/8569:233,Testability,assert,assert,233,"@johnc1231 I realized there's a really easy way to catch the kinds of dead-code-after-a-goto bugs we were running into, so I went ahead and did it. The basic idea is: most operations on a CodeBuilder (which ultimately call `append`) assert that the current `code` doesn't end in a `ControlX`, such as a jump. `CodeBuilder.define` is the escape hatch, which works whether `code` ends in a jump, or doesn't, in which case the previous code falls through to the new label.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8569
https://github.com/hail-is/hail/pull/8570:130,Safety,avoid,avoid,130,The overloading made development a huge headache. I think we should consider overloading more than ~2-way bad practice and try to avoid it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8570
https://github.com/hail-is/hail/pull/8571:56,Deployability,configurat,configuration,56,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571
https://github.com/hail-is/hail/pull/8571:201,Deployability,configurat,configuration,201,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571
https://github.com/hail-is/hail/pull/8571:529,Deployability,install,installed,529,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571
https://github.com/hail-is/hail/pull/8571:302,Integrability,depend,dependencies,302,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571
https://github.com/hail-is/hail/pull/8571:56,Modifiability,config,configuration,56,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571
https://github.com/hail-is/hail/pull/8571:201,Modifiability,config,configuration,201,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571
https://github.com/hail-is/hail/pull/8571:93,Testability,log,logs,93,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571
https://github.com/hail-is/hail/pull/8576:11,Integrability,interface,interface,11,"Our public interface is python. The public interface of Spark that we use, AFAICT, does not depend on any non-Spark-defined classes. Ergo, we should hide away all our dependencies. I intended to shade every bundled package except for breeze-natives, because I think we want to take that from the cluster (and we're willing to lock into Spark's version). `RandBasis` (a breeze class) takes as argument a random generator from `math3`, so I keep the Apache Commons Math3 library unbundled and not shaded. Everything else should now be shaded.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576
https://github.com/hail-is/hail/pull/8576:43,Integrability,interface,interface,43,"Our public interface is python. The public interface of Spark that we use, AFAICT, does not depend on any non-Spark-defined classes. Ergo, we should hide away all our dependencies. I intended to shade every bundled package except for breeze-natives, because I think we want to take that from the cluster (and we're willing to lock into Spark's version). `RandBasis` (a breeze class) takes as argument a random generator from `math3`, so I keep the Apache Commons Math3 library unbundled and not shaded. Everything else should now be shaded.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576
https://github.com/hail-is/hail/pull/8576:92,Integrability,depend,depend,92,"Our public interface is python. The public interface of Spark that we use, AFAICT, does not depend on any non-Spark-defined classes. Ergo, we should hide away all our dependencies. I intended to shade every bundled package except for breeze-natives, because I think we want to take that from the cluster (and we're willing to lock into Spark's version). `RandBasis` (a breeze class) takes as argument a random generator from `math3`, so I keep the Apache Commons Math3 library unbundled and not shaded. Everything else should now be shaded.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576
https://github.com/hail-is/hail/pull/8576:167,Integrability,depend,dependencies,167,"Our public interface is python. The public interface of Spark that we use, AFAICT, does not depend on any non-Spark-defined classes. Ergo, we should hide away all our dependencies. I intended to shade every bundled package except for breeze-natives, because I think we want to take that from the cluster (and we're willing to lock into Spark's version). `RandBasis` (a breeze class) takes as argument a random generator from `math3`, so I keep the Apache Commons Math3 library unbundled and not shaded. Everything else should now be shaded.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576
https://github.com/hail-is/hail/pull/8581:802,Deployability,configurat,configuration,802,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8581
https://github.com/hail-is/hail/pull/8581:802,Modifiability,config,configuration,802,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8581
https://github.com/hail-is/hail/pull/8581:789,Performance,load,load,789,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8581
https://github.com/hail-is/hail/pull/8581:1489,Performance,load,loaded,1489,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8581
https://github.com/hail-is/hail/pull/8581:674,Security,access,access,674,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8581
https://github.com/hail-is/hail/pull/8583:15,Integrability,rout,router-resolver,15,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583
https://github.com/hail-is/hail/pull/8583:260,Safety,avoid,avoid,260,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583
https://github.com/hail-is/hail/pull/8583:136,Security,authoriz,authorized,136,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583
https://github.com/hail-is/hail/pull/8583:185,Security,authoriz,authorized,185,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583
https://github.com/hail-is/hail/pull/8583:335,Security,attack,attacker,335,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583
https://github.com/hail-is/hail/pull/8583:213,Testability,log,logged,213,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583
https://github.com/hail-is/hail/pull/8583:424,Testability,log,log,424,"Currently, the router-resolver returns 500 if the session id is invalid. Instead,; it should return 401. This collapses two states: not authorized due to not being; a developer and not authorized due to not being logged in. This is unfortunate, but; we should avoid leaking information as to *why* this endpoint is unauthorized to; an attacker. Developers, presumably, are knowledgable enough to figure out why; they cannot log in on their own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583
https://github.com/hail-is/hail/pull/8584:377,Deployability,deploy,deploy,377,"Currently, `hailctl curl` uses `external_url` instead of `url`. As a result,; if `hailctl curl` is used inside a GCE VM or on a k8s pod, the url will always; be `....hail.is` to which GCE VMs and k8s pods likely lack credentials. This was a mistake when I first wrote curl. At that time, I was only using it for; local testing. It will still work for local testing because our deploy configs are; all `external`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8584
https://github.com/hail-is/hail/pull/8584:384,Modifiability,config,configs,384,"Currently, `hailctl curl` uses `external_url` instead of `url`. As a result,; if `hailctl curl` is used inside a GCE VM or on a k8s pod, the url will always; be `....hail.is` to which GCE VMs and k8s pods likely lack credentials. This was a mistake when I first wrote curl. At that time, I was only using it for; local testing. It will still work for local testing because our deploy configs are; all `external`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8584
https://github.com/hail-is/hail/pull/8584:319,Testability,test,testing,319,"Currently, `hailctl curl` uses `external_url` instead of `url`. As a result,; if `hailctl curl` is used inside a GCE VM or on a k8s pod, the url will always; be `....hail.is` to which GCE VMs and k8s pods likely lack credentials. This was a mistake when I first wrote curl. At that time, I was only using it for; local testing. It will still work for local testing because our deploy configs are; all `external`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8584
https://github.com/hail-is/hail/pull/8584:357,Testability,test,testing,357,"Currently, `hailctl curl` uses `external_url` instead of `url`. As a result,; if `hailctl curl` is used inside a GCE VM or on a k8s pod, the url will always; be `....hail.is` to which GCE VMs and k8s pods likely lack credentials. This was a mistake when I first wrote curl. At that time, I was only using it for; local testing. It will still work for local testing because our deploy configs are; all `external`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8584
https://github.com/hail-is/hail/pull/8585:101,Integrability,message,message,101,"- HAIL_QUERY_BACKEND selects the backend: spark or service; - remove Spark from default initializing message; - Add HAIL_DONT_RETRY_500=1 to disable retrying 500. This is necessary to see the faction of passing tests against the partially-functioning query service. Otherwise the tests hang.; - normalize paths in GoogleStorageFS. Various code uses .., and normalizing removes them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8585
https://github.com/hail-is/hail/pull/8585:211,Testability,test,tests,211,"- HAIL_QUERY_BACKEND selects the backend: spark or service; - remove Spark from default initializing message; - Add HAIL_DONT_RETRY_500=1 to disable retrying 500. This is necessary to see the faction of passing tests against the partially-functioning query service. Otherwise the tests hang.; - normalize paths in GoogleStorageFS. Various code uses .., and normalizing removes them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8585
https://github.com/hail-is/hail/pull/8585:280,Testability,test,tests,280,"- HAIL_QUERY_BACKEND selects the backend: spark or service; - remove Spark from default initializing message; - Add HAIL_DONT_RETRY_500=1 to disable retrying 500. This is necessary to see the faction of passing tests against the partially-functioning query service. Otherwise the tests hang.; - normalize paths in GoogleStorageFS. Various code uses .., and normalizing removes them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8585
https://github.com/hail-is/hail/issues/8586:2,Testability,assert,assert,2,"I assert that if you create a directory structure by recursively applying this rule:; - create a file named a, recur into it, then; - create a file named b, recur into it. up to some maximum depth D, then the following call will take O(2^D) time:; ```; blobs = bucket.list_blobs(prefix=""dsub-magma-out-maf01/"", max_results=1, delimiter='/'); list(islice(blobs, 1)); ```. In particular, the second operation will take all the time. The first operation returns almost instantaneously. I do not fully understand how Google Cloud Storage is implemented. I conjecture that it uses some ordered structure of keys/paths. The main issue is the `delimiter` argument. When that argument is specified, the returned object will have a `prefixes` property which is populated with one page's worth of ""prefixes"" or strings matching `dsub-magma-out-mfa01/[^/]+/`. Unfortunately, it appears that even with `max_results=1`, the API calls takes a lot of time (as long as 30s). Removing `delimiter` causes the API return immediately. This seems like a bug on Google's end. If `max_results=1`, then Google's API docs claim you'll receive at most 1 prefix per page. If retrieving a single object (`list_blobs(prefix=""foo/"", max_results=1)`) takes time `t`, then retrieving the first prefix should also take time `t` (calculate it from that single object).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8586
https://github.com/hail-is/hail/issues/8587:194,Usability,UX,UX,194,https://github.com/hail-is/jgscm/pull/3 will resolve this. `jgscm` sequentially issues API calls for each folder or file in a directory. Each call takes about a second so this quickly kills the UX.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8587
https://github.com/hail-is/hail/pull/8596:51,Testability,test,tests,51,This PR lowers `TableParallelize` and beefs up the tests for parallelize a bit. If you see a simpler formulation of the math to compute `start` and `length` let me know.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8596
https://github.com/hail-is/hail/pull/8596:93,Usability,simpl,simpler,93,This PR lowers `TableParallelize` and beefs up the tests for parallelize a bit. If you see a simpler formulation of the math to compute `start` and `length` let me know.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8596
https://github.com/hail-is/hail/pull/8602:362,Testability,test,tests,362,"This PR is already getting kind of massive, so I think I'm going to plan on implementing (extracted) aggregator support separately. I may also pull out the TableIR stuff into a separate PR and leave this as just implementations of pure value IR nodes, since I ended up needing to add a bunch of stuff to be able to pull minimal PTypes out of reads. I've written tests for most of the values nodes, but not all of them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8602
https://github.com/hail-is/hail/pull/8605:64,Testability,test,tested,64,"This PR lowers `TableRename`. The only thing that is not really tested is the renaming of key fields, because we don't support lowering `TableKeyBy` yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8605
https://github.com/hail-is/hail/pull/8606:54,Deployability,deploy,deploy,54,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8606
https://github.com/hail-is/hail/pull/8606:241,Deployability,deploy,deploy,241,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8606
https://github.com/hail-is/hail/pull/8606:297,Deployability,deploy,deploys,297,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8606
https://github.com/hail-is/hail/pull/8606:373,Deployability,deploy,deploys,373,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8606
https://github.com/hail-is/hail/pull/8606:79,Testability,test,tested,79,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8606
https://github.com/hail-is/hail/pull/8606:236,Testability,test,test,236,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8606
https://github.com/hail-is/hail/pull/8606:368,Testability,test,test,368,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8606
https://github.com/hail-is/hail/pull/8623:56,Performance,cache,cached,56,The wheel container prevents this image from ever being cached.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8623
https://github.com/hail-is/hail/pull/8624:313,Integrability,interface,interface,313,"This is the first step in lowering the importers. GenericTableValue is a table value that can either be converted to a normal TableValue, or can be lowered to a TableStage. MatrixPlinkReader now generates a GenericTableValue, but haven't wired it up to the TableIR lowering logic yet (next PR). This includes two interface changes: import_plink now takes n_partitions and block_size, like import_bgen. It still takes min_partitions, which is actual min_partitions. When all is said and done, this interface should be consistent across the importers. Second, the Plink reader now splits along variants instead of bytes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8624
https://github.com/hail-is/hail/pull/8624:497,Integrability,interface,interface,497,"This is the first step in lowering the importers. GenericTableValue is a table value that can either be converted to a normal TableValue, or can be lowered to a TableStage. MatrixPlinkReader now generates a GenericTableValue, but haven't wired it up to the TableIR lowering logic yet (next PR). This includes two interface changes: import_plink now takes n_partitions and block_size, like import_bgen. It still takes min_partitions, which is actual min_partitions. When all is said and done, this interface should be consistent across the importers. Second, the Plink reader now splits along variants instead of bytes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8624
https://github.com/hail-is/hail/pull/8624:274,Testability,log,logic,274,"This is the first step in lowering the importers. GenericTableValue is a table value that can either be converted to a normal TableValue, or can be lowered to a TableStage. MatrixPlinkReader now generates a GenericTableValue, but haven't wired it up to the TableIR lowering logic yet (next PR). This includes two interface changes: import_plink now takes n_partitions and block_size, like import_bgen. It still takes min_partitions, which is actual min_partitions. When all is said and done, this interface should be consistent across the importers. Second, the Plink reader now splits along variants instead of bytes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8624
https://github.com/hail-is/hail/pull/8627:137,Integrability,wrap,wrapped,137,"Addressing what we discussed in lowering meeting yesterday. I've refactored TableStage to accumulate a bunch of let bindings that can be wrapped around the IR you want to generate with `TableStage.wrapInBindings`. See `TableCollect` for an example of this. This definitely generates the IR we wanted, but I'd be very open to suggestions about how to make this more ergonomic to use (had to generate a lot of UIDs and Refs manually for `TableParallelize`). . cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8627
https://github.com/hail-is/hail/pull/8627:197,Integrability,wrap,wrapInBindings,197,"Addressing what we discussed in lowering meeting yesterday. I've refactored TableStage to accumulate a bunch of let bindings that can be wrapped around the IR you want to generate with `TableStage.wrapInBindings`. See `TableCollect` for an example of this. This definitely generates the IR we wanted, but I'd be very open to suggestions about how to make this more ergonomic to use (had to generate a lot of UIDs and Refs manually for `TableParallelize`). . cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8627
https://github.com/hail-is/hail/pull/8627:65,Modifiability,refactor,refactored,65,"Addressing what we discussed in lowering meeting yesterday. I've refactored TableStage to accumulate a bunch of let bindings that can be wrapped around the IR you want to generate with `TableStage.wrapInBindings`. See `TableCollect` for an example of this. This definitely generates the IR we wanted, but I'd be very open to suggestions about how to make this more ergonomic to use (had to generate a lot of UIDs and Refs manually for `TableParallelize`). . cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8627
https://github.com/hail-is/hail/pull/8631:104,Availability,down,down,104,"My change https://github.com/hail-is/hail/pull/8581 introduced a bug, count of native reads with pushed down intervals could give the wrong answer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8631
https://github.com/hail-is/hail/pull/8634:670,Performance,Optimiz,Optimized,670,"http://34.207.246.132/ for the demo. The copy in the ""About"" section should have a once-over @mkveerapen in particular, and @danking, @cseed, @tpoterba. Future work will provide a multi-column, easier to read version of About, and a linking section with a beautiful code-snippet-containing introduction to Hail (demonstrating file import). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634
https://github.com/hail-is/hail/pull/8634:1327,Performance,perform,performance,1327,"rt). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally unnecessary OO abstractions, and provides some additional effects (hover highlighting of vertices), hence the fork. License is MIT, no issue for us.; * Made sure this all works, looks nice with docs. Future works will bring doc style in line with homepage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634
https://github.com/hail-is/hail/pull/8634:1376,Performance,load,loaded,1376,"rt). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally unnecessary OO abstractions, and provides some additional effects (hover highlighting of vertices), hence the fork. License is MIT, no issue for us.; * Made sure this all works, looks nice with docs. Future works will bring doc style in line with homepage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634
https://github.com/hail-is/hail/pull/8634:1414,Performance,cache,cached,1414,"rt). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally unnecessary OO abstractions, and provides some additional effects (hover highlighting of vertices), hence the fork. License is MIT, no issue for us.; * Made sure this all works, looks nice with docs. Future works will bring doc style in line with homepage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634
https://github.com/hail-is/hail/pull/8634:1884,Performance,perform,performs,1884,"rt). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally unnecessary OO abstractions, and provides some additional effects (hover highlighting of vertices), hence the fork. License is MIT, no issue for us.; * Made sure this all works, looks nice with docs. Future works will bring doc style in line with homepage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634
https://github.com/hail-is/hail/pull/8634:1093,Usability,simpl,simpler,1093," a once-over @mkveerapen in particular, and @danking, @cseed, @tpoterba. Future work will provide a multi-column, easier to read version of About, and a linking section with a beautiful code-snippet-containing introduction to Hail (demonstrating file import). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634
https://github.com/hail-is/hail/pull/8634:1131,Usability,simpl,simplify,1131," provide a multi-column, easier to read version of About, and a linking section with a beautiful code-snippet-containing introduction to Hail (demonstrating file import). I leave these as future tasks, because enough changes are present in this PR. Summary of changes:; * Refined homepage styles, ensured navbar matches to pixel between docs and hail.is root (surprisingly difficult); * Improved mobile styles, especially mobile nav menu (much smoother animation, larger, easier to click on links); * Optimized icon sizes (50KB -> 3.3KB); * Removed all use of bootstrap on hail.is/*.html pages (bootstrap remains on docs, future pr).; * Removed index.md contents. The markdown format is pretty limited. To have a richly-marked up site with consistent styling, the syntax it provides is not enough. The solution is either to add html to index.md, or just write html in a the index.xslt file. I chose the latter, because it's simpler. Future reorganization should simplify this and docs further, though I think I still recommend NextJS and the build system that provides.; * Added threeR115.min.js. This is regrettably large, but doesn't impact page rendering performance in any meaningful way, because it is loaded after all html content (and is cached after the first visit). Future work can go to webgl directly, potentially. There is also ongoing work by the ThreeJS maintainers to allow tree-shaking and smaller builds.; * Added heavily modified fork of VantaJS. Because we are not using something like NextJS, there is no package manager to rely on, so I just checked the file in manually (I have this in a separate repo, we can use that if preferred). License is in line with a note about modifications. Vanta performs very, very poorly (order of 40% CPU usage, old/slow ways of observing whether animated element is in view, unnecessary object generation, etc), this does not, removes a bunch of totally unnecessary OO abstractions, and provides some additional effects (hover highlighting of ve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634
https://github.com/hail-is/hail/pull/8636:75,Availability,error,error,75,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8636
https://github.com/hail-is/hail/pull/8636:198,Deployability,deploy,deploying,198,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8636
https://github.com/hail-is/hail/pull/8636:81,Integrability,message,message,81,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8636
https://github.com/hail-is/hail/pull/8636:316,Performance,cache,cache,316,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8636
https://github.com/hail-is/hail/pull/8637:23,Availability,error,error,23,- generate appropriate error message; - don't test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8637
https://github.com/hail-is/hail/pull/8637:29,Integrability,message,message,29,- generate appropriate error message; - don't test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8637
https://github.com/hail-is/hail/pull/8637:46,Testability,test,test,46,- generate appropriate error message; - don't test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8637
https://github.com/hail-is/hail/pull/8638:56,Testability,test,tests,56,- SparkBackend has the right context; - skip the parser tests unless on Spark backend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8638
https://github.com/hail-is/hail/pull/8640:24,Integrability,depend,depends,24,"ExecuteContext.scoped() depends on the Spark backend, and all uses of it need to get removed. A separate PR removes it from the IRParser. The remaining uses are in BlockMatrix and the Graph/maximal independent set stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8640
https://github.com/hail-is/hail/pull/8647:5,Usability,clear,clear,5,"It's clear we are failing to explain the necessity of a compatible BLAS library. I hope this will stem the flow of bug tickets and support questions related to a misconfigured, missing, or incompatible BLAS library.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8647
https://github.com/hail-is/hail/pull/8650:11,Testability,test,tests,11,Still need tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8650
https://github.com/hail-is/hail/pull/8651:41,Availability,error,error,41,"The default behavior, is, apparently, to error, not to return `None`. This change makes the missing value case return `None` which triggers the ValueError on the next line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8651
https://github.com/hail-is/hail/pull/8652:122,Availability,avail,available,122,"When I last tried to fix this up, I somehow missed the GCS client dependency. This; uses the in cluster location if it is available otherwise it lets Google try to find; the key.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8652
https://github.com/hail-is/hail/pull/8652:66,Integrability,depend,dependency,66,"When I last tried to fix this up, I somehow missed the GCS client dependency. This; uses the in cluster location if it is available otherwise it lets Google try to find; the key.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8652
https://github.com/hail-is/hail/pull/8653:79,Testability,assert,assert,79,"Fix bugs that show up when we typecheck the literal value on construction. The assert is commented out for now because it is expensive. Later, I'll add a flag to control expensive checks like this and checkRVDKeys and the tests should be run with expensive checks turned on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8653
https://github.com/hail-is/hail/pull/8653:222,Testability,test,tests,222,"Fix bugs that show up when we typecheck the literal value on construction. The assert is commented out for now because it is expensive. Later, I'll add a flag to control expensive checks like this and checkRVDKeys and the tests should be run with expensive checks turned on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8653
https://github.com/hail-is/hail/pull/8656:381,Deployability,install,installed,381,Split the Hail query backends into separate file. One goal here is to have spark_backend.py the only file that imports pyspark. Also: Added init_service(). init() is for initialising with the Spark backend. init_service() is for initialising with the service backend. Don't import the backends unelss they are used. This will allow us to avoid importing pyspark (or even having it installed) when using the service backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8656
https://github.com/hail-is/hail/pull/8656:338,Safety,avoid,avoid,338,Split the Hail query backends into separate file. One goal here is to have spark_backend.py the only file that imports pyspark. Also: Added init_service(). init() is for initialising with the Spark backend. init_service() is for initialising with the service backend. Don't import the backends unelss they are used. This will allow us to avoid importing pyspark (or even having it installed) when using the service backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8656
https://github.com/hail-is/hail/pull/8658:11,Modifiability,refactor,refactoring,11,"Some light refactoring of TableStage to change how context/body is defined; this is pretty similar to how BlockMatrixStage currently handles contexts. I prefer this way of defining the body of a partition because I think it will make writing table joins more natural and lead to less boilerplate ref generation; I think BlockMatrixDot provides a pretty good illustration of what a join body will look like, in general. (The context handling is different, and I don't think we want to do what BlockMatrixStage does w.r.t contexts right now.) This is slightly different from the code in master; I've rewritten `blockBody` to make use of `bindIR` and (currently non-existent) `foldIR` instead of manually generating refs to better illustrate flow. ```; case x@BlockMatrixDot(leftIR, rightIR) =>; val left = lower(leftIR); val right = lower(rightIR); val newCtxType = TArray(TTuple(left.ctxType, right.ctxType)); new BlockMatrixStage(left.globalVals ++ right.globalVals, newCtxType) {; def blockContext(idx: (Int, Int)): IR = {; val (i, j) = idx; MakeArray(Array.tabulate[Option[IR]](leftIR.typ.nColBlocks) { k =>; if (leftIR.typ.hasBlock(i -> k) && rightIR.typ.hasBlock(k -> j)); Some(MakeTuple.ordered(FastSeq(; left.blockContext(i -> k), right.blockContext(k -> j)))); else None; }.flatten[IR], newCtxType); }. def blockBody(ctxRef: Ref): IR = {; def blockMultiply(elt: Ref) =; bindIR(GetTupleElement(elt, 0)) { leftElt =>; bindIR(GetTupleElement(elt, 1)) { rightElt =>; NDArrayMatMul(left.blockBody(leftElt), right.blockBody(rightElt)); }; }; foldIR(ToStream(invoke(""sliceRight"", ctxType, ctxRef, I32(1))),; bindIR(ArrayRef(ctxRef, 0))(blockMultiply)) { (sum, elt) =>; NDArrayMap2(sum, blockMultiply(elt), ""l"", ""r"",; Ref(""l"", x.typ.elementType) + Ref(""r"", x.typ.elementType)); }; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8658
https://github.com/hail-is/hail/pull/8662:622,Integrability,depend,depending,622,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8662:6,Performance,load,loading,6,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8662:348,Performance,load,loaded,348,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8662:380,Testability,test,test,380,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8662:449,Testability,test,test,449,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8662:491,Testability,test,tests,491,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8662:523,Testability,test,tested,523,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8662:549,Testability,test,tests,549,"Write loading lines from text files in a way that can be used by Spark or table lowering. This will be to lower the VCF and text file importers. This handles three cases:; - an (splittable) uncompressed file,; - a (splittable) bgzip compressed file, and; - an non-splittable compressed file compressed by some other codec. In this case, it will be loaded as a single partition. I test each of the three cases. I also ported the prexisting partition test that generates random splittings and tests that in the bgzip case. I tested with the number of tests turned up to 10,000. Two ideas here:. Each line has a fixed offset depending on the case (file offset, virtual offset, or decompressed offset). The split defines a partition of the offset spaces. For a partition (start, end), and line with offset x, the line belongs in the partition if x lies in the range (start, end] or, if it is the first partition, [start, end]. This is because, if at the beginning of a split, you can't tell if the line started earlier or not. The second idea is to copy the blocks one at a time into buf. bgzip blocks are max 64KB. `read` on a BGzipInputStream doesn't return data that spans blocks. So buf always contains the entire data for a block. This is used to track the offset o the beginning of the line.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8662
https://github.com/hail-is/hail/pull/8665:55,Testability,test,test,55,This PR lowers `TableGroupWithinPartitions` and adds a test for it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8665
https://github.com/hail-is/hail/pull/8667:139,Availability,error,error,139,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8667
https://github.com/hail-is/hail/pull/8667:244,Availability,echo,echoes,244,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8667
https://github.com/hail-is/hail/pull/8667:681,Availability,error,error,681,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8667
https://github.com/hail-is/hail/pull/8667:720,Availability,error,error,720,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8667
https://github.com/hail-is/hail/pull/8667:733,Availability,Error,Error,733,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8667
https://github.com/hail-is/hail/pull/8667:930,Modifiability,config,config,930,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8667
https://github.com/hail-is/hail/pull/8671:134,Integrability,wrap,wrapping,134,We previously were just passing the Code in EmitParams to method; invocations without testing missingness. This change fixes this by; wrapping the PCode in a missingness that will return a default value if; the original EmitCode is missing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671
https://github.com/hail-is/hail/pull/8671:86,Testability,test,testing,86,We previously were just passing the Code in EmitParams to method; invocations without testing missingness. This change fixes this by; wrapping the PCode in a missingness that will return a default value if; the original EmitCode is missing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671
https://github.com/hail-is/hail/pull/8676:241,Availability,reliab,reliably,241,"The logic trying to infer the version of various Spark dependencies; was total garbage and almost certainly except for a few specific; cases. I was feeling aggressive. I nuked it. If we want to support; building with other versions of Spark reliably (whcih we don't test)`,; we should find another way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8676
https://github.com/hail-is/hail/pull/8676:55,Integrability,depend,dependencies,55,"The logic trying to infer the version of various Spark dependencies; was total garbage and almost certainly except for a few specific; cases. I was feeling aggressive. I nuked it. If we want to support; building with other versions of Spark reliably (whcih we don't test)`,; we should find another way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8676
https://github.com/hail-is/hail/pull/8676:4,Testability,log,logic,4,"The logic trying to infer the version of various Spark dependencies; was total garbage and almost certainly except for a few specific; cases. I was feeling aggressive. I nuked it. If we want to support; building with other versions of Spark reliably (whcih we don't test)`,; we should find another way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8676
https://github.com/hail-is/hail/pull/8676:266,Testability,test,test,266,"The logic trying to infer the version of various Spark dependencies; was total garbage and almost certainly except for a few specific; cases. I was feeling aggressive. I nuked it. If we want to support; building with other versions of Spark reliably (whcih we don't test)`,; we should find another way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8676
https://github.com/hail-is/hail/pull/8680:157,Deployability,configurat,configuration,157,"Changes:; - removed unused py4jVersion from build.gradle; - pin breeze native version to version required by spark. This was not easy! I do it by creating a configuration that just depends on Spark, and then a resolution rule for all configurations that says only accept the breeze natives version corresponding to the version requested by spark.; - determine sparkMajorVersion from sparkVersion (strip patch version); - in docs, everywhere we use SPARK_VERSION, also specify SCALA_VERSION. I verified 2.4.0.cloudera is built against Scala 2.11.; - added SCALA_VERSION to hail/Makefile; - make Makefile versions match build.gradle defaults (somewhat annoying they are duplicated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8680
https://github.com/hail-is/hail/pull/8680:234,Deployability,configurat,configurations,234,"Changes:; - removed unused py4jVersion from build.gradle; - pin breeze native version to version required by spark. This was not easy! I do it by creating a configuration that just depends on Spark, and then a resolution rule for all configurations that says only accept the breeze natives version corresponding to the version requested by spark.; - determine sparkMajorVersion from sparkVersion (strip patch version); - in docs, everywhere we use SPARK_VERSION, also specify SCALA_VERSION. I verified 2.4.0.cloudera is built against Scala 2.11.; - added SCALA_VERSION to hail/Makefile; - make Makefile versions match build.gradle defaults (somewhat annoying they are duplicated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8680
https://github.com/hail-is/hail/pull/8680:403,Deployability,patch,patch,403,"Changes:; - removed unused py4jVersion from build.gradle; - pin breeze native version to version required by spark. This was not easy! I do it by creating a configuration that just depends on Spark, and then a resolution rule for all configurations that says only accept the breeze natives version corresponding to the version requested by spark.; - determine sparkMajorVersion from sparkVersion (strip patch version); - in docs, everywhere we use SPARK_VERSION, also specify SCALA_VERSION. I verified 2.4.0.cloudera is built against Scala 2.11.; - added SCALA_VERSION to hail/Makefile; - make Makefile versions match build.gradle defaults (somewhat annoying they are duplicated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8680
https://github.com/hail-is/hail/pull/8680:181,Integrability,depend,depends,181,"Changes:; - removed unused py4jVersion from build.gradle; - pin breeze native version to version required by spark. This was not easy! I do it by creating a configuration that just depends on Spark, and then a resolution rule for all configurations that says only accept the breeze natives version corresponding to the version requested by spark.; - determine sparkMajorVersion from sparkVersion (strip patch version); - in docs, everywhere we use SPARK_VERSION, also specify SCALA_VERSION. I verified 2.4.0.cloudera is built against Scala 2.11.; - added SCALA_VERSION to hail/Makefile; - make Makefile versions match build.gradle defaults (somewhat annoying they are duplicated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8680
https://github.com/hail-is/hail/pull/8680:157,Modifiability,config,configuration,157,"Changes:; - removed unused py4jVersion from build.gradle; - pin breeze native version to version required by spark. This was not easy! I do it by creating a configuration that just depends on Spark, and then a resolution rule for all configurations that says only accept the breeze natives version corresponding to the version requested by spark.; - determine sparkMajorVersion from sparkVersion (strip patch version); - in docs, everywhere we use SPARK_VERSION, also specify SCALA_VERSION. I verified 2.4.0.cloudera is built against Scala 2.11.; - added SCALA_VERSION to hail/Makefile; - make Makefile versions match build.gradle defaults (somewhat annoying they are duplicated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8680
https://github.com/hail-is/hail/pull/8680:234,Modifiability,config,configurations,234,"Changes:; - removed unused py4jVersion from build.gradle; - pin breeze native version to version required by spark. This was not easy! I do it by creating a configuration that just depends on Spark, and then a resolution rule for all configurations that says only accept the breeze natives version corresponding to the version requested by spark.; - determine sparkMajorVersion from sparkVersion (strip patch version); - in docs, everywhere we use SPARK_VERSION, also specify SCALA_VERSION. I verified 2.4.0.cloudera is built against Scala 2.11.; - added SCALA_VERSION to hail/Makefile; - make Makefile versions match build.gradle defaults (somewhat annoying they are duplicated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8680
https://github.com/hail-is/hail/pull/8683:303,Deployability,configurat,configuration,303,"The date time changes added `-target:jvm-1.8` to build.gradle which quietly; broke SBT. It wasn't a problem for me because I wasn't hacking on Hail until; recently. Moreover, [Ensime](https://ensime.github.io) is dead, so I'm switching over to; bloop. This plugin is necessary for SBT to generate bloop configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8683
https://github.com/hail-is/hail/pull/8683:257,Modifiability,plugin,plugin,257,"The date time changes added `-target:jvm-1.8` to build.gradle which quietly; broke SBT. It wasn't a problem for me because I wasn't hacking on Hail until; recently. Moreover, [Ensime](https://ensime.github.io) is dead, so I'm switching over to; bloop. This plugin is necessary for SBT to generate bloop configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8683
https://github.com/hail-is/hail/pull/8683:303,Modifiability,config,configuration,303,"The date time changes added `-target:jvm-1.8` to build.gradle which quietly; broke SBT. It wasn't a problem for me because I wasn't hacking on Hail until; recently. Moreover, [Ensime](https://ensime.github.io) is dead, so I'm switching over to; bloop. This plugin is necessary for SBT to generate bloop configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8683
https://github.com/hail-is/hail/pull/8686:161,Deployability,deploy,deployment,161,- `stat -c` doesn't work on Mac; - I have a bloop directory now (related to metals which is a Scala Emacs IDE); - I had a rogue x in the definition of gateway's deployment; - I had missing semicolons in gateway's deployment; - the router resolver *is* TLS enabled; - the internal-gateway is *not* TLS enabled,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8686
https://github.com/hail-is/hail/pull/8686:213,Deployability,deploy,deployment,213,- `stat -c` doesn't work on Mac; - I have a bloop directory now (related to metals which is a Scala Emacs IDE); - I had a rogue x in the definition of gateway's deployment; - I had missing semicolons in gateway's deployment; - the router resolver *is* TLS enabled; - the internal-gateway is *not* TLS enabled,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8686
https://github.com/hail-is/hail/pull/8686:231,Integrability,rout,router,231,- `stat -c` doesn't work on Mac; - I have a bloop directory now (related to metals which is a Scala Emacs IDE); - I had a rogue x in the definition of gateway's deployment; - I had missing semicolons in gateway's deployment; - the router resolver *is* TLS enabled; - the internal-gateway is *not* TLS enabled,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8686
https://github.com/hail-is/hail/pull/8691:115,Testability,log,log,115,"This really frustrated me while I was working on the Shuffler. Does it not bother; everyone else? Without this, my log statements get buffered and placed into some; HTML page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8691
https://github.com/hail-is/hail/issues/8694:40,Availability,Error,Error,40,"```; Hail version: 0.2.38-16624ac88829; Error summary: AssertionError: assertion failed: +PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],gene:+PCString,annotation:+PCString,__iruid_97596:+PCArray[+PCStruct{gene:+PCString,annotation:+PCString,`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+PCArray[PCStruct{AF_Allele2:PFloat64,imputationInfo:PFloat64,BETA:PFloat64,SE:PFloat64,`p.value.NA`:PFloat64,`AF.Cases`:PFloat64,`AF.Controls`:PFloat64,Pvalue:PFloat64}]}]}, struct{locus: locus<GRCh37>, alleles: array<str>, __iruid_97596: array<struct{gene: str, annotation: str, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{AF_Allele2: float64, imputationInfo: float64, BETA: float64, SE: float64, `p.value.NA`: float64, `AF.Cases`: float64, `AF.Controls`: float64, Pvalue: float64}>}>}; ...; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.ir.TableAggregateByKey.execute(TableIR.scala:1879); at is.hail.expr.ir.TableFilter.execute(TableIR.scala:581); at is.hail.expr.ir.TableOrderBy.execute(TableIR.scala:1971); at is.hail.expr.ir.TableSubset$class.execute(TableIR.scala:626); at is.hail.expr.ir.TableHead.execute(TableIR.scala:634); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1176); ```; pipeline:; ```; mt = hl.read_matrix_table(...); x = mt._filter_partitions(range(1)); x.entries().show(); ```; version is some minor commits off of f836e49cb179117837aaae7614b6bdd28febe857",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8694
https://github.com/hail-is/hail/issues/8694:1235,Deployability,pipeline,pipeline,1235,"```; Hail version: 0.2.38-16624ac88829; Error summary: AssertionError: assertion failed: +PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],gene:+PCString,annotation:+PCString,__iruid_97596:+PCArray[+PCStruct{gene:+PCString,annotation:+PCString,`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+PCArray[PCStruct{AF_Allele2:PFloat64,imputationInfo:PFloat64,BETA:PFloat64,SE:PFloat64,`p.value.NA`:PFloat64,`AF.Cases`:PFloat64,`AF.Controls`:PFloat64,Pvalue:PFloat64}]}]}, struct{locus: locus<GRCh37>, alleles: array<str>, __iruid_97596: array<struct{gene: str, annotation: str, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{AF_Allele2: float64, imputationInfo: float64, BETA: float64, SE: float64, `p.value.NA`: float64, `AF.Cases`: float64, `AF.Controls`: float64, Pvalue: float64}>}>}; ...; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.ir.TableAggregateByKey.execute(TableIR.scala:1879); at is.hail.expr.ir.TableFilter.execute(TableIR.scala:581); at is.hail.expr.ir.TableOrderBy.execute(TableIR.scala:1971); at is.hail.expr.ir.TableSubset$class.execute(TableIR.scala:626); at is.hail.expr.ir.TableHead.execute(TableIR.scala:634); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1176); ```; pipeline:; ```; mt = hl.read_matrix_table(...); x = mt._filter_partitions(range(1)); x.entries().show(); ```; version is some minor commits off of f836e49cb179117837aaae7614b6bdd28febe857",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8694
https://github.com/hail-is/hail/issues/8694:55,Testability,Assert,AssertionError,55,"```; Hail version: 0.2.38-16624ac88829; Error summary: AssertionError: assertion failed: +PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],gene:+PCString,annotation:+PCString,__iruid_97596:+PCArray[+PCStruct{gene:+PCString,annotation:+PCString,`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+PCArray[PCStruct{AF_Allele2:PFloat64,imputationInfo:PFloat64,BETA:PFloat64,SE:PFloat64,`p.value.NA`:PFloat64,`AF.Cases`:PFloat64,`AF.Controls`:PFloat64,Pvalue:PFloat64}]}]}, struct{locus: locus<GRCh37>, alleles: array<str>, __iruid_97596: array<struct{gene: str, annotation: str, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{AF_Allele2: float64, imputationInfo: float64, BETA: float64, SE: float64, `p.value.NA`: float64, `AF.Cases`: float64, `AF.Controls`: float64, Pvalue: float64}>}>}; ...; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.ir.TableAggregateByKey.execute(TableIR.scala:1879); at is.hail.expr.ir.TableFilter.execute(TableIR.scala:581); at is.hail.expr.ir.TableOrderBy.execute(TableIR.scala:1971); at is.hail.expr.ir.TableSubset$class.execute(TableIR.scala:626); at is.hail.expr.ir.TableHead.execute(TableIR.scala:634); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1176); ```; pipeline:; ```; mt = hl.read_matrix_table(...); x = mt._filter_partitions(range(1)); x.entries().show(); ```; version is some minor commits off of f836e49cb179117837aaae7614b6bdd28febe857",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8694
https://github.com/hail-is/hail/issues/8694:71,Testability,assert,assertion,71,"```; Hail version: 0.2.38-16624ac88829; Error summary: AssertionError: assertion failed: +PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],gene:+PCString,annotation:+PCString,__iruid_97596:+PCArray[+PCStruct{gene:+PCString,annotation:+PCString,`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+PCArray[PCStruct{AF_Allele2:PFloat64,imputationInfo:PFloat64,BETA:PFloat64,SE:PFloat64,`p.value.NA`:PFloat64,`AF.Cases`:PFloat64,`AF.Controls`:PFloat64,Pvalue:PFloat64}]}]}, struct{locus: locus<GRCh37>, alleles: array<str>, __iruid_97596: array<struct{gene: str, annotation: str, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{AF_Allele2: float64, imputationInfo: float64, BETA: float64, SE: float64, `p.value.NA`: float64, `AF.Cases`: float64, `AF.Controls`: float64, Pvalue: float64}>}>}; ...; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.ir.TableAggregateByKey.execute(TableIR.scala:1879); at is.hail.expr.ir.TableFilter.execute(TableIR.scala:581); at is.hail.expr.ir.TableOrderBy.execute(TableIR.scala:1971); at is.hail.expr.ir.TableSubset$class.execute(TableIR.scala:626); at is.hail.expr.ir.TableHead.execute(TableIR.scala:634); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1176); ```; pipeline:; ```; mt = hl.read_matrix_table(...); x = mt._filter_partitions(range(1)); x.entries().show(); ```; version is some minor commits off of f836e49cb179117837aaae7614b6bdd28febe857",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8694
https://github.com/hail-is/hail/issues/8694:833,Testability,assert,assert,833,"```; Hail version: 0.2.38-16624ac88829; Error summary: AssertionError: assertion failed: +PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],gene:+PCString,annotation:+PCString,__iruid_97596:+PCArray[+PCStruct{gene:+PCString,annotation:+PCString,`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+PCArray[PCStruct{AF_Allele2:PFloat64,imputationInfo:PFloat64,BETA:PFloat64,SE:PFloat64,`p.value.NA`:PFloat64,`AF.Cases`:PFloat64,`AF.Controls`:PFloat64,Pvalue:PFloat64}]}]}, struct{locus: locus<GRCh37>, alleles: array<str>, __iruid_97596: array<struct{gene: str, annotation: str, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{AF_Allele2: float64, imputationInfo: float64, BETA: float64, SE: float64, `p.value.NA`: float64, `AF.Cases`: float64, `AF.Controls`: float64, Pvalue: float64}>}>}; ...; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.ir.TableAggregateByKey.execute(TableIR.scala:1879); at is.hail.expr.ir.TableFilter.execute(TableIR.scala:581); at is.hail.expr.ir.TableOrderBy.execute(TableIR.scala:1971); at is.hail.expr.ir.TableSubset$class.execute(TableIR.scala:626); at is.hail.expr.ir.TableHead.execute(TableIR.scala:634); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1176); ```; pipeline:; ```; mt = hl.read_matrix_table(...); x = mt._filter_partitions(range(1)); x.entries().show(); ```; version is some minor commits off of f836e49cb179117837aaae7614b6bdd28febe857",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8694
https://github.com/hail-is/hail/pull/8695:59,Testability,test,test,59,This PR adds lowering rule for `TableHead`. It also adds a test for `TableHead` since there wasn't any good one.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8695
https://github.com/hail-is/hail/pull/8700:47,Integrability,depend,dependency,47,"In our use case, the shaded Hail jar runs into dependency conflicts due to `org.apache.commons.io`, which is pulled in by `com.indeed:lsmtree-core`. This PR shades `org.apache.commons.io`; unfortunately, shading all of `org.apache.commons` was problematic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700
https://github.com/hail-is/hail/pull/8703:20,Energy Efficiency,monitor,monitoring,20,I already moved the monitoring namespace by hand. The monitoring web UI header dropdown now has links to the Google; Cloud console for monitoring and logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8703
https://github.com/hail-is/hail/pull/8703:54,Energy Efficiency,monitor,monitoring,54,I already moved the monitoring namespace by hand. The monitoring web UI header dropdown now has links to the Google; Cloud console for monitoring and logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8703
https://github.com/hail-is/hail/pull/8703:135,Energy Efficiency,monitor,monitoring,135,I already moved the monitoring namespace by hand. The monitoring web UI header dropdown now has links to the Google; Cloud console for monitoring and logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8703
https://github.com/hail-is/hail/pull/8703:150,Testability,log,logs,150,I already moved the monitoring namespace by hand. The monitoring web UI header dropdown now has links to the Google; Cloud console for monitoring and logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8703
https://github.com/hail-is/hail/pull/8706:41,Safety,avoid,avoid,41,Ruchi and Sam need this functionality to avoid an unnecessary scan.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8706
https://github.com/hail-is/hail/pull/8710:97,Integrability,rout,router,97,"So I both screwed up by including the TLS proxy settings in the location as; well as setting the router to port 80, which the gateway does not respect.; The gateway tries to speak to the router on 443. All the router services; should speak TLS on 443, even if they proxy to a service speaking plain; HTTP on 80.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8710
https://github.com/hail-is/hail/pull/8710:187,Integrability,rout,router,187,"So I both screwed up by including the TLS proxy settings in the location as; well as setting the router to port 80, which the gateway does not respect.; The gateway tries to speak to the router on 443. All the router services; should speak TLS on 443, even if they proxy to a service speaking plain; HTTP on 80.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8710
https://github.com/hail-is/hail/pull/8710:210,Integrability,rout,router,210,"So I both screwed up by including the TLS proxy settings in the location as; well as setting the router to port 80, which the gateway does not respect.; The gateway tries to speak to the router on 443. All the router services; should speak TLS on 443, even if they proxy to a service speaking plain; HTTP on 80.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8710
https://github.com/hail-is/hail/pull/8711:137,Energy Efficiency,reduce,reduce,137,"to be a little more in line with need, and to make the non-preemptible pods more packable. The ukbb-rg pods are pretty slow, so I didn't reduce them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8711
https://github.com/hail-is/hail/pull/8718:473,Modifiability,variab,variables,473,"I rage programmed here a bit. I was pretty frustrated with this class when; trying to debug some other issues in the Shuffler IR. I have two aims with this; PR:; 1. Use a clear, consistent naming scheme throughout the file; 2. Use concise definitions. In particular, I unified the terminology to use these words:; - name: the function's name; - valueParameterTypes: the type of each value-level parameter; - typeParameters: the type-level parameters, these are always type variables, I; considered calling them typeVariables, but I like the symmetry with valueParameters; - returnType and returnPType; - typeArguments and valueArguments: these refer to the concrete values to which the parameters are bound. I also simplified some anonymous class definitions by providing constructor arguments; instead of methods that are always overridden by `val`s.; Oh, and I changed `IRFunction` to `JVMFunction` because there are already ""IR"" functions; and an ""irRegistry"" and it was super confusing to not have the IRFunctions inside the; ""irRegistry"". I did not use `CodeFunction` because these are actually implemented by; a number of different JVM Bytecode building tools: `Code`, `PCode`, `EmitCode`, and; `IEmitCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8718
https://github.com/hail-is/hail/pull/8718:171,Usability,clear,clear,171,"I rage programmed here a bit. I was pretty frustrated with this class when; trying to debug some other issues in the Shuffler IR. I have two aims with this; PR:; 1. Use a clear, consistent naming scheme throughout the file; 2. Use concise definitions. In particular, I unified the terminology to use these words:; - name: the function's name; - valueParameterTypes: the type of each value-level parameter; - typeParameters: the type-level parameters, these are always type variables, I; considered calling them typeVariables, but I like the symmetry with valueParameters; - returnType and returnPType; - typeArguments and valueArguments: these refer to the concrete values to which the parameters are bound. I also simplified some anonymous class definitions by providing constructor arguments; instead of methods that are always overridden by `val`s.; Oh, and I changed `IRFunction` to `JVMFunction` because there are already ""IR"" functions; and an ""irRegistry"" and it was super confusing to not have the IRFunctions inside the; ""irRegistry"". I did not use `CodeFunction` because these are actually implemented by; a number of different JVM Bytecode building tools: `Code`, `PCode`, `EmitCode`, and; `IEmitCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8718
https://github.com/hail-is/hail/pull/8718:715,Usability,simpl,simplified,715,"I rage programmed here a bit. I was pretty frustrated with this class when; trying to debug some other issues in the Shuffler IR. I have two aims with this; PR:; 1. Use a clear, consistent naming scheme throughout the file; 2. Use concise definitions. In particular, I unified the terminology to use these words:; - name: the function's name; - valueParameterTypes: the type of each value-level parameter; - typeParameters: the type-level parameters, these are always type variables, I; considered calling them typeVariables, but I like the symmetry with valueParameters; - returnType and returnPType; - typeArguments and valueArguments: these refer to the concrete values to which the parameters are bound. I also simplified some anonymous class definitions by providing constructor arguments; instead of methods that are always overridden by `val`s.; Oh, and I changed `IRFunction` to `JVMFunction` because there are already ""IR"" functions; and an ""irRegistry"" and it was super confusing to not have the IRFunctions inside the; ""irRegistry"". I did not use `CodeFunction` because these are actually implemented by; a number of different JVM Bytecode building tools: `Code`, `PCode`, `EmitCode`, and; `IEmitCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8718
https://github.com/hail-is/hail/pull/8726:741,Testability,test,testShuffleIR,741,"This implements the Shuffler IR, which has become these four nodes:; - ShuffleWith: owns the shuffle, passes the shuffle id to children as a binding.; - ShuffleWrite: writes a stream into the shuffler.; - ShufflePartitionBounds: gets the boundaries of approximately balanced partitions.; - ShuffleRead: reads a stream from the shuffler. It adds `TShuffle` which represents a shuffle and contains all the EType, PType, and Type information necessary to implement all nodes (thanks @catoverdrive !). `TShuffle` is realized at runtime by a `PCanonicalShuffle(true)`, which is a straight copy of `PCanonicalBinary`. Unfortunately, it appears not possible to have two distinct virtual types whose run-time realization is the same physical type. `testShuffleIR` is most of the work towards a lowering of TableKeyBy/TableSortBy, I believe. Next Up:; - Actually implement ""successful partition ids,"" which is ignored by the IR. The server doesn't even have functionality for this right now. Note there are FIXME comments about this in this PR. I will remove them in the next PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726
https://github.com/hail-is/hail/pull/8737:219,Integrability,interface,interface,219,"Implements a StagedIndexWriter with a very similar structure to the unstaged version. To test this, I threaded this through `IndexWriter.build` so that it now compiles a function that implements the CompiledIndexWriter interface:; ```; trait CompiledIndexWriter {; def init(path: String): Unit; def apply(x: Long, offset: Long, annotation: Long): Unit; def close(): Unit; }; ```; with a wrapper class that mimics the interface of the old IndexWriter. Eventually, we'll need this to lower TableWrite. (Kind of non-randomly assigning @chrisvittal, as I'd like feedback on whether the new-style imperative codegen looks right.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8737
https://github.com/hail-is/hail/pull/8737:387,Integrability,wrap,wrapper,387,"Implements a StagedIndexWriter with a very similar structure to the unstaged version. To test this, I threaded this through `IndexWriter.build` so that it now compiles a function that implements the CompiledIndexWriter interface:; ```; trait CompiledIndexWriter {; def init(path: String): Unit; def apply(x: Long, offset: Long, annotation: Long): Unit; def close(): Unit; }; ```; with a wrapper class that mimics the interface of the old IndexWriter. Eventually, we'll need this to lower TableWrite. (Kind of non-randomly assigning @chrisvittal, as I'd like feedback on whether the new-style imperative codegen looks right.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8737
https://github.com/hail-is/hail/pull/8737:417,Integrability,interface,interface,417,"Implements a StagedIndexWriter with a very similar structure to the unstaged version. To test this, I threaded this through `IndexWriter.build` so that it now compiles a function that implements the CompiledIndexWriter interface:; ```; trait CompiledIndexWriter {; def init(path: String): Unit; def apply(x: Long, offset: Long, annotation: Long): Unit; def close(): Unit; }; ```; with a wrapper class that mimics the interface of the old IndexWriter. Eventually, we'll need this to lower TableWrite. (Kind of non-randomly assigning @chrisvittal, as I'd like feedback on whether the new-style imperative codegen looks right.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8737
https://github.com/hail-is/hail/pull/8737:89,Testability,test,test,89,"Implements a StagedIndexWriter with a very similar structure to the unstaged version. To test this, I threaded this through `IndexWriter.build` so that it now compiles a function that implements the CompiledIndexWriter interface:; ```; trait CompiledIndexWriter {; def init(path: String): Unit; def apply(x: Long, offset: Long, annotation: Long): Unit; def close(): Unit; }; ```; with a wrapper class that mimics the interface of the old IndexWriter. Eventually, we'll need this to lower TableWrite. (Kind of non-randomly assigning @chrisvittal, as I'd like feedback on whether the new-style imperative codegen looks right.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8737
https://github.com/hail-is/hail/pull/8737:558,Usability,feedback,feedback,558,"Implements a StagedIndexWriter with a very similar structure to the unstaged version. To test this, I threaded this through `IndexWriter.build` so that it now compiles a function that implements the CompiledIndexWriter interface:; ```; trait CompiledIndexWriter {; def init(path: String): Unit; def apply(x: Long, offset: Long, annotation: Long): Unit; def close(): Unit; }; ```; with a wrapper class that mimics the interface of the old IndexWriter. Eventually, we'll need this to lower TableWrite. (Kind of non-randomly assigning @chrisvittal, as I'd like feedback on whether the new-style imperative codegen looks right.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8737
https://github.com/hail-is/hail/pull/8738:304,Deployability,pipeline,pipeline,304,"Adds support for TableIR nodes, as well as some test coverage for all supported TableIR nodes. I think I'm going to choose not to support RelationalLetTable at this point; I think we lift relational lets before we lower MatrixTables, so we would never need to support it currently in our normal lowering pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8738
https://github.com/hail-is/hail/pull/8738:48,Testability,test,test,48,"Adds support for TableIR nodes, as well as some test coverage for all supported TableIR nodes. I think I'm going to choose not to support RelationalLetTable at this point; I think we lift relational lets before we lower MatrixTables, so we would never need to support it currently in our normal lowering pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8738
https://github.com/hail-is/hail/pull/8740:547,Modifiability,extend,extend,547,"This adds the function to TableReader:; ```; rowAndGlobalPTypes(ctx: ExecuteContext, requrestedType: Type): (PStruct, PStruct); ```. (The context is necessary for native readers to be able to get the filesystem in order to read the metadata.). I'm not sure if this is the best way to implement this, but it feels like TableReaders ought to be able to tell you what PType they expect to be decoding into at compile time, since we can use this information to make decisions about requiredness, etc. on IRs that contain TableReads. I rely on this to extend the requiredness analysis to TableIR nodes; I think it's probably reasonable to have one function that goes from requestedType => PType instead of separate functions for requiredness and other analyses we might want to do un the future, since I don't think we have imminent plans to decode directly into different PTypes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8740
https://github.com/hail-is/hail/pull/8741:75,Testability,Test,Test,75,"Also build infrastructure for repartitioning table stages without shuffle. Test does not pass with `allRelational` due to method splitting issues, but did pass when I fixed a few locals=>fields and disabled EstimableEmitter splitting logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8741
https://github.com/hail-is/hail/pull/8741:234,Testability,log,logic,234,"Also build infrastructure for repartitioning table stages without shuffle. Test does not pass with `allRelational` due to method splitting issues, but did pass when I fixed a few locals=>fields and disabled EstimableEmitter splitting logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8741
https://github.com/hail-is/hail/pull/8742:24,Testability,log,logs,24,Added hail-batch-worker-logs bucket with a 7-day retention policy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8742
https://github.com/hail-is/hail/pull/8743:185,Performance,Load,LoadVCF,185,"It was wrong for me to check `hasNext` before checking `childIterationCount != blockSize`. If you look at https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1306, you can see that `LoadVCF`'s `hasNext` loads values into memory. By calling `hasNext` and then not actually processing that value, I was loading a value into memory and then invalidating the pointer before it processed in the next group of rows. . Thanks to @tpoterba for helping me figure this out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8743
https://github.com/hail-is/hail/pull/8743:224,Performance,Load,LoadVCF,224,"It was wrong for me to check `hasNext` before checking `childIterationCount != blockSize`. If you look at https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1306, you can see that `LoadVCF`'s `hasNext` loads values into memory. By calling `hasNext` and then not actually processing that value, I was loading a value into memory and then invalidating the pointer before it processed in the next group of rows. . Thanks to @tpoterba for helping me figure this out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8743
https://github.com/hail-is/hail/pull/8743:245,Performance,load,loads,245,"It was wrong for me to check `hasNext` before checking `childIterationCount != blockSize`. If you look at https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1306, you can see that `LoadVCF`'s `hasNext` loads values into memory. By calling `hasNext` and then not actually processing that value, I was loading a value into memory and then invalidating the pointer before it processed in the next group of rows. . Thanks to @tpoterba for helping me figure this out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8743
https://github.com/hail-is/hail/pull/8743:343,Performance,load,loading,343,"It was wrong for me to check `hasNext` before checking `childIterationCount != blockSize`. If you look at https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1306, you can see that `LoadVCF`'s `hasNext` loads values into memory. By calling `hasNext` and then not actually processing that value, I was loading a value into memory and then invalidating the pointer before it processed in the next group of rows. . Thanks to @tpoterba for helping me figure this out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8743
https://github.com/hail-is/hail/pull/8744:13,Integrability,message,message,13,except for a message around spacing in parameters with type hints and; default values that I'm not quite sure how to resolve. I couldn't help myself.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8744
https://github.com/hail-is/hail/issues/8751:87,Availability,down,downsample,87,"`0.2.39-d38fca12930d`. ```; ht_out = ht.group_by(_x=True).aggregate(; ...: data=hl.agg.downsample(; ...: hl.log10(eur_freq.gnomad_genomes_af), hl.log10(eur_freq.af / 2),; ...: [ht.pass_status, hl.str(hl.abs(hl.log(eur_freq.gnomad_genomes_af / eur_freq.gnomad_exomes_af, 2)) > 2)]); ...: ).explode('data').key_by().drop('_x'); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-59-3a16f061373b>"", line 5, in <module>; ).explode('data').key_by().drop('_x'); File ""<decorator-gen-1731>"", line 2, in drop; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 965, in drop; protected_key = set(self._row_indices.protected_key); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 42, in protected_key; self._cached_key = self._get_key(); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 56, in _get_key; assert isinstance(self.source, hl.MatrixTable); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8751
https://github.com/hail-is/hail/issues/8751:789,Integrability,wrap,wrapper,789,"`0.2.39-d38fca12930d`. ```; ht_out = ht.group_by(_x=True).aggregate(; ...: data=hl.agg.downsample(; ...: hl.log10(eur_freq.gnomad_genomes_af), hl.log10(eur_freq.af / 2),; ...: [ht.pass_status, hl.str(hl.abs(hl.log(eur_freq.gnomad_genomes_af / eur_freq.gnomad_exomes_af, 2)) > 2)]); ...: ).explode('data').key_by().drop('_x'); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-59-3a16f061373b>"", line 5, in <module>; ).explode('data').key_by().drop('_x'); File ""<decorator-gen-1731>"", line 2, in drop; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 965, in drop; protected_key = set(self._row_indices.protected_key); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 42, in protected_key; self._cached_key = self._get_key(); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 56, in _get_key; assert isinstance(self.source, hl.MatrixTable); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8751
https://github.com/hail-is/hail/issues/8751:210,Testability,log,log,210,"`0.2.39-d38fca12930d`. ```; ht_out = ht.group_by(_x=True).aggregate(; ...: data=hl.agg.downsample(; ...: hl.log10(eur_freq.gnomad_genomes_af), hl.log10(eur_freq.af / 2),; ...: [ht.pass_status, hl.str(hl.abs(hl.log(eur_freq.gnomad_genomes_af / eur_freq.gnomad_exomes_af, 2)) > 2)]); ...: ).explode('data').key_by().drop('_x'); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-59-3a16f061373b>"", line 5, in <module>; ).explode('data').key_by().drop('_x'); File ""<decorator-gen-1731>"", line 2, in drop; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 965, in drop; protected_key = set(self._row_indices.protected_key); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 42, in protected_key; self._cached_key = self._get_key(); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 56, in _get_key; assert isinstance(self.source, hl.MatrixTable); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8751
https://github.com/hail-is/hail/issues/8751:1199,Testability,assert,assert,1199,"`0.2.39-d38fca12930d`. ```; ht_out = ht.group_by(_x=True).aggregate(; ...: data=hl.agg.downsample(; ...: hl.log10(eur_freq.gnomad_genomes_af), hl.log10(eur_freq.af / 2),; ...: [ht.pass_status, hl.str(hl.abs(hl.log(eur_freq.gnomad_genomes_af / eur_freq.gnomad_exomes_af, 2)) > 2)]); ...: ).explode('data').key_by().drop('_x'); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-59-3a16f061373b>"", line 5, in <module>; ).explode('data').key_by().drop('_x'); File ""<decorator-gen-1731>"", line 2, in drop; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 965, in drop; protected_key = set(self._row_indices.protected_key); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 42, in protected_key; self._cached_key = self._get_key(); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 56, in _get_key; assert isinstance(self.source, hl.MatrixTable); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8751
https://github.com/hail-is/hail/issues/8751:1247,Testability,Assert,AssertionError,1247,"`0.2.39-d38fca12930d`. ```; ht_out = ht.group_by(_x=True).aggregate(; ...: data=hl.agg.downsample(; ...: hl.log10(eur_freq.gnomad_genomes_af), hl.log10(eur_freq.af / 2),; ...: [ht.pass_status, hl.str(hl.abs(hl.log(eur_freq.gnomad_genomes_af / eur_freq.gnomad_exomes_af, 2)) > 2)]); ...: ).explode('data').key_by().drop('_x'); Traceback (most recent call last):; File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-59-3a16f061373b>"", line 5, in <module>; ).explode('data').key_by().drop('_x'); File ""<decorator-gen-1731>"", line 2, in drop; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 965, in drop; protected_key = set(self._row_indices.protected_key); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 42, in protected_key; self._cached_key = self._get_key(); File ""/Users/konradk/hail/hail/python/hail/expr/expressions/indices.py"", line 56, in _get_key; assert isinstance(self.source, hl.MatrixTable); AssertionError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8751
https://github.com/hail-is/hail/pull/8757:503,Availability,error,error,503,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757
https://github.com/hail-is/hail/pull/8757:907,Deployability,integrat,integration,907,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757
https://github.com/hail-is/hail/pull/8757:907,Integrability,integrat,integration,907,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757
https://github.com/hail-is/hail/pull/8757:786,Performance,perform,performs,786,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757
https://github.com/hail-is/hail/pull/8757:735,Testability,benchmark,benchmark,735,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757
https://github.com/hail-is/hail/pull/8757:865,Testability,test,tests,865,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757
https://github.com/hail-is/hail/pull/8757:919,Testability,test,tests,919,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757
https://github.com/hail-is/hail/pull/8759:276,Availability,error,errors,276,"I think this can go in instead of #8730. I ran dev deploy with master and then didn't delete the database and ran the tests with the new version. The billing UI page reported the correct values. I also ran the new version with the check functions in the background and got no errors. I can probably double check the UI batches cost are correct, but let's wait until we're happy with the code before I do anymore testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759
https://github.com/hail-is/hail/pull/8759:51,Deployability,deploy,deploy,51,"I think this can go in instead of #8730. I ran dev deploy with master and then didn't delete the database and ran the tests with the new version. The billing UI page reported the correct values. I also ran the new version with the check functions in the background and got no errors. I can probably double check the UI batches cost are correct, but let's wait until we're happy with the code before I do anymore testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759
https://github.com/hail-is/hail/pull/8759:118,Testability,test,tests,118,"I think this can go in instead of #8730. I ran dev deploy with master and then didn't delete the database and ran the tests with the new version. The billing UI page reported the correct values. I also ran the new version with the check functions in the background and got no errors. I can probably double check the UI batches cost are correct, but let's wait until we're happy with the code before I do anymore testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759
https://github.com/hail-is/hail/pull/8759:412,Testability,test,testing,412,"I think this can go in instead of #8730. I ran dev deploy with master and then didn't delete the database and ran the tests with the new version. The billing UI page reported the correct values. I also ran the new version with the check functions in the background and got no errors. I can probably double check the UI batches cost are correct, but let's wait until we're happy with the code before I do anymore testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759
https://github.com/hail-is/hail/pull/8760:79,Deployability,update,update,79,"Fixes the problem where if there is a bad secret / job config fails, we didn't update the attempt id in the jobs table in the database.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8760
https://github.com/hail-is/hail/pull/8760:55,Modifiability,config,config,55,"Fixes the problem where if there is a bad secret / job config fails, we didn't update the attempt id in the jobs table in the database.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8760
https://github.com/hail-is/hail/pull/8761:9,Availability,error,error,9,"Got this error in a migration step when dev deploying. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 598, in _read_bytes; data = await self._reader.readexactly(num_bytes); File ""/usr/lib/python3.7/asyncio/streams.py"", line 679, in readexactly; await self._wait_for_data('readexactly'); File ""/usr/lib/python3.7/asyncio/streams.py"", line 473, in _wait_for_data; await self._waiter; File ""/usr/lib/python3.7/asyncio/selector_events.py"", line 804, in _read_ready__data_received; data = self._sock.recv(self.max_size); ConnectionResetError: [Errno 104] Connection reset by peer. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""create_database.py"", line 263, in <module>; loop.run_until_complete(async_main()); File ""/usr/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""create_database.py"", line 259, in async_main; await migrate(database_name, db, i, m); File ""create_database.py"", line 201, in migrate; (to_version, to_version, name, script_sha1)); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 26, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 229, in just_execute; async with self.start() as tx:; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 114, in __aenter__; await tx.async_init(self.db_pool, self.read_only); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 135, in async_init; await cursor.execute('START TRANSACTION;'); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8761
https://github.com/hail-is/hail/pull/8761:44,Deployability,deploy,deploying,44,"Got this error in a migration step when dev deploying. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 598, in _read_bytes; data = await self._reader.readexactly(num_bytes); File ""/usr/lib/python3.7/asyncio/streams.py"", line 679, in readexactly; await self._wait_for_data('readexactly'); File ""/usr/lib/python3.7/asyncio/streams.py"", line 473, in _wait_for_data; await self._waiter; File ""/usr/lib/python3.7/asyncio/selector_events.py"", line 804, in _read_ready__data_received; data = self._sock.recv(self.max_size); ConnectionResetError: [Errno 104] Connection reset by peer. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""create_database.py"", line 263, in <module>; loop.run_until_complete(async_main()); File ""/usr/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""create_database.py"", line 259, in async_main; await migrate(database_name, db, i, m); File ""create_database.py"", line 201, in migrate; (to_version, to_version, name, script_sha1)); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 26, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 229, in just_execute; async with self.start() as tx:; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 114, in __aenter__; await tx.async_init(self.db_pool, self.read_only); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 135, in async_init; await cursor.execute('START TRANSACTION;'); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8761
https://github.com/hail-is/hail/pull/8761:1210,Integrability,wrap,wrapper,1210,"es); File ""/usr/lib/python3.7/asyncio/streams.py"", line 679, in readexactly; await self._wait_for_data('readexactly'); File ""/usr/lib/python3.7/asyncio/streams.py"", line 473, in _wait_for_data; await self._waiter; File ""/usr/lib/python3.7/asyncio/selector_events.py"", line 804, in _read_ready__data_received; data = self._sock.recv(self.max_size); ConnectionResetError: [Errno 104] Connection reset by peer. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""create_database.py"", line 263, in <module>; loop.run_until_complete(async_main()); File ""/usr/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""create_database.py"", line 259, in async_main; await migrate(database_name, db, i, m); File ""create_database.py"", line 201, in migrate; (to_version, to_version, name, script_sha1)); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 26, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 229, in just_execute; async with self.start() as tx:; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 114, in __aenter__; await tx.async_init(self.db_pool, self.read_only); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 135, in async_init; await cursor.execute('START TRANSACTION;'); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 622, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 1105, in read; first_p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8761
https://github.com/hail-is/hail/pull/8763:209,Modifiability,rewrite,rewrite,209,"This fixes site to work inside PRs and dev namespaces. The main fix is to teach; site that, when the namespace is not default, all its resources are located at; `/$NAMESPACE/site`. I also use `subs_filter` to rewrite images, anchors, and; stylesheets that have absolute links (this can be `<a href=""/""` or `<a; href=""/foo/bar/baz.html""`) to include the namespace prefix. I also added a test that the website is up and returning 200 with our infrastructure. I also added `updated_host` which uses the X-Fowarded-For host if it exists (i.e. in a namespace).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8763
https://github.com/hail-is/hail/pull/8763:386,Testability,test,test,386,"This fixes site to work inside PRs and dev namespaces. The main fix is to teach; site that, when the namespace is not default, all its resources are located at; `/$NAMESPACE/site`. I also use `subs_filter` to rewrite images, anchors, and; stylesheets that have absolute links (this can be `<a href=""/""` or `<a; href=""/foo/bar/baz.html""`) to include the namespace prefix. I also added a test that the website is up and returning 200 with our infrastructure. I also added `updated_host` which uses the X-Fowarded-For host if it exists (i.e. in a namespace).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8763
https://github.com/hail-is/hail/pull/8765:22,Integrability,message,message,22,I also converted a no-message assertion into an if with an AssertionError; because I found it difficult to debug without the added information. This removes the duplication of the list of supported types for RPrimitive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8765
https://github.com/hail-is/hail/pull/8765:30,Testability,assert,assertion,30,I also converted a no-message assertion into an if with an AssertionError; because I found it difficult to debug without the added information. This removes the duplication of the list of supported types for RPrimitive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8765
https://github.com/hail-is/hail/pull/8765:59,Testability,Assert,AssertionError,59,I also converted a no-message assertion into an if with an AssertionError; because I found it difficult to debug without the added information. This removes the duplication of the list of supported types for RPrimitive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8765
https://github.com/hail-is/hail/pull/8766:31,Availability,avail,available,31,"This mirrors the functionality available on `TypedCodecSpec`. In some cases (the shuffler),; you might be handed a buffer that is already configured, but you still want to create a; decoder whose PType is known to be a subtype of PStruct.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8766
https://github.com/hail-is/hail/pull/8766:138,Modifiability,config,configured,138,"This mirrors the functionality available on `TypedCodecSpec`. In some cases (the shuffler),; you might be handed a buffer that is already configured, but you still want to create a; decoder whose PType is known to be a subtype of PStruct.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8766
https://github.com/hail-is/hail/pull/8770:17,Deployability,deploy,deployed,17,"Missed one. Hand deployed, verified it works end-to-end with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8770
https://github.com/hail-is/hail/pull/8774:122,Energy Efficiency,Power,Powered,122,"""Landscape of multi-nucleotide variants in 125,748 human exomes and 15,708 genomes"" is currently listed twice on the Hail-Powered Science page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8774
https://github.com/hail-is/hail/pull/8776:55,Testability,test,test,55,"I'm PRing this because it's done / short / adds a good test, but it may be possible to eliminate `TableDistinct` in the future in favor of just `TableAggregateByKey`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8776
https://github.com/hail-is/hail/pull/8779:44,Deployability,Deploy,DeployConfig,44,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8779
https://github.com/hail-is/hail/pull/8779:492,Deployability,update,updated,492,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8779
https://github.com/hail-is/hail/pull/8779:197,Safety,safe,safe,197,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8779
https://github.com/hail-is/hail/pull/8779:224,Safety,safe,safe,224,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8779
https://github.com/hail-is/hail/pull/8779:282,Testability,test,test,282,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8779
https://github.com/hail-is/hail/pull/8779:325,Testability,test,tests,325,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8779
https://github.com/hail-is/hail/pull/8779:261,Usability,Simpl,Simple,261,"- created is.hail.services package; - added DeployConfig, Tokens with the necessary functionality to get BatchClient working; - BatchClient is built on Apache HttpComponents; - Synchronous, thread safe. HttpClient is thread safe, BatchClient should be, too.; - Simple hello, world! test; - Added build step for Java services tests. FYI @jigold this might be a possible model if we ever rework the Python BatchClient. Also, if there are Batch changes going forward this code will also need to updated. The client is incredibly light weight, so that shouldn't be often, similar to the aiogoogle clients I wrote recently. Next up: Query Batch backend!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8779
https://github.com/hail-is/hail/pull/8781:56,Usability,clear,clearer,56,"I hope this makes file localization and the Batch setup clearer. I didn't know how to do this better in words. <img width=""728"" alt=""Screen Shot 2020-05-13 at 11 03 16 AM"" src=""https://user-images.githubusercontent.com/1693348/81830010-8792a380-9509-11ea-9f74-9068bd9872fd.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8781
https://github.com/hail-is/hail/pull/8782:39,Availability,down,download,39,This addresses issues where the gradle download may fail. We retry a command; that is cheap (`--version`) but which requries downloading the gradle binary.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8782
https://github.com/hail-is/hail/pull/8782:125,Availability,down,downloading,125,This addresses issues where the gradle download may fail. We retry a command; that is cheap (`--version`) but which requries downloading the gradle binary.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8782
https://github.com/hail-is/hail/pull/8783:63,Performance,optimiz,optimization,63,"I added a `StreamLen` IR node. The justification for this were optimization issues in `Simplify` situations like. ```; case ArrayLen(ToArray(StreamMap(s, _, _))) => ArrayLen(ToArray(s)); ```. The above is not a valid optimization in all cases because if the elements of `s` are not realizable, the optimization will have created an invalid IR that will fail at emit time since elements of an array must be realizable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8783
https://github.com/hail-is/hail/pull/8783:217,Performance,optimiz,optimization,217,"I added a `StreamLen` IR node. The justification for this were optimization issues in `Simplify` situations like. ```; case ArrayLen(ToArray(StreamMap(s, _, _))) => ArrayLen(ToArray(s)); ```. The above is not a valid optimization in all cases because if the elements of `s` are not realizable, the optimization will have created an invalid IR that will fail at emit time since elements of an array must be realizable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8783
https://github.com/hail-is/hail/pull/8783:298,Performance,optimiz,optimization,298,"I added a `StreamLen` IR node. The justification for this were optimization issues in `Simplify` situations like. ```; case ArrayLen(ToArray(StreamMap(s, _, _))) => ArrayLen(ToArray(s)); ```. The above is not a valid optimization in all cases because if the elements of `s` are not realizable, the optimization will have created an invalid IR that will fail at emit time since elements of an array must be realizable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8783
https://github.com/hail-is/hail/pull/8783:87,Usability,Simpl,Simplify,87,"I added a `StreamLen` IR node. The justification for this were optimization issues in `Simplify` situations like. ```; case ArrayLen(ToArray(StreamMap(s, _, _))) => ArrayLen(ToArray(s)); ```. The above is not a valid optimization in all cases because if the elements of `s` are not realizable, the optimization will have created an invalid IR that will fail at emit time since elements of an array must be realizable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8783
https://github.com/hail-is/hail/pull/8784:14,Availability,error,errors,14,There are two errors in the status returned by the worker: one is caught when executing the job and the other is when executing the container (such as uploading log to google storage or timeout error). We were only handling job-level errors correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8784
https://github.com/hail-is/hail/pull/8784:194,Availability,error,error,194,There are two errors in the status returned by the worker: one is caught when executing the job and the other is when executing the container (such as uploading log to google storage or timeout error). We were only handling job-level errors correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8784
https://github.com/hail-is/hail/pull/8784:234,Availability,error,errors,234,There are two errors in the status returned by the worker: one is caught when executing the job and the other is when executing the container (such as uploading log to google storage or timeout error). We were only handling job-level errors correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8784
https://github.com/hail-is/hail/pull/8784:186,Safety,timeout,timeout,186,There are two errors in the status returned by the worker: one is caught when executing the job and the other is when executing the container (such as uploading log to google storage or timeout error). We were only handling job-level errors correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8784
https://github.com/hail-is/hail/pull/8784:161,Testability,log,log,161,There are two errors in the status returned by the worker: one is caught when executing the job and the other is when executing the container (such as uploading log to google storage or timeout error). We were only handling job-level errors correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8784
https://github.com/hail-is/hail/pull/8786:209,Availability,error,error-in-hail-,209,"Requiredness stuff was assuming that tuples were well-ordered and contiguously indexed, which is wrong because the pruner can prune tuple elements. See https://discuss.hail.is/t/arrayindexoutofboundsexception-error-in-hail-0-2-40/1413/5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8786
https://github.com/hail-is/hail/pull/8788:27,Deployability,deploy,deploy,27,"Unfortunately, I can't dev deploy the rest of the resources branch until this is merged. I need these functions in the services-base image as that's what CreateDatabase uses to run the migration scripts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8788
https://github.com/hail-is/hail/pull/8790:195,Deployability,configurat,configuration,195,`hailctl dataproc connect` and `hailctl dataproc modify` hard-code a default compute zone of us-central1-b. This changes those two commands to use the `compute/zone` value from the user's gcloud configuration if a zone argument is not provided. @johnc1231 [mentioned this in Zulip](https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Unable.20to.20launch.20notebook) the other day.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8790
https://github.com/hail-is/hail/pull/8790:195,Modifiability,config,configuration,195,`hailctl dataproc connect` and `hailctl dataproc modify` hard-code a default compute zone of us-central1-b. This changes those two commands to use the `compute/zone` value from the user's gcloud configuration if a zone argument is not provided. @johnc1231 [mentioned this in Zulip](https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Unable.20to.20launch.20notebook) the other day.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8790
https://github.com/hail-is/hail/pull/8791:135,Availability,error,error,135,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:566,Availability,error,error,566,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:822,Availability,error,error,822,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:141,Integrability,message,message,141,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:572,Integrability,message,message,572,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:828,Integrability,message,message,828,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:88,Modifiability,config,configured,88,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:645,Modifiability,config,config,645,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8791:544,Testability,log,logic,544,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8791
https://github.com/hail-is/hail/pull/8794:37,Energy Efficiency,reduce,reduce,37,"This PR makes a number of changes to reduce the overhead of the interpreted `TableAggregate` in general, plus a couple of tweaks to `ApproxCDFCombiner` to eliminate sources of boxed primitives. The main changes are:; * Make `RegionMemory` track the number of Java objects being held, and include that in the log.; * Make the combOp in `TableAggregate` interpreter operate directly on `RegionValue`s. It modifies and returns the left state, and frees the right one.; * To generate the combOp function, I had to compile a function with two agg states (instead of concatenating two `TupleAggregatorStates`, which must live in a single region). That meant not using the `CombOp` IR node. I couldn't quite get rid of the `CombOp` node completely, because I don't understand how it's being used in `Aggregators2Suite` well enough to rewrite it, but that is now the only use.; * In `TableAggregate`, work with `RDD[WrappedByteArray]` instead of `RDD[Array[Byte]]`, to allow the incoming `Array[Byte]` to be GCed as soon as we have decoded it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8794
https://github.com/hail-is/hail/pull/8794:908,Integrability,Wrap,WrappedByteArray,908,"This PR makes a number of changes to reduce the overhead of the interpreted `TableAggregate` in general, plus a couple of tweaks to `ApproxCDFCombiner` to eliminate sources of boxed primitives. The main changes are:; * Make `RegionMemory` track the number of Java objects being held, and include that in the log.; * Make the combOp in `TableAggregate` interpreter operate directly on `RegionValue`s. It modifies and returns the left state, and frees the right one.; * To generate the combOp function, I had to compile a function with two agg states (instead of concatenating two `TupleAggregatorStates`, which must live in a single region). That meant not using the `CombOp` IR node. I couldn't quite get rid of the `CombOp` node completely, because I don't understand how it's being used in `Aggregators2Suite` well enough to rewrite it, but that is now the only use.; * In `TableAggregate`, work with `RDD[WrappedByteArray]` instead of `RDD[Array[Byte]]`, to allow the incoming `Array[Byte]` to be GCed as soon as we have decoded it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8794
https://github.com/hail-is/hail/pull/8794:827,Modifiability,rewrite,rewrite,827,"This PR makes a number of changes to reduce the overhead of the interpreted `TableAggregate` in general, plus a couple of tweaks to `ApproxCDFCombiner` to eliminate sources of boxed primitives. The main changes are:; * Make `RegionMemory` track the number of Java objects being held, and include that in the log.; * Make the combOp in `TableAggregate` interpreter operate directly on `RegionValue`s. It modifies and returns the left state, and frees the right one.; * To generate the combOp function, I had to compile a function with two agg states (instead of concatenating two `TupleAggregatorStates`, which must live in a single region). That meant not using the `CombOp` IR node. I couldn't quite get rid of the `CombOp` node completely, because I don't understand how it's being used in `Aggregators2Suite` well enough to rewrite it, but that is now the only use.; * In `TableAggregate`, work with `RDD[WrappedByteArray]` instead of `RDD[Array[Byte]]`, to allow the incoming `Array[Byte]` to be GCed as soon as we have decoded it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8794
https://github.com/hail-is/hail/pull/8794:308,Testability,log,log,308,"This PR makes a number of changes to reduce the overhead of the interpreted `TableAggregate` in general, plus a couple of tweaks to `ApproxCDFCombiner` to eliminate sources of boxed primitives. The main changes are:; * Make `RegionMemory` track the number of Java objects being held, and include that in the log.; * Make the combOp in `TableAggregate` interpreter operate directly on `RegionValue`s. It modifies and returns the left state, and frees the right one.; * To generate the combOp function, I had to compile a function with two agg states (instead of concatenating two `TupleAggregatorStates`, which must live in a single region). That meant not using the `CombOp` IR node. I couldn't quite get rid of the `CombOp` node completely, because I don't understand how it's being used in `Aggregators2Suite` well enough to rewrite it, but that is now the only use.; * In `TableAggregate`, work with `RDD[WrappedByteArray]` instead of `RDD[Array[Byte]]`, to allow the incoming `Array[Byte]` to be GCed as soon as we have decoded it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8794
https://github.com/hail-is/hail/pull/8796:112,Testability,log,logging,112,This fixes `balding_nichols_model` in the service. I also fixed warn => warning. Warn is deprecated in Python's logging library.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8796
https://github.com/hail-is/hail/pull/8797:92,Deployability,install,install-editable,92,Not sure why these are not already ignored. They seem to get created when I; run `./gradlew install-editable`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8797
https://github.com/hail-is/hail/pull/8799:91,Availability,error,error-in-hail-,91,This should fix Julia's bug here: https://discuss.hail.is/t/arrayindexoutofboundsexception-error-in-hail-0-2-40/1413/11. I will make a follow up PR tomorrow splitting things up and making a `PCanonicalTupleCode`. I just wanted to get rid of this blatantly wrong thing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8799
https://github.com/hail-is/hail/pull/8801:27,Testability,Test,TestNG,27,I am encountering a bug in TestNG version 6.8.21 that is resovled in 7.1.0. For; more detail see https://stackoverflow.com/questions/39613927/testng-with-dataprovider-skips-all-tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801
https://github.com/hail-is/hail/pull/8801:142,Testability,test,testng-with-dataprovider-skips-all-tests,142,I am encountering a bug in TestNG version 6.8.21 that is resovled in 7.1.0. For; more detail see https://stackoverflow.com/questions/39613927/testng-with-dataprovider-skips-all-tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801
https://github.com/hail-is/hail/pull/8802:137,Availability,error,errors,137,Add retry infrastructure mirroring Python. This will hopefully fix the deploy issue. I think we'll have to grow another set of transient errors to retry related to lower-level networking issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8802
https://github.com/hail-is/hail/pull/8802:71,Deployability,deploy,deploy,71,Add retry infrastructure mirroring Python. This will hopefully fix the deploy issue. I think we'll have to grow another set of transient errors to retry related to lower-level networking issues.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8802
https://github.com/hail-is/hail/pull/8803:1643,Safety,safe,safe,1643,"This adds `partitionKeys` which partitions the keys into partitions containing as-near-to-equal-as-possible number of records. The partitions are represented by an array, `pb`, of keys of length `nPartitions + 1`. The partitions are taken to include records with keys in; ```; [pb(0), pb(1)); [pb(1), pb(2)); ...; [pb(nPartitions-1), pb(nPartitions)]; ```; Note that the last partition is inclusive of both end-points. I have two simple examples of the partitioning behavior at the top of `LSMSuite`. In these cases, the number of elements is so small that the LSM perfectly stored the distribution, so there is no approximation. `LSMSuite` also contains tests that use enough keys so as to force the LSM to not keep them all. When the LSM has more than 10,000 keys, it starts sampling. It flips a coin that is true with probability `10,000 / n_keys_seen`. As we see more keys, the probability that the next seen key is kept decreases. If we decide to keep a key, we uniformly randomly choose a key to evict. The above is not entirely true. In reality, we keep 9998 keys in an array and separately keep the greatest and the least. Those are the true greatest seen key and the true least seen key. The probabilities above are adjusted accordingly. If the sample of keys is unbiased, then we expect the partitions chosen to be roughly equal in number of records. ---. The TestNG changes are already in another PR, I'll rebase when that lands. I separately fixed a bug in KeyedCodecSpec wherein it incorrectly assumed the Key and Value types were the same. I also fixed a bug in that the LSM used non-thread-local regions. Regions are not thread safe and the Indeed LSM implementation uses many threads. In order to track every addition to the LSM, I made the Indeed LSM object private and made the Hail LSM class have the necessary methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803
https://github.com/hail-is/hail/pull/8803:655,Testability,test,tests,655,"This adds `partitionKeys` which partitions the keys into partitions containing as-near-to-equal-as-possible number of records. The partitions are represented by an array, `pb`, of keys of length `nPartitions + 1`. The partitions are taken to include records with keys in; ```; [pb(0), pb(1)); [pb(1), pb(2)); ...; [pb(nPartitions-1), pb(nPartitions)]; ```; Note that the last partition is inclusive of both end-points. I have two simple examples of the partitioning behavior at the top of `LSMSuite`. In these cases, the number of elements is so small that the LSM perfectly stored the distribution, so there is no approximation. `LSMSuite` also contains tests that use enough keys so as to force the LSM to not keep them all. When the LSM has more than 10,000 keys, it starts sampling. It flips a coin that is true with probability `10,000 / n_keys_seen`. As we see more keys, the probability that the next seen key is kept decreases. If we decide to keep a key, we uniformly randomly choose a key to evict. The above is not entirely true. In reality, we keep 9998 keys in an array and separately keep the greatest and the least. Those are the true greatest seen key and the true least seen key. The probabilities above are adjusted accordingly. If the sample of keys is unbiased, then we expect the partitions chosen to be roughly equal in number of records. ---. The TestNG changes are already in another PR, I'll rebase when that lands. I separately fixed a bug in KeyedCodecSpec wherein it incorrectly assumed the Key and Value types were the same. I also fixed a bug in that the LSM used non-thread-local regions. Regions are not thread safe and the Indeed LSM implementation uses many threads. In order to track every addition to the LSM, I made the Indeed LSM object private and made the Hail LSM class have the necessary methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803
https://github.com/hail-is/hail/pull/8803:1370,Testability,Test,TestNG,1370,"This adds `partitionKeys` which partitions the keys into partitions containing as-near-to-equal-as-possible number of records. The partitions are represented by an array, `pb`, of keys of length `nPartitions + 1`. The partitions are taken to include records with keys in; ```; [pb(0), pb(1)); [pb(1), pb(2)); ...; [pb(nPartitions-1), pb(nPartitions)]; ```; Note that the last partition is inclusive of both end-points. I have two simple examples of the partitioning behavior at the top of `LSMSuite`. In these cases, the number of elements is so small that the LSM perfectly stored the distribution, so there is no approximation. `LSMSuite` also contains tests that use enough keys so as to force the LSM to not keep them all. When the LSM has more than 10,000 keys, it starts sampling. It flips a coin that is true with probability `10,000 / n_keys_seen`. As we see more keys, the probability that the next seen key is kept decreases. If we decide to keep a key, we uniformly randomly choose a key to evict. The above is not entirely true. In reality, we keep 9998 keys in an array and separately keep the greatest and the least. Those are the true greatest seen key and the true least seen key. The probabilities above are adjusted accordingly. If the sample of keys is unbiased, then we expect the partitions chosen to be roughly equal in number of records. ---. The TestNG changes are already in another PR, I'll rebase when that lands. I separately fixed a bug in KeyedCodecSpec wherein it incorrectly assumed the Key and Value types were the same. I also fixed a bug in that the LSM used non-thread-local regions. Regions are not thread safe and the Indeed LSM implementation uses many threads. In order to track every addition to the LSM, I made the Indeed LSM object private and made the Hail LSM class have the necessary methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803
https://github.com/hail-is/hail/pull/8803:430,Usability,simpl,simple,430,"This adds `partitionKeys` which partitions the keys into partitions containing as-near-to-equal-as-possible number of records. The partitions are represented by an array, `pb`, of keys of length `nPartitions + 1`. The partitions are taken to include records with keys in; ```; [pb(0), pb(1)); [pb(1), pb(2)); ...; [pb(nPartitions-1), pb(nPartitions)]; ```; Note that the last partition is inclusive of both end-points. I have two simple examples of the partitioning behavior at the top of `LSMSuite`. In these cases, the number of elements is so small that the LSM perfectly stored the distribution, so there is no approximation. `LSMSuite` also contains tests that use enough keys so as to force the LSM to not keep them all. When the LSM has more than 10,000 keys, it starts sampling. It flips a coin that is true with probability `10,000 / n_keys_seen`. As we see more keys, the probability that the next seen key is kept decreases. If we decide to keep a key, we uniformly randomly choose a key to evict. The above is not entirely true. In reality, we keep 9998 keys in an array and separately keep the greatest and the least. Those are the true greatest seen key and the true least seen key. The probabilities above are adjusted accordingly. If the sample of keys is unbiased, then we expect the partitions chosen to be roughly equal in number of records. ---. The TestNG changes are already in another PR, I'll rebase when that lands. I separately fixed a bug in KeyedCodecSpec wherein it incorrectly assumed the Key and Value types were the same. I also fixed a bug in that the LSM used non-thread-local regions. Regions are not thread safe and the Indeed LSM implementation uses many threads. In order to track every addition to the LSM, I made the Indeed LSM object private and made the Hail LSM class have the necessary methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803
https://github.com/hail-is/hail/pull/8804:4,Availability,down,down,4,I'm down to about 350 issues. The next one will probably be the last one.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8804
https://github.com/hail-is/hail/pull/8805:306,Deployability,deploy,deploy,306,"The CSS makes some strong statements about how the web browser should display it. I; removed all these statements which lets the web browser choose if it should wrap some; in-cell text, truncate the table, or expand the table. I fixed some bad formatting and removed tabs () from some pages. Check my dev deploy: https://internal.hail.is/dking/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805
https://github.com/hail-is/hail/pull/8805:161,Integrability,wrap,wrap,161,"The CSS makes some strong statements about how the web browser should display it. I; removed all these statements which lets the web browser choose if it should wrap some; in-cell text, truncate the table, or expand the table. I fixed some bad formatting and removed tabs () from some pages. Check my dev deploy: https://internal.hail.is/dking/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805
https://github.com/hail-is/hail/pull/8808:10,Deployability,upgrade,upgrade,10,Fix is to upgrade to Bloop 1.4.1. https://github.com/scalacenter/bloop/issues/1276,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8808
https://github.com/hail-is/hail/pull/8809:39,Deployability,deploy,deploy,39,FYI @konradjk . I tested this with dev deploy for the batch pages. I assume the ci page is the same. This changes the behavior of CI focus to require to enter slash first. I think it's better to be consistent.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8809
https://github.com/hail-is/hail/pull/8809:18,Testability,test,tested,18,FYI @konradjk . I tested this with dev deploy for the batch pages. I assume the ci page is the same. This changes the behavior of CI focus to require to enter slash first. I think it's better to be consistent.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8809
https://github.com/hail-is/hail/pull/8811:862,Availability,down,down,862,"Our current HTML display of tables flattens the table and concatenates the field; names to produce table headers. This leads to long, unreadable headers. This change reproduces the nesting structure of the types with several table; header layers. The essential activity is to convert:. ```; bing; foo:; bar:; baz; quux; fizzle:; fazz:; fazz1; fazz2; fezz; quork; bang; ```. into. ```; foo; -------------------------------; fizzle; ----------------; bar fazz; -------- -----------; bing baz quux fazz1 fazz2 fezz quork bang; ```. The bottom layer are the names of the leaves of this tree. Working from the; bottom, a name appears when the row corresponds to that name's tree height. For; this reason `bar` appears lower than `fizzle`. This frustrates finding peer; fields. However, I prefer it. I think I have some sense of visual gravity that; wants bar to fall down. Anyway, I implement this with some html grunginess in `Table._Show` and a new; class named `PlacementTree`. We construct a `PlacementTree` from a type. It is a; tree whose internal and leaf nodes contain a name, width, and height. It has a; method `to_grid` which converts it to an HTML-table-like structure with ""spacer""; elements. Our above example looks like:. ```python3; [[(None, 1), ('foo', 6), (None, 1)],; [(None, 1), (None, 2), ('fizzle', 3), (None, 1), (None, 1)],; [(None, 1), ('bar', 2), ('fazz', 2), (None, 1) (None, 1), (None, 1)],; [('bing', 1), ('baz', 1), ('quux', 1), ('fazz1', 1), ('fazz2', 1), ('fezz', 1) ('quork', 1), ('bang', 1)]]; ```. The code in `Table._Show` converts this to HTML table rows that looks like:; ```html; <tr><td></td><td colspan=""6"">foo</td><td></td></tr>; <tr><td></td><td colspan=""2""></td><td colspan=3>fizzle</td><td></td><td></td></tr>; <tr><td></td><td colspan=""2"">bar</td><td colspan=2>fazz</td><td></td><td></td><td></td></tr>; <tr><td>bing</td><td>baz</td><td>quux</td><td>fazz1</td><td>fazz2</td><td>fezz</td><td>quork</td><td>bang</td></tr>; ```. Which looks like:. <table>; <tr><t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8811
https://github.com/hail-is/hail/pull/8811:2385,Integrability,wrap,wrap,2385,"es of the leaves of this tree. Working from the; bottom, a name appears when the row corresponds to that name's tree height. For; this reason `bar` appears lower than `fizzle`. This frustrates finding peer; fields. However, I prefer it. I think I have some sense of visual gravity that; wants bar to fall down. Anyway, I implement this with some html grunginess in `Table._Show` and a new; class named `PlacementTree`. We construct a `PlacementTree` from a type. It is a; tree whose internal and leaf nodes contain a name, width, and height. It has a; method `to_grid` which converts it to an HTML-table-like structure with ""spacer""; elements. Our above example looks like:. ```python3; [[(None, 1), ('foo', 6), (None, 1)],; [(None, 1), (None, 2), ('fizzle', 3), (None, 1), (None, 1)],; [(None, 1), ('bar', 2), ('fazz', 2), (None, 1) (None, 1), (None, 1)],; [('bing', 1), ('baz', 1), ('quux', 1), ('fazz1', 1), ('fazz2', 1), ('fezz', 1) ('quork', 1), ('bang', 1)]]; ```. The code in `Table._Show` converts this to HTML table rows that looks like:; ```html; <tr><td></td><td colspan=""6"">foo</td><td></td></tr>; <tr><td></td><td colspan=""2""></td><td colspan=3>fizzle</td><td></td><td></td></tr>; <tr><td></td><td colspan=""2"">bar</td><td colspan=2>fazz</td><td></td><td></td><td></td></tr>; <tr><td>bing</td><td>baz</td><td>quux</td><td>fazz1</td><td>fazz2</td><td>fezz</td><td>quork</td><td>bang</td></tr>; ```. Which looks like:. <table>; <tr><td></td><td colspan=""6"">foo</td><td></td></tr>; <tr><td></td><td colspan=""2""></td><td colspan=3>fizzle</td><td></td><td></td></tr>; <tr><td></td><td colspan=""2"">bar</td><td colspan=2>fazz</td><td></td><td></td><td></td></tr>; <tr><td>bing</td><td>baz</td><td>quux</td><td>fazz1</td><td>fazz2</td><td>fezz</td><td>quork</td><td>bang</td></tr>; </table>. ---. I also forced cells to not wrap lines. Instead, if they are larger than 500px; they're truncated by the web browser with an ellipsis. Hovering over the cell; displays the full contents in a tool tip.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8811
https://github.com/hail-is/hail/pull/8811:239,Modifiability,layers,layers,239,"Our current HTML display of tables flattens the table and concatenates the field; names to produce table headers. This leads to long, unreadable headers. This change reproduces the nesting structure of the types with several table; header layers. The essential activity is to convert:. ```; bing; foo:; bar:; baz; quux; fizzle:; fazz:; fazz1; fazz2; fezz; quork; bang; ```. into. ```; foo; -------------------------------; fizzle; ----------------; bar fazz; -------- -----------; bing baz quux fazz1 fazz2 fezz quork bang; ```. The bottom layer are the names of the leaves of this tree. Working from the; bottom, a name appears when the row corresponds to that name's tree height. For; this reason `bar` appears lower than `fizzle`. This frustrates finding peer; fields. However, I prefer it. I think I have some sense of visual gravity that; wants bar to fall down. Anyway, I implement this with some html grunginess in `Table._Show` and a new; class named `PlacementTree`. We construct a `PlacementTree` from a type. It is a; tree whose internal and leaf nodes contain a name, width, and height. It has a; method `to_grid` which converts it to an HTML-table-like structure with ""spacer""; elements. Our above example looks like:. ```python3; [[(None, 1), ('foo', 6), (None, 1)],; [(None, 1), (None, 2), ('fizzle', 3), (None, 1), (None, 1)],; [(None, 1), ('bar', 2), ('fazz', 2), (None, 1) (None, 1), (None, 1)],; [('bing', 1), ('baz', 1), ('quux', 1), ('fazz1', 1), ('fazz2', 1), ('fezz', 1) ('quork', 1), ('bang', 1)]]; ```. The code in `Table._Show` converts this to HTML table rows that looks like:; ```html; <tr><td></td><td colspan=""6"">foo</td><td></td></tr>; <tr><td></td><td colspan=""2""></td><td colspan=3>fizzle</td><td></td><td></td></tr>; <tr><td></td><td colspan=""2"">bar</td><td colspan=2>fazz</td><td></td><td></td><td></td></tr>; <tr><td>bing</td><td>baz</td><td>quux</td><td>fazz1</td><td>fazz2</td><td>fezz</td><td>quork</td><td>bang</td></tr>; ```. Which looks like:. <table>; <tr><t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8811
https://github.com/hail-is/hail/pull/8812:181,Availability,error,error,181,"These incremental improvements set us up for a cleaner diff when the Shuffler IR lands. The bigger changes:. - add `getOrNull` to `HailContextFlags`, previously you'd always get an error if the key did not exist.; - add a few currently unused flags for the shuffler; they all default to settings appropriate for tests.; - transmit ETypes instead of codec specs; the buffer is fixed at compile-time.; - use buffers instead of raw input streams for all communication. This resolved some latent bugs that arise from mixing Hail's (In|Out)putBuffers with operations on the underlying streams. Encoders and Decoders appear to have no issue being interleaved with Buffer operations, so I now freely use the buffer of the (En|De)coder as is convenient.; - get now accepts inclusivity flags for both start and end (this was critical to use partition bounds correctly).; - implement partitionBounds.; - add a `close` to `ShuffleClient` so it can clean up ExecuteContext and the socket.; - the server and client now handshake (each sends and receives one byte) on a close so as to raise errors sooner if either one of them did not expect the conversation to end.; - KeyedCodecSpec => ShuffleCodecSpec, changed to support the all-etypes, no-codec-specs methodology.; - shrink uuid to 32 bytes, still a lot, but fits on one log line.; - implement a *whole pile* of write/read methods on `Wire.scala` that give a unified language to our mess of serializers and deserializers. I tried to make the rule: write: to buffer, read: from buffer, serialize: to string, deserialize: from string. Why are some things missing? Why are some thing unused? This is the set of things I need to ultimately make the Shuffler work. My apologies for the mammoth size of this PR. I've been trimming and trimming to get little fixes in, but now we're down to almost exclusive Shuffler changes. It seems less valuable to try and educate the team about the Shuffler via PR since it will be owned by services team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8812
https://github.com/hail-is/hail/pull/8812:1077,Availability,error,errors,1077,"These incremental improvements set us up for a cleaner diff when the Shuffler IR lands. The bigger changes:. - add `getOrNull` to `HailContextFlags`, previously you'd always get an error if the key did not exist.; - add a few currently unused flags for the shuffler; they all default to settings appropriate for tests.; - transmit ETypes instead of codec specs; the buffer is fixed at compile-time.; - use buffers instead of raw input streams for all communication. This resolved some latent bugs that arise from mixing Hail's (In|Out)putBuffers with operations on the underlying streams. Encoders and Decoders appear to have no issue being interleaved with Buffer operations, so I now freely use the buffer of the (En|De)coder as is convenient.; - get now accepts inclusivity flags for both start and end (this was critical to use partition bounds correctly).; - implement partitionBounds.; - add a `close` to `ShuffleClient` so it can clean up ExecuteContext and the socket.; - the server and client now handshake (each sends and receives one byte) on a close so as to raise errors sooner if either one of them did not expect the conversation to end.; - KeyedCodecSpec => ShuffleCodecSpec, changed to support the all-etypes, no-codec-specs methodology.; - shrink uuid to 32 bytes, still a lot, but fits on one log line.; - implement a *whole pile* of write/read methods on `Wire.scala` that give a unified language to our mess of serializers and deserializers. I tried to make the rule: write: to buffer, read: from buffer, serialize: to string, deserialize: from string. Why are some things missing? Why are some thing unused? This is the set of things I need to ultimately make the Shuffler work. My apologies for the mammoth size of this PR. I've been trimming and trimming to get little fixes in, but now we're down to almost exclusive Shuffler changes. It seems less valuable to try and educate the team about the Shuffler via PR since it will be owned by services team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8812
https://github.com/hail-is/hail/pull/8812:1817,Availability,down,down,1817,"These incremental improvements set us up for a cleaner diff when the Shuffler IR lands. The bigger changes:. - add `getOrNull` to `HailContextFlags`, previously you'd always get an error if the key did not exist.; - add a few currently unused flags for the shuffler; they all default to settings appropriate for tests.; - transmit ETypes instead of codec specs; the buffer is fixed at compile-time.; - use buffers instead of raw input streams for all communication. This resolved some latent bugs that arise from mixing Hail's (In|Out)putBuffers with operations on the underlying streams. Encoders and Decoders appear to have no issue being interleaved with Buffer operations, so I now freely use the buffer of the (En|De)coder as is convenient.; - get now accepts inclusivity flags for both start and end (this was critical to use partition bounds correctly).; - implement partitionBounds.; - add a `close` to `ShuffleClient` so it can clean up ExecuteContext and the socket.; - the server and client now handshake (each sends and receives one byte) on a close so as to raise errors sooner if either one of them did not expect the conversation to end.; - KeyedCodecSpec => ShuffleCodecSpec, changed to support the all-etypes, no-codec-specs methodology.; - shrink uuid to 32 bytes, still a lot, but fits on one log line.; - implement a *whole pile* of write/read methods on `Wire.scala` that give a unified language to our mess of serializers and deserializers. I tried to make the rule: write: to buffer, read: from buffer, serialize: to string, deserialize: from string. Why are some things missing? Why are some thing unused? This is the set of things I need to ultimately make the Shuffler work. My apologies for the mammoth size of this PR. I've been trimming and trimming to get little fixes in, but now we're down to almost exclusive Shuffler changes. It seems less valuable to try and educate the team about the Shuffler via PR since it will be owned by services team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8812
https://github.com/hail-is/hail/pull/8812:312,Testability,test,tests,312,"These incremental improvements set us up for a cleaner diff when the Shuffler IR lands. The bigger changes:. - add `getOrNull` to `HailContextFlags`, previously you'd always get an error if the key did not exist.; - add a few currently unused flags for the shuffler; they all default to settings appropriate for tests.; - transmit ETypes instead of codec specs; the buffer is fixed at compile-time.; - use buffers instead of raw input streams for all communication. This resolved some latent bugs that arise from mixing Hail's (In|Out)putBuffers with operations on the underlying streams. Encoders and Decoders appear to have no issue being interleaved with Buffer operations, so I now freely use the buffer of the (En|De)coder as is convenient.; - get now accepts inclusivity flags for both start and end (this was critical to use partition bounds correctly).; - implement partitionBounds.; - add a `close` to `ShuffleClient` so it can clean up ExecuteContext and the socket.; - the server and client now handshake (each sends and receives one byte) on a close so as to raise errors sooner if either one of them did not expect the conversation to end.; - KeyedCodecSpec => ShuffleCodecSpec, changed to support the all-etypes, no-codec-specs methodology.; - shrink uuid to 32 bytes, still a lot, but fits on one log line.; - implement a *whole pile* of write/read methods on `Wire.scala` that give a unified language to our mess of serializers and deserializers. I tried to make the rule: write: to buffer, read: from buffer, serialize: to string, deserialize: from string. Why are some things missing? Why are some thing unused? This is the set of things I need to ultimately make the Shuffler work. My apologies for the mammoth size of this PR. I've been trimming and trimming to get little fixes in, but now we're down to almost exclusive Shuffler changes. It seems less valuable to try and educate the team about the Shuffler via PR since it will be owned by services team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8812
https://github.com/hail-is/hail/pull/8812:1312,Testability,log,log,1312,"These incremental improvements set us up for a cleaner diff when the Shuffler IR lands. The bigger changes:. - add `getOrNull` to `HailContextFlags`, previously you'd always get an error if the key did not exist.; - add a few currently unused flags for the shuffler; they all default to settings appropriate for tests.; - transmit ETypes instead of codec specs; the buffer is fixed at compile-time.; - use buffers instead of raw input streams for all communication. This resolved some latent bugs that arise from mixing Hail's (In|Out)putBuffers with operations on the underlying streams. Encoders and Decoders appear to have no issue being interleaved with Buffer operations, so I now freely use the buffer of the (En|De)coder as is convenient.; - get now accepts inclusivity flags for both start and end (this was critical to use partition bounds correctly).; - implement partitionBounds.; - add a `close` to `ShuffleClient` so it can clean up ExecuteContext and the socket.; - the server and client now handshake (each sends and receives one byte) on a close so as to raise errors sooner if either one of them did not expect the conversation to end.; - KeyedCodecSpec => ShuffleCodecSpec, changed to support the all-etypes, no-codec-specs methodology.; - shrink uuid to 32 bytes, still a lot, but fits on one log line.; - implement a *whole pile* of write/read methods on `Wire.scala` that give a unified language to our mess of serializers and deserializers. I tried to make the rule: write: to buffer, read: from buffer, serialize: to string, deserialize: from string. Why are some things missing? Why are some thing unused? This is the set of things I need to ultimately make the Shuffler work. My apologies for the mammoth size of this PR. I've been trimming and trimming to get little fixes in, but now we're down to almost exclusive Shuffler changes. It seems less valuable to try and educate the team about the Shuffler via PR since it will be owned by services team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8812
https://github.com/hail-is/hail/pull/8816:136,Availability,error,error,136,"This moves `self.activate` into a `wait_for` with a timeout. We use a tight try-except; around the activation code to provide a precise error message if activation fails. If activation succeeds, we enter the else branch which operates as before. If activation; times out we do not deactivate. This is OK, we probably did not activate. If we did activate,; batch will eventually find out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8816
https://github.com/hail-is/hail/pull/8816:142,Integrability,message,message,142,"This moves `self.activate` into a `wait_for` with a timeout. We use a tight try-except; around the activation code to provide a precise error message if activation fails. If activation succeeds, we enter the else branch which operates as before. If activation; times out we do not deactivate. This is OK, we probably did not activate. If we did activate,; batch will eventually find out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8816
https://github.com/hail-is/hail/pull/8816:52,Safety,timeout,timeout,52,"This moves `self.activate` into a `wait_for` with a timeout. We use a tight try-except; around the activation code to provide a precise error message if activation fails. If activation succeeds, we enter the else branch which operates as before. If activation; times out we do not deactivate. This is OK, we probably did not activate. If we did activate,; batch will eventually find out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8816
https://github.com/hail-is/hail/pull/8818:108,Integrability,wrap,wrap,108,"The main issue is that `nth-child-of` still sees the hidden elements. We can instead; use `nth-of-type` and wrap hidden `tr`s in a `div`. Because the `div` is not a `tr`,; it does not appear as part of the `nth-of-type` calculation. The content is still; present on the page, so we can restore it by removing the `display: none` div.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8818
https://github.com/hail-is/hail/pull/8819:456,Availability,down,downstream,456,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8819
https://github.com/hail-is/hail/pull/8819:622,Deployability,update,updates,622,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8819
https://github.com/hail-is/hail/pull/8819:676,Deployability,update,update,676,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8819
https://github.com/hail-is/hail/pull/8824:11,Testability,log,log,11,"The change log only supports (#NNNN) for PRs and issues, not a comma separated; list of #NNNNs inside parentheses. See: https://hail.is/docs/0.2/change_log.html#bug-fixes; for the issue currently. A sed rule in the makefile implements this syntax.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8824
https://github.com/hail-is/hail/pull/8825:456,Availability,down,downstream,456,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8825
https://github.com/hail-is/hail/pull/8825:622,Deployability,update,updates,622,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8825
https://github.com/hail-is/hail/pull/8825:676,Deployability,update,update,676,"Five things may already exist:; - creating the database; - creating the admin or user user; - creating the admin or user secret. Creating the database is now idempotent with `IF NOT EXISTS`. Creating a user is idempotent because we create a user with a random name. If; we're racing with someone else, the database might have extra users; created. Finally, we race to create the secrets. Whoever runs last wins the race and; their secret create is the one downstream tasks see. Secret creation was made; idempotent by using `create --dry-run` piped to `apply`. Apply does not fail if; the secret already exists, it merely updates it. In a sense, apply is an atomic; create-or-update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8825
https://github.com/hail-is/hail/pull/8828:261,Deployability,deploy,deploy,261,I avoid printing the full exception into the body in most cases. Seems prudent to not expose too much about our internals. CI already uses a broad except and prints the full message when building PRs so I adopted that for building the branch (`unwatched_branch.deploy`) in dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8828
https://github.com/hail-is/hail/pull/8828:277,Deployability,deploy,deploy,277,I avoid printing the full exception into the body in most cases. Seems prudent to not expose too much about our internals. CI already uses a broad except and prints the full message when building PRs so I adopted that for building the branch (`unwatched_branch.deploy`) in dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8828
https://github.com/hail-is/hail/pull/8828:174,Integrability,message,message,174,I avoid printing the full exception into the body in most cases. Seems prudent to not expose too much about our internals. CI already uses a broad except and prints the full message when building PRs so I adopted that for building the branch (`unwatched_branch.deploy`) in dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8828
https://github.com/hail-is/hail/pull/8828:2,Safety,avoid,avoid,2,I avoid printing the full exception into the body in most cases. Seems prudent to not expose too much about our internals. CI already uses a broad except and prints the full message when building PRs so I adopted that for building the branch (`unwatched_branch.deploy`) in dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8828
https://github.com/hail-is/hail/pull/8828:86,Security,expose,expose,86,I avoid printing the full exception into the body in most cases. Seems prudent to not expose too much about our internals. CI already uses a broad except and prints the full message when building PRs so I adopted that for building the branch (`unwatched_branch.deploy`) in dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8828
https://github.com/hail-is/hail/pull/8830:92,Availability,error,errors,92,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8830:220,Availability,error,error,220,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8830:402,Availability,error,error,402,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8830:450,Availability,error,error,450,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8830:456,Integrability,message,message,456,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8830:82,Security,integrity,integrity,82,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8830:292,Security,Integrity,IntegrityError,292,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8830:392,Security,integrity,integrity,392,"If a batch contains a job who lists the same parent twice, Batch will encounter; [integrity errors from; MySQL](https://hail.zulipchat.com/#narrow/stream/127527-team/topic/ci.20broken/near/195236580). For; example, this error was raised when I duplicated a parent in build.yaml:. pymysql.err.IntegrityError: (1062, ""Duplicate entry '35921-13-1' for key 'PRIMARY'"")""}. This change catches the integrity error and raises a more useful 400 bad request; error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8830
https://github.com/hail-is/hail/pull/8833:148,Security,password,password,148,"Database idempotency is easy, use `IF NOT EXISTS`. User cleanup is hard because; the job isn't deterministic. Each attempt will produce a different password. You; cannot retrieve a password of an extant user. The bulk of the changes are in service of creating a job whose attempts will; race to create the password. Only one attempt will win that race and pass its; password on to the `create_database_job`. Those jobs will race to create; databases and users and secrets, but all those operations are now; idempotent (`CREATE USER` uses `IF NOT EXISTS`; the secret is always the same).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833
https://github.com/hail-is/hail/pull/8833:181,Security,password,password,181,"Database idempotency is easy, use `IF NOT EXISTS`. User cleanup is hard because; the job isn't deterministic. Each attempt will produce a different password. You; cannot retrieve a password of an extant user. The bulk of the changes are in service of creating a job whose attempts will; race to create the password. Only one attempt will win that race and pass its; password on to the `create_database_job`. Those jobs will race to create; databases and users and secrets, but all those operations are now; idempotent (`CREATE USER` uses `IF NOT EXISTS`; the secret is always the same).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833
https://github.com/hail-is/hail/pull/8833:306,Security,password,password,306,"Database idempotency is easy, use `IF NOT EXISTS`. User cleanup is hard because; the job isn't deterministic. Each attempt will produce a different password. You; cannot retrieve a password of an extant user. The bulk of the changes are in service of creating a job whose attempts will; race to create the password. Only one attempt will win that race and pass its; password on to the `create_database_job`. Those jobs will race to create; databases and users and secrets, but all those operations are now; idempotent (`CREATE USER` uses `IF NOT EXISTS`; the secret is always the same).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833
https://github.com/hail-is/hail/pull/8833:366,Security,password,password,366,"Database idempotency is easy, use `IF NOT EXISTS`. User cleanup is hard because; the job isn't deterministic. Each attempt will produce a different password. You; cannot retrieve a password of an extant user. The bulk of the changes are in service of creating a job whose attempts will; race to create the password. Only one attempt will win that race and pass its; password on to the `create_database_job`. Those jobs will race to create; databases and users and secrets, but all those operations are now; idempotent (`CREATE USER` uses `IF NOT EXISTS`; the secret is always the same).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833
https://github.com/hail-is/hail/pull/8834:124,Modifiability,config,config,124,"CHANGELOG: Batch ServiceBackend now requires a bucket for intermediate files, either explicitly or through the batch/bucket config setting. Other changes:; - moved notebook/user to auth/user (more appropriate there); - auth no longer creates bucket during user creation",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8834
https://github.com/hail-is/hail/pull/8836:66,Safety,safe,safest,66,I was seeing Reason: None. This is probably overkill but it seems safest. We should probably do it consistently.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8836
https://github.com/hail-is/hail/pull/8844:835,Availability,error,error,835,CHANGELOG: Changed cost per instance from $0.02170 to $0.021935 from switching to using local SSDs. - Added 1 local SSD (375 GB) and formatted it in the worker run script.; - Changed the resource for boot-disk to just disk and modified the worker config. I figured there was no reason to have a separate boot disk in the resources as long as all disks are assumed to be fractions of the instance based on the number of cores being used.; - Changed the worker boot disk from 100 GB to 20 GB; - Changed the worker to move all docker files and batch files to the Local SSD from the boot disk. Can you double check my math for the documentation?. Is it possible it takes longer for an instance to boot up with a local SSD? One of my earlier tests had workers stuck in STAGING. This resolved itself later on so I'm assuming it was a Google error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8844
https://github.com/hail-is/hail/pull/8844:247,Modifiability,config,config,247,CHANGELOG: Changed cost per instance from $0.02170 to $0.021935 from switching to using local SSDs. - Added 1 local SSD (375 GB) and formatted it in the worker run script.; - Changed the resource for boot-disk to just disk and modified the worker config. I figured there was no reason to have a separate boot disk in the resources as long as all disks are assumed to be fractions of the instance based on the number of cores being used.; - Changed the worker boot disk from 100 GB to 20 GB; - Changed the worker to move all docker files and batch files to the Local SSD from the boot disk. Can you double check my math for the documentation?. Is it possible it takes longer for an instance to boot up with a local SSD? One of my earlier tests had workers stuck in STAGING. This resolved itself later on so I'm assuming it was a Google error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8844
https://github.com/hail-is/hail/pull/8844:737,Testability,test,tests,737,CHANGELOG: Changed cost per instance from $0.02170 to $0.021935 from switching to using local SSDs. - Added 1 local SSD (375 GB) and formatted it in the worker run script.; - Changed the resource for boot-disk to just disk and modified the worker config. I figured there was no reason to have a separate boot disk in the resources as long as all disks are assumed to be fractions of the instance based on the number of cores being used.; - Changed the worker boot disk from 100 GB to 20 GB; - Changed the worker to move all docker files and batch files to the Local SSD from the boot disk. Can you double check my math for the documentation?. Is it possible it takes longer for an instance to boot up with a local SSD? One of my earlier tests had workers stuck in STAGING. This resolved itself later on so I'm assuming it was a Google error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8844
https://github.com/hail-is/hail/pull/8845:29,Security,access,accessing,29,"CHANGELOG: Fixed issue where accessing an element of an ndarray in a call to Table.transmute would fail. Because `_indices` was not being set, transmute would fail when it tried to see which row fields to delete.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8845
https://github.com/hail-is/hail/pull/8846:241,Availability,error,error,241,"Previously we get a stack trace without the http response body. I tested this; locally on a branch that does not exist:. # hailctl dev deploy --branch danking/hail:shuffler-deploymefdsafdsa --steps test_shuffler; HTTP Response code was 400; error finding {""repo"": {""owner"": ""danking"", ""name"": ""hail""}, ""name"": ""shuffler-deploymefdsafdsa""} at GitHub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8846
https://github.com/hail-is/hail/pull/8846:135,Deployability,deploy,deploy,135,"Previously we get a stack trace without the http response body. I tested this; locally on a branch that does not exist:. # hailctl dev deploy --branch danking/hail:shuffler-deploymefdsafdsa --steps test_shuffler; HTTP Response code was 400; error finding {""repo"": {""owner"": ""danking"", ""name"": ""hail""}, ""name"": ""shuffler-deploymefdsafdsa""} at GitHub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8846
https://github.com/hail-is/hail/pull/8846:173,Deployability,deploy,deploymefdsafdsa,173,"Previously we get a stack trace without the http response body. I tested this; locally on a branch that does not exist:. # hailctl dev deploy --branch danking/hail:shuffler-deploymefdsafdsa --steps test_shuffler; HTTP Response code was 400; error finding {""repo"": {""owner"": ""danking"", ""name"": ""hail""}, ""name"": ""shuffler-deploymefdsafdsa""} at GitHub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8846
https://github.com/hail-is/hail/pull/8846:320,Deployability,deploy,deploymefdsafdsa,320,"Previously we get a stack trace without the http response body. I tested this; locally on a branch that does not exist:. # hailctl dev deploy --branch danking/hail:shuffler-deploymefdsafdsa --steps test_shuffler; HTTP Response code was 400; error finding {""repo"": {""owner"": ""danking"", ""name"": ""hail""}, ""name"": ""shuffler-deploymefdsafdsa""} at GitHub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8846
https://github.com/hail-is/hail/pull/8846:66,Testability,test,tested,66,"Previously we get a stack trace without the http response body. I tested this; locally on a branch that does not exist:. # hailctl dev deploy --branch danking/hail:shuffler-deploymefdsafdsa --steps test_shuffler; HTTP Response code was 400; error finding {""repo"": {""owner"": ""danking"", ""name"": ""hail""}, ""name"": ""shuffler-deploymefdsafdsa""} at GitHub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8846
https://github.com/hail-is/hail/pull/8847:0,Energy Efficiency,Allocate,Allocate,0,"Allocate a fixed stack, as it was the simplest thing I could do. 128 stack; slots should be more than enough for this implementation as it requires one; stack slot per level of the tree. There are many improvements that can be made here, but hopefully this should; unblock some amount of the method splitting work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8847
https://github.com/hail-is/hail/pull/8847:38,Usability,simpl,simplest,38,"Allocate a fixed stack, as it was the simplest thing I could do. 128 stack; slots should be more than enough for this implementation as it requires one; stack slot per level of the tree. There are many improvements that can be made here, but hopefully this should; unblock some amount of the method splitting work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8847
https://github.com/hail-is/hail/pull/8851:18,Usability,learn,learn,18,"CHANGELOG: remove learn more, which should not have gone in until the learn more content was finalized.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8851
https://github.com/hail-is/hail/pull/8851:70,Usability,learn,learn,70,"CHANGELOG: remove learn more, which should not have gone in until the learn more content was finalized.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8851
https://github.com/hail-is/hail/pull/8852:169,Deployability,release,released,169,"And use internally. This adds the bucket parameter/config setting, but doesn't require it, and falls back to getting the bucket from the user information. After this is released, I'll rip out the user bucket and make this mandatory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8852
https://github.com/hail-is/hail/pull/8852:51,Modifiability,config,config,51,"And use internally. This adds the bucket parameter/config setting, but doesn't require it, and falls back to getting the bucket from the user information. After this is released, I'll rip out the user bucket and make this mandatory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8852
https://github.com/hail-is/hail/pull/8854:858,Integrability,interoperab,interoperability,858,"CHANGELOG: Add `composable` option to parallel text export for use with `gsutil compose`. The BGZF spec reccommends one empty BGZF block be written at the end of; at the end a BGZF file. The `gsutil compose` command concatenates a list; of objects into one composite object. We recently discovered that when; these empty blocks are present in the middle of a file, utilities like; tabix will output pointers to them (as from a reading perspective, the; empty blocks are equivalent to the next block). This will hit assertions; in code like htsjdk that checks to make sure that seek operations from; tabix virtual pointers point to the end of a block if and only if that; block is end of file. This is a bug in tabix implementations.; Furthermore, the end-of-file marker probably shouldn't be appended to; BGZF streams in the first place. In order to improve interoperability of hail with other tools, we add; the 'composable' output option to export types. 'composable' behaves; like 'separate_header', except we do not write the end-of-file marker at; the end of the header or every partition written, and an extra, empty; bgz file with the end-of-file marker is written to `part-composable-end`; which should sort later than any partfile written from the RDD and thus; should be amenable to globbing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8854
https://github.com/hail-is/hail/pull/8854:515,Testability,assert,assertions,515,"CHANGELOG: Add `composable` option to parallel text export for use with `gsutil compose`. The BGZF spec reccommends one empty BGZF block be written at the end of; at the end a BGZF file. The `gsutil compose` command concatenates a list; of objects into one composite object. We recently discovered that when; these empty blocks are present in the middle of a file, utilities like; tabix will output pointers to them (as from a reading perspective, the; empty blocks are equivalent to the next block). This will hit assertions; in code like htsjdk that checks to make sure that seek operations from; tabix virtual pointers point to the end of a block if and only if that; block is end of file. This is a bug in tabix implementations.; Furthermore, the end-of-file marker probably shouldn't be appended to; BGZF streams in the first place. In order to improve interoperability of hail with other tools, we add; the 'composable' output option to export types. 'composable' behaves; like 'separate_header', except we do not write the end-of-file marker at; the end of the header or every partition written, and an extra, empty; bgz file with the end-of-file marker is written to `part-composable-end`; which should sort later than any partfile written from the RDD and thus; should be amenable to globbing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8854
https://github.com/hail-is/hail/pull/8858:44,Availability,down,downstream,44,"Although we generally want unshared IRs for downstream passes, there are many instances where we construct IRs with shared nodes (common case: using the same Ref for the table row in TableMapRows expressions). . Requiredness correctness should not be affected by node-sharing, and deduplicating every time we want to run the requiredness analysis is potentially very expensive since the goal is to be able to run it at arbitrary points in the lowering stack, so I'm going to allow Requiredness to handle shared nodes. This could potentially mess up if there are instances where a ref is created and used to represent two separate values for whatever reason, but I think we should consider that to be a bug---e.g.; ```; val r = Ref(""foo"", TInt32); If(, ; Let(""foo"", NA(TInt32), r + 3); Let(""foo"", I32(5), r + 5)); ```; should never exist. I've done the same for ComputeUsesAndDefs, which Requiredness needs, but same comment applies---correctness should not be affected by node sharing, unless we have improperly constructed IR with scoping issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8858
https://github.com/hail-is/hail/pull/8860:608,Availability,error,error,608,"Bascially grabbed the relevant bits from SparkBackend and ServiceBackend. Enabled by setting HAIL_QUERY_BACKEND=local. Needs HAIL_HOME and SPARK_HOME set to find jars, and hardcodes the py4j jar version that comes with Spark 2.4.x. Will have to work on ripping out Spark dependency. Currently uses HadoopFS for the file system in Java. GoogleFS in Python works with gs:// or local files, I just copied it and ripped out the Google stuff. Some some rough ideas from some of your old work, @johnc1231 (py4jbackend). Current results on the Python tests:. > == 470 failed, 245 passed, 87 skipped, 15 warnings, 1 error in 270.61 seconds ==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8860
https://github.com/hail-is/hail/pull/8860:271,Integrability,depend,dependency,271,"Bascially grabbed the relevant bits from SparkBackend and ServiceBackend. Enabled by setting HAIL_QUERY_BACKEND=local. Needs HAIL_HOME and SPARK_HOME set to find jars, and hardcodes the py4j jar version that comes with Spark 2.4.x. Will have to work on ripping out Spark dependency. Currently uses HadoopFS for the file system in Java. GoogleFS in Python works with gs:// or local files, I just copied it and ripped out the Google stuff. Some some rough ideas from some of your old work, @johnc1231 (py4jbackend). Current results on the Python tests:. > == 470 failed, 245 passed, 87 skipped, 15 warnings, 1 error in 270.61 seconds ==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8860
https://github.com/hail-is/hail/pull/8860:544,Testability,test,tests,544,"Bascially grabbed the relevant bits from SparkBackend and ServiceBackend. Enabled by setting HAIL_QUERY_BACKEND=local. Needs HAIL_HOME and SPARK_HOME set to find jars, and hardcodes the py4j jar version that comes with Spark 2.4.x. Will have to work on ripping out Spark dependency. Currently uses HadoopFS for the file system in Java. GoogleFS in Python works with gs:// or local files, I just copied it and ripped out the Google stuff. Some some rough ideas from some of your old work, @johnc1231 (py4jbackend). Current results on the Python tests:. > == 470 failed, 245 passed, 87 skipped, 15 warnings, 1 error in 270.61 seconds ==",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8860
https://github.com/hail-is/hail/pull/8864:500,Performance,optimiz,optimizer,500,"I plan to move this to `hl.dnd.DNDArray`. I had to make a couple changes to RVD and Table to make this work. They all; revolve around convincing Hail not to elide *critical* `key_by`s. The critical insight is that 1:1 partitioners (partitioners where each range; bound interval contains exactly one key) are special: permuting their keys is; free. I can take advantage of this by combining two changes:; 1. `RVD.enforceKey` is aware of these partitioners and avoids scans in that case; 2. Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:885,Performance,optimiz,optimizer,885,"I plan to move this to `hl.dnd.DNDArray`. I had to make a couple changes to RVD and Table to make this work. They all; revolve around convincing Hail not to elide *critical* `key_by`s. The critical insight is that 1:1 partitioners (partitioners where each range; bound interval contains exactly one key) are special: permuting their keys is; free. I can take advantage of this by combining two changes:; 1. `RVD.enforceKey` is aware of these partitioners and avoids scans in that case; 2. Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:1984,Performance,Perform,Performance,1984,"d`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).write('/tmp/out.da', overwrite=True); ```. Block matrix performed the matrix multiply in 19.3s. DNDArray performed the; matrix multiply in 37.6s. Block Matrix:; ![Screen Shot 2020-05-26 at 1 37 51 PM](https://user-images.githubusercontent.com/106194/82932367-54630200-9f56-11ea-86f4-94726c36d727.png). DNDArray:; ![Screen Shot 2020-05-26 at 1 37 08 PM](https://user-images.githubusercontent.com/106194/829323",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:2644,Performance,perform,performed,2644," able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).write('/tmp/out.da', overwrite=True); ```. Block matrix performed the matrix multiply in 19.3s. DNDArray performed the; matrix multiply in 37.6s. Block Matrix:; ![Screen Shot 2020-05-26 at 1 37 51 PM](https://user-images.githubusercontent.com/106194/82932367-54630200-9f56-11ea-86f4-94726c36d727.png). DNDArray:; ![Screen Shot 2020-05-26 at 1 37 08 PM](https://user-images.githubusercontent.com/106194/82932387-5927b600-9f56-11ea-820e-97d1eb443d8f.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:2693,Performance,perform,performed,2693," able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).write('/tmp/out.da', overwrite=True); ```. Block matrix performed the matrix multiply in 19.3s. DNDArray performed the; matrix multiply in 37.6s. Block Matrix:; ![Screen Shot 2020-05-26 at 1 37 51 PM](https://user-images.githubusercontent.com/106194/82932367-54630200-9f56-11ea-86f4-94726c36d727.png). DNDArray:; ![Screen Shot 2020-05-26 at 1 37 08 PM](https://user-images.githubusercontent.com/106194/82932387-5927b600-9f56-11ea-820e-97d1eb443d8f.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:459,Safety,avoid,avoids,459,"I plan to move this to `hl.dnd.DNDArray`. I had to make a couple changes to RVD and Table to make this work. They all; revolve around convincing Hail not to elide *critical* `key_by`s. The critical insight is that 1:1 partitioners (partitioners where each range; bound interval contains exactly one key) are special: permuting their keys is; free. I can take advantage of this by combining two changes:; 1. `RVD.enforceKey` is aware of these partitioners and avoids scans in that case; 2. Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:1510,Safety,avoid,avoid,1510,"Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:1528,Safety,avoid,avoid,1528,"ire shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).writ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8864:936,Security,expose,exposed,936,"I plan to move this to `hl.dnd.DNDArray`. I had to make a couple changes to RVD and Table to make this work. They all; revolve around convincing Hail not to elide *critical* `key_by`s. The critical insight is that 1:1 partitioners (partitioners where each range; bound interval contains exactly one key) are special: permuting their keys is; free. I can take advantage of this by combining two changes:; 1. `RVD.enforceKey` is aware of these partitioners and avoids scans in that case; 2. Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864
https://github.com/hail-is/hail/pull/8865:62,Availability,error,errors,62,CHANGELOG: Add `hl.die` function that can be used to generate errors. Useful in data validation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865
https://github.com/hail-is/hail/pull/8865:85,Security,validat,validation,85,CHANGELOG: Add `hl.die` function that can be used to generate errors. Useful in data validation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865
https://github.com/hail-is/hail/pull/8867:399,Availability,error,errors,399,"CHANGELOG: Fix a major correctness bug ocurring when calling `BlockMatrix.transpose` on sparse BlockMatrices. Symmetric matrices are not affected. It seems like `BlockMatrix.transpose` has been broken for a while when the matrix is sparse. . I added `PerBlockMatrixSparsifier` as a way to sparsify particular blocks of a `BlockMatrix` from python. This is just so we can write tests / diagnose user errors based on sparsity patterns. I also wrote a helper function to sparsify numpy matrices for testing purposes. . The crucial fix here is to `GridPartitioner.transpose`. That function is supposed to return a pair of the form `(GridPartitioner, Int => Int)`, where the first of the pair is the new `GridPartitioner` for the transposed thing, and the second of the pair is a function that takes in a partition number and returns the partition number of its parent partition. Crucially, it's a function from new partition ids to old partition ids. I believe that code I'm removing did the opposite. Refresher on `GridPartitioner`: There are 3 coordinate systems:. There's ""coordinate"", which is (row, column). There's ""blockIndex"", which is the column major numbering of all blocks. And there's ""partitionIndex"", which is similar to numbering by blockIndex but it skips the sparse blocks",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867
https://github.com/hail-is/hail/pull/8867:377,Testability,test,tests,377,"CHANGELOG: Fix a major correctness bug ocurring when calling `BlockMatrix.transpose` on sparse BlockMatrices. Symmetric matrices are not affected. It seems like `BlockMatrix.transpose` has been broken for a while when the matrix is sparse. . I added `PerBlockMatrixSparsifier` as a way to sparsify particular blocks of a `BlockMatrix` from python. This is just so we can write tests / diagnose user errors based on sparsity patterns. I also wrote a helper function to sparsify numpy matrices for testing purposes. . The crucial fix here is to `GridPartitioner.transpose`. That function is supposed to return a pair of the form `(GridPartitioner, Int => Int)`, where the first of the pair is the new `GridPartitioner` for the transposed thing, and the second of the pair is a function that takes in a partition number and returns the partition number of its parent partition. Crucially, it's a function from new partition ids to old partition ids. I believe that code I'm removing did the opposite. Refresher on `GridPartitioner`: There are 3 coordinate systems:. There's ""coordinate"", which is (row, column). There's ""blockIndex"", which is the column major numbering of all blocks. And there's ""partitionIndex"", which is similar to numbering by blockIndex but it skips the sparse blocks",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867
https://github.com/hail-is/hail/pull/8867:496,Testability,test,testing,496,"CHANGELOG: Fix a major correctness bug ocurring when calling `BlockMatrix.transpose` on sparse BlockMatrices. Symmetric matrices are not affected. It seems like `BlockMatrix.transpose` has been broken for a while when the matrix is sparse. . I added `PerBlockMatrixSparsifier` as a way to sparsify particular blocks of a `BlockMatrix` from python. This is just so we can write tests / diagnose user errors based on sparsity patterns. I also wrote a helper function to sparsify numpy matrices for testing purposes. . The crucial fix here is to `GridPartitioner.transpose`. That function is supposed to return a pair of the form `(GridPartitioner, Int => Int)`, where the first of the pair is the new `GridPartitioner` for the transposed thing, and the second of the pair is a function that takes in a partition number and returns the partition number of its parent partition. Crucially, it's a function from new partition ids to old partition ids. I believe that code I'm removing did the opposite. Refresher on `GridPartitioner`: There are 3 coordinate systems:. There's ""coordinate"", which is (row, column). There's ""blockIndex"", which is the column major numbering of all blocks. And there's ""partitionIndex"", which is similar to numbering by blockIndex but it skips the sparse blocks",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867
https://github.com/hail-is/hail/pull/8875:64,Deployability,upgrade,upgraded,64,- changed worker image from batch-worker-7 to batch-worker-8; - upgraded version of ubuntu from ubuntu-minimal-1804-bionic-v20191024 to ubuntu-minimal-1804-bionic-v20200520; - upgraded docker ; - added debug = true mode to docker daemon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8875
https://github.com/hail-is/hail/pull/8875:176,Deployability,upgrade,upgraded,176,- changed worker image from batch-worker-7 to batch-worker-8; - upgraded version of ubuntu from ubuntu-minimal-1804-bionic-v20191024 to ubuntu-minimal-1804-bionic-v20200520; - upgraded docker ; - added debug = true mode to docker daemon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8875
https://github.com/hail-is/hail/pull/8878:400,Availability,down,down,400,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8878:674,Availability,failure,failures,674,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8878:719,Availability,failure,failures,719,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8878:758,Integrability,depend,dependent,758,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8878:177,Security,access,accesses,177,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8878:626,Testability,test,tests,626,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8878:792,Testability,test,tests,792,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8878:839,Testability,test,test,839,"I suspect some of the inconsistent behavior we're seeing could be due to memory corruption. So I put in another debugging allocator. What does this do? It makes sure all memory accesses in `Memory` are valid. Also, for each allocation, it puts a sentinel values before and after the allocation, and verifies they are undisturbed on free. How will this work normally? Obviously, this will slow things down. This checked `Memory` will be stored outside the main source, and can be copied over `Memory` to run with checked memory. Once this is passing, I will organize it that way. We should probably always run a version of the tests with memory checking enabled. Am I seeing failures? Yes, a handful. Unfortunately, the failures themselves don't seem context dependent, and when I run all the tests things fail, but when I run the isolated test, they pass. Getting this on the board while we debug it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878
https://github.com/hail-is/hail/pull/8882:8,Testability,test,tested,8,"Haven't tested at all yet, expect there will be things to fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8882
https://github.com/hail-is/hail/pull/8885:85,Modifiability,config,config,85,"Crosslink to dataproc docs that explain how to start a VEP cluster, mention that the config info is not necessary if you're using dataproc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8885
https://github.com/hail-is/hail/pull/8886:214,Testability,assert,asserts,214,"Allow the inner stream in a `StreamGrouped` or `StreamGroupByKey` to be unused, by recognizing when the `apply` method on the inner stream is never called, and calling it ourselves with a dummy consumer which just asserts the contained code paths are unreachable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8886
https://github.com/hail-is/hail/pull/8887:37,Performance,perform,performance,37,CHANGELOG: Substantially improve the performance of `import_gtf`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8887
https://github.com/hail-is/hail/pull/8890:193,Availability,error,error,193,"In a TableValue, the RVD key may be longer than the TableType key, so it's wrong for the row type of the result of a TableIR execute to depend on the RVD key. I tried to find all cases of this error in TableIR execute methods, and only found these two.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8890
https://github.com/hail-is/hail/pull/8890:136,Integrability,depend,depend,136,"In a TableValue, the RVD key may be longer than the TableType key, so it's wrong for the row type of the result of a TableIR execute to depend on the RVD key. I tried to find all cases of this error in TableIR execute methods, and only found these two.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8890
https://github.com/hail-is/hail/pull/8891:771,Safety,avoid,avoid,771,"I'd like to introduce a function to generate UUIDs in our code. The use case I have in mind is currently to append unique identifiers to partition file names for write to mimic our current behavior, but the ability to generate uuids in our IR seems pretty nice, in general. I added it as an IR node, but there's several problems arising from the fact that UUID is non-deterministic, and we can't effectively seed it. . The fact that UUID4 is non-idempotent is actually the feature I'm looking for here---running the exact same IR twice should get me two different results, because if my WritePartition fails with a certain UUID I want to retry the execution, generating a new UUID. . We can put a ""seed"" in the node to effectively treat each UUID4() node as non-equal to avoid any theoretical CSEing that might get done. I don't think we need to worry about other forms of let binding; we treat it as non-constant, and we already don't push lets inside of nested array scopes, so the only time we'd ever forward a binding with UUID4 is if we have one usage of the binding within the same array scoping as its definition, which is perfectly valid. The part that I'm having trouble with is related to the following concept: What happens when I make a stream of uuids, and then try to do a self-join? Let's look at the example of zipping a stream with itself, and just concatenating the two elements into an array.; ```; val stream = mapIR(rangeIR(5)) { _ => UUID4() }; val zipped = StreamZip(Array(stream, stream), Array(""1"", ""2""),; MakeArray(Array(Ref(""1"", TString), Ref(""1"", TString)), TArray(TString)),; ArrayZipBehavior.AssumeSameLength); ```; In the general case, we'd expect the zipped stream to be composed of arrays of duplicated elements---we rely on this being true when lowering, for example, a TableJoin against itself or a slightly transformed version of itself. Neither `zipped` nor `boundAndZipped` will exhibit that behavior here---since we process each stream independently, the UUIDs g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8891
https://github.com/hail-is/hail/pull/8892:45,Testability,test,tests,45,"This PR lowers `TableKeyByAndAggregate`. The tests are set to only run in the interpreter though, since method splitting issues cause them to fail currently. If I turn off method splitting, the tests I've added pass.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8892
https://github.com/hail-is/hail/pull/8892:194,Testability,test,tests,194,"This PR lowers `TableKeyByAndAggregate`. The tests are set to only run in the interpreter though, since method splitting issues cause them to fail currently. If I turn off method splitting, the tests I've added pass.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8892
https://github.com/hail-is/hail/pull/8893:168,Testability,test,test,168,"Use `emitI` to emit `ApplySeeded` nodes and port the seeded function implementations to rely on IEmitCode instead of EmitCode. The changes in IRSuite are arguably to a test that's no longer relevant---it was a regression test for a bug where we were occasionally double-emitting some part of the EmitTriplet in some cases. On the other hand, it feels kind of useful to keep as an illustration of how to thread IEmitCode through the function registry in a way that allows us to have weird interactions between the setup code of various IEmitCode args and the function itself. It was certainly instructive for me to work through how to translate the test functions in a way that made sense. I've removed one of the functions since we no longer have a single notion for ""this code should be run after all the IEmitCode setup but before the missingness is calculated""---they've been kind of merged into one concept.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8893
https://github.com/hail-is/hail/pull/8893:221,Testability,test,test,221,"Use `emitI` to emit `ApplySeeded` nodes and port the seeded function implementations to rely on IEmitCode instead of EmitCode. The changes in IRSuite are arguably to a test that's no longer relevant---it was a regression test for a bug where we were occasionally double-emitting some part of the EmitTriplet in some cases. On the other hand, it feels kind of useful to keep as an illustration of how to thread IEmitCode through the function registry in a way that allows us to have weird interactions between the setup code of various IEmitCode args and the function itself. It was certainly instructive for me to work through how to translate the test functions in a way that made sense. I've removed one of the functions since we no longer have a single notion for ""this code should be run after all the IEmitCode setup but before the missingness is calculated""---they've been kind of merged into one concept.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8893
https://github.com/hail-is/hail/pull/8893:648,Testability,test,test,648,"Use `emitI` to emit `ApplySeeded` nodes and port the seeded function implementations to rely on IEmitCode instead of EmitCode. The changes in IRSuite are arguably to a test that's no longer relevant---it was a regression test for a bug where we were occasionally double-emitting some part of the EmitTriplet in some cases. On the other hand, it feels kind of useful to keep as an illustration of how to thread IEmitCode through the function registry in a way that allows us to have weird interactions between the setup code of various IEmitCode args and the function itself. It was certainly instructive for me to work through how to translate the test functions in a way that made sense. I've removed one of the functions since we no longer have a single notion for ""this code should be run after all the IEmitCode setup but before the missingness is calculated""---they've been kind of merged into one concept.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8893
https://github.com/hail-is/hail/pull/8896:16,Availability,error,error,16,"should fix your error from friday, Dan.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8896
https://github.com/hail-is/hail/pull/8899:156,Deployability,deploy,deployment,156,"We use a readiness probe with a generous timeout to ensure the browser; loads the relevant files into a cache before any users see the website. This; slows deployment because we wait about 30s before sending any user traffic (in; the meantime, users will see 502s). On the bright side, after start-up, users will; always have a face experience, even if the pod was restarted since someone last; visited the site. I also codified some of the file update steps and clarified the README wrt Duncan's; GitHub repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8899
https://github.com/hail-is/hail/pull/8899:446,Deployability,update,update,446,"We use a readiness probe with a generous timeout to ensure the browser; loads the relevant files into a cache before any users see the website. This; slows deployment because we wait about 30s before sending any user traffic (in; the meantime, users will see 502s). On the bright side, after start-up, users will; always have a face experience, even if the pod was restarted since someone last; visited the site. I also codified some of the file update steps and clarified the README wrt Duncan's; GitHub repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8899
https://github.com/hail-is/hail/pull/8899:72,Performance,load,loads,72,"We use a readiness probe with a generous timeout to ensure the browser; loads the relevant files into a cache before any users see the website. This; slows deployment because we wait about 30s before sending any user traffic (in; the meantime, users will see 502s). On the bright side, after start-up, users will; always have a face experience, even if the pod was restarted since someone last; visited the site. I also codified some of the file update steps and clarified the README wrt Duncan's; GitHub repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8899
https://github.com/hail-is/hail/pull/8899:104,Performance,cache,cache,104,"We use a readiness probe with a generous timeout to ensure the browser; loads the relevant files into a cache before any users see the website. This; slows deployment because we wait about 30s before sending any user traffic (in; the meantime, users will see 502s). On the bright side, after start-up, users will; always have a face experience, even if the pod was restarted since someone last; visited the site. I also codified some of the file update steps and clarified the README wrt Duncan's; GitHub repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8899
https://github.com/hail-is/hail/pull/8899:41,Safety,timeout,timeout,41,"We use a readiness probe with a generous timeout to ensure the browser; loads the relevant files into a cache before any users see the website. This; slows deployment because we wait about 30s before sending any user traffic (in; the meantime, users will see 502s). On the bright side, after start-up, users will; always have a face experience, even if the pod was restarted since someone last; visited the site. I also codified some of the file update steps and clarified the README wrt Duncan's; GitHub repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8899
https://github.com/hail-is/hail/issues/8908:27,Availability,error,error,27,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8908
https://github.com/hail-is/hail/issues/8908:120,Availability,Error,Error,120,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8908
https://github.com/hail-is/hail/issues/8908:450,Availability,down,downstream,450,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8908
https://github.com/hail-is/hail/issues/8908:517,Availability,down,downstream,517,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8908
https://github.com/hail-is/hail/issues/8908:461,Performance,perform,performance,461,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8908
https://github.com/hail-is/hail/pull/8910:23,Testability,test,tests,23,"This PR introduces and tests a new `EType`, `ENDArray`. It is not used anywhere yet except for tests, as there are some issues with `fundamentalType` and backcompatibility to be worked out, but in the interest of smaller PRs I figured I'd get the type implementation in first and do a follow up where we start using it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8910
https://github.com/hail-is/hail/pull/8910:95,Testability,test,tests,95,"This PR introduces and tests a new `EType`, `ENDArray`. It is not used anywhere yet except for tests, as there are some issues with `fundamentalType` and backcompatibility to be worked out, but in the interest of smaller PRs I figured I'd get the type implementation in first and do a follow up where we start using it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8910
https://github.com/hail-is/hail/pull/8916:57,Deployability,deploy,deploy,57,I think I got all of the fixes. I tested the UI with dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8916
https://github.com/hail-is/hail/pull/8916:34,Testability,test,tested,34,I think I got all of the fixes. I tested the UI with dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8916
https://github.com/hail-is/hail/pull/8917:85,Integrability,depend,depends,85,"~Stacked on #8874, not by necessity, but because the following PR lowering TableJoin depends on both this and that.~. * Before, `TableStage` defined it's per-partition behavior in an abstract method `def partition(ctxRef: Ref): IR`. This makes defining a new `TableStage` a bit heavy syntactically, and means we can't inspect the type of the partition IR without apply the function to something. This PR replaces the abstract method with two fields `ctxRefName: String` and `partitionIR: IR`. It defines a method `def partition(ctx: IR): IR` using these, which is more ergonomic to use because the context isn't forced to be a `Ref`. * With the type of the partition result accessible, this PR requires that type to be a `TStream<TStruct>`. Without this requirement, there is no clear connection between the partitioner and the rest of the `TableStage`. The only violation of this requirement was mapping to some other type right before collecting; this use is accommodated by a `mapCollect` method which combines the steps. * The binding structure of `TableStage` has been slightly reorganized. The `letBindings`, which are used on the master, and the `broadcastVals` which are used on the driver (previously, they had to also be usable on master), have been teased apart. In the new structure:; * `letBindings` are as before: a sequence of bindings which are evaluated in sequence on the master, whose bindings are visible in the `contexts` expression and in the `broadcastVals`; * `broadcastVals` are now a separate sequence of bindings, which are evaluated on the master in parallel (each broadcast binding sees only the `letBindings`, no previous `broadcastVals`), and whose bindings are visible only in the `partitionIR`; * `globals` is now required to be a `Ref`, which is defined in `letBindings` and redefined in `broadcastVals`, so that the `Ref` is valid in later `letBindings`, in `contexts`, as well as in the `partitionIR`. This does mean that `globals` is always broadcast, even when it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8917
https://github.com/hail-is/hail/pull/8917:674,Security,access,accessible,674,"~Stacked on #8874, not by necessity, but because the following PR lowering TableJoin depends on both this and that.~. * Before, `TableStage` defined it's per-partition behavior in an abstract method `def partition(ctxRef: Ref): IR`. This makes defining a new `TableStage` a bit heavy syntactically, and means we can't inspect the type of the partition IR without apply the function to something. This PR replaces the abstract method with two fields `ctxRefName: String` and `partitionIR: IR`. It defines a method `def partition(ctx: IR): IR` using these, which is more ergonomic to use because the context isn't forced to be a `Ref`. * With the type of the partition result accessible, this PR requires that type to be a `TStream<TStruct>`. Without this requirement, there is no clear connection between the partitioner and the rest of the `TableStage`. The only violation of this requirement was mapping to some other type right before collecting; this use is accommodated by a `mapCollect` method which combines the steps. * The binding structure of `TableStage` has been slightly reorganized. The `letBindings`, which are used on the master, and the `broadcastVals` which are used on the driver (previously, they had to also be usable on master), have been teased apart. In the new structure:; * `letBindings` are as before: a sequence of bindings which are evaluated in sequence on the master, whose bindings are visible in the `contexts` expression and in the `broadcastVals`; * `broadcastVals` are now a separate sequence of bindings, which are evaluated on the master in parallel (each broadcast binding sees only the `letBindings`, no previous `broadcastVals`), and whose bindings are visible only in the `partitionIR`; * `globals` is now required to be a `Ref`, which is defined in `letBindings` and redefined in `broadcastVals`, so that the `Ref` is valid in later `letBindings`, in `contexts`, as well as in the `partitionIR`. This does mean that `globals` is always broadcast, even when it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8917
https://github.com/hail-is/hail/pull/8917:779,Usability,clear,clear,779,"~Stacked on #8874, not by necessity, but because the following PR lowering TableJoin depends on both this and that.~. * Before, `TableStage` defined it's per-partition behavior in an abstract method `def partition(ctxRef: Ref): IR`. This makes defining a new `TableStage` a bit heavy syntactically, and means we can't inspect the type of the partition IR without apply the function to something. This PR replaces the abstract method with two fields `ctxRefName: String` and `partitionIR: IR`. It defines a method `def partition(ctx: IR): IR` using these, which is more ergonomic to use because the context isn't forced to be a `Ref`. * With the type of the partition result accessible, this PR requires that type to be a `TStream<TStruct>`. Without this requirement, there is no clear connection between the partitioner and the rest of the `TableStage`. The only violation of this requirement was mapping to some other type right before collecting; this use is accommodated by a `mapCollect` method which combines the steps. * The binding structure of `TableStage` has been slightly reorganized. The `letBindings`, which are used on the master, and the `broadcastVals` which are used on the driver (previously, they had to also be usable on master), have been teased apart. In the new structure:; * `letBindings` are as before: a sequence of bindings which are evaluated in sequence on the master, whose bindings are visible in the `contexts` expression and in the `broadcastVals`; * `broadcastVals` are now a separate sequence of bindings, which are evaluated on the master in parallel (each broadcast binding sees only the `letBindings`, no previous `broadcastVals`), and whose bindings are visible only in the `partitionIR`; * `globals` is now required to be a `Ref`, which is defined in `letBindings` and redefined in `broadcastVals`, so that the `Ref` is valid in later `letBindings`, in `contexts`, as well as in the `partitionIR`. This does mean that `globals` is always broadcast, even when it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8917
https://github.com/hail-is/hail/pull/8917:1231,Usability,usab,usable,1231,"bstract method `def partition(ctxRef: Ref): IR`. This makes defining a new `TableStage` a bit heavy syntactically, and means we can't inspect the type of the partition IR without apply the function to something. This PR replaces the abstract method with two fields `ctxRefName: String` and `partitionIR: IR`. It defines a method `def partition(ctx: IR): IR` using these, which is more ergonomic to use because the context isn't forced to be a `Ref`. * With the type of the partition result accessible, this PR requires that type to be a `TStream<TStruct>`. Without this requirement, there is no clear connection between the partitioner and the rest of the `TableStage`. The only violation of this requirement was mapping to some other type right before collecting; this use is accommodated by a `mapCollect` method which combines the steps. * The binding structure of `TableStage` has been slightly reorganized. The `letBindings`, which are used on the master, and the `broadcastVals` which are used on the driver (previously, they had to also be usable on master), have been teased apart. In the new structure:; * `letBindings` are as before: a sequence of bindings which are evaluated in sequence on the master, whose bindings are visible in the `contexts` expression and in the `broadcastVals`; * `broadcastVals` are now a separate sequence of bindings, which are evaluated on the master in parallel (each broadcast binding sees only the `letBindings`, no previous `broadcastVals`), and whose bindings are visible only in the `partitionIR`; * `globals` is now required to be a `Ref`, which is defined in `letBindings` and redefined in `broadcastVals`, so that the `Ref` is valid in later `letBindings`, in `contexts`, as well as in the `partitionIR`. This does mean that `globals` is always broadcast, even when it's just an empty struct. But since we're generating a `CollectDistributedArray` which always broadcasts a struct, adding a nested required empty struct shouldn't have any overhead. I ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8917
https://github.com/hail-is/hail/pull/8917:2227,Usability,clear,clearer,2227,"he binding structure of `TableStage` has been slightly reorganized. The `letBindings`, which are used on the master, and the `broadcastVals` which are used on the driver (previously, they had to also be usable on master), have been teased apart. In the new structure:; * `letBindings` are as before: a sequence of bindings which are evaluated in sequence on the master, whose bindings are visible in the `contexts` expression and in the `broadcastVals`; * `broadcastVals` are now a separate sequence of bindings, which are evaluated on the master in parallel (each broadcast binding sees only the `letBindings`, no previous `broadcastVals`), and whose bindings are visible only in the `partitionIR`; * `globals` is now required to be a `Ref`, which is defined in `letBindings` and redefined in `broadcastVals`, so that the `Ref` is valid in later `letBindings`, in `contexts`, as well as in the `partitionIR`. This does mean that `globals` is always broadcast, even when it's just an empty struct. But since we're generating a `CollectDistributedArray` which always broadcasts a struct, adding a nested required empty struct shouldn't have any overhead. I think this more layered organization has a clearer semantics than before, where `broadcastVals` were a subset of the `letBindings`, and `globals` was an arbitrary IR in the `broadcastVals` scope. It's also closer to the structure of the resulting `CollectDistributedArray`. I tried to pull `letBindings` out of `TableStage`, letting them be built up imperatively in the lowering pass, similar to what we do in `LowerMatrixIR`. I had it mostly working, but was blocked by `localSort`, which compiles and executes an intermediate `TableStage`. This needs to be able to compile only those bindings needed in that intermediate stage, and to throw out those used bindings, neither of which is easy when the bindings are accumulated in a single list throughout the lowering pass. @catoverdrive Would appreciate if you could take a quick look as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8917
https://github.com/hail-is/hail/pull/8918:31,Deployability,update,update-hail-version,31,`hailctl dataproc modify`'s `--update-hail-version` and `--wheel` arguments cannot be used together. Using argparse's [mutually exclusive group](https://docs.python.org/3/library/argparse.html#mutual-exclusion) to enforce that makes the usage/help text clearer. ```; usage: hailctl dataproc modify [-h] [--num-workers NUM_WORKERS]; [--num-preemptible-workers NUM_PREEMPTIBLE_WORKERS]; [--graceful-decommission-timeout GRACEFUL_DECOMMISSION_TIMEOUT]; [--max-idle MAX_IDLE] [--dry-run] [--zone ZONE]; [--update-hail-version | --wheel WHEEL]; name; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8918
https://github.com/hail-is/hail/pull/8918:502,Deployability,update,update-hail-version,502,`hailctl dataproc modify`'s `--update-hail-version` and `--wheel` arguments cannot be used together. Using argparse's [mutually exclusive group](https://docs.python.org/3/library/argparse.html#mutual-exclusion) to enforce that makes the usage/help text clearer. ```; usage: hailctl dataproc modify [-h] [--num-workers NUM_WORKERS]; [--num-preemptible-workers NUM_PREEMPTIBLE_WORKERS]; [--graceful-decommission-timeout GRACEFUL_DECOMMISSION_TIMEOUT]; [--max-idle MAX_IDLE] [--dry-run] [--zone ZONE]; [--update-hail-version | --wheel WHEEL]; name; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8918
https://github.com/hail-is/hail/pull/8918:410,Safety,timeout,timeout,410,`hailctl dataproc modify`'s `--update-hail-version` and `--wheel` arguments cannot be used together. Using argparse's [mutually exclusive group](https://docs.python.org/3/library/argparse.html#mutual-exclusion) to enforce that makes the usage/help text clearer. ```; usage: hailctl dataproc modify [-h] [--num-workers NUM_WORKERS]; [--num-preemptible-workers NUM_PREEMPTIBLE_WORKERS]; [--graceful-decommission-timeout GRACEFUL_DECOMMISSION_TIMEOUT]; [--max-idle MAX_IDLE] [--dry-run] [--zone ZONE]; [--update-hail-version | --wheel WHEEL]; name; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8918
https://github.com/hail-is/hail/pull/8918:253,Usability,clear,clearer,253,`hailctl dataproc modify`'s `--update-hail-version` and `--wheel` arguments cannot be used together. Using argparse's [mutually exclusive group](https://docs.python.org/3/library/argparse.html#mutual-exclusion) to enforce that makes the usage/help text clearer. ```; usage: hailctl dataproc modify [-h] [--num-workers NUM_WORKERS]; [--num-preemptible-workers NUM_PREEMPTIBLE_WORKERS]; [--graceful-decommission-timeout GRACEFUL_DECOMMISSION_TIMEOUT]; [--max-idle MAX_IDLE] [--dry-run] [--zone ZONE]; [--update-hail-version | --wheel WHEEL]; name; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8918
https://github.com/hail-is/hail/pull/8920:119,Deployability,install,install,119,"Adds the side-by-side clickable code block, some styling changes (fix view width to 2048px on wider displays), and the install section. Stacked on #8848",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920
https://github.com/hail-is/hail/pull/8921:43,Modifiability,extend,extendKeyPreservesPartitioning,43,"~Stacked on #8917~. Adds `zipPartitions`, `extendKeyPreservesPartitioning`, `orderedJoin`, and `alignAndZipPartitions` methods to `TableStage`, trying to mirror the `RVD` implementation, especially concerning the partitioning logic. Adds lowering case for `TableJoin`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8921
https://github.com/hail-is/hail/pull/8921:226,Testability,log,logic,226,"~Stacked on #8917~. Adds `zipPartitions`, `extendKeyPreservesPartitioning`, `orderedJoin`, and `alignAndZipPartitions` methods to `TableStage`, trying to mirror the `RVD` implementation, especially concerning the partitioning logic. Adds lowering case for `TableJoin`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8921
https://github.com/hail-is/hail/pull/8922:12,Deployability,deploy,deploying,12,"1. When dev deploying or PR deploying, the root URL is not the usual one. We fix `layout.html` to produce the correct url when prefixed.; 2. The NGINX rule which fixes paths in dev deploys or PR deploys incorrectly assumed only double quotes were permissible around HTML attributes. Single quotoes are also permissible. The layout changes will not take effect until we next deploy the Hail docs. Until then, dev deploy's will not have a nav bar on the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8922
https://github.com/hail-is/hail/pull/8922:28,Deployability,deploy,deploying,28,"1. When dev deploying or PR deploying, the root URL is not the usual one. We fix `layout.html` to produce the correct url when prefixed.; 2. The NGINX rule which fixes paths in dev deploys or PR deploys incorrectly assumed only double quotes were permissible around HTML attributes. Single quotoes are also permissible. The layout changes will not take effect until we next deploy the Hail docs. Until then, dev deploy's will not have a nav bar on the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8922
https://github.com/hail-is/hail/pull/8922:181,Deployability,deploy,deploys,181,"1. When dev deploying or PR deploying, the root URL is not the usual one. We fix `layout.html` to produce the correct url when prefixed.; 2. The NGINX rule which fixes paths in dev deploys or PR deploys incorrectly assumed only double quotes were permissible around HTML attributes. Single quotoes are also permissible. The layout changes will not take effect until we next deploy the Hail docs. Until then, dev deploy's will not have a nav bar on the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8922
https://github.com/hail-is/hail/pull/8922:195,Deployability,deploy,deploys,195,"1. When dev deploying or PR deploying, the root URL is not the usual one. We fix `layout.html` to produce the correct url when prefixed.; 2. The NGINX rule which fixes paths in dev deploys or PR deploys incorrectly assumed only double quotes were permissible around HTML attributes. Single quotoes are also permissible. The layout changes will not take effect until we next deploy the Hail docs. Until then, dev deploy's will not have a nav bar on the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8922
https://github.com/hail-is/hail/pull/8922:374,Deployability,deploy,deploy,374,"1. When dev deploying or PR deploying, the root URL is not the usual one. We fix `layout.html` to produce the correct url when prefixed.; 2. The NGINX rule which fixes paths in dev deploys or PR deploys incorrectly assumed only double quotes were permissible around HTML attributes. Single quotoes are also permissible. The layout changes will not take effect until we next deploy the Hail docs. Until then, dev deploy's will not have a nav bar on the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8922
https://github.com/hail-is/hail/pull/8922:412,Deployability,deploy,deploy,412,"1. When dev deploying or PR deploying, the root URL is not the usual one. We fix `layout.html` to produce the correct url when prefixed.; 2. The NGINX rule which fixes paths in dev deploys or PR deploys incorrectly assumed only double quotes were permissible around HTML attributes. Single quotoes are also permissible. The layout changes will not take effect until we next deploy the Hail docs. Until then, dev deploy's will not have a nav bar on the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8922
https://github.com/hail-is/hail/pull/8923:503,Availability,down,down,503,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:144,Deployability,deploy,deployment,144,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:581,Deployability,deploy,deploy,581,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:626,Deployability,deploy,deployed,626,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:722,Deployability,deploy,deploy,722,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:513,Integrability,rout,route,513,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:381,Testability,test,test,381,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:455,Testability,test,test,455,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:131,Usability,simpl,simpler,131,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8923:269,Usability,simpl,simpler,269,"1. Move `hail/www` to `site/www` and associated build commands into `site/Makefile` and a new `build.yaml` step.; 2. Prepare for a simpler docs deployment by supporting both the current 0.2 structure (top-level `www` containing `docs/0.2` and `docs/0.1`) and a future, simpler structure (top-level `docs` containing `0.2` and `0.1`).; 3. Fix `site/Makefile` which had bit-rotted. `test` doesn't really work anymore so I removed it. We could restore `make test` by figuring out a local SSL story. I went down this route but couldn't get NGINX to respond to my HTTPS requests. `make deploy` is rather fast now anyway. Currently deployed at https://internal.hail.is/dking/site/index.html. There are two known issues with dev deploy, those are resolved at https://github.com/hail-is/hail/pull/8922.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923
https://github.com/hail-is/hail/pull/8925:82,Testability,stub,stub,82,Defines (but doesn't implement) WritePartition and WriteMetadata nodes as well as stub classes for the writers needed for native TableWrite lowering. WriteMetadata uses IEmitCode; WritePartition is implemented with EmitCode because it consumes a stream.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8925
https://github.com/hail-is/hail/pull/8926:131,Availability,error,error,131,"This implementation lowers TableWrite with a TableNativeReader. Punting on the `stageLocally` path for now (it'll throw a lowering error) since our current implementation adds a task listener to the spark task to clean up files, and I'm not sure how we want to handle that in the general case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8926
https://github.com/hail-is/hail/pull/8928:46,Security,authenticat,authenticated,46,"Not sure why crossorigin is necessary to make authenticated imports work, but it works. No information at MDN: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/script. https://internal.hail.is/dking/site/ demonstrates this working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8928
https://github.com/hail-is/hail/pull/8929:38,Modifiability,config,config,38,VEP will now automatically look for a config file in environment variable `VEP_CONFIG_URI` if one isn't specified. This environment variable is prepopulated on dataproc clusters started with `hailctl dataproc`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8929
https://github.com/hail-is/hail/pull/8929:65,Modifiability,variab,variable,65,VEP will now automatically look for a config file in environment variable `VEP_CONFIG_URI` if one isn't specified. This environment variable is prepopulated on dataproc clusters started with `hailctl dataproc`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8929
https://github.com/hail-is/hail/pull/8929:132,Modifiability,variab,variable,132,VEP will now automatically look for a config file in environment variable `VEP_CONFIG_URI` if one isn't specified. This environment variable is prepopulated on dataproc clusters started with `hailctl dataproc`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8929
https://github.com/hail-is/hail/pull/8930:173,Deployability,deploy,deployed,173,I do not know where `static` came from. The shiny docs indicate that static files; should be placed inside a `www` directory which is a sibling to the `app.R` file. Already deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8930
https://github.com/hail-is/hail/pull/8934:655,Testability,test,tested,655,"There might be additional PRs, but for now this PR does the following:. - Names folders for jobs based on the job_id computed after ordering the jobs; - Gets rid of double slashes in path names; - Names JobResourceFiles based on the name of the attribute.; Example: j.ofile => /batch/1/ofile; - Names resource groups from jobs as follows:; Example: j.declare_resource_group(ofile={'bed': '{root}.bed'}); /batch/1/ofile.bed; - Names input resource files based on the file name; /batch/inputs/{uid}/my_vcf.bgz; - Names input resource groups two-ways; input = batch.declare_resource_group(vcf=...); /batch/inputs/{uid}/my_vcf.bgz; /batch/inputs/{uid}.vcf. I tested this locally and on the service to make sure it's doing what I want.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8934
https://github.com/hail-is/hail/pull/8937:42,Deployability,configurat,configuration,42,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8937
https://github.com/hail-is/hail/pull/8937:144,Deployability,configurat,configuration,144,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8937
https://github.com/hail-is/hail/pull/8937:42,Modifiability,config,configuration,42,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8937
https://github.com/hail-is/hail/pull/8937:144,Modifiability,config,configuration,144,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8937
https://github.com/hail-is/hail/pull/8937:178,Testability,test,tests,178,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8937
https://github.com/hail-is/hail/pull/8937:232,Testability,test,test,232,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8937
https://github.com/hail-is/hail/pull/8938:119,Availability,checkpoint,checkpoint,119,"Not sure why the extra line is required here... riddles of Sphinx. https://hail.is/docs/0.2/hail.Table.html#hail.Table.checkpoint; https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.checkpoint. Before; <img width=""1029"" alt=""Screen Shot 2020-06-08 at 9 30 12 PM"" src=""https://user-images.githubusercontent.com/1156625/84096487-0cb98d00-a9d0-11ea-8623-12288df6eace.png"">. After; <img width=""1042"" alt=""Screen Shot 2020-06-08 at 9 35 06 PM"" src=""https://user-images.githubusercontent.com/1156625/84096490-10e5aa80-a9d0-11ea-9be9-c3dfd8b049a9.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8938
https://github.com/hail-is/hail/pull/8938:195,Availability,checkpoint,checkpoint,195,"Not sure why the extra line is required here... riddles of Sphinx. https://hail.is/docs/0.2/hail.Table.html#hail.Table.checkpoint; https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.checkpoint. Before; <img width=""1029"" alt=""Screen Shot 2020-06-08 at 9 30 12 PM"" src=""https://user-images.githubusercontent.com/1156625/84096487-0cb98d00-a9d0-11ea-8623-12288df6eace.png"">. After; <img width=""1042"" alt=""Screen Shot 2020-06-08 at 9 35 06 PM"" src=""https://user-images.githubusercontent.com/1156625/84096490-10e5aa80-a9d0-11ea-9be9-c3dfd8b049a9.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8938
https://github.com/hail-is/hail/pull/8939:111,Testability,test,test,111,"With this table; ```; ds = hl.utils.range_table(10); ds = ds.annotate(t=hl.tuple([1, ""Foo"", True])); ds.write(""test.ht""); ```. `hailctl dataproc describe test.ht` currently outputs the following for ""Row fields"":; ```; ----------------------------------------; Row fields:; 'idx': int32; 't': tuple<>; ----------------------------------------; ```. With this change, it outputs:; ```; ----------------------------------------; Row fields:; 'idx': int32; 't': tuple<int32, str, bool>; ----------------------------------------; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8939
https://github.com/hail-is/hail/pull/8939:154,Testability,test,test,154,"With this table; ```; ds = hl.utils.range_table(10); ds = ds.annotate(t=hl.tuple([1, ""Foo"", True])); ds.write(""test.ht""); ```. `hailctl dataproc describe test.ht` currently outputs the following for ""Row fields"":; ```; ----------------------------------------; Row fields:; 'idx': int32; 't': tuple<>; ----------------------------------------; ```. With this change, it outputs:; ```; ----------------------------------------; Row fields:; 'idx': int32; 't': tuple<int32, str, bool>; ----------------------------------------; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8939
https://github.com/hail-is/hail/issues/8940:229,Availability,error,error,229,"The [documented](https://hail.is/docs/0.2/getting_started_developing.html#building-the-docs-and-website) process for building documentation is:; ```; cd hail; make docs-no-test; ```. That now fails with; ```; Warning, treated as error:; html_extra_path entry '/path/to/hail/hail/build/docs/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../www/hail-logo-cropped.png' does not exist; make[1]: *** [html] Error 2; make: *** [docs-no-test] Error 2; ```. It looks like the source of the problem is that docs/conf.py can't find the `www` directory.; https://github.com/hail-is/hail/blob/0b3823af5310a735bc9544fb73308f82426292be/hail/python/hail/docs/conf.py#L225-L232. I'm guessing this is related to changes in #8923.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8940
https://github.com/hail-is/hail/issues/8940:503,Availability,Error,Error,503,"The [documented](https://hail.is/docs/0.2/getting_started_developing.html#building-the-docs-and-website) process for building documentation is:; ```; cd hail; make docs-no-test; ```. That now fails with; ```; Warning, treated as error:; html_extra_path entry '/path/to/hail/hail/build/docs/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../www/hail-logo-cropped.png' does not exist; make[1]: *** [html] Error 2; make: *** [docs-no-test] Error 2; ```. It looks like the source of the problem is that docs/conf.py can't find the `www` directory.; https://github.com/hail-is/hail/blob/0b3823af5310a735bc9544fb73308f82426292be/hail/python/hail/docs/conf.py#L225-L232. I'm guessing this is related to changes in #8923.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8940
https://github.com/hail-is/hail/issues/8940:537,Availability,Error,Error,537,"The [documented](https://hail.is/docs/0.2/getting_started_developing.html#building-the-docs-and-website) process for building documentation is:; ```; cd hail; make docs-no-test; ```. That now fails with; ```; Warning, treated as error:; html_extra_path entry '/path/to/hail/hail/build/docs/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../www/hail-logo-cropped.png' does not exist; make[1]: *** [html] Error 2; make: *** [docs-no-test] Error 2; ```. It looks like the source of the problem is that docs/conf.py can't find the `www` directory.; https://github.com/hail-is/hail/blob/0b3823af5310a735bc9544fb73308f82426292be/hail/python/hail/docs/conf.py#L225-L232. I'm guessing this is related to changes in #8923.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8940
https://github.com/hail-is/hail/issues/8940:172,Testability,test,test,172,"The [documented](https://hail.is/docs/0.2/getting_started_developing.html#building-the-docs-and-website) process for building documentation is:; ```; cd hail; make docs-no-test; ```. That now fails with; ```; Warning, treated as error:; html_extra_path entry '/path/to/hail/hail/build/docs/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../www/hail-logo-cropped.png' does not exist; make[1]: *** [html] Error 2; make: *** [docs-no-test] Error 2; ```. It looks like the source of the problem is that docs/conf.py can't find the `www` directory.; https://github.com/hail-is/hail/blob/0b3823af5310a735bc9544fb73308f82426292be/hail/python/hail/docs/conf.py#L225-L232. I'm guessing this is related to changes in #8923.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8940
https://github.com/hail-is/hail/issues/8940:449,Testability,log,logo-cropped,449,"The [documented](https://hail.is/docs/0.2/getting_started_developing.html#building-the-docs-and-website) process for building documentation is:; ```; cd hail; make docs-no-test; ```. That now fails with; ```; Warning, treated as error:; html_extra_path entry '/path/to/hail/hail/build/docs/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../www/hail-logo-cropped.png' does not exist; make[1]: *** [html] Error 2; make: *** [docs-no-test] Error 2; ```. It looks like the source of the problem is that docs/conf.py can't find the `www` directory.; https://github.com/hail-is/hail/blob/0b3823af5310a735bc9544fb73308f82426292be/hail/python/hail/docs/conf.py#L225-L232. I'm guessing this is related to changes in #8923.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8940
https://github.com/hail-is/hail/issues/8940:531,Testability,test,test,531,"The [documented](https://hail.is/docs/0.2/getting_started_developing.html#building-the-docs-and-website) process for building documentation is:; ```; cd hail; make docs-no-test; ```. That now fails with; ```; Warning, treated as error:; html_extra_path entry '/path/to/hail/hail/build/docs/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../www/hail-logo-cropped.png' does not exist; make[1]: *** [html] Error 2; make: *** [docs-no-test] Error 2; ```. It looks like the source of the problem is that docs/conf.py can't find the `www` directory.; https://github.com/hail-is/hail/blob/0b3823af5310a735bc9544fb73308f82426292be/hail/python/hail/docs/conf.py#L225-L232. I'm guessing this is related to changes in #8923.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8940
https://github.com/hail-is/hail/pull/8942:11,Modifiability,Refactor,Refactored,11,CHANGELOG: Refactored VCF combiner to support other GVCF schemas.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8942
https://github.com/hail-is/hail/issues/8944:1495,Availability,avail,available,1495,"rs the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (50000, 998); 	2020-06-10 10:30:10 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:3628,Availability,Error,Error,3628,"=======================> (222 + 80) / 353]; 	[PASS] with 353 partitions: (50000, 973); 	2020-06-10 10:30:15 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 9:> (0 + 18) / 18]; 	[FAIL] with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:3774,Availability,failure,failure,3774,"chols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 9:> (0 + 18) / 18]; 	[FAIL] with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.ite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:3888,Availability,failure,failure,3888," with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:6179,Availability,error,error,6179,MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(Exec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:8439,Availability,failure,failure,8439,.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:8553,Availability,failure,failure,8553,.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:15638,Availability,Error,Error,15638,ExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:15694,Availability,failure,failure,15694,ExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:15808,Availability,failure,failure,15808,.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18498,Availability,error,error,18498,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18539,Availability,error,error,18539,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18580,Availability,error,error,18580,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18621,Availability,error,error,18621,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18662,Availability,error,error,18662,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18703,Availability,error,error,18703,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18746,Availability,error,error,18746,"RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18769,Availability,error,error,18769,"ecutor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18863,Availability,error,error,18863,"r(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18913,Availability,error,error,18913,"r(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18954,Availability,error,error,18954,"r(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19272,Availability,error,error,19272,"ches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19318,Availability,error,error,19318,"ches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19650,Availability,error,error,19650,"46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19694,Availability,error,error,19694,"46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19708,Availability,error,error,19708,"46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19863,Availability,error,error,19863,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19907,Availability,error,error,19907,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19948,Availability,error,error,19948,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:80,Deployability,release,releases,80,"This may be an environment specific bug, but we have showed it only occurs with releases after 0.2.42 (in our environment) and only under very specific conditions. When we run `subject_qc` on a matrix table with >= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:5588,Energy Efficiency,schedul,scheduler,5588,cala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:3,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:5658,Energy Efficiency,schedul,scheduler,5658,:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10253,Energy Efficiency,schedul,scheduler,10253,cala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.Array,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10323,Energy Efficiency,schedul,scheduler,10323,:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10818,Energy Efficiency,schedul,scheduler,10818,ator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(Event,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10858,Energy Efficiency,schedul,scheduler,10858,D.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGSchedul,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10959,Energy Efficiency,schedul,scheduler,10959,kpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11059,Energy Efficiency,schedul,scheduler,11059,pPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11319,Energy Efficiency,schedul,scheduler,11319,cheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11402,Energy Efficiency,schedul,scheduler,11402,$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11495,Energy Efficiency,schedul,scheduler,11495,s.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompil,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11595,Energy Efficiency,schedul,scheduler,11595,concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11693,Energy Efficiency,schedul,scheduler,11693,adPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 			at is.hail.expr.ir.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11862,Energy Efficiency,schedul,scheduler,11862,$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:17508,Energy Efficiency,schedul,scheduler,17508,cala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:17578,Energy Efficiency,schedul,scheduler,17578,:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an err,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:6552,Integrability,Wrap,WrappedArray,6552,cala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 			at is.hail.utils.package$.using(package.scala:600); 			at is.hail.annotations.Region$.scoped(Region.scala:18); 			at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 			at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:6573,Integrability,Wrap,WrappedArray,6573,apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 			at is.hail.utils.package$.using(package.scala:600); 			at is.hail.annotations.Region$.scoped(Region.scala:18); 			at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 			at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:13773,Integrability,Wrap,WrappedArray,13773,ail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 			at is.hail.utils.package$.using(package.scala:600); 			at is.hail.annotations.Region$.scoped(Region.scala:18); 			at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 			at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:13794,Integrability,Wrap,WrappedArray,13794,ing.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 			at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 			at is.hail.utils.package$.using(package.scala:600); 			at is.hail.annotations.Region$.scoped(Region.scala:18); 			at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 			at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:12416,Modifiability,rewrite,rewrite,12416,DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 			at is.hail.expr.ir.lowering.InterpretNonCom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:12546,Modifiability,rewrite,rewrite,12546,ive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1219,Performance,load,load,1219,">= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (500",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:3354,Performance,load,loads,3354,"tage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 7:=================================> (222 + 80) / 353]; 	[PASS] with 353 partitions: (50000, 973); 	2020-06-10 10:30:15 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 9:> (0 + 18) / 18]; 	[FAIL] with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutput",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:5935,Performance,concurren,concurrent,5935,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkB,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:6018,Performance,concurren,concurrent,6018,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10600,Performance,concurren,concurrent,10600,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.D,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10683,Performance,concurren,concurrent,10683,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:17855,Performance,concurren,concurrent,17855,ache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:17938,Performance,concurren,concurrent,17938,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 al,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:3753,Safety,abort,aborted,3753,"chols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 9:> (0 + 18) / 18]; 	[FAIL] with 354 partitions; 	Traceback (most recent call last):; 	 File ""test_11_cluster_sampleqc.py"", line 20, in <module>; 		print(""\n[PASS] with"", N, ""partitions:"", Y.count()); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/matrixtable.py"", line 2426, in count; 		return Env.backend().execute(count_ir); 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 296, in execute; 		result = json.loads(self._jhc.backend().executeJSON(jir)); 	 File ""/bmrn/apps/spark/2.4.5/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; 	 File ""/bmrn/apps/hail/0.2.44/python/hail-0.2.44-py3-none-any.egg/hail/backend/spark_backend.py"", line 41, in deco; 		'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 	hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.ite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:8418,Safety,abort,aborted,8418,.SparkBackend.withExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:10991,Safety,abort,abortStage,10991,at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11091,Safety,abort,abortStage,11091,(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RV,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:11342,Safety,abort,abortStage,11342,23) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.In,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:15673,Safety,abort,aborted,15673,ExecuteContext(SparkBackend.scala:229); 			at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); 			at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); 			at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 			at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 			at java.lang.reflect.Method.invoke(Method.java:498); 			at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 			at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 			at py4j.Gateway.invoke(Gateway.java:282); 			at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 			at py4j.commands.CallCommand.execute(CallCommand.java:79); 			at py4j.GatewayConnection.run(GatewayConnection.java:238); 			at java.lang.Thread.run(Thread.java:748). 	Hail version: 0.2.44-6cfa355a1954; 	Error summary: SparkException: Job aborted due to stage failure: ResultStage 9 (runJob at RVD.scala:688) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18784,Safety,detect,detected,18784,"ecutor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:475,Testability,test,test,475,"This may be an environment specific bug, but we have showed it only occurs with releases after 0.2.42 (in our environment) and only under very specific conditions. When we run `subject_qc` on a matrix table with >= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:722,Testability,Test,Test,722,"This may be an environment specific bug, but we have showed it only occurs with releases after 0.2.42 (in our environment) and only under very specific conditions. When we run `subject_qc` on a matrix table with >= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1096,Testability,Test,Test,1096,"nly under very specific conditions. When we run `subject_qc` on a matrix table with >= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Sta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1145,Testability,log,logs,1145,"nly under very specific conditions. When we run `subject_qc` on a matrix table with >= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Sta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1329,Testability,log,log,1329,"e spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (50000, 998); 	2020-06-10 10:30:10 Hail: INFO: balding_nichols_model: generating genotypes for 1 po",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1362,Testability,log,logging,1362,"ccur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (50000, 998); 	2020-06-10 10:30:10 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 5:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1658,Testability,LOG,LOGGING,1658,"park://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (50000, 998); 	2020-06-10 10:30:10 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 7:=================================> (222 + 80) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1708,Testability,test,test,1708,". Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (50000, 998); 	2020-06-10 10:30:10 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 7:=================================> (222 + 80) / 353]; 	[PASS] with 353 partitions: (50000, 973",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:1752,Testability,log,log,1752," in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (50000, 998); 	2020-06-10 10:30:10 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 5:=====================================================> (344 + 8) / 352]; 	[PASS] with 352 partitions: (50000, 1000); 	2020-06-10 10:30:13 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 7:=================================> (222 + 80) / 353]; 	[PASS] with 353 partitions: (50000, 973); 	2020-06-10 10:30:15 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:18063,Testability,Log,Logs,18063,".RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:19822,Testability,log,log,19822,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:20253,Testability,test,tested,20253,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:20417,Testability,test,tested,20417,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:20509,Testability,test,tested,20509,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/issues/8944:20670,Testability,test,tested,20670,"6926895564544 also had an error][thread 46926900827904 also had an error]. 	SIGSEGV (0xb) at pc=0x00002aaab5115c88, pid=34051, tid=0x00002aae05d1a700; 	#; 	# JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-b08); 	# Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 compressed oops); 	# Problematic frame:; 	[thread 46926929250048 also had an error]# ; 	[thread 46926881888000 also had an error]; 	J 5583 C2 __C111CompiledWithAggs.__m131wrapped(Lis/hail/annotations/Region;J)V (280 bytes) @ 0x00002aaab5115c88 [0x00002aaab5115ae0+0x1a8]; 	#; 	# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; 	#; 	[thread 46924863489792 also had an error]; 	[thread 46924861384448 also had an error]; 	# An error report file with more information is saved as:; 	# /local/scratch/app-20200610100916-0000/0/hs_err_pid34051.log; 	[thread 46926913459968 also had an error]; 	[thread 46924843489024 also had an error][thread 46926917670656 also had an error]. 	#; 	# If you would like to submit a bug report, please visit:; 	# http://bugreport.java.com/bugreport/crash.jsp; 	#. To summarize our observations:; * The issue does not occur when hail is initialized without an existing spark master; * The issue does not occur in HAIL versions prior to 0.2.43 (tested: 0.2.42, 0.2.40, 0.2.38, 0.2.34, 0.2.33 all passed and 0.2.43, 0.2.44 both failed); * The issue occurs consistently when the number of partitions is >= 354 (tested: 500, 450, 400, 360, 354, 1000) and does not occur with lower numbers of partitions (tested: 5, 10, 20, 50, 100, 200, 300, 350, 351, 352, 353); * Changing the number of variants and/or subjects does not appear to change the issue (but we haven't tested that rigorously; increased/decreased by an order of magnitude and observed the same behavior at the same number of partitions); * The issue also occurs on real datasets (large datasets imported from VCF files).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944
https://github.com/hail-is/hail/pull/8947:114,Security,access,access,114,"Currently, this is set inside GCP, but if anyone were to apply this service.yaml file,; it would reset the global access to `false`. This change ensures that the service.yaml; for internal-gateway reflects the actual desired state. This annotation was added in GKE 1.16 which we now have.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8947
https://github.com/hail-is/hail/pull/8948:78,Availability,error,error,78,"Can't really test this easily, unfortunately. CHANGELOG: Fix integer overflow error when reading files >2G with `hl.import_plink`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8948
https://github.com/hail-is/hail/pull/8948:13,Testability,test,test,13,"Can't really test this easily, unfortunately. CHANGELOG: Fix integer overflow error when reading files >2G with `hl.import_plink`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8948
https://github.com/hail-is/hail/pull/8950:188,Integrability,rout,routing,188,"Algolia crawls anchor tags, and treats query strings as distinct urls (since apriori there is no way to know that that doesn't represent a unique page, since query strings can be used for routing). We don't use ?highlight consistently anyway, and it frankly doesn't add anything useful (it highlights what the page already scrolls to).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8950
https://github.com/hail-is/hail/pull/8952:223,Deployability,patch,patches,223,"This PR:. - Switches the unused `ENDArray` to be called `EColumnMajorNDArray`, always writes out a column major thing, never writes out strides. - Makes the `fundamentalType` of `PNDArray` be `PNDArray`, and makes required patches throughout code base to facilitate this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8952
https://github.com/hail-is/hail/pull/8955:259,Performance,perform,performance,259,"cc @mkveerapen @tpoterba . 1) Adds natural language documentation search bar with autocompletion; 2) Adds paginated search page to browse all search results; 3) Makes navbar scripts async, to allow inclusion of navbar scripts at top of file without impacting performance (since we include the navbar.html content in other templates we cannot easily locate those scripts at bottom of the includers' pages); 4) Increases hero content width on mobile.; 5) Hides sphinx search bar. http://34.207.246.132/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8955
https://github.com/hail-is/hail/pull/8957:311,Testability,test,tests,311,"Summary:; - remove debug statement (?) in PartitionIteratorLongReader; - LowerTableIR needs to truncate the partitioner when truncating the keys. Otherwise, the partitioner will refer to rows that can be modified elsewhere.; - Added a missing ToStream in TableExplode lowering. Unfortunately, the local backend tests are now segfaulting. How's the debugging allocator fix coming @tpoterba @johnc1231?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8957
https://github.com/hail-is/hail/pull/8958:605,Availability,recover,recover,605,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958
https://github.com/hail-is/hail/pull/8958:39,Energy Efficiency,allocate,allocated,39,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958
https://github.com/hail-is/hail/pull/8958:874,Integrability,wrap,wrapping,874,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958
https://github.com/hail-is/hail/pull/8958:605,Safety,recover,recover,605,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958
https://github.com/hail-is/hail/pull/8958:281,Usability,simpl,simplification,281,"Due to bytecode verification rules, an allocated but uninitalized object cannot be stored into a field, so the NEW and INVOKESPECIAL constructor call bytecodes cannot be split across methods. Therefore, I modified newInstance to fuse those operations together. I broke out control simplification and made it a stronger. Added method splitting. Currently, method splitting splits out basic blocks into their own, straight-line methods and all the control flow remains in the original method. All locals are spilled to fields which is terrible, but what we're doing now. I expect two changes in the future: recover the structured control flow (there are standard algorithms for this) so we can split out control flow, and use the dataflow analysis from InitializeLocals to only spill locals split across method boundaries. I will make a stacked PR on this that removes method wrapping from Emit and enables lir method splitting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958
https://github.com/hail-is/hail/pull/8960:302,Deployability,deploy,deploys,302,"This PR only adds support in the worker and exposes the functionality in the batch client. Another PR will be added to make these changes user facing in the user version of batch. I added the WIP tag because I want to make sure the test passes once before I comment it out. That's because when this PR deploys, the workers will still be the old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960
https://github.com/hail-is/hail/pull/8960:44,Security,expose,exposes,44,"This PR only adds support in the worker and exposes the functionality in the batch client. Another PR will be added to make these changes user facing in the user version of batch. I added the WIP tag because I want to make sure the test passes once before I comment it out. That's because when this PR deploys, the workers will still be the old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960
https://github.com/hail-is/hail/pull/8960:232,Testability,test,test,232,"This PR only adds support in the worker and exposes the functionality in the batch client. Another PR will be added to make these changes user facing in the user version of batch. I added the WIP tag because I want to make sure the test passes once before I comment it out. That's because when this PR deploys, the workers will still be the old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960
https://github.com/hail-is/hail/pull/8963:38,Integrability,wrap,wrapping,38,"Summary of changes:; - rip out method wrapping logic in ir.Emit; - add lir.{Blocks, Locals} for enumerating and indexing blocks and locals; - add SplitLargeBlocks to break up large blocks (the minimum unit of splitting in SplitMethod is the block); - add PST to compute our non-standard variant of the program structure tree (see the comment in PST); - add SplitMethod to split large methods based on the PST",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963
https://github.com/hail-is/hail/pull/8963:47,Testability,log,logic,47,"Summary of changes:; - rip out method wrapping logic in ir.Emit; - add lir.{Blocks, Locals} for enumerating and indexing blocks and locals; - add SplitLargeBlocks to break up large blocks (the minimum unit of splitting in SplitMethod is the block); - add PST to compute our non-standard variant of the program structure tree (see the comment in PST); - add SplitMethod to split large methods based on the PST",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963
https://github.com/hail-is/hail/pull/8970:15,Safety,safe,safe,15,"Working on the safe memory allocator that checks if we are accessing invalid memory, it's catching this 0 pointer. . There are already tests that hit `TableWriter`, and this should result in no functionality change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970
https://github.com/hail-is/hail/pull/8970:59,Security,access,accessing,59,"Working on the safe memory allocator that checks if we are accessing invalid memory, it's catching this 0 pointer. . There are already tests that hit `TableWriter`, and this should result in no functionality change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970
https://github.com/hail-is/hail/pull/8970:135,Testability,test,tests,135,"Working on the safe memory allocator that checks if we are accessing invalid memory, it's catching this 0 pointer. . There are already tests that hit `TableWriter`, and this should result in no functionality change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970
https://github.com/hail-is/hail/pull/8975:42,Availability,ERROR,ERROR,42,"I'm seeing this in the driver logs:. ```; ERROR 2020-06-16 23:37:18,446 in event loop Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 500, in event_loop; await self.handle_event(event); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 430, in handle_event; timestamp = event.timestamp.timestamp() * 1000; AttributeError: 'dict' object has no attribute 'timestamp'; ```. `event['timestamp']` is in RFC3339 Zulu format with nanosecond precision, for example: 2020-06-08T16:49:53.374657381Z, see: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry. There is no native RFC3339 Python parser. RFC3339 is nearly identicaly to ISO 8601, except maybe some timezone differences which aren't relevant in Zulu format, see: https://en.wikipedia.org/wiki/ISO_8601. There isn't a native Python ISO 8601 parser. dateutil.parser.isoparse is a ISO 8601 parser (and is maybe also supports RFC3339? I can't quite tell.). Note, Python datetime only has microsecond accuracy, but that's fine because we only store millisecond accuracy. Time is the worst.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8975
https://github.com/hail-is/hail/pull/8975:30,Testability,log,logs,30,"I'm seeing this in the driver logs:. ```; ERROR 2020-06-16 23:37:18,446 in event loop Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 500, in event_loop; await self.handle_event(event); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 430, in handle_event; timestamp = event.timestamp.timestamp() * 1000; AttributeError: 'dict' object has no attribute 'timestamp'; ```. `event['timestamp']` is in RFC3339 Zulu format with nanosecond precision, for example: 2020-06-08T16:49:53.374657381Z, see: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry. There is no native RFC3339 Python parser. RFC3339 is nearly identicaly to ISO 8601, except maybe some timezone differences which aren't relevant in Zulu format, see: https://en.wikipedia.org/wiki/ISO_8601. There isn't a native Python ISO 8601 parser. dateutil.parser.isoparse is a ISO 8601 parser (and is maybe also supports RFC3339? I can't quite tell.). Note, Python datetime only has microsecond accuracy, but that's fine because we only store millisecond accuracy. Time is the worst.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8975
https://github.com/hail-is/hail/pull/8975:622,Testability,log,logging,622,"I'm seeing this in the driver logs:. ```; ERROR 2020-06-16 23:37:18,446 in event loop Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 500, in event_loop; await self.handle_event(event); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 430, in handle_event; timestamp = event.timestamp.timestamp() * 1000; AttributeError: 'dict' object has no attribute 'timestamp'; ```. `event['timestamp']` is in RFC3339 Zulu format with nanosecond precision, for example: 2020-06-08T16:49:53.374657381Z, see: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry. There is no native RFC3339 Python parser. RFC3339 is nearly identicaly to ISO 8601, except maybe some timezone differences which aren't relevant in Zulu format, see: https://en.wikipedia.org/wiki/ISO_8601. There isn't a native Python ISO 8601 parser. dateutil.parser.isoparse is a ISO 8601 parser (and is maybe also supports RFC3339? I can't quite tell.). Note, Python datetime only has microsecond accuracy, but that's fine because we only store millisecond accuracy. Time is the worst.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8975
https://github.com/hail-is/hail/pull/8975:656,Testability,Log,LogEntry,656,"I'm seeing this in the driver logs:. ```; ERROR 2020-06-16 23:37:18,446 in event loop Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 500, in event_loop; await self.handle_event(event); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/instance_pool.py"", line 430, in handle_event; timestamp = event.timestamp.timestamp() * 1000; AttributeError: 'dict' object has no attribute 'timestamp'; ```. `event['timestamp']` is in RFC3339 Zulu format with nanosecond precision, for example: 2020-06-08T16:49:53.374657381Z, see: https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry. There is no native RFC3339 Python parser. RFC3339 is nearly identicaly to ISO 8601, except maybe some timezone differences which aren't relevant in Zulu format, see: https://en.wikipedia.org/wiki/ISO_8601. There isn't a native Python ISO 8601 parser. dateutil.parser.isoparse is a ISO 8601 parser (and is maybe also supports RFC3339? I can't quite tell.). Note, Python datetime only has microsecond accuracy, but that's fine because we only store millisecond accuracy. Time is the worst.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8975
https://github.com/hail-is/hail/pull/8979:84,Safety,avoid,avoid,84,"- Also moved the location of where buckets are mounted to not be in /batch so as to avoid accidentally deleting entire buckets.; - The file mode didn't do what I expected (allowed you to write to a bucket), but now that I think about it, we probably do want to expose this and my first intuition was right. We probably want files to be specified as read only when they're created on the local file system. I can make this a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8979
https://github.com/hail-is/hail/pull/8979:261,Security,expose,expose,261,"- Also moved the location of where buckets are mounted to not be in /batch so as to avoid accidentally deleting entire buckets.; - The file mode didn't do what I expected (allowed you to write to a bucket), but now that I think about it, we probably do want to expose this and my first intuition was right. We probably want files to be specified as read only when they're created on the local file system. I can make this a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8979
https://github.com/hail-is/hail/pull/8979:286,Usability,intuit,intuition,286,"- Also moved the location of where buckets are mounted to not be in /batch so as to avoid accidentally deleting entire buckets.; - The file mode didn't do what I expected (allowed you to write to a bucket), but now that I think about it, we probably do want to expose this and my first intuition was right. We probably want files to be specified as read only when they're created on the local file system. I can make this a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8979
https://github.com/hail-is/hail/pull/8987:208,Availability,error,errors,208,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8987
https://github.com/hail-is/hail/pull/8987:307,Availability,error,errors,307,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8987
https://github.com/hail-is/hail/pull/8987:126,Performance,cache,cache,126,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8987
https://github.com/hail-is/hail/pull/8987:67,Safety,unsafe,unsafe,67,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8987
https://github.com/hail-is/hail/pull/8987:88,Safety,unsafe,unsafe,88,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8987
https://github.com/hail-is/hail/pull/8987:177,Usability,clear,cleared,177,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8987
https://github.com/hail-is/hail/pull/8991:366,Modifiability,refactor,refactoring,366,"This pull request lowers the TableAggregate node. It also reimplements; the AggStateValue and CombOpValue nodes. However, this is not the final design -- I think that there should also; be an InitFromSerializedValue node that is used to prevent us from; initializing twice (and inlining aggs.init in the lowering). feel free to push back on merging this before that refactoring, but it works and I have a preference for making that change in a followup. After that, I'll move on to lowering TableMapRows with scans, as well as designing the pass to lift out nodes as relational lets that we discussed. Woohoo!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8991
https://github.com/hail-is/hail/pull/8992:103,Deployability,install,install,103,"On views narrower than the height of the graph image + 100px, the slide box will overflow the adjacent install section. Fixed using inline style to avoid css caching issues until the build process is fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8992
https://github.com/hail-is/hail/pull/8992:148,Safety,avoid,avoid,148,"On views narrower than the height of the graph image + 100px, the slide box will overflow the adjacent install section. Fixed using inline style to avoid css caching issues until the build process is fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8992
https://github.com/hail-is/hail/pull/8999:104,Energy Efficiency,power,powered,104,"A few links to references.md were broken when website files were moved in #8923. This changes the ""Hail-powered science"" link in README.md to point to the references page on the website instead of the references.md file in GitHub and fixes the ""editing this page directly"" link on https://hail.is/references.html.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8999
https://github.com/hail-is/hail/pull/9000:118,Usability,Simpl,SimplifyControl,118,I need this for upcoming improvements to SplitMethod. Summary:; - add Block.replace; - have Block track uses; - makes SimplifyControl simpler; - Method.findBlocks prunes dead uses,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9000
https://github.com/hail-is/hail/pull/9000:134,Usability,simpl,simpler,134,I need this for upcoming improvements to SplitMethod. Summary:; - add Block.replace; - have Block track uses; - makes SimplifyControl simpler; - Method.findBlocks prunes dead uses,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9000
https://github.com/hail-is/hail/pull/9001:42,Availability,resilien,resilient,42,Just make all the FS streams double close resilient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9001
https://github.com/hail-is/hail/pull/9005:180,Deployability,update,updates,180,Adds the tutorial page (http://34.207.246.132/tutorial.html) and Learn More button bottom-right justified on home page. cc @mkveerapen includes some further content changes. minor updates to style (mostly width) found in the subsequent pr: #9007,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9005
https://github.com/hail-is/hail/pull/9005:65,Usability,Learn,Learn,65,Adds the tutorial page (http://34.207.246.132/tutorial.html) and Learn More button bottom-right justified on home page. cc @mkveerapen includes some further content changes. minor updates to style (mostly width) found in the subsequent pr: #9007,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9005
https://github.com/hail-is/hail/pull/9006:308,Usability,clear,clear,308,"CHANGELOG: Fix memory leak in ExportBgen. If you look a dozen or so lines under the `boundary` I added, you'll see a :. ```; it.foreach { ptr =>; val (b, d) = bpw.emitVariant(ptr); out.write(b); dropped += d; }; ```. So that thing is stepping through the iterator without freeing. The `boundary` is to do a `clear` after each read from the `it`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9006
https://github.com/hail-is/hail/pull/9011:19,Testability,test,test,19,"I've not written a test for this because we use this pattern a lot, and we haven't been writing explicit test cases for all of our simplify rules, but let me know if you want one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9011
https://github.com/hail-is/hail/pull/9011:105,Testability,test,test,105,"I've not written a test for this because we use this pattern a lot, and we haven't been writing explicit test cases for all of our simplify rules, but let me know if you want one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9011
https://github.com/hail-is/hail/pull/9011:131,Usability,simpl,simplify,131,"I've not written a test for this because we use this pattern a lot, and we haven't been writing explicit test cases for all of our simplify rules, but let me know if you want one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9011
https://github.com/hail-is/hail/pull/9013:3,Usability,Learn,Learn,3,"1) Learn More button shouldn't have target blank; it's a link to a page in the same domain, better to allow forward/back nav and no new window; 2) Reference page text would overflow because default word-break is none.; 3) Height of blue background to title was constrained to a certain height, and while that helped consistency, it cost inner margin (padding) consistency; tradeoff not worth it; 4) Fix code block alignments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9013
https://github.com/hail-is/hail/issues/9016:4530,Availability,Error,Error,4530,"1095>"", line 2, in take; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 2087, in take; return self.head(n).collect(_localize); File ""<decorator-gen-1089>"", line 2, in collect; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1886, in collect; return Env.backend().execute(e._ir); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/Users/konradk/programs/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchElementException: key not found: 1; [...]; java.util.NoSuchElementException: key not found: 1; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.MapLike$class.apply(MapLike.scala:141); at scala.collection.AbstractMap.apply(Map.scala:59); at is.hail.types.encoded.EBaseStruct.fieldType(EBaseStruct.scala:34); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:84); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:83); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.Traver",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9016
https://github.com/hail-is/hail/issues/9016:3641,Integrability,wrap,wrapper,3641,"on.framework/Versions/3.6/lib/python3.6/site-packages/IPython/lib/pretty.py"", line 394, in pretty; return _repr_pprint(obj, self, cycle); File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/lib/pretty.py"", line 700, in _repr_pprint; output = repr(obj); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1269, in __repr__; return self.__str__(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1266, in __str__; return self._ascii_str(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1292, in _ascii_str; rows, has_more, dtype = self.data(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1276, in data; rows, has_more = t._take_n(self.n); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1423, in _take_n; rows = self.take(n + 1); File ""<decorator-gen-1095>"", line 2, in take; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 2087, in take; return self.head(n).collect(_localize); File ""<decorator-gen-1089>"", line 2, in collect; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1886, in collect; return Env.backend().execute(e._ir); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/Users/konradk/programs/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchElementException: key not found: 1; [...]; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9016
https://github.com/hail-is/hail/issues/9016:3933,Integrability,wrap,wrapper,3933,"repr(obj); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1269, in __repr__; return self.__str__(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1266, in __str__; return self._ascii_str(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1292, in _ascii_str; rows, has_more, dtype = self.data(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1276, in data; rows, has_more = t._take_n(self.n); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1423, in _take_n; rows = self.take(n + 1); File ""<decorator-gen-1095>"", line 2, in take; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 2087, in take; return self.head(n).collect(_localize); File ""<decorator-gen-1089>"", line 2, in collect; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1886, in collect; return Env.backend().execute(e._ir); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/Users/konradk/programs/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchElementException: key not found: 1; [...]; java.util.NoSuchElementException: key not found: 1; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.MapLike$class.apply(MapLike.scala:141); at scala.collection.AbstractMap.apply(Map.scala:59); at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9016
https://github.com/hail-is/hail/issues/9016:6228,Integrability,Wrap,WrappedArray,6228,ruct.scala:83); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.types.encoded.EBaseStruct._decodedPType(EBaseStruct.scala:83); at is.hail.types.encoded.EType.decodedPType(EType.scala:159); at is.hail.types.encoded.EBaseStruct$$anonfun$7.apply(EBaseStruct.scala:78); at is.hail.types.encoded.EBaseStruct$$anonfun$7.apply(EBaseStruct.scala:77); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.types.encoded.EBaseStruct._decodedPType(EBaseStruct.scala:77); at is.hail.types.encoded.EType.decodedPType(EType.scala:159); at is.hail.types.encoded.EType$.buildDecoder(EType.scala:255); at is.hail.types.encoded.EType.buildDecoder(EType.scala:36); at is.hail.io.TypedCodecSpec.buildDecoder(TypedCodecSpec.scala:41); at is.hail.expr.ir.TableParallelize.execute(TableIR.scala:784); at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:835); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1432); at is.hail.expr.ir.TableOrderBy.execute(TableIR.scala:2214); at is.hail.expr.ir.TableSubset$class.execute(TableIR.scala:950); at is.hail.expr.ir.TableHead.execute(TableIR.scala:958); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1432); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9016
https://github.com/hail-is/hail/issues/9016:6249,Integrability,Wrap,WrappedArray,6249,ruct.scala:83); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.types.encoded.EBaseStruct._decodedPType(EBaseStruct.scala:83); at is.hail.types.encoded.EType.decodedPType(EType.scala:159); at is.hail.types.encoded.EBaseStruct$$anonfun$7.apply(EBaseStruct.scala:78); at is.hail.types.encoded.EBaseStruct$$anonfun$7.apply(EBaseStruct.scala:77); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.types.encoded.EBaseStruct._decodedPType(EBaseStruct.scala:77); at is.hail.types.encoded.EType.decodedPType(EType.scala:159); at is.hail.types.encoded.EType$.buildDecoder(EType.scala:255); at is.hail.types.encoded.EType.buildDecoder(EType.scala:36); at is.hail.io.TypedCodecSpec.buildDecoder(TypedCodecSpec.scala:41); at is.hail.expr.ir.TableParallelize.execute(TableIR.scala:784); at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:835); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1432); at is.hail.expr.ir.TableOrderBy.execute(TableIR.scala:2214); at is.hail.expr.ir.TableSubset$class.execute(TableIR.scala:950); at is.hail.expr.ir.TableHead.execute(TableIR.scala:958); at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1432); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9016
https://github.com/hail-is/hail/issues/9016:4205,Performance,load,loads,4205,"hail/hail/python/hail/table.py"", line 1292, in _ascii_str; rows, has_more, dtype = self.data(); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1276, in data; rows, has_more = t._take_n(self.n); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1423, in _take_n; rows = self.take(n + 1); File ""<decorator-gen-1095>"", line 2, in take; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 2087, in take; return self.head(n).collect(_localize); File ""<decorator-gen-1089>"", line 2, in collect; File ""/Users/konradk/hail/hail/python/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/konradk/hail/hail/python/hail/table.py"", line 1886, in collect; return Env.backend().execute(e._ir); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/Users/konradk/programs/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/hail/hail/python/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: NoSuchElementException: key not found: 1; [...]; java.util.NoSuchElementException: key not found: 1; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.MapLike$class.apply(MapLike.scala:141); at scala.collection.AbstractMap.apply(Map.scala:59); at is.hail.types.encoded.EBaseStruct.fieldType(EBaseStruct.scala:34); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:84); at is.hail.types.encoded.EBaseStruct$$anonfun$8.apply(EBaseStruct.scala:83); at scala.collection.Tr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9016
https://github.com/hail-is/hail/pull/9017:19,Deployability,install,installation,19,"I find the current installation docs totally overwhelming. If you are using Hail; on a Mac you should not see any unnecessary crap about Linux, clusters, and; BLAS. This change introduces four flows: mac, linux, dataproc, cluster. Each page's; complexity matches the true complexity of installing Hail on that platform. In; particular, note how simple the Linux and Mac OS X pages are. It also clears up the ""other cluster"" case. Our current docs are too complex and; don't push people towards simple solutions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9017
https://github.com/hail-is/hail/pull/9017:286,Deployability,install,installing,286,"I find the current installation docs totally overwhelming. If you are using Hail; on a Mac you should not see any unnecessary crap about Linux, clusters, and; BLAS. This change introduces four flows: mac, linux, dataproc, cluster. Each page's; complexity matches the true complexity of installing Hail on that platform. In; particular, note how simple the Linux and Mac OS X pages are. It also clears up the ""other cluster"" case. Our current docs are too complex and; don't push people towards simple solutions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9017
https://github.com/hail-is/hail/pull/9017:345,Usability,simpl,simple,345,"I find the current installation docs totally overwhelming. If you are using Hail; on a Mac you should not see any unnecessary crap about Linux, clusters, and; BLAS. This change introduces four flows: mac, linux, dataproc, cluster. Each page's; complexity matches the true complexity of installing Hail on that platform. In; particular, note how simple the Linux and Mac OS X pages are. It also clears up the ""other cluster"" case. Our current docs are too complex and; don't push people towards simple solutions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9017
https://github.com/hail-is/hail/pull/9017:394,Usability,clear,clears,394,"I find the current installation docs totally overwhelming. If you are using Hail; on a Mac you should not see any unnecessary crap about Linux, clusters, and; BLAS. This change introduces four flows: mac, linux, dataproc, cluster. Each page's; complexity matches the true complexity of installing Hail on that platform. In; particular, note how simple the Linux and Mac OS X pages are. It also clears up the ""other cluster"" case. Our current docs are too complex and; don't push people towards simple solutions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9017
https://github.com/hail-is/hail/pull/9017:494,Usability,simpl,simple,494,"I find the current installation docs totally overwhelming. If you are using Hail; on a Mac you should not see any unnecessary crap about Linux, clusters, and; BLAS. This change introduces four flows: mac, linux, dataproc, cluster. Each page's; complexity matches the true complexity of installing Hail on that platform. In; particular, note how simple the Linux and Mac OS X pages are. It also clears up the ""other cluster"" case. Our current docs are too complex and; don't push people towards simple solutions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9017
https://github.com/hail-is/hail/pull/9022:90,Availability,error,error,90,A KeyError's __str__ is just the key that failed. This prints the traceback and the full; error message. It is far more useful.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9022
https://github.com/hail-is/hail/pull/9022:96,Integrability,message,message,96,A KeyError's __str__ is just the key that failed. This prints the traceback and the full; error message. It is far more useful.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9022
https://github.com/hail-is/hail/pull/9028:259,Availability,down,down,259,"The previous implementation, while seeming to be well-abstracted at; first, actually had a rather devious property of creating agg states; for multiple classes multiple times. I'm still working on figuring out; *exactly* the place where our assumptions broke down, but this change; definitely fixes the problem, and simplifies the implementation by; directly using IR, instead of other compiled functions. This problem was a symptom of a larger issue, which is that the; ownership semantics of the current aggregator system is way too complex; to be coding against regularly. This all will go away when lowering is; complete, in favor of the *much* simpler set of IR nodes that are used; in lowering. We may need to address this problem sooner, though. CHANGELOG: Fixed memory leak affecting `Table.annotate` with scans, `hl.experimental.densify`, and `Table.group_by` / `aggregate`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9028
https://github.com/hail-is/hail/pull/9028:316,Usability,simpl,simplifies,316,"The previous implementation, while seeming to be well-abstracted at; first, actually had a rather devious property of creating agg states; for multiple classes multiple times. I'm still working on figuring out; *exactly* the place where our assumptions broke down, but this change; definitely fixes the problem, and simplifies the implementation by; directly using IR, instead of other compiled functions. This problem was a symptom of a larger issue, which is that the; ownership semantics of the current aggregator system is way too complex; to be coding against regularly. This all will go away when lowering is; complete, in favor of the *much* simpler set of IR nodes that are used; in lowering. We may need to address this problem sooner, though. CHANGELOG: Fixed memory leak affecting `Table.annotate` with scans, `hl.experimental.densify`, and `Table.group_by` / `aggregate`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9028
https://github.com/hail-is/hail/pull/9028:649,Usability,simpl,simpler,649,"The previous implementation, while seeming to be well-abstracted at; first, actually had a rather devious property of creating agg states; for multiple classes multiple times. I'm still working on figuring out; *exactly* the place where our assumptions broke down, but this change; definitely fixes the problem, and simplifies the implementation by; directly using IR, instead of other compiled functions. This problem was a symptom of a larger issue, which is that the; ownership semantics of the current aggregator system is way too complex; to be coding against regularly. This all will go away when lowering is; complete, in favor of the *much* simpler set of IR nodes that are used; in lowering. We may need to address this problem sooner, though. CHANGELOG: Fixed memory leak affecting `Table.annotate` with scans, `hl.experimental.densify`, and `Table.group_by` / `aggregate`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9028
https://github.com/hail-is/hail/pull/9032:9,Performance,load,loadField,9,"Struct's loadField has been changed to load the address directly in the; case of the field being a collection type. PCanonicalIntervalValue was; using loadStart and loadEnd rather than startOffset and endOffset which; caused addresses to be loaded twice in the case of intervals of; collections, leading to segfaults. This code is currently unused, but will be used in EmitCodeOrdering #8725",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9032
https://github.com/hail-is/hail/pull/9032:39,Performance,load,load,39,"Struct's loadField has been changed to load the address directly in the; case of the field being a collection type. PCanonicalIntervalValue was; using loadStart and loadEnd rather than startOffset and endOffset which; caused addresses to be loaded twice in the case of intervals of; collections, leading to segfaults. This code is currently unused, but will be used in EmitCodeOrdering #8725",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9032
https://github.com/hail-is/hail/pull/9032:151,Performance,load,loadStart,151,"Struct's loadField has been changed to load the address directly in the; case of the field being a collection type. PCanonicalIntervalValue was; using loadStart and loadEnd rather than startOffset and endOffset which; caused addresses to be loaded twice in the case of intervals of; collections, leading to segfaults. This code is currently unused, but will be used in EmitCodeOrdering #8725",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9032
https://github.com/hail-is/hail/pull/9032:165,Performance,load,loadEnd,165,"Struct's loadField has been changed to load the address directly in the; case of the field being a collection type. PCanonicalIntervalValue was; using loadStart and loadEnd rather than startOffset and endOffset which; caused addresses to be loaded twice in the case of intervals of; collections, leading to segfaults. This code is currently unused, but will be used in EmitCodeOrdering #8725",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9032
https://github.com/hail-is/hail/pull/9032:241,Performance,load,loaded,241,"Struct's loadField has been changed to load the address directly in the; case of the field being a collection type. PCanonicalIntervalValue was; using loadStart and loadEnd rather than startOffset and endOffset which; caused addresses to be loaded twice in the case of intervals of; collections, leading to segfaults. This code is currently unused, but will be used in EmitCodeOrdering #8725",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9032
https://github.com/hail-is/hail/pull/9033:200,Integrability,interface,interface,200,"This PR adds stream nodes `StreamMultiMerge` and `StreamZipJoin`, which will be used to implement `TableUnion` and `TableMultiWayZipJoin`. The two nodes were so similar, both in implementation and in interface, that I thought bundling them into one PR would actually make it *easier* to review, but I can split them up if you disagree. The implementations of both nodes use tournament trees, a very simple data structure ideal for this problem. Think of it as a priority queue specialized to holding exactly `k` elements, so when you pop the top element, you must immediately replace it with a new value. A tournament tree is just what it sounds like. It is a complete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9033
https://github.com/hail-is/hail/pull/9033:1718,Modifiability,variab,variable,1718,"lete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, where each element is a pointer to the head of the corresponding stream, and I store the `k-1` internal nodes in a `Array[Int]`, in the usual breadth-first order, where each element is the index of the stream which lost the comparison at that node (had the larger value). I use an index of `-1` to represent an imaginary element smaller than all real elements, and similarly an index of `k` is larger than all real elements. The tournament tree begins filled with `-1`, and each stream is advanced once, as their values push out all the `-1`s. When a steam ends, it's leaf is given the value `k`, and once the overall winner is `k`, we know all streams have ended.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9033
https://github.com/hail-is/hail/pull/9033:471,Performance,queue,queue,471,"This PR adds stream nodes `StreamMultiMerge` and `StreamZipJoin`, which will be used to implement `TableUnion` and `TableMultiWayZipJoin`. The two nodes were so similar, both in implementation and in interface, that I thought bundling them into one PR would actually make it *easier* to review, but I can split them up if you disagree. The implementations of both nodes use tournament trees, a very simple data structure ideal for this problem. Think of it as a priority queue specialized to holding exactly `k` elements, so when you pop the top element, you must immediately replace it with a new value. A tournament tree is just what it sounds like. It is a complete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9033
https://github.com/hail-is/hail/pull/9033:399,Usability,simpl,simple,399,"This PR adds stream nodes `StreamMultiMerge` and `StreamZipJoin`, which will be used to implement `TableUnion` and `TableMultiWayZipJoin`. The two nodes were so similar, both in implementation and in interface, that I thought bundling them into one PR would actually make it *easier* to review, but I can split them up if you disagree. The implementations of both nodes use tournament trees, a very simple data structure ideal for this problem. Think of it as a priority queue specialized to holding exactly `k` elements, so when you pop the top element, you must immediately replace it with a new value. A tournament tree is just what it sounds like. It is a complete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9033
https://github.com/hail-is/hail/pull/9034:716,Security,access,accesses,716,"`hl.experimental.import_gtf` is documented as accepting either a string or a `ReferenceGenome` object for its `reference_genome` argument. However, it actually only works correctly with a string because it compares the `reference_genome` argument to the string ""GRCh37"" and calls `hl.get_reference` with the `reference_genome` argument.; https://github.com/hail-is/hail/blob/c9b2ddfa92d619d0e4e22a169157853dc391f29a/hail/python/hail/experimental/import_gtf.py#L162-L173. This changes `import_gtf` to accept a string or `ReferenceGenome` by using `typecheck` to cast the argument into a `ReferenceGenome`. This also corrects the docstring for `_load_gencode_gtf`, which only works with a `ReferenceGenome` because it accesses the `name` property of its `reference_genome` argument.; https://github.com/hail-is/hail/blob/c9b2ddfa92d619d0e4e22a169157853dc391f29a/hail/python/hail/experimental/import_gtf.py#L270-L273",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9034
https://github.com/hail-is/hail/pull/9035:369,Integrability,interface,interface,369,I can break this up further if you want. Big changes:. - change batch.py to support multi-line commands (use `{\n...\n}`); - change batch.py and job.py to support per-job environment variables (and add tests to test_batch.py); - add `partition` to hail top mirroring the implementation in Scala; - implement BatchPoolExecutor which attempts to faithfully implement the interface of concurrent.futures.Executor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9035
https://github.com/hail-is/hail/pull/9035:183,Modifiability,variab,variables,183,I can break this up further if you want. Big changes:. - change batch.py to support multi-line commands (use `{\n...\n}`); - change batch.py and job.py to support per-job environment variables (and add tests to test_batch.py); - add `partition` to hail top mirroring the implementation in Scala; - implement BatchPoolExecutor which attempts to faithfully implement the interface of concurrent.futures.Executor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9035
https://github.com/hail-is/hail/pull/9035:382,Performance,concurren,concurrent,382,I can break this up further if you want. Big changes:. - change batch.py to support multi-line commands (use `{\n...\n}`); - change batch.py and job.py to support per-job environment variables (and add tests to test_batch.py); - add `partition` to hail top mirroring the implementation in Scala; - implement BatchPoolExecutor which attempts to faithfully implement the interface of concurrent.futures.Executor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9035
https://github.com/hail-is/hail/pull/9035:202,Testability,test,tests,202,I can break this up further if you want. Big changes:. - change batch.py to support multi-line commands (use `{\n...\n}`); - change batch.py and job.py to support per-job environment variables (and add tests to test_batch.py); - add `partition` to hail top mirroring the implementation in Scala; - implement BatchPoolExecutor which attempts to faithfully implement the interface of concurrent.futures.Executor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9035
https://github.com/hail-is/hail/pull/9044:4,Integrability,interface,interfaces,4,The interfaces are `newStaticField` and `newStaticMethod` on (Emit)ClassBuilder. The first; usage of this feature is to move the `FS` off of the class itself and onto a container class.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044
https://github.com/hail-is/hail/issues/9050:83,Availability,error,error,83,"I accidentally passed a list instead of a string as the hb.Batch name and got this error; ```; Traceback (most recent call last):; File ""outrider_batch_pipeline.py"", line 216, in <module>; main(); File ""outrider_batch_pipeline.py"", line 212, in main; logger.info(f""Output: {output_file}""); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py"", line 119, in __exit__; next(self.gen); File ""/Users/weisburd/code/methods/batch/batch_utils.py"", line 66, in run_batch; batch.run(dry_run=args.dry_run, verbose=args.verbose); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/batch.py"", line 423, in run; return self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 435, in _run; bc_batch = bc_batch.submit(disable_progress_bar=disable_progress_bar); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 167, in submit; async_batch = async_to_blocking(self._async_builder.submit(*args, **kwargs)); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/lib/python3.7/site-packages/nest_asyncio.py"", line 63, in run_until_complete; return self._run_until_complete_orig(future); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/aioclient.py"", line 492, in submit; batch = aw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9050
https://github.com/hail-is/hail/issues/9050:2937,Integrability,message,message,2937,"cking(self._async_builder.submit(*args, **kwargs)); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/lib/python3.7/site-packages/nest_asyncio.py"", line 63, in run_until_complete; return self._run_until_complete_orig(future); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/aioclient.py"", line 492, in submit; batch = await self._create(); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/aioclient.py"", line 478, in _create; json=batch_spec)).json(); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/aioclient.py"", line 576, in _post; self.url + path, data=data, json=json, headers=self._headers); File ""/usr/local/lib/python3.7/site-packages/hailtop/utils/utils.py"", line 349, in request_retry_transient_errors; return await retry_transient_errors(session.request, method, url, **kwargs); File ""/usr/local/lib/python3.7/site-packages/hailtop/utils/utils.py"", line 321, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/site-packages/aiohttp/client.py"", line 588, in _request; resp.raise_for_status(); File ""/usr/local/lib/python3.7/site-packages/aiohttp/client_reqrep.py"", line 946, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 400, message='batch.attributes has non-str value', url='https://batch.hail.is/api/v1alpha/batches/create. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9050
https://github.com/hail-is/hail/issues/9050:251,Testability,log,logger,251,"I accidentally passed a list instead of a string as the hb.Batch name and got this error; ```; Traceback (most recent call last):; File ""outrider_batch_pipeline.py"", line 216, in <module>; main(); File ""outrider_batch_pipeline.py"", line 212, in main; logger.info(f""Output: {output_file}""); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py"", line 119, in __exit__; next(self.gen); File ""/Users/weisburd/code/methods/batch/batch_utils.py"", line 66, in run_batch; batch.run(dry_run=args.dry_run, verbose=args.verbose); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/batch.py"", line 423, in run; return self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch/backend.py"", line 435, in _run; bc_batch = bc_batch.submit(disable_progress_bar=disable_progress_bar); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 167, in submit; async_batch = async_to_blocking(self._async_builder.submit(*args, **kwargs)); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/lib/python3.7/site-packages/nest_asyncio.py"", line 63, in run_until_complete; return self._run_until_complete_orig(future); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/hailtop/batch_client/aioclient.py"", line 492, in submit; batch = aw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9050
https://github.com/hail-is/hail/issues/9051:113,Availability,error,error,113,"```; (b'', b""+ mkdir -p repos/hail-is/hail; + cd repos/hail-is/hail; + '[' '!' -d .git ']'; + git reset --merge; error: Entry 'hail/src/main/scala/is/hail/HailContext.scala' not uptodate. Cannot merge.; fatal: Could not reset index file to revision 'HEAD'.; ""); ```; https://ci.hail.is/watched_branches/0/pr/9048",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9051
https://github.com/hail-is/hail/pull/9054:495,Availability,error,error,495,"TableIRSuite extensively uses the function; ```; def collect(tir: TableIR): TableCollect = TableCollect(TableKeyBy(tir, FastIndexedSeq())); ```; to compare the result of a `TableIR` with the expected collection. But `TableCollect` makes no promises what order the results will be in, and in particular the optimizer is allowed to remove that `TableKeyBy`. This PR redefines that function to use the collect aggregator, which does promise the order rows are collected. It also fixes a small type error in the `TableJoin` lowering case that was uncovered.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9054
https://github.com/hail-is/hail/pull/9054:306,Performance,optimiz,optimizer,306,"TableIRSuite extensively uses the function; ```; def collect(tir: TableIR): TableCollect = TableCollect(TableKeyBy(tir, FastIndexedSeq())); ```; to compare the result of a `TableIR` with the expected collection. But `TableCollect` makes no promises what order the results will be in, and in particular the optimizer is allowed to remove that `TableKeyBy`. This PR redefines that function to use the collect aggregator, which does promise the order rows are collected. It also fixes a small type error in the `TableJoin` lowering case that was uncovered.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9054
https://github.com/hail-is/hail/issues/9059:1039,Availability,down,download,1039,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/issues/9059:342,Deployability,integrat,integrated,342,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/issues/9059:848,Deployability,configurat,configuration,848,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/issues/9059:1132,Deployability,configurat,configuration,1132,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/issues/9059:342,Integrability,integrat,integrated,342,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/issues/9059:848,Modifiability,config,configuration,848,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/issues/9059:1132,Modifiability,config,configuration,1132,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/issues/9059:363,Usability,learn,learned,363,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9059
https://github.com/hail-is/hail/pull/9061:102,Deployability,install,installations,102,"Pycharm thought `hl.nd` didn't exist, and I'm pretty sure we could; have had issues on certain python installations without this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9061
https://github.com/hail-is/hail/pull/9066:153,Deployability,configurat,configuration,153,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:477,Deployability,configurat,configuration,477,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:153,Modifiability,config,configuration,153,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:477,Modifiability,config,configuration,477,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:23,Testability,test,test,23,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:249,Testability,mock,mocks,249,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:258,Testability,test,tests,258,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:310,Testability,mock,mock,310,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:357,Testability,test,tests,357,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:407,Testability,test,tests,407,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:461,Testability,mock,mocked,461,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:512,Testability,test,tests,512,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:554,Testability,test,tests,554,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/pull/9066:598,Testability,assert,assertions,598,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9066
https://github.com/hail-is/hail/issues/9067:108,Performance,perform,performance,108,"Brought up by TJ in a recent conversation. He wants to not use the browser to work on Jupyter notebooks for performance / IDE convenience reasons. From a brief look, there appear to be two issues in getting this to work. First, VS Code will need to be started with proxy flags. As its runtime is Electron, all Chromium flags will work, so could almost specify HAILCTL_CHROME=code hailctl connect ... , but this doesn't directly work because VS Code also needs a workspace (so the cli invocation will need to be slightly different). Second, password-less may not work without `disable_xsrf_check`. Relevant issue: https://github.com/microsoft/vscode-python/issues/7137. There may be ways to hijack a proxied localhost connection, so unless we fully understand those issues, if disable_xsrf_check is necessary to enable password-less, it would be better to generate a token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9067
https://github.com/hail-is/hail/issues/9067:540,Security,password,password-less,540,"Brought up by TJ in a recent conversation. He wants to not use the browser to work on Jupyter notebooks for performance / IDE convenience reasons. From a brief look, there appear to be two issues in getting this to work. First, VS Code will need to be started with proxy flags. As its runtime is Electron, all Chromium flags will work, so could almost specify HAILCTL_CHROME=code hailctl connect ... , but this doesn't directly work because VS Code also needs a workspace (so the cli invocation will need to be slightly different). Second, password-less may not work without `disable_xsrf_check`. Relevant issue: https://github.com/microsoft/vscode-python/issues/7137. There may be ways to hijack a proxied localhost connection, so unless we fully understand those issues, if disable_xsrf_check is necessary to enable password-less, it would be better to generate a token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9067
https://github.com/hail-is/hail/issues/9067:818,Security,password,password-less,818,"Brought up by TJ in a recent conversation. He wants to not use the browser to work on Jupyter notebooks for performance / IDE convenience reasons. From a brief look, there appear to be two issues in getting this to work. First, VS Code will need to be started with proxy flags. As its runtime is Electron, all Chromium flags will work, so could almost specify HAILCTL_CHROME=code hailctl connect ... , but this doesn't directly work because VS Code also needs a workspace (so the cli invocation will need to be slightly different). Second, password-less may not work without `disable_xsrf_check`. Relevant issue: https://github.com/microsoft/vscode-python/issues/7137. There may be ways to hijack a proxied localhost connection, so unless we fully understand those issues, if disable_xsrf_check is necessary to enable password-less, it would be better to generate a token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9067
https://github.com/hail-is/hail/pull/9071:11,Usability,simpl,simple,11,Is it this simple?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9071
https://github.com/hail-is/hail/pull/9074:257,Deployability,deploy,deploy,257,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9074
https://github.com/hail-is/hail/pull/9074:348,Performance,cache,cache,348,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9074
https://github.com/hail-is/hail/pull/9074:581,Performance,cache,cache,581,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9074
https://github.com/hail-is/hail/pull/9074:221,Testability,test,tested,221,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9074
https://github.com/hail-is/hail/pull/9074:506,Testability,test,test,506,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9074
https://github.com/hail-is/hail/pull/9078:45,Deployability,update,update-hail-version,45,Follow on to #9066. Still todo: tests for `--update-hail-version`. Those will require a mock for deploy metadata.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9078
https://github.com/hail-is/hail/pull/9078:97,Deployability,deploy,deploy,97,Follow on to #9066. Still todo: tests for `--update-hail-version`. Those will require a mock for deploy metadata.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9078
https://github.com/hail-is/hail/pull/9078:32,Testability,test,tests,32,Follow on to #9066. Still todo: tests for `--update-hail-version`. Those will require a mock for deploy metadata.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9078
https://github.com/hail-is/hail/pull/9078:88,Testability,mock,mock,88,Follow on to #9066. Still todo: tests for `--update-hail-version`. Those will require a mock for deploy metadata.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9078
https://github.com/hail-is/hail/pull/9080:1092,Performance,cache,cache-friendly,1092,"Implement an arbitrary inner product operation. This is the critical piece of; infrastructure that underlies all relatedness inference algorithms. The typical; inner product on real number matrices is defined as:. ```; L_ij : matrix of shape a by b; M_kl : matrix of shape b by c; N_il : matrix of shape a by c, the inner product of L and M. N_il = Sum_k (L_ik * M_kl); ```. This PR allows the user to define what `*` means and what `Sum` means. For; example, the KING paper defines an estimator for relatedness of homogeneous; populations called KING-homo. KING-homo's numerator is given by; `score_difference` below. ```python3; mt = hl.balding_nichols_model(2, 5, 5); mt = mt.select_entries(genotype_score=hl.float(mt.GT.n_alt_alleles())); da = hl.experimental.dnd.array(mt, 'genotype_score', block_size=3); score_difference = da.T.inner_product(; da,; lambda l, r: sqr(l - r),; lambda l, r: l + r,; hl.float(0),; hl.agg.sum; ); ```. The rest of KING-homo is just manipulation of row fields. Eventually we need to implement `ndarray_inner_product(self, other, product, sum)`; which does a cache-friendly pass over the data but applies arbitrary user; product and sum operations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9080
https://github.com/hail-is/hail/pull/9082:6,Testability,test,tests,6,Todo: tests for `--pyfiles`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9082
https://github.com/hail-is/hail/pull/9087:31,Availability,error,errors,31,Couldn't reopen #9074. This PR errors if any of the input or output paths have unescaped wildcard characters in them.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9087
https://github.com/hail-is/hail/pull/9088:63,Deployability,install,install,63,"For reasons completely unclear to me, on Mac OS X, you have to install a JDK to; get the `java` command line tool. https://stackoverflow.com/questions/34074039/java-command-line-requires-jdk-on-mac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9088
https://github.com/hail-is/hail/pull/9089:17,Security,Expose,Expose,17,"To come next:; * Expose in Python, add more tests of Python functionality",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9089
https://github.com/hail-is/hail/pull/9089:44,Testability,test,tests,44,"To come next:; * Expose in Python, add more tests of Python functionality",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9089
https://github.com/hail-is/hail/pull/9094:17,Testability,test,tests,17,"Enabling NDArray tests revealed a bug relative to the test suite: MakeNDArray doesn't actually consider row/column major. I added a simple fix for this, predicated on rowMajorIR being a literal, which seems like a reasonable choice.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9094
https://github.com/hail-is/hail/pull/9094:54,Testability,test,test,54,"Enabling NDArray tests revealed a bug relative to the test suite: MakeNDArray doesn't actually consider row/column major. I added a simple fix for this, predicated on rowMajorIR being a literal, which seems like a reasonable choice.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9094
https://github.com/hail-is/hail/pull/9094:132,Usability,simpl,simple,132,"Enabling NDArray tests revealed a bug relative to the test suite: MakeNDArray doesn't actually consider row/column major. I added a simple fix for this, predicated on rowMajorIR being a literal, which seems like a reasonable choice.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9094
https://github.com/hail-is/hail/pull/9095:24,Performance,cache,caches,24,"Based on #9076. This PR caches input files at `/cache` on the worker. I do not support wildcard characters in filenames. I had to change the Flock implementation to only lock the directories and not a file if a file name is given. This is because we don't know if a user means a file or directory a priori. For example, `gs://jigold/test`. Is that a directory test or a file test? I figured more coarse-grained locking was fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095
https://github.com/hail-is/hail/pull/9095:48,Performance,cache,cache,48,"Based on #9076. This PR caches input files at `/cache` on the worker. I do not support wildcard characters in filenames. I had to change the Flock implementation to only lock the directories and not a file if a file name is given. This is because we don't know if a user means a file or directory a priori. For example, `gs://jigold/test`. Is that a directory test or a file test? I figured more coarse-grained locking was fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095
https://github.com/hail-is/hail/pull/9095:333,Testability,test,test,333,"Based on #9076. This PR caches input files at `/cache` on the worker. I do not support wildcard characters in filenames. I had to change the Flock implementation to only lock the directories and not a file if a file name is given. This is because we don't know if a user means a file or directory a priori. For example, `gs://jigold/test`. Is that a directory test or a file test? I figured more coarse-grained locking was fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095
https://github.com/hail-is/hail/pull/9095:360,Testability,test,test,360,"Based on #9076. This PR caches input files at `/cache` on the worker. I do not support wildcard characters in filenames. I had to change the Flock implementation to only lock the directories and not a file if a file name is given. This is because we don't know if a user means a file or directory a priori. For example, `gs://jigold/test`. Is that a directory test or a file test? I figured more coarse-grained locking was fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095
https://github.com/hail-is/hail/pull/9095:375,Testability,test,test,375,"Based on #9076. This PR caches input files at `/cache` on the worker. I do not support wildcard characters in filenames. I had to change the Flock implementation to only lock the directories and not a file if a file name is given. This is because we don't know if a user means a file or directory a priori. For example, `gs://jigold/test`. Is that a directory test or a file test? I figured more coarse-grained locking was fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095
https://github.com/hail-is/hail/pull/9096:258,Security,access,accesses,258,I debated whether to have this option on a batch which would require a database migration to add a new field to the batches table or just have it in the job spec for all jobs (same for all jobs). What I did is the easiest thing. It assumes all requester pay accesses are billed to the same project.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096
https://github.com/hail-is/hail/pull/9101:93,Security,expose,expose,93,"Used in the regenie implementation I'm referring to, and a useful/common ndarray function to expose.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9101
https://github.com/hail-is/hail/pull/9106:1428,Deployability,pipeline,pipelines,1428,"onment needs to be a function `Code[Region] => SizedStream`. For the most part, this PR leaves the `eltRegion`s unused. In the new semantics, explained below, it is still correct for a producer to only use the `outerRegion`, and for a consumer to pass the same region for both `outerRegion` and `eltRegion`. This lets us migrate to the new memory management one node at a time. When I talk about correctness, I mean that: no pointer is used after the region it points into has been freed; and, every region which is created is eventually freed. When reasoning about correctness, I consider `r.clear()` equivalent to `r.invalidate(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9106:1844,Integrability,contract,contract,1844,"lly freed. When reasoning about correctness, I consider `r.clear()` equivalent to `r.invalidate(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the consumer's `push` is called, the pushed value must be owned by either `outerRegion` or `eltRegion` (really `eltRegion`; using `outerRegion` is correct but unnecessarily extends the lifetime of the value). This can be accomplished by writing directly to the consumer's region, or by giving/sharing ownership of a producer-owned region to it. Thus the consumer may assume the pushed value is valid until it frees/clears `eltRegion`. If the consumer needs the value to live longer, it must deep-copy the value to another region. These allow ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9106:2326,Integrability,contract,contract,2326,"te(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the consumer's `push` is called, the pushed value must be owned by either `outerRegion` or `eltRegion` (really `eltRegion`; using `outerRegion` is correct but unnecessarily extends the lifetime of the value). This can be accomplished by writing directly to the consumer's region, or by giving/sharing ownership of a producer-owned region to it. Thus the consumer may assume the pushed value is valid until it frees/clears `eltRegion`. If the consumer needs the value to live longer, it must deep-copy the value to another region. These allow us to reason about (and, in principle, to prove) the correctness of each node independently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9106:2517,Modifiability,extend,extends,2517,"te(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the consumer's `push` is called, the pushed value must be owned by either `outerRegion` or `eltRegion` (really `eltRegion`; using `outerRegion` is correct but unnecessarily extends the lifetime of the value). This can be accomplished by writing directly to the consumer's region, or by giving/sharing ownership of a producer-owned region to it. Thus the consumer may assume the pushed value is valid until it frees/clears `eltRegion`. If the consumer needs the value to live longer, it must deep-copy the value to another region. These allow us to reason about (and, in principle, to prove) the correctness of each node independently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9106:1418,Testability,test,tested,1418,"onment needs to be a function `Code[Region] => SizedStream`. For the most part, this PR leaves the `eltRegion`s unused. In the new semantics, explained below, it is still correct for a producer to only use the `outerRegion`, and for a consumer to pass the same region for both `outerRegion` and `eltRegion`. This lets us migrate to the new memory management one node at a time. When I talk about correctness, I mean that: no pointer is used after the region it points into has been freed; and, every region which is created is eventually freed. When reasoning about correctness, I consider `r.clear()` equivalent to `r.invalidate(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9106:944,Usability,clear,clear,944,"This PR introduces a second region `eltRegion` into the `emitStream` environment, and plumbs it through everywhere it is needed. One complication is that the `eltRegion` for a stream bound in the environment (e.g. in a `StreamMap` over a `Stream[Stream[A]]`) must be set by the consumer of the stream, so the `PCanonicalStreamCode` stored in the environment needs to be a function `Code[Region] => SizedStream`. For the most part, this PR leaves the `eltRegion`s unused. In the new semantics, explained below, it is still correct for a producer to only use the `outerRegion`, and for a consumer to pass the same region for both `outerRegion` and `eltRegion`. This lets us migrate to the new memory management one node at a time. When I talk about correctness, I mean that: no pointer is used after the region it points into has been freed; and, every region which is created is eventually freed. When reasoning about correctness, I consider `r.clear()` equivalent to `r.invalidate(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9106:1650,Usability,clear,clear,1650,"r both `outerRegion` and `eltRegion`. This lets us migrate to the new memory management one node at a time. When I talk about correctness, I mean that: no pointer is used after the region it points into has been freed; and, every region which is created is eventually freed. When reasoning about correctness, I consider `r.clear()` equivalent to `r.invalidate(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the consumer's `push` is called, the pushed value must be owned by either `outerRegion` or `eltRegion` (really `eltRegion`; using `outerRegion` is correct but unnecessarily extends the lifetime of the value). This can be accomplished by writing directly to the consumer's region",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9106:2759,Usability,clear,clears,2759,"te(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the consumer's `push` is called, the pushed value must be owned by either `outerRegion` or `eltRegion` (really `eltRegion`; using `outerRegion` is correct but unnecessarily extends the lifetime of the value). This can be accomplished by writing directly to the consumer's region, or by giving/sharing ownership of a producer-owned region to it. Thus the consumer may assume the pushed value is valid until it frees/clears `eltRegion`. If the consumer needs the value to live longer, it must deep-copy the value to another region. These allow us to reason about (and, in principle, to prove) the correctness of each node independently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106
https://github.com/hail-is/hail/pull/9107:143,Deployability,deploy,deploy,143,This needs to be done at least once. The root SSL/TLS key is used to sign all; other keys used in the namespace. We do not recreate it when we deploy into; default or into dev namesapces.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9107
https://github.com/hail-is/hail/pull/9109:210,Deployability,deploy,deploys,210,"The shuffler lives!. I'm really quite satisfied with how small this PR is. Make sure you enable ""Hide whitespace changes"". I removed a try-catch from each test which changed a bunch of formatting. This PR just deploys the shuffler and retargets all tests at the shuffler service rather than a local version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9109
https://github.com/hail-is/hail/pull/9109:155,Testability,test,test,155,"The shuffler lives!. I'm really quite satisfied with how small this PR is. Make sure you enable ""Hide whitespace changes"". I removed a try-catch from each test which changed a bunch of formatting. This PR just deploys the shuffler and retargets all tests at the shuffler service rather than a local version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9109
https://github.com/hail-is/hail/pull/9109:249,Testability,test,tests,249,"The shuffler lives!. I'm really quite satisfied with how small this PR is. Make sure you enable ""Hide whitespace changes"". I removed a try-catch from each test which changed a bunch of formatting. This PR just deploys the shuffler and retargets all tests at the shuffler service rather than a local version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9109
https://github.com/hail-is/hail/pull/9113:189,Usability,simpl,simplified,189,"The auth driver currently uses the old secret format. The new secret format; includes SSL information. This change defines the secret format once in hailtop; and uses it everywhere. I also simplified the auth secret format: everyone uses; SSL/TLS to talk to databases now, so assume all necessary files are present.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9113
https://github.com/hail-is/hail/pull/9117:156,Modifiability,extend,extend,156,"Numpy both supports this and prominently uses it in documentation. We should support this for people translating Numpy code to Hail code. Note that I don't extend this feature to Hail TupleExpressions, because Hail tuple types are distinct from Hail array types (in that the latter has a single element_type, and the former has a collection of possible types). Python tuples are distinct from lists only in their immutability. Also, we support tuples in concatenate/NDArrayConcat, which makes the constructor difference from Numpy extra confusing. Example Numpy use: ; https://numpy.org/doc/stable/reference/generated/numpy.hstack.html?highlight=hstack#numpy.hstack; `a = np.array((1,2,3))` (this is the first example, which would fail if translated to hail); https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array; `x = np.array([(1,2),(3,4)])`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9117
https://github.com/hail-is/hail/pull/9120:599,Availability,error,error,599,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:1087,Availability,error,error,1087," In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:1792,Availability,error,error,1792,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:2405,Availability,error,errors,2405,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:1817,Deployability,configurat,configuration,1817,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:1817,Modifiability,config,configuration,1817,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:380,Performance,load,loads,380,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:2188,Performance,load,load,2188,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:2242,Performance,load,load,2242,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:2300,Performance,load,load,2300,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:1543,Safety,timeout,timeout,1543,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:404,Security,certificate,certificates,404,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:641,Security,certificate,certificate,641,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:701,Security,certificate,certificate,701,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:748,Security,certificate,certificate,748,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:818,Security,certificate,certificates,818,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:1019,Security,certificate,certificates,1019,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/pull/9120:1270,Security,certificate,certificates,1270,"t's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it wi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9120
https://github.com/hail-is/hail/issues/9121:78,Availability,error,error,78,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:142,Availability,error,error,142,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:84,Integrability,message,message,84,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:148,Integrability,message,message,148,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:608,Integrability,wrap,wrapper,608,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:659,Integrability,wrap,wrapper,659,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:864,Integrability,wrap,wrapper,864,"Note that `mt.cols()[mt.col_key]` is obviously wrong but instead we get a big error message that is ultimately really quite confusing. A good error message would be ""cannot index matrix table with itself"". (randomly assigning someone). ```; ExpressionException Traceback (most recent call last); <ipython-input-47-76acaa85d728> in <module>; 9 #combined.show(); 10; ---> 11 combined = combined.annotate_rows (N_Aa1 = mt.cols()[mt.col_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:1340,Integrability,wrap,wrapper,1340,"_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/base_expression.py in unify_all(*exprs); 351 n=len(sources),; 352 fields=''.join(""\n {}: {}"".format(src, fds) for src, fds in sources.items()); --> 353 )) from None; 354 first, rest = exprs[0], exprs[1:]; 355 aggregations = first._aggregations. ExpressionException: Cannot combine expressions from different source objects.; Found fields from 1 objects:; <hail.matrixtable.MatrixTable object at 0x7ff0ce0d3c50>: ['s']; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:1391,Integrability,wrap,wrapper,1391,"_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/base_expression.py in unify_all(*exprs); 351 n=len(sources),; 352 fields=''.join(""\n {}: {}"".format(src, fds) for src, fds in sources.items()); --> 353 )) from None; 354 first, rest = exprs[0], exprs[1:]; 355 aggregations = first._aggregations. ExpressionException: Cannot combine expressions from different source objects.; Found fields from 1 objects:; <hail.matrixtable.MatrixTable object at 0x7ff0ce0d3c50>: ['s']; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/issues/9121:1596,Integrability,wrap,wrapper,1596,"_key].N_Aa); 12; 13 combined.cols().show(). <decorator-gen-1171> in annotate_rows(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/matrixtable.py in annotate_rows(self, **named_exprs); 955 caller = ""MatrixTable.annotate_rows""; 956 check_annotate_exprs(caller, named_exprs, self._row_indices); --> 957 return self._select_rows(caller, self._rvrow.annotate(**named_exprs)); 958; 959 @typecheck_method(named_exprs=expr_any). <decorator-gen-651> in annotate(self, **named_exprs). ~/opt/miniconda3/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615; 616 return wrapper. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/typed_expressions.py in annotate(self, **named_exprs); 1624; 1625 result_type = tstruct(**new_types); -> 1626 indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]); 1627; 1628 return construct_expr(ir.InsertFields.construct_with_deduplication(. ~/opt/miniconda3/lib/python3.7/site-packages/hail/expr/expressions/base_expression.py in unify_all(*exprs); 351 n=len(sources),; 352 fields=''.join(""\n {}: {}"".format(src, fds) for src, fds in sources.items()); --> 353 )) from None; 354 first, rest = exprs[0], exprs[1:]; 355 aggregations = first._aggregations. ExpressionException: Cannot combine expressions from different source objects.; Found fields from 1 objects:; <hail.matrixtable.MatrixTable object at 0x7ff0ce0d3c50>: ['s']; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121
https://github.com/hail-is/hail/pull/9122:111,Testability,test,test,111,"`gear` should *only* be used in services, *never* in developer or user facing; tools. This change fixes `scale-test.py` to not use `gear`. I fixed some use of `\` to use `( ... )`.; I verified that I didn't add any new required packages to hail top.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9122
https://github.com/hail-is/hail/pull/9127:31,Availability,down,download,31,Old versions of pip could only download some packages in source form which; requires compiling them. New version of pip can download these packages in; binary form which requires no compilation. This *substantially* improves docker; build times when you have to run `pip` in any layer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9127
https://github.com/hail-is/hail/pull/9127:124,Availability,down,download,124,Old versions of pip could only download some packages in source form which; requires compiling them. New version of pip can download these packages in; binary form which requires no compilation. This *substantially* improves docker; build times when you have to run `pip` in any layer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9127
https://github.com/hail-is/hail/issues/9128:1300,Integrability,Wrap,WrappedArray,1300,l(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1321,Integrability,Wrap,WrappedArray,1321,; Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2040,Integrability,Wrap,WrappedArray,2040,ike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2061,Integrability,Wrap,WrappedArray,2061,1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2780,Integrability,Wrap,WrappedArray,2780,ike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2801,Integrability,Wrap,WrappedArray,2801,1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3520,Integrability,Wrap,WrappedArray,3520,ike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3541,Integrability,Wrap,WrappedArray,3541,1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4260,Integrability,Wrap,WrappedArray,4260,ike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4281,Integrability,Wrap,WrappedArray,4281,1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:7260,Integrability,Wrap,WrappedArray,7260,form(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:7281,Integrability,Wrap,WrappedArray,7281,.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304); 	at is.hail.backend.spark.Spark,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:10639,Integrability,wrap,wrapToMethod,10639,$apply$24.apply(Emit.scala:811); 	at is.hail.expr.ir.IEmitCode.map(Emit.scala:234); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:811); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:810); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:810); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:809); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:809); 	at is.hail.expr.ir.Emit.is$hail$expr$ir$Emit$$emitI$3(Emit.scala:1035); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2385); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2384); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:283); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2384); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1013); 	at is.hail.expr.ir.Emit$$anonfun$13$$anon$2.emit(Emit.scala:464); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.EmitUtils$.wrapToMethod(Emit.scala:426); 	at is.hail.expr.ir.Emit.wrapToMethod(Emit.scala:468); 	at is.hail.expr.ir.Emit.wrapToMethod$2(Emit.scala:1044); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1594); 	at is.hail.expr.ir.Emit.emitFallback$1(Emit.scala:692); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:975); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:675); 	at is.hail.expr.ir.Emit$$anonfun$ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:10716,Integrability,wrap,wrapToMethod,10716,:234); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:811); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:810); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:810); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:809); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:809); 	at is.hail.expr.ir.Emit.is$hail$expr$ir$Emit$$emitI$3(Emit.scala:1035); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2385); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2384); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:283); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2384); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1013); 	at is.hail.expr.ir.Emit$$anonfun$13$$anon$2.emit(Emit.scala:464); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.EmitUtils$.wrapToMethod(Emit.scala:426); 	at is.hail.expr.ir.Emit.wrapToMethod(Emit.scala:468); 	at is.hail.expr.ir.Emit.wrapToMethod$2(Emit.scala:1044); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1594); 	at is.hail.expr.ir.Emit.emitFallback$1(Emit.scala:692); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:975); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:675); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:91); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:11257,Integrability,wrap,wrapToMethod,11257,pr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2385); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2384); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:283); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2384); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1013); 	at is.hail.expr.ir.Emit$$anonfun$13$$anon$2.emit(Emit.scala:464); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.EmitUtils$.wrapToMethod(Emit.scala:426); 	at is.hail.expr.ir.Emit.wrapToMethod(Emit.scala:468); 	at is.hail.expr.ir.Emit.wrapToMethod$2(Emit.scala:1044); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1594); 	at is.hail.expr.ir.Emit.emitFallback$1(Emit.scala:692); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:975); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:675); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:91); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:90); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:14); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1091); 	at is.hail.expr.ir.WrappedEmitMethodBuilder$class.emitWithBuilder(EmitClassBuilder.scala:1111); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1215); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:90); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:71); 	at is.hail.expr.ir.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:11312,Integrability,wrap,wrapToMethod,11312, is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2384); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:283); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2384); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1013); 	at is.hail.expr.ir.Emit$$anonfun$13$$anon$2.emit(Emit.scala:464); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.EmitUtils$.wrapToMethod(Emit.scala:426); 	at is.hail.expr.ir.Emit.wrapToMethod(Emit.scala:468); 	at is.hail.expr.ir.Emit.wrapToMethod$2(Emit.scala:1044); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1594); 	at is.hail.expr.ir.Emit.emitFallback$1(Emit.scala:692); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:975); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:675); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:91); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:90); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:14); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1091); 	at is.hail.expr.ir.WrappedEmitMethodBuilder$class.emitWithBuilder(EmitClassBuilder.scala:1111); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1215); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:90); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:71); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$2.apply(CompileAndEvaluate.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:11367,Integrability,wrap,wrapToMethod,11367,2384); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:283); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2384); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1013); 	at is.hail.expr.ir.Emit$$anonfun$13$$anon$2.emit(Emit.scala:464); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at is.hail.expr.ir.EmitUtils$$anonfun$wrapToMethod$1.apply(Emit.scala:426); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.EmitUtils$.wrapToMethod(Emit.scala:426); 	at is.hail.expr.ir.Emit.wrapToMethod(Emit.scala:468); 	at is.hail.expr.ir.Emit.wrapToMethod$2(Emit.scala:1044); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1594); 	at is.hail.expr.ir.Emit.emitFallback$1(Emit.scala:692); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:975); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:675); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:91); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:90); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:14); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1091); 	at is.hail.expr.ir.WrappedEmitMethodBuilder$class.emitWithBuilder(EmitClassBuilder.scala:1111); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1215); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:90); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:71); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$2.apply(CompileAndEvaluate.scala:34); 	at is.hail.expr.ir.CompileAndEvaluate$$anonf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:11981,Integrability,Wrap,WrappedEmitMethodBuilder,11981,ay.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.EmitUtils$.wrapToMethod(Emit.scala:426); 	at is.hail.expr.ir.Emit.wrapToMethod(Emit.scala:468); 	at is.hail.expr.ir.Emit.wrapToMethod$2(Emit.scala:1044); 	at is.hail.expr.ir.Emit.emit(Emit.scala:1594); 	at is.hail.expr.ir.Emit.emitFallback$1(Emit.scala:692); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:975); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:675); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:91); 	at is.hail.expr.ir.Emit$$anonfun$apply$13.apply(Emit.scala:90); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:14); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1091); 	at is.hail.expr.ir.WrappedEmitMethodBuilder$class.emitWithBuilder(EmitClassBuilder.scala:1111); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1215); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:90); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:71); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$2.apply(CompileAndEvaluate.scala:34); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$2.apply(CompileAndEvaluate.scala:34); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:34); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(Execute,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:768,Modifiability,Rewrite,RewriteBottomUp,768,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:801,Modifiability,Rewrite,RewriteBottomUp,801,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:818,Modifiability,rewrite,rewrite,818,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:828,Modifiability,Rewrite,RewriteBottomUp,828,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:875,Modifiability,Rewrite,RewriteBottomUp,875,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:908,Modifiability,Rewrite,RewriteBottomUp,908,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:954,Modifiability,Rewrite,RewriteBottomUp,954,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:987,Modifiability,Rewrite,RewriteBottomUp,987,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1509,Modifiability,Rewrite,RewriteBottomUp,1509,.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1542,Modifiability,Rewrite,RewriteBottomUp,1542,l$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1559,Modifiability,rewrite,rewrite,1559,l$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1569,Modifiability,Rewrite,RewriteBottomUp,1569,l$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1615,Modifiability,Rewrite,RewriteBottomUp,1615,at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1648,Modifiability,Rewrite,RewriteBottomUp,1648,nstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1694,Modifiability,Rewrite,RewriteBottomUp,1694,nstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:1727,Modifiability,Rewrite,RewriteBottomUp,1727,ants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2249,Modifiability,Rewrite,RewriteBottomUp,2249,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2282,Modifiability,Rewrite,RewriteBottomUp,2282,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2299,Modifiability,rewrite,rewrite,2299,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2309,Modifiability,Rewrite,RewriteBottomUp,2309,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2355,Modifiability,Rewrite,RewriteBottomUp,2355,aversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2388,Modifiability,Rewrite,RewriteBottomUp,2388,raversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2434,Modifiability,Rewrite,RewriteBottomUp,2434,ctTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2467,Modifiability,Rewrite,RewriteBottomUp,2467,able.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:2989,Modifiability,Rewrite,RewriteBottomUp,2989,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3022,Modifiability,Rewrite,RewriteBottomUp,3022,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3039,Modifiability,rewrite,rewrite,3039,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3049,Modifiability,Rewrite,RewriteBottomUp,3049,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3095,Modifiability,Rewrite,RewriteBottomUp,3095,aversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3128,Modifiability,Rewrite,RewriteBottomUp,3128,raversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3174,Modifiability,Rewrite,RewriteBottomUp,3174,ctTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3207,Modifiability,Rewrite,RewriteBottomUp,3207,able.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3729,Modifiability,Rewrite,RewriteBottomUp,3729,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3762,Modifiability,Rewrite,RewriteBottomUp,3762,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3779,Modifiability,rewrite,rewrite,3779,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3789,Modifiability,Rewrite,RewriteBottomUp,3789,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3835,Modifiability,Rewrite,RewriteBottomUp,3835,aversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3868,Modifiability,Rewrite,RewriteBottomUp,3868,raversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldC,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3914,Modifiability,Rewrite,RewriteBottomUp,3914,ctTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:3947,Modifiability,Rewrite,RewriteBottomUp,3947,able.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4469,Modifiability,Rewrite,RewriteBottomUp,4469,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$an,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4502,Modifiability,Rewrite,RewriteBottomUp,4502,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4519,Modifiability,rewrite,rewrite,4519,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4529,Modifiability,Rewrite,RewriteBottomUp,4529,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4575,Modifiability,Rewrite,RewriteBottomUp,4575,on.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optim,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:4598,Modifiability,Rewrite,RewriteBottomUp,4598,lass.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5353,Performance,Optimiz,Optimize,5353,; 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(Lowerin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5409,Performance,Optimiz,Optimize,5409,raversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5449,Performance,Optimiz,Optimize,5449,teBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(Low,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5505,Performance,Optimiz,Optimize,5505,iteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5545,Performance,Optimiz,Optimize,5545,ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5579,Performance,Optimiz,Optimize,5579,ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5606,Performance,Optimiz,Optimize,5606,ttomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5646,Performance,Optimiz,Optimize,5646,ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.loweri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5680,Performance,Optimiz,Optimize,5680,ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.loweri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5707,Performance,Optimiz,Optimize,5707,dConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5811,Performance,Optimiz,Optimize,5811,s.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.exp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5837,Performance,Optimiz,Optimize,5837,s$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5856,Performance,Optimiz,Optimize,5856,s$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5896,Performance,Optimiz,Optimize,5896,ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.O,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5935,Performance,Optimiz,Optimize,5935,un$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(Loweri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:5975,Performance,Optimiz,Optimize,5975,t is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.Lowering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:6007,Performance,Optimiz,Optimize,6007,ecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:6047,Performance,Optimiz,Optimize,6047,eContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:6079,Performance,Optimiz,Optimize,6079,	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:6183,Performance,Optimiz,Optimize,6183,; 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:6199,Performance,Optimiz,Optimize,6199,l.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:6248,Performance,Optimiz,OptimizePass,6248,xt.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:6908,Performance,Optimiz,OptimizePass,6908,apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:9235,Testability,Assert,AssertionError,9235,".scala:304); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ```. Attempting to evaluate the array first and create and ndarray from that yields a different issue, again should work:. ```python; >>> b = hl.eval(a); >>> hl.eval(hl.nd.array(b)); FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.types.physical.PCanonicalArray.checkedConvertFrom(PCanonicalArray.scala:320); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:815); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:811); 	at is.hail.expr.ir.IEmitCode.map(Emit.scala:234); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:811); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:810); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:810); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:809); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:809); 	at is.hail.expr.ir.Emit.is$hail$expr$ir$Emit$$emitI$3(Emit.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:9251,Testability,assert,assertion,9251,".scala:304); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ```. Attempting to evaluate the array first and create and ndarray from that yields a different issue, again should work:. ```python; >>> b = hl.eval(a); >>> hl.eval(hl.nd.array(b)); FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.types.physical.PCanonicalArray.checkedConvertFrom(PCanonicalArray.scala:320); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:815); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:811); 	at is.hail.expr.ir.IEmitCode.map(Emit.scala:234); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:811); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:810); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:810); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:809); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:809); 	at is.hail.expr.ir.Emit.is$hail$expr$ir$Emit$$emitI$3(Emit.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:9298,Testability,Assert,AssertionError,9298,"la:324); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ```. Attempting to evaluate the array first and create and ndarray from that yields a different issue, again should work:. ```python; >>> b = hl.eval(a); >>> hl.eval(hl.nd.array(b)); FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.types.physical.PCanonicalArray.checkedConvertFrom(PCanonicalArray.scala:320); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:815); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:811); 	at is.hail.expr.ir.IEmitCode.map(Emit.scala:234); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:811); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:810); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:810); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:809); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:809); 	at is.hail.expr.ir.Emit.is$hail$expr$ir$Emit$$emitI$3(Emit.scala:1035); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2385); 	a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:9314,Testability,assert,assertion,9314,"la:324); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ```. Attempting to evaluate the array first and create and ndarray from that yields a different issue, again should work:. ```python; >>> b = hl.eval(a); >>> hl.eval(hl.nd.array(b)); FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.types.physical.PCanonicalArray.checkedConvertFrom(PCanonicalArray.scala:320); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:815); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:811); 	at is.hail.expr.ir.IEmitCode.map(Emit.scala:234); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:811); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:810); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:810); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:809); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:809); 	at is.hail.expr.ir.Emit.is$hail$expr$ir$Emit$$emitI$3(Emit.scala:1035); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2385); 	a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/issues/9128:9350,Testability,assert,assert,9350,"AccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); ```. Attempting to evaluate the array first and create and ndarray from that yields a different issue, again should work:. ```python; >>> b = hl.eval(a); >>> hl.eval(hl.nd.array(b)); FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.types.physical.PCanonicalArray.checkedConvertFrom(PCanonicalArray.scala:320); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:815); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23$$anonfun$apply$24.apply(Emit.scala:811); 	at is.hail.expr.ir.IEmitCode.map(Emit.scala:234); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:811); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10$$anonfun$apply$23.apply(Emit.scala:810); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:810); 	at is.hail.expr.ir.Emit$$anonfun$emitI$10.apply(Emit.scala:809); 	at is.hail.expr.ir.IEmitCode.flatMap(Emit.scala:241); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:809); 	at is.hail.expr.ir.Emit.is$hail$expr$ir$Emit$$emitI$3(Emit.scala:1035); 	at is.hail.expr.ir.Emit$$anonfun$emit$11.apply(Emit.scala:2385); 	at is.hail.expr.ir.Emit$$anonfun$emit$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128
https://github.com/hail-is/hail/pull/9129:625,Deployability,pipeline,pipeline,625,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:659,Deployability,deploy,deploy,659,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:754,Deployability,Deploy,Deploy,754,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:666,Modifiability,config,config,666,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:761,Modifiability,config,config,761,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:222,Performance,load,load,222,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:339,Performance,load,load,339,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:51,Security,expose,exposes,51,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:128,Security,access,access,128,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:159,Security,expose,expose,159,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:871,Testability,test,test,871,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9129:864,Usability,simpl,simple,864,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129
https://github.com/hail-is/hail/pull/9130:33,Deployability,deploy,deploying,33,This is only a problem when hand deploying batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9130
https://github.com/hail-is/hail/pull/9131:177,Integrability,message,message,177,"Numerous times, I have sent REST credentials to a web endpoint. It is supremely annoying; to run around in circles wondering how your credentials got out of date. I think; this message is safe because the user really is a credentialed user, they just; used the wrong endpoint.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9131
https://github.com/hail-is/hail/pull/9131:188,Safety,safe,safe,188,"Numerous times, I have sent REST credentials to a web endpoint. It is supremely annoying; to run around in circles wondering how your credentials got out of date. I think; this message is safe because the user really is a credentialed user, they just; used the wrong endpoint.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9131
https://github.com/hail-is/hail/pull/9140:212,Usability,simpl,simplify,212,"This basically implements the collect-and-broadcast strategy that we use for the current BlockMatrix.execute. I think it might be good to eventually be able to lift scalar broadcasts out as a relational let in a simplify pass instead of in the lowering, but in the meantime I've implemented it in the lowerer also.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9140
https://github.com/hail-is/hail/pull/9150:59,Safety,avoid,avoid,59,* Improves pruning of BlockMatrix.write_from_entry_expr to avoid large; allocation in the slow test.; * Delete the slow test.; * Add two benchmarks that should catch this,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9150
https://github.com/hail-is/hail/pull/9150:95,Testability,test,test,95,* Improves pruning of BlockMatrix.write_from_entry_expr to avoid large; allocation in the slow test.; * Delete the slow test.; * Add two benchmarks that should catch this,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9150
https://github.com/hail-is/hail/pull/9150:120,Testability,test,test,120,* Improves pruning of BlockMatrix.write_from_entry_expr to avoid large; allocation in the slow test.; * Delete the slow test.; * Add two benchmarks that should catch this,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9150
https://github.com/hail-is/hail/pull/9150:137,Testability,benchmark,benchmarks,137,* Improves pruning of BlockMatrix.write_from_entry_expr to avoid large; allocation in the slow test.; * Delete the slow test.; * Add two benchmarks that should catch this,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9150
https://github.com/hail-is/hail/pull/9152:474,Modifiability,polymorphi,polymorphism,474,"This PR supersedes #9146, sets ndarray slicing behavior to follow ndarray/python semantics when out of bounds: upper_bound = len_dim if step > 0 else len_dim - 1, lower_bound = 0 if step > 0 else -1 (since our -1 has the effect of a slice bound that is None). The other PR doesn't clamp dimensions properly (min and max out of bounds for start and stop are identical), and missed a 2nd issue, which is that python changes clamping behavior when step is negative. I bet this polymorphism is the reason we don't allow non-1 steps for ArrayExpressions.; - @johnc1231 could you still follow up on the byte code generation issue?. I've verified this works in the boundary cases.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9152
https://github.com/hail-is/hail/pull/9156:55,Security,expose,exposed,55,Do I have to do anything else to get BatchPoolExecutor exposed in the Batch docs?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9156
https://github.com/hail-is/hail/pull/9157:83,Testability,test,tests,83,"make LocalBroadcastValue serializable.; make writeReference public. Gets the local tests to 419 passing:. > ====== 344 failed, 419 passed, 75 skipped, 16 warnings in 362.06 seconds =======",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9157
https://github.com/hail-is/hail/pull/9161:204,Integrability,wrap,wrapped,204,"CHANGELOG: Fix bug that prevented concatenating ndarrays that are fields of a table. Two bugs:. 1. We can't call `.typ` on `concat_ir`, since that kicks off the type inference process before this IR gets wrapped in a `TableIR` that provides the necessary context to infer the type.; 2. We need to `_compute_type` on `self.nds` as a whole, not just each of its `args`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9161
https://github.com/hail-is/hail/pull/9162:8,Testability,test,testing,8,"I tried testing the output with pandoc, but that failed to render well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9162
https://github.com/hail-is/hail/issues/9163:92,Availability,error,error,92,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:1159,Availability,avail,available,1159,"lt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/base_expression.py"", line 567, in _bin_op_numeric; return me._bin_op(name, other, ret_type); File ""/Users/dking/projects/hail/hail/python/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:98,Integrability,message,message,98,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:579,Integrability,interface,interface,579,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:747,Performance,load,load,747,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:856,Testability,log,log,856,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:888,Testability,log,logging,888,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:1302,Testability,LOG,LOGGING,1302,"...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/base_expression.py"", line 567, in _bin_op_numeric; return me._bin_op(name, other, ret_type); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/base_expression.py"", line 577, in _bin_op; indices, aggregations = unify_all(self, other); File ""/Users/dking/projects/hail/hail/python/hail/e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/issues/9163:1393,Testability,log,log,1393,"0:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/base_expression.py"", line 567, in _bin_op_numeric; return me._bin_op(name, other, ret_type); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/base_expression.py"", line 577, in _bin_op; indices, aggregations = unify_all(self, other); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/base_expression.py"", line 353, in unify_all; )) from None; ExpressionException: Cannot combine expressions fro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9163
https://github.com/hail-is/hail/pull/9164:29,Availability,error,error,29,Let me know if you think the error message is clear.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9164
https://github.com/hail-is/hail/pull/9164:35,Integrability,message,message,35,Let me know if you think the error message is clear.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9164
https://github.com/hail-is/hail/pull/9164:46,Usability,clear,clear,46,Let me know if you think the error message is clear.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9164
https://github.com/hail-is/hail/pull/9166:1070,Availability,echo,echo,1070,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:1144,Availability,echo,echo,1144,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:9,Deployability,deploy,deployment,9,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:120,Deployability,deploy,deployment,120,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:4,Testability,test,test,4,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:102,Testability,test,tests,102,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:131,Testability,test,tests,131,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:393,Testability,test,test,393,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:416,Testability,test,test,416,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:478,Testability,log,log,478,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:745,Testability,Test,Test,745,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:805,Testability,test,test,805,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:821,Testability,Test,Test,821,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:826,Testability,test,testMethod,826,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:1223,Testability,assert,assert,1223,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:1565,Testability,assert,assert,1565,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:1657,Testability,assert,assert,1657,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:1713,Testability,Assert,AssertionError,1713,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:1750,Testability,test,test,1750,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9166:1818,Testability,assert,assert,1818,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9166
https://github.com/hail-is/hail/pull/9168:376,Testability,Test,Test,376,"Same issue where I didn't specify the storage to be 1Gi, so it got more memory than intended. I changed all the explicit resource requests to be 1Gi. ```; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:497:submit created batch 71161; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:533:submit closed batch 71161; FAILED; ___________________________ Test.test_out_of_memory ____________________________. self = <test.test_batch.Test testMethod=test_out_of_memory>. def test_out_of_memory(self):; builder = self.client.create_batch(); resources = {'cpu': '0.1', 'memory': '10M'}; j = builder.create_job('python:3.6-slim-stretch',; ['python', '-c', 'x = ""a"" * 1000**3'],; resources=resources); builder.submit(); status = j.wait(); > assert j._get_out_of_memory(status, 'main'); E AssertionError: assert False; E + where False = <function Job._get_out_of_memory at 0x7f2781b87050>({'batch_id': 71161, 'cost': '$0.0000', 'duration': 5847, 'exit_code': 0, ...}, 'main'); E + where <function Job._get_out_of_memory at 0x7f2781b87050> = <hailtop.batch_client.client.Job object at 0x7f277d7a71d0>._get_out_of_memory. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9168
https://github.com/hail-is/hail/pull/9168:438,Testability,test,test,438,"Same issue where I didn't specify the storage to be 1Gi, so it got more memory than intended. I changed all the explicit resource requests to be 1Gi. ```; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:497:submit created batch 71161; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:533:submit closed batch 71161; FAILED; ___________________________ Test.test_out_of_memory ____________________________. self = <test.test_batch.Test testMethod=test_out_of_memory>. def test_out_of_memory(self):; builder = self.client.create_batch(); resources = {'cpu': '0.1', 'memory': '10M'}; j = builder.create_job('python:3.6-slim-stretch',; ['python', '-c', 'x = ""a"" * 1000**3'],; resources=resources); builder.submit(); status = j.wait(); > assert j._get_out_of_memory(status, 'main'); E AssertionError: assert False; E + where False = <function Job._get_out_of_memory at 0x7f2781b87050>({'batch_id': 71161, 'cost': '$0.0000', 'duration': 5847, 'exit_code': 0, ...}, 'main'); E + where <function Job._get_out_of_memory at 0x7f2781b87050> = <hailtop.batch_client.client.Job object at 0x7f277d7a71d0>._get_out_of_memory. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9168
https://github.com/hail-is/hail/pull/9168:454,Testability,Test,Test,454,"Same issue where I didn't specify the storage to be 1Gi, so it got more memory than intended. I changed all the explicit resource requests to be 1Gi. ```; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:497:submit created batch 71161; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:533:submit closed batch 71161; FAILED; ___________________________ Test.test_out_of_memory ____________________________. self = <test.test_batch.Test testMethod=test_out_of_memory>. def test_out_of_memory(self):; builder = self.client.create_batch(); resources = {'cpu': '0.1', 'memory': '10M'}; j = builder.create_job('python:3.6-slim-stretch',; ['python', '-c', 'x = ""a"" * 1000**3'],; resources=resources); builder.submit(); status = j.wait(); > assert j._get_out_of_memory(status, 'main'); E AssertionError: assert False; E + where False = <function Job._get_out_of_memory at 0x7f2781b87050>({'batch_id': 71161, 'cost': '$0.0000', 'duration': 5847, 'exit_code': 0, ...}, 'main'); E + where <function Job._get_out_of_memory at 0x7f2781b87050> = <hailtop.batch_client.client.Job object at 0x7f277d7a71d0>._get_out_of_memory. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9168
https://github.com/hail-is/hail/pull/9168:459,Testability,test,testMethod,459,"Same issue where I didn't specify the storage to be 1Gi, so it got more memory than intended. I changed all the explicit resource requests to be 1Gi. ```; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:497:submit created batch 71161; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:533:submit closed batch 71161; FAILED; ___________________________ Test.test_out_of_memory ____________________________. self = <test.test_batch.Test testMethod=test_out_of_memory>. def test_out_of_memory(self):; builder = self.client.create_batch(); resources = {'cpu': '0.1', 'memory': '10M'}; j = builder.create_job('python:3.6-slim-stretch',; ['python', '-c', 'x = ""a"" * 1000**3'],; resources=resources); builder.submit(); status = j.wait(); > assert j._get_out_of_memory(status, 'main'); E AssertionError: assert False; E + where False = <function Job._get_out_of_memory at 0x7f2781b87050>({'batch_id': 71161, 'cost': '$0.0000', 'duration': 5847, 'exit_code': 0, ...}, 'main'); E + where <function Job._get_out_of_memory at 0x7f2781b87050> = <hailtop.batch_client.client.Job object at 0x7f277d7a71d0>._get_out_of_memory. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9168
https://github.com/hail-is/hail/pull/9168:757,Testability,assert,assert,757,"Same issue where I didn't specify the storage to be 1Gi, so it got more memory than intended. I changed all the explicit resource requests to be 1Gi. ```; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:497:submit created batch 71161; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:533:submit closed batch 71161; FAILED; ___________________________ Test.test_out_of_memory ____________________________. self = <test.test_batch.Test testMethod=test_out_of_memory>. def test_out_of_memory(self):; builder = self.client.create_batch(); resources = {'cpu': '0.1', 'memory': '10M'}; j = builder.create_job('python:3.6-slim-stretch',; ['python', '-c', 'x = ""a"" * 1000**3'],; resources=resources); builder.submit(); status = j.wait(); > assert j._get_out_of_memory(status, 'main'); E AssertionError: assert False; E + where False = <function Job._get_out_of_memory at 0x7f2781b87050>({'batch_id': 71161, 'cost': '$0.0000', 'duration': 5847, 'exit_code': 0, ...}, 'main'); E + where <function Job._get_out_of_memory at 0x7f2781b87050> = <hailtop.batch_client.client.Job object at 0x7f277d7a71d0>._get_out_of_memory. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9168
https://github.com/hail-is/hail/pull/9168:804,Testability,Assert,AssertionError,804,"Same issue where I didn't specify the storage to be 1Gi, so it got more memory than intended. I changed all the explicit resource requests to be 1Gi. ```; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:497:submit created batch 71161; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:533:submit closed batch 71161; FAILED; ___________________________ Test.test_out_of_memory ____________________________. self = <test.test_batch.Test testMethod=test_out_of_memory>. def test_out_of_memory(self):; builder = self.client.create_batch(); resources = {'cpu': '0.1', 'memory': '10M'}; j = builder.create_job('python:3.6-slim-stretch',; ['python', '-c', 'x = ""a"" * 1000**3'],; resources=resources); builder.submit(); status = j.wait(); > assert j._get_out_of_memory(status, 'main'); E AssertionError: assert False; E + where False = <function Job._get_out_of_memory at 0x7f2781b87050>({'batch_id': 71161, 'cost': '$0.0000', 'duration': 5847, 'exit_code': 0, ...}, 'main'); E + where <function Job._get_out_of_memory at 0x7f2781b87050> = <hailtop.batch_client.client.Job object at 0x7f277d7a71d0>._get_out_of_memory. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9168
https://github.com/hail-is/hail/pull/9168:820,Testability,assert,assert,820,"Same issue where I didn't specify the storage to be 1Gi, so it got more memory than intended. I changed all the explicit resource requests to be 1Gi. ```; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:497:submit created batch 71161; 2020-07-28T18:18:12 INFO batch_client.aioclient aioclient.py:533:submit closed batch 71161; FAILED; ___________________________ Test.test_out_of_memory ____________________________. self = <test.test_batch.Test testMethod=test_out_of_memory>. def test_out_of_memory(self):; builder = self.client.create_batch(); resources = {'cpu': '0.1', 'memory': '10M'}; j = builder.create_job('python:3.6-slim-stretch',; ['python', '-c', 'x = ""a"" * 1000**3'],; resources=resources); builder.submit(); status = j.wait(); > assert j._get_out_of_memory(status, 'main'); E AssertionError: assert False; E + where False = <function Job._get_out_of_memory at 0x7f2781b87050>({'batch_id': 71161, 'cost': '$0.0000', 'duration': 5847, 'exit_code': 0, ...}, 'main'); E + where <function Job._get_out_of_memory at 0x7f2781b87050> = <hailtop.batch_client.client.Job object at 0x7f277d7a71d0>._get_out_of_memory. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9168
https://github.com/hail-is/hail/pull/9169:37,Availability,error,error,37,"Fixes #8944. CHANGELOG: Fixed crash (error 134 or SIGSEGV) in `MatrixTable.annotate_cols`, `hl.sample_qc`, and more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9169
https://github.com/hail-is/hail/pull/9172:264,Deployability,install,install,264,"CHANGELOG: fix missing google-cloud-storage python dependency, required by batch. This makes the tutorial work: https://hail.is/docs/batch/getting_started.html. Version chosen based on our dataproc dependency declaration in init_notebook.py. Note that 1.25.* will install a new version of google-auth. On that note, it is possible that we can upgrade /auth to use a newer google-auth (@danking), as the issue listed in the /auth docker file (https://github.com/googleapis/google-auth-library-python/issues/443) is obviated by the 1.11.2 release: https://github.com/googleapis/google-auth-library-python/pull/446.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9172
https://github.com/hail-is/hail/pull/9172:343,Deployability,upgrade,upgrade,343,"CHANGELOG: fix missing google-cloud-storage python dependency, required by batch. This makes the tutorial work: https://hail.is/docs/batch/getting_started.html. Version chosen based on our dataproc dependency declaration in init_notebook.py. Note that 1.25.* will install a new version of google-auth. On that note, it is possible that we can upgrade /auth to use a newer google-auth (@danking), as the issue listed in the /auth docker file (https://github.com/googleapis/google-auth-library-python/issues/443) is obviated by the 1.11.2 release: https://github.com/googleapis/google-auth-library-python/pull/446.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9172
https://github.com/hail-is/hail/pull/9172:537,Deployability,release,release,537,"CHANGELOG: fix missing google-cloud-storage python dependency, required by batch. This makes the tutorial work: https://hail.is/docs/batch/getting_started.html. Version chosen based on our dataproc dependency declaration in init_notebook.py. Note that 1.25.* will install a new version of google-auth. On that note, it is possible that we can upgrade /auth to use a newer google-auth (@danking), as the issue listed in the /auth docker file (https://github.com/googleapis/google-auth-library-python/issues/443) is obviated by the 1.11.2 release: https://github.com/googleapis/google-auth-library-python/pull/446.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9172
https://github.com/hail-is/hail/pull/9172:51,Integrability,depend,dependency,51,"CHANGELOG: fix missing google-cloud-storage python dependency, required by batch. This makes the tutorial work: https://hail.is/docs/batch/getting_started.html. Version chosen based on our dataproc dependency declaration in init_notebook.py. Note that 1.25.* will install a new version of google-auth. On that note, it is possible that we can upgrade /auth to use a newer google-auth (@danking), as the issue listed in the /auth docker file (https://github.com/googleapis/google-auth-library-python/issues/443) is obviated by the 1.11.2 release: https://github.com/googleapis/google-auth-library-python/pull/446.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9172
https://github.com/hail-is/hail/pull/9172:198,Integrability,depend,dependency,198,"CHANGELOG: fix missing google-cloud-storage python dependency, required by batch. This makes the tutorial work: https://hail.is/docs/batch/getting_started.html. Version chosen based on our dataproc dependency declaration in init_notebook.py. Note that 1.25.* will install a new version of google-auth. On that note, it is possible that we can upgrade /auth to use a newer google-auth (@danking), as the issue listed in the /auth docker file (https://github.com/googleapis/google-auth-library-python/issues/443) is obviated by the 1.11.2 release: https://github.com/googleapis/google-auth-library-python/pull/446.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9172
https://github.com/hail-is/hail/pull/9173:17,Deployability,upgrade,upgrade,17,"CHANGELOG: auth: upgrade google-auth-oauthlib to 0.4.1 an google-auth to 1.20.0. Note that google-auth-oauthlib 0.4.1 fixes to bug 0.4.0 introduced that prevented us from using that version: https://github.com/googleapis/google-auth-library-python-oauthlib/pull/48/files. 1.20.0 is the (latest <2.0-dev) version installed with google-cloud-storage, use by hailctl dataproc, and now hail batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9173
https://github.com/hail-is/hail/pull/9173:312,Deployability,install,installed,312,"CHANGELOG: auth: upgrade google-auth-oauthlib to 0.4.1 an google-auth to 1.20.0. Note that google-auth-oauthlib 0.4.1 fixes to bug 0.4.0 introduced that prevented us from using that version: https://github.com/googleapis/google-auth-library-python-oauthlib/pull/48/files. 1.20.0 is the (latest <2.0-dev) version installed with google-cloud-storage, use by hailctl dataproc, and now hail batch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9173
https://github.com/hail-is/hail/pull/9175:1,Testability,benchmark,benchmark,1,[benchmark] fix crash loop backoff from init_app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9175
https://github.com/hail-is/hail/pull/9178:556,Integrability,wrap,wraps,556,"~~Stacked on #9177~~. These are tricky because of the overlapping lifetimes of the accumulator values. The solution implemented here is to keep two regions. When running the fold body, one region is empty, and the other holds just the previous accumulator. Then we deep copy the new accumulator to the empty region, clear the other one, then swap the two regions before moving to the next iteration. This required expanding the StagedRegion API a bit. First, it now strictly tracks the parent StagedRegion, even in the non-allocating case where the parent wraps the same run-time region. Second, in addition to copying a value to the parent, we can now copy a value to a sibling. It enforces at compile time that the two regions have the same parent. It follows that either both are actually separate regions, or both are just wrappers around the parent region. In the latter case, copying to a sibling is a no-op.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9178
https://github.com/hail-is/hail/pull/9178:827,Integrability,wrap,wrappers,827,"~~Stacked on #9177~~. These are tricky because of the overlapping lifetimes of the accumulator values. The solution implemented here is to keep two regions. When running the fold body, one region is empty, and the other holds just the previous accumulator. Then we deep copy the new accumulator to the empty region, clear the other one, then swap the two regions before moving to the next iteration. This required expanding the StagedRegion API a bit. First, it now strictly tracks the parent StagedRegion, even in the non-allocating case where the parent wraps the same run-time region. Second, in addition to copying a value to the parent, we can now copy a value to a sibling. It enforces at compile time that the two regions have the same parent. It follows that either both are actually separate regions, or both are just wrappers around the parent region. In the latter case, copying to a sibling is a no-op.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9178
https://github.com/hail-is/hail/pull/9178:316,Usability,clear,clear,316,"~~Stacked on #9177~~. These are tricky because of the overlapping lifetimes of the accumulator values. The solution implemented here is to keep two regions. When running the fold body, one region is empty, and the other holds just the previous accumulator. Then we deep copy the new accumulator to the empty region, clear the other one, then swap the two regions before moving to the next iteration. This required expanding the StagedRegion API a bit. First, it now strictly tracks the parent StagedRegion, even in the non-allocating case where the parent wraps the same run-time region. Second, in addition to copying a value to the parent, we can now copy a value to a sibling. It enforces at compile time that the two regions have the same parent. It follows that either both are actually separate regions, or both are just wrappers around the parent region. In the latter case, copying to a sibling is a no-op.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9178
https://github.com/hail-is/hail/pull/9179:26,Availability,error,errors,26,CHANGELOG: Fixed compiler errors related to interval filter pushdown.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9179
https://github.com/hail-is/hail/pull/9181:426,Safety,safe,safest,426,"Stacked on #9220. The region parameter to emit ostensibly means ""construct the value in this region"". But in many places `StagedRegionValueBuilder`s are constructed with the default region, which is the argument to the method being emitted into. This PR fixes all the uses in the emitter to build values in the right region. Most of the remaining users of the constructor with the default region are in tests, so I thought it safest to remove those constructors, and require explicitly passing the region to build into. Otherwise it's too easy to create memory leaks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9181
https://github.com/hail-is/hail/pull/9181:403,Testability,test,tests,403,"Stacked on #9220. The region parameter to emit ostensibly means ""construct the value in this region"". But in many places `StagedRegionValueBuilder`s are constructed with the default region, which is the argument to the method being emitted into. This PR fixes all the uses in the emitter to build values in the right region. Most of the remaining users of the constructor with the default region are in tests, so I thought it safest to remove those constructors, and require explicitly passing the region to build into. Otherwise it's too easy to create memory leaks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9181
https://github.com/hail-is/hail/pull/9184:53,Modifiability,config,config,53,I wasn't sure if we should print at least one Docker config or not. I'm worried about the test for out_of_memory where that prints out a bunch of the letter 'a' in the script.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9184
https://github.com/hail-is/hail/pull/9184:90,Testability,test,test,90,I wasn't sure if we should print at least one Docker config or not. I'm worried about the test for out_of_memory where that prints out a bunch of the letter 'a' in the script.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9184
https://github.com/hail-is/hail/pull/9185:215,Testability,test,test,215,"I mixed the required changes with some cosmetic changes to, e.g. docs and comments. I also added a missing </div> in references.md. AFAICT, there's no way to change the default branch yet, so I didn't change the PR test CI's test repo's branch yet. This change will change the references page on the site as soon as its merged, so we should be prepared to switch over when we merge it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9185
https://github.com/hail-is/hail/pull/9185:225,Testability,test,test,225,"I mixed the required changes with some cosmetic changes to, e.g. docs and comments. I also added a missing </div> in references.md. AFAICT, there's no way to change the default branch yet, so I didn't change the PR test CI's test repo's branch yet. This change will change the references page on the site as soon as its merged, so we should be prepared to switch over when we merge it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9185
https://github.com/hail-is/hail/pull/9189:691,Integrability,message,message,691,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:143,Security,authoriz,authorization,143,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:76,Testability,log,logs,76,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:84,Testability,Log,Logs,84,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:301,Testability,Log,Logs,301,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:607,Testability,log,log,607,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:687,Testability,log,log,687,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:706,Testability,Log,Logs,706,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:811,Testability,log,logName,811,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:838,Testability,log,logs,838,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:850,Testability,log,log,850,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:971,Testability,log,logName,971,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:998,Testability,log,logs,998,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:1010,Testability,log,log,1010,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:1078,Testability,log,logging,1078,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9189:1126,Testability,log,logs,1126,"This should wait for #9184 . This PR adds fluentd support for streaming the logs to Logs Viewer. Things to Note:; - I didn't have to setup the authorization at all, so I believe it's using the service account on the worker which would be the batch2 service account. I had to give that service account Logs Writer permissions. This is the same service account regardless of the namespace.; - All namespaces have output written.; - I added some labels of the instance id and namespace to help with searching. Let me know if you think we need something else.; - I used a filter to parse the JSON in the worker log to get the right timestamp of the record rather than the publication of the log message in the Logs Viewer. Example Stackdriver Queries:. ```; resource.type=""gce_instance""; labels.namespace=""jigold""; logName=""projects/hail-vdc/logs/worker.log""; ```. ```; resource.type=""gce_instance""; labels.""compute.googleapis.com/resource_name""=""batch-worker-jigold-uyvo5""; logName=""projects/hail-vdc/logs/worker.log""; ```. I basically just followed this: https://cloud.google.com/logging/docs/agent. Will attempt to get dockerd logs in some other time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189
https://github.com/hail-is/hail/pull/9191:91,Energy Efficiency,reduce,reduces,91,"CHANGELOG: Adds close, default implementation pass, to Backend as an abstract method. This reduces the number of conditional statements needed by 1 when using variable backends. Variable backends (Local or Service) are useful when prototyping batches, or when the user will know in advance that a particular batch routine will work locally (since in future work it will be much easier to consume dockerized methods using Batch than anything else). Additionally, as provided, the only Batch tutorial, GWAS clumping will not work with LocalBackend without this. Use case:. ```python; parser.add_argument('--local', required=False, action=""store_true""); if is_local:; backend = hb.LocalBackend(); run_opts = {}; else:; backend = hb.ServiceBackend(); run_opts = {open: True, wait: True}. # do a bunch of Batch stuff to ; batch.run(**run_opts); backend.close(); ```. In a similar vein, I'd like to allow LocalBackend to ignore unused run opts. Again, GWAS tutorial would not work with LocalBackend without this (or an opts dict as above).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191
https://github.com/hail-is/hail/pull/9191:314,Integrability,rout,routine,314,"CHANGELOG: Adds close, default implementation pass, to Backend as an abstract method. This reduces the number of conditional statements needed by 1 when using variable backends. Variable backends (Local or Service) are useful when prototyping batches, or when the user will know in advance that a particular batch routine will work locally (since in future work it will be much easier to consume dockerized methods using Batch than anything else). Additionally, as provided, the only Batch tutorial, GWAS clumping will not work with LocalBackend without this. Use case:. ```python; parser.add_argument('--local', required=False, action=""store_true""); if is_local:; backend = hb.LocalBackend(); run_opts = {}; else:; backend = hb.ServiceBackend(); run_opts = {open: True, wait: True}. # do a bunch of Batch stuff to ; batch.run(**run_opts); backend.close(); ```. In a similar vein, I'd like to allow LocalBackend to ignore unused run opts. Again, GWAS tutorial would not work with LocalBackend without this (or an opts dict as above).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191
https://github.com/hail-is/hail/pull/9191:159,Modifiability,variab,variable,159,"CHANGELOG: Adds close, default implementation pass, to Backend as an abstract method. This reduces the number of conditional statements needed by 1 when using variable backends. Variable backends (Local or Service) are useful when prototyping batches, or when the user will know in advance that a particular batch routine will work locally (since in future work it will be much easier to consume dockerized methods using Batch than anything else). Additionally, as provided, the only Batch tutorial, GWAS clumping will not work with LocalBackend without this. Use case:. ```python; parser.add_argument('--local', required=False, action=""store_true""); if is_local:; backend = hb.LocalBackend(); run_opts = {}; else:; backend = hb.ServiceBackend(); run_opts = {open: True, wait: True}. # do a bunch of Batch stuff to ; batch.run(**run_opts); backend.close(); ```. In a similar vein, I'd like to allow LocalBackend to ignore unused run opts. Again, GWAS tutorial would not work with LocalBackend without this (or an opts dict as above).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191
https://github.com/hail-is/hail/pull/9191:178,Modifiability,Variab,Variable,178,"CHANGELOG: Adds close, default implementation pass, to Backend as an abstract method. This reduces the number of conditional statements needed by 1 when using variable backends. Variable backends (Local or Service) are useful when prototyping batches, or when the user will know in advance that a particular batch routine will work locally (since in future work it will be much easier to consume dockerized methods using Batch than anything else). Additionally, as provided, the only Batch tutorial, GWAS clumping will not work with LocalBackend without this. Use case:. ```python; parser.add_argument('--local', required=False, action=""store_true""); if is_local:; backend = hb.LocalBackend(); run_opts = {}; else:; backend = hb.ServiceBackend(); run_opts = {open: True, wait: True}. # do a bunch of Batch stuff to ; batch.run(**run_opts); backend.close(); ```. In a similar vein, I'd like to allow LocalBackend to ignore unused run opts. Again, GWAS tutorial would not work with LocalBackend without this (or an opts dict as above).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191
https://github.com/hail-is/hail/pull/9194:726,Energy Efficiency,allocate,allocated,726,"CHANGELOG: early implementation of regenie. I'd like to get this basic version in and iterate. It works on local. Has a few todos and fixmes; I've seen Cotton, others use these, and they'll be gone in fairly short order, saves a bit of time over making a formal issue, for something that is clearly wip. . This looks like a scary amount of lines, but almost all of the work is held in regenie-batch.py. Example files are included in contrib/regenie/regenie, which is a redacted copy of their repo, and which is sufficient, lighter-weight than the full. When you have a chance, I'd like to discuss increasing the max capacity of the SSD (striping partitions). I want this as a fallback mechanism, when not enough memory can be allocated (lowmem). It will likely perform terribly with persistent storage. I'm happy to contribute that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194
https://github.com/hail-is/hail/pull/9194:761,Performance,perform,perform,761,"CHANGELOG: early implementation of regenie. I'd like to get this basic version in and iterate. It works on local. Has a few todos and fixmes; I've seen Cotton, others use these, and they'll be gone in fairly short order, saves a bit of time over making a formal issue, for something that is clearly wip. . This looks like a scary amount of lines, but almost all of the work is held in regenie-batch.py. Example files are included in contrib/regenie/regenie, which is a redacted copy of their repo, and which is sufficient, lighter-weight than the full. When you have a chance, I'd like to discuss increasing the max capacity of the SSD (striping partitions). I want this as a fallback mechanism, when not enough memory can be allocated (lowmem). It will likely perform terribly with persistent storage. I'm happy to contribute that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194
https://github.com/hail-is/hail/pull/9194:291,Usability,clear,clearly,291,"CHANGELOG: early implementation of regenie. I'd like to get this basic version in and iterate. It works on local. Has a few todos and fixmes; I've seen Cotton, others use these, and they'll be gone in fairly short order, saves a bit of time over making a formal issue, for something that is clearly wip. . This looks like a scary amount of lines, but almost all of the work is held in regenie-batch.py. Example files are included in contrib/regenie/regenie, which is a redacted copy of their repo, and which is sufficient, lighter-weight than the full. When you have a chance, I'd like to discuss increasing the max capacity of the SSD (striping partitions). I want this as a fallback mechanism, when not enough memory can be allocated (lowmem). It will likely perform terribly with persistent storage. I'm happy to contribute that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194
https://github.com/hail-is/hail/pull/9196:11,Energy Efficiency,adapt,adaptation,11,"This is an adaptation of my comment on the TLS PR. I moved the old `tls.md` to `tls-cookbook.md`. Git doesn't realize that. Dania & @catoverdrive, y'all are probably the two folks most likely to benefit from tls.md, so I'd appreciate your comments on the readability of this document.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9196
https://github.com/hail-is/hail/pull/9196:11,Modifiability,adapt,adaptation,11,"This is an adaptation of my comment on the TLS PR. I moved the old `tls.md` to `tls-cookbook.md`. Git doesn't realize that. Dania & @catoverdrive, y'all are probably the two folks most likely to benefit from tls.md, so I'd appreciate your comments on the readability of this document.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9196
https://github.com/hail-is/hail/pull/9198:485,Performance,concurren,concurrent,485,cc: @tpoterba. There's a Sphinx extension that uses Python type annotations to figure out; parameter types so we don't have to write it twice. I implement that here. I; wrestled for a while with the cyclical import structure until I realized that. ```; from . import module; ```. side steps a lot of the cyclical issues (pylint still dislikes it). I also added `sphinx.ext.intersphinx` which makes every reference to a Python; standard library thing work correctly. Now when I write; `concurrent.futures.ThreadPoolExecutor` it links right to the Python docs; page. It's really quite nice.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9198
https://github.com/hail-is/hail/pull/9200:82,Availability,error,errors,82,"This PR changes the `addresses` function on `DeployConfig` to retry all transient errors. In particular, if the address service is temporarily down (maybe its getting redeployed), this change allows the client to repeatedly retry until the address service comes back to life. I also added some type annotations to `retry_transient_errors`. It takes a function that returns something we can `await` and then applies that function in a loop until it does not raise an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9200
https://github.com/hail-is/hail/pull/9200:143,Availability,down,down,143,"This PR changes the `addresses` function on `DeployConfig` to retry all transient errors. In particular, if the address service is temporarily down (maybe its getting redeployed), this change allows the client to repeatedly retry until the address service comes back to life. I also added some type annotations to `retry_transient_errors`. It takes a function that returns something we can `await` and then applies that function in a loop until it does not raise an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9200
https://github.com/hail-is/hail/pull/9200:466,Availability,error,error,466,"This PR changes the `addresses` function on `DeployConfig` to retry all transient errors. In particular, if the address service is temporarily down (maybe its getting redeployed), this change allows the client to repeatedly retry until the address service comes back to life. I also added some type annotations to `retry_transient_errors`. It takes a function that returns something we can `await` and then applies that function in a loop until it does not raise an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9200
https://github.com/hail-is/hail/pull/9200:45,Deployability,Deploy,DeployConfig,45,"This PR changes the `addresses` function on `DeployConfig` to retry all transient errors. In particular, if the address service is temporarily down (maybe its getting redeployed), this change allows the client to repeatedly retry until the address service comes back to life. I also added some type annotations to `retry_transient_errors`. It takes a function that returns something we can `await` and then applies that function in a loop until it does not raise an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9200
https://github.com/hail-is/hail/pull/9202:6,Availability,error,error,6,"* Fix error in TableKeyByAndAggregate caused by field name clobbering; * Fix error in TableLeftJoinRightDistinct with non-strict left tables; * Fix erroneous key preservation in TableStage.mapPartition; * Add Consume to TypeCheck; * Fix error where globals were not exposed in TableAggregate; * Fix error where globals were not exposed in TableAggregate. New local backend success rate:. ```; 271 failed, 496 passed, 75 skipped, 15 warnings in 368.17 seconds; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9202
https://github.com/hail-is/hail/pull/9202:77,Availability,error,error,77,"* Fix error in TableKeyByAndAggregate caused by field name clobbering; * Fix error in TableLeftJoinRightDistinct with non-strict left tables; * Fix erroneous key preservation in TableStage.mapPartition; * Add Consume to TypeCheck; * Fix error where globals were not exposed in TableAggregate; * Fix error where globals were not exposed in TableAggregate. New local backend success rate:. ```; 271 failed, 496 passed, 75 skipped, 15 warnings in 368.17 seconds; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9202
https://github.com/hail-is/hail/pull/9202:237,Availability,error,error,237,"* Fix error in TableKeyByAndAggregate caused by field name clobbering; * Fix error in TableLeftJoinRightDistinct with non-strict left tables; * Fix erroneous key preservation in TableStage.mapPartition; * Add Consume to TypeCheck; * Fix error where globals were not exposed in TableAggregate; * Fix error where globals were not exposed in TableAggregate. New local backend success rate:. ```; 271 failed, 496 passed, 75 skipped, 15 warnings in 368.17 seconds; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9202
https://github.com/hail-is/hail/pull/9202:299,Availability,error,error,299,"* Fix error in TableKeyByAndAggregate caused by field name clobbering; * Fix error in TableLeftJoinRightDistinct with non-strict left tables; * Fix erroneous key preservation in TableStage.mapPartition; * Add Consume to TypeCheck; * Fix error where globals were not exposed in TableAggregate; * Fix error where globals were not exposed in TableAggregate. New local backend success rate:. ```; 271 failed, 496 passed, 75 skipped, 15 warnings in 368.17 seconds; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9202
https://github.com/hail-is/hail/pull/9202:266,Security,expose,exposed,266,"* Fix error in TableKeyByAndAggregate caused by field name clobbering; * Fix error in TableLeftJoinRightDistinct with non-strict left tables; * Fix erroneous key preservation in TableStage.mapPartition; * Add Consume to TypeCheck; * Fix error where globals were not exposed in TableAggregate; * Fix error where globals were not exposed in TableAggregate. New local backend success rate:. ```; 271 failed, 496 passed, 75 skipped, 15 warnings in 368.17 seconds; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9202
https://github.com/hail-is/hail/pull/9202:328,Security,expose,exposed,328,"* Fix error in TableKeyByAndAggregate caused by field name clobbering; * Fix error in TableLeftJoinRightDistinct with non-strict left tables; * Fix erroneous key preservation in TableStage.mapPartition; * Add Consume to TypeCheck; * Fix error where globals were not exposed in TableAggregate; * Fix error where globals were not exposed in TableAggregate. New local backend success rate:. ```; 271 failed, 496 passed, 75 skipped, 15 warnings in 368.17 seconds; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9202
https://github.com/hail-is/hail/pull/9205:131,Energy Efficiency,reduce,reduce,131,"This is the first in a series of changes that will push PCode through more of; the PType construction interfaces. The intent is to reduce code duplication,; and have as few interfaces as possible where `Code[_]` or `Code[Long]`; represents a hail type.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9205
https://github.com/hail-is/hail/pull/9205:102,Integrability,interface,interfaces,102,"This is the first in a series of changes that will push PCode through more of; the PType construction interfaces. The intent is to reduce code duplication,; and have as few interfaces as possible where `Code[_]` or `Code[Long]`; represents a hail type.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9205
https://github.com/hail-is/hail/pull/9205:173,Integrability,interface,interfaces,173,"This is the first in a series of changes that will push PCode through more of; the PType construction interfaces. The intent is to reduce code duplication,; and have as few interfaces as possible where `Code[_]` or `Code[Long]`; represents a hail type.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9205
https://github.com/hail-is/hail/issues/9216:398,Integrability,wrap,wrapping,398,"Batch should allow a user to get an output that that doesn't modify a resource group's identifier. This is useful in cases when the resource group is a series of files that are distinguished by name, not extension. Currently batch will output these as ""someOtherPrefix"".identifier1name.ext; ""someOtherPrefix"".identifier2name.ext; ""someOtherPrefix"".identifier3name.ext. even if the program batch is wrapping would otherwise output; identifier1name.ext; identifier2name.ext; identifier3name.ext. Regenie hits this",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9216
https://github.com/hail-is/hail/pull/9220:391,Availability,error,errors,391,"~~Stacked on #9106.~~. Adds an enumeration to control the allocation strategy in emitted code. There are currently three options, `Default`, `OneRegion`, and `ManyRegions`. `OneRegion` replicates the current behavior, and is the default for now. `ManyRegions` always allocates new regions when given the choice. This PR makes Java tests use the `ManyRegions` strategy to catch more lifetime errors. `Default` is somewhere in the middle, and will become the default. It will use one region (at least a constant number of regions) per row of a table. `Default` currently behaves the same as `ManyRegions`. Future work will implement the per-row behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9220
https://github.com/hail-is/hail/pull/9220:267,Energy Efficiency,allocate,allocates,267,"~~Stacked on #9106.~~. Adds an enumeration to control the allocation strategy in emitted code. There are currently three options, `Default`, `OneRegion`, and `ManyRegions`. `OneRegion` replicates the current behavior, and is the default for now. `ManyRegions` always allocates new regions when given the choice. This PR makes Java tests use the `ManyRegions` strategy to catch more lifetime errors. `Default` is somewhere in the middle, and will become the default. It will use one region (at least a constant number of regions) per row of a table. `Default` currently behaves the same as `ManyRegions`. Future work will implement the per-row behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9220
https://github.com/hail-is/hail/pull/9220:331,Testability,test,tests,331,"~~Stacked on #9106.~~. Adds an enumeration to control the allocation strategy in emitted code. There are currently three options, `Default`, `OneRegion`, and `ManyRegions`. `OneRegion` replicates the current behavior, and is the default for now. `ManyRegions` always allocates new regions when given the choice. This PR makes Java tests use the `ManyRegions` strategy to catch more lifetime errors. `Default` is somewhere in the middle, and will become the default. It will use one region (at least a constant number of regions) per row of a table. `Default` currently behaves the same as `ManyRegions`. Future work will implement the per-row behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9220
https://github.com/hail-is/hail/pull/9221:13,Deployability,update,update,13,"Furthermore, update build.gradle to (finally) not print all the; Unsafe warnings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9221
https://github.com/hail-is/hail/pull/9221:65,Safety,Unsafe,Unsafe,65,"Furthermore, update build.gradle to (finally) not print all the; Unsafe warnings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9221
https://github.com/hail-is/hail/pull/9222:894,Availability,error,error,894,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:486,Deployability,update,update,486,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:83,Integrability,message,message,83,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:295,Integrability,interface,interface,295,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:672,Integrability,interface,interface,672,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:727,Performance,load,loadings,727,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:337,Testability,test,test,337,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:463,Testability,benchmark,benchmarking,463,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:599,Testability,benchmark,benchmarking,599,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9222:622,Testability,test,test,622,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222
https://github.com/hail-is/hail/pull/9223:301,Availability,error,errors,301,"CHANGELOG: Fixed issue where ndarrays being sliced and indexed into in one expression didn't have sufficient bounds checks. Fixes #9144 by checking if indices provided in a slicing expression that mixes slices and single indices are out of bounds. . Alex, please tell me if you're able to get anymore errors from #9144 with this change. I wasn't able to, but that issue was not super well defined at first so it's unclear to me if this covers all of your problems. I retitled the issue to reflect my understanding of the problems. . More detail:. In numpy, it is ok for the upper bound of a slice to go past the end of an array, but it is not ok for an indexing operation to do the same. For example:. ```; n = np.array([[1,2,3,4], [1,2,3,4]]); n[0:1000, 0:1000]; ```. is allowed, the bounds just get clamped. However, this is not allowed:. ```; n[1000, 1000]; ```. nor is:. ```; n[1000, 0:1000]; ```. Hail was not handling that last case with a mix of slices and indices correctly. Namely, it was not throwing an out bounds error on the first axis index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223
https://github.com/hail-is/hail/pull/9223:1025,Availability,error,error,1025,"CHANGELOG: Fixed issue where ndarrays being sliced and indexed into in one expression didn't have sufficient bounds checks. Fixes #9144 by checking if indices provided in a slicing expression that mixes slices and single indices are out of bounds. . Alex, please tell me if you're able to get anymore errors from #9144 with this change. I wasn't able to, but that issue was not super well defined at first so it's unclear to me if this covers all of your problems. I retitled the issue to reflect my understanding of the problems. . More detail:. In numpy, it is ok for the upper bound of a slice to go past the end of an array, but it is not ok for an indexing operation to do the same. For example:. ```; n = np.array([[1,2,3,4], [1,2,3,4]]); n[0:1000, 0:1000]; ```. is allowed, the bounds just get clamped. However, this is not allowed:. ```; n[1000, 1000]; ```. nor is:. ```; n[1000, 0:1000]; ```. Hail was not handling that last case with a mix of slices and indices correctly. Namely, it was not throwing an out bounds error on the first axis index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223
https://github.com/hail-is/hail/issues/9226:2358,Availability,Error,Error,2358,"r_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/projects/hail/hail/python/hail/backend/spark_backend.py in deco(*args, **kwargs); 39 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 40 'Hail version: %s\n'; ---> 41 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 42 except pyspark.sql.utils.CapturedException as e:; 43 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Index 5 is out of bounds for axis 0 with size 2. Java stack trace:; is.hail.utils.HailException: Index 5 is out of bounds for axis 0 with size 2; 	at __C889Compiled.apply(Unknown Source); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply$mcJ$sp(CompileAndEvaluate.scala:41); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply(CompileAndEvaluate.scala:41); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$1.apply(CompileAndEvaluate.scala:41); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:41); 	at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318); 	at is.hail.backend.spark.SparkBackend$$anonfun$exe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:4774,Availability,Error,Error,4774," 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305); 	at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); 	at is.hail.utils.package$.using(package.scala:609); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304); 	at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.53-e245ebe1d9a6; Error summary: HailException: Index 5 is out of bounds for axis 0 with size 2; ```. Numpy:; ```sh; In [22]: hl.eval(ndarray_t[5,0:1000]) ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-22-0643c9661056> in <module>; ----> 1 hl.eval(ndarray_t[5,0:1000]). IndexError: index 5 is out of bounds for axis 0 with size 2; ```. The stack trace is uninformative, and the typical Hail user will not want to see it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:419,Integrability,wrap,wrapper,419,"Found during ndarray debugging, but generally applies:. Sumary of issue:; ```sh; In [21]: hl.eval(hl_nd_array_t[5,0:1000]) ; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-21-f94ec164ffe0> in <module>; ----> 1 hl.eval(t[5,0:1000]). <decorator-gen-716> in eval(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval(expression); 188 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:470,Integrability,wrap,wrapper,470,"Found during ndarray debugging, but generally applies:. Sumary of issue:; ```sh; In [21]: hl.eval(hl_nd_array_t[5,0:1000]) ; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-21-f94ec164ffe0> in <module>; ----> 1 hl.eval(t[5,0:1000]). <decorator-gen-716> in eval(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval(expression); 188 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:676,Integrability,wrap,wrapper,676,"Found during ndarray debugging, but generally applies:. Sumary of issue:; ```sh; In [21]: hl.eval(hl_nd_array_t[5,0:1000]) ; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-21-f94ec164ffe0> in <module>; ----> 1 hl.eval(t[5,0:1000]). <decorator-gen-716> in eval(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval(expression); 188 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:950,Integrability,wrap,wrapper,950,"; In [21]: hl.eval(hl_nd_array_t[5,0:1000]) ; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-21-f94ec164ffe0> in <module>; ----> 1 hl.eval(t[5,0:1000]). <decorator-gen-716> in eval(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval(expression); 188 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:1001,Integrability,wrap,wrapper,1001,"; In [21]: hl.eval(hl_nd_array_t[5,0:1000]) ; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-21-f94ec164ffe0> in <module>; ----> 1 hl.eval(t[5,0:1000]). <decorator-gen-716> in eval(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval(expression); 188 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:1207,Integrability,wrap,wrapper,1207,"; In [21]: hl.eval(hl_nd_array_t[5,0:1000]) ; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-21-f94ec164ffe0> in <module>; ----> 1 hl.eval(t[5,0:1000]). <decorator-gen-716> in eval(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval(expression); 188 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/issues/9226:1772,Performance,load,loads,1772,"88 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/projects/hail/hail/python/hail/backend/spark_backend.py in deco(*args, **kwargs); 39 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 40 'Hail version: %s\n'; ---> 41 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 42 except pyspark.sql.utils.CapturedException as e:; 43 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Index 5 is out of bounds for axis 0 with size 2. Java stack trace:; is.hail.utils.HailException: Index 5 is out of bounds for axis 0 with size 2; 	at __C889Compiled.apply(Unknown Source); 	at is.hail.expr.ir.Com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226
https://github.com/hail-is/hail/pull/9228:145,Integrability,interface,interface,145,"- implements ""Block Lanczos"" / Blanczos algorithm for randomized SVD in Hail with HailTables; - adds method for Blanczos PCA following the exact interface of the current PCA method in Hail; - adds tests comparing Blanczos PCA with numpy and with Hail's current PCA",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9228
https://github.com/hail-is/hail/pull/9228:197,Testability,test,tests,197,"- implements ""Block Lanczos"" / Blanczos algorithm for randomized SVD in Hail with HailTables; - adds method for Blanczos PCA following the exact interface of the current PCA method in Hail; - adds tests comparing Blanczos PCA with numpy and with Hail's current PCA",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9228
https://github.com/hail-is/hail/pull/9229:80,Testability,test,testing,80,A commit from a few months ago removed the ability to specify a single file for testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9229
https://github.com/hail-is/hail/pull/9233:169,Safety,avoid,avoid,169,In #9176 Dan and I settled on Docker Hub as the preferred name for that service. This PR makes that consistent across batch docs. I left `docker_resources.rst` alone to avoid conflicting with #9176.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9233
https://github.com/hail-is/hail/pull/9234:544,Usability,clear,clears,544,"~~Stacked on #9232.~~. This adds memory management to StreamGrouped and StreamGroupByKey. As always, the case where the inner stream is never consumed causes complications. Because the child stream gets its per-element region from the consumer of the inner stream, if the inner stream is unconsumed we have to pass some other region. The solution implemented here uses the outer stream's eltRegion. If the outer stream's consumer allows creating new regions, the grouping node creates an owned region to construct child stream elements in, and clears it every element (because it is never passed to a consumer). Otherwise, all child elements get created in the outer stream's eltRegion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9234
https://github.com/hail-is/hail/pull/9239:7,Testability,benchmark,benchmarks,7,"To run benchmarks, you'll need to grant your batch service account permissions:. 1. grab service account name from https://auth.hail.is/user. Then:; ```; export BATCH_SERVICE_ACCOUNT=""...""; gsutil iam ch serviceAccount:${BATCH_SERVICE_ACCOUNT}:objectAdmin gs://hail-benchmarks-2; gsutil iam ch serviceAccount:${BATCH_SERVICE_ACCOUNT}:objectViewer gs://artifacts.broad-ctsa.appspot.com; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9239
https://github.com/hail-is/hail/pull/9239:266,Testability,benchmark,benchmarks-,266,"To run benchmarks, you'll need to grant your batch service account permissions:. 1. grab service account name from https://auth.hail.is/user. Then:; ```; export BATCH_SERVICE_ACCOUNT=""...""; gsutil iam ch serviceAccount:${BATCH_SERVICE_ACCOUNT}:objectAdmin gs://hail-benchmarks-2; gsutil iam ch serviceAccount:${BATCH_SERVICE_ACCOUNT}:objectViewer gs://artifacts.broad-ctsa.appspot.com; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9239
https://github.com/hail-is/hail/pull/9241:47,Energy Efficiency,monitor,monitoring,47,- Added BigQuery client for aiogoogle; - Added monitoring service; - Added billing portion which queries BigQuery for the billing data in broad-ctsa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9241
https://github.com/hail-is/hail/pull/9242:446,Modifiability,inherit,inheriting,446,"Was hitting a very annoying bug because `PCanonicalNDArray` didn't explicitly override the default. Creators of new `PType`s should not have to magically know that this method exists and override it. It should not have a default implementation. . I had to specify that `PCanonicalNDArray` does have pointers, and that `PVoid` and `PPrimitive` don't. I said `false` for `PCanonicalStream` too, but that was less clear. `false` is the value it was inheriting previously, but idk if you can even `deepCopy` a stream. If you can, then maybe it should recur to the `elementType`? You'd know best Patrick.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9242
https://github.com/hail-is/hail/pull/9242:411,Usability,clear,clear,411,"Was hitting a very annoying bug because `PCanonicalNDArray` didn't explicitly override the default. Creators of new `PType`s should not have to magically know that this method exists and override it. It should not have a default implementation. . I had to specify that `PCanonicalNDArray` does have pointers, and that `PVoid` and `PPrimitive` don't. I said `false` for `PCanonicalStream` too, but that was less clear. `false` is the value it was inheriting previously, but idk if you can even `deepCopy` a stream. If you can, then maybe it should recur to the `elementType`? You'd know best Patrick.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9242
https://github.com/hail-is/hail/pull/9249:53,Deployability,deploy,deploy,53,"I don't know how to test this works unless I can dev deploy to my own copy of CI that's running with the new changes. The issue I was seeing is the database step runs fine, but the new tables weren't actually created.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9249
https://github.com/hail-is/hail/pull/9249:20,Testability,test,test,20,"I don't know how to test this works unless I can dev deploy to my own copy of CI that's running with the new changes. The issue I was seeing is the database step runs fine, but the new tables weren't actually created.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9249
https://github.com/hail-is/hail/pull/9250:51,Deployability,release,release-notes,51,"[gcloud 284.0.0](https://cloud.google.com/sdk/docs/release-notes#28400_2020-03-10) deprecated the `--num-preemptible-workers` and `--preemptible-worker-boot-disk-size` arguments to `gcloud dataproc clusters create`. [gcloud 285.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) deprecated the `--num-preemptible-workers` argument to `gcloud dataproc clusters update`. This replaces `--num-preemptible-workers` with `--num-secondary-workers` and `--preemptible-worker-boot-disk-size` with `--secondary-worker-boot-disk-size` in calls to `gcloud` from `hailctl dataproc start` and `hailctl dataproc modify`. Since the new arguments were added in gcloud version 285.0.0 (released in March 2020), this also adds a requirement that gcloud be at least version 285.0.0. Alternatively, the arguments could be switched based on the gcloud version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9250
https://github.com/hail-is/hail/pull/9250:267,Deployability,release,release-notes,267,"[gcloud 284.0.0](https://cloud.google.com/sdk/docs/release-notes#28400_2020-03-10) deprecated the `--num-preemptible-workers` and `--preemptible-worker-boot-disk-size` arguments to `gcloud dataproc clusters create`. [gcloud 285.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) deprecated the `--num-preemptible-workers` argument to `gcloud dataproc clusters update`. This replaces `--num-preemptible-workers` with `--num-secondary-workers` and `--preemptible-worker-boot-disk-size` with `--secondary-worker-boot-disk-size` in calls to `gcloud` from `hailctl dataproc start` and `hailctl dataproc modify`. Since the new arguments were added in gcloud version 285.0.0 (released in March 2020), this also adds a requirement that gcloud be at least version 285.0.0. Alternatively, the arguments could be switched based on the gcloud version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9250
https://github.com/hail-is/hail/pull/9250:380,Deployability,update,update,380,"[gcloud 284.0.0](https://cloud.google.com/sdk/docs/release-notes#28400_2020-03-10) deprecated the `--num-preemptible-workers` and `--preemptible-worker-boot-disk-size` arguments to `gcloud dataproc clusters create`. [gcloud 285.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) deprecated the `--num-preemptible-workers` argument to `gcloud dataproc clusters update`. This replaces `--num-preemptible-workers` with `--num-secondary-workers` and `--preemptible-worker-boot-disk-size` with `--secondary-worker-boot-disk-size` in calls to `gcloud` from `hailctl dataproc start` and `hailctl dataproc modify`. Since the new arguments were added in gcloud version 285.0.0 (released in March 2020), this also adds a requirement that gcloud be at least version 285.0.0. Alternatively, the arguments could be switched based on the gcloud version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9250
https://github.com/hail-is/hail/pull/9250:689,Deployability,release,released,689,"[gcloud 284.0.0](https://cloud.google.com/sdk/docs/release-notes#28400_2020-03-10) deprecated the `--num-preemptible-workers` and `--preemptible-worker-boot-disk-size` arguments to `gcloud dataproc clusters create`. [gcloud 285.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) deprecated the `--num-preemptible-workers` argument to `gcloud dataproc clusters update`. This replaces `--num-preemptible-workers` with `--num-secondary-workers` and `--preemptible-worker-boot-disk-size` with `--secondary-worker-boot-disk-size` in calls to `gcloud` from `hailctl dataproc start` and `hailctl dataproc modify`. Since the new arguments were added in gcloud version 285.0.0 (released in March 2020), this also adds a requirement that gcloud be at least version 285.0.0. Alternatively, the arguments could be switched based on the gcloud version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9250
https://github.com/hail-is/hail/pull/9255:30,Deployability,deploy,deploy,30,I checked this works with dev deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9255
https://github.com/hail-is/hail/pull/9256:50,Testability,benchmark,benchmark,50,I think this is all that needs to be added to get benchmark in the header. There's no caret after so we don't need to format that at all.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9256
https://github.com/hail-is/hail/pull/9257:20,Modifiability,config,config,20,The contents of ssl-config are not named after the service in question. See create_certs.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9257
https://github.com/hail-is/hail/pull/9259:301,Availability,error,error,301,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:1173,Availability,error,error,1173,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:1278,Availability,error,error,1278,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:634,Deployability,configurat,configuration,634,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:1256,Energy Efficiency,monitor,monitoring,1256,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:109,Modifiability,config,config,109,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:634,Modifiability,config,configuration,634,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:598,Safety,avoid,avoid,598,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:769,Testability,log,logic,769,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:1193,Testability,log,logs,1193,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:1284,Testability,log,logs,1284,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9259:1372,Testability,log,logs,1372,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9259
https://github.com/hail-is/hail/pull/9262:234,Availability,avail,available,234,"CHANGELOG: Teach `hailctl dataproc` to use new `gcloud` flag names which suppresses the warnings about `--num-secondary-workers`. `hailctl` now requires at least gcloud 284.0.0. Run `gcloud components update` to update. This has been available since March, it seems high time to fix this and recommend people update gcloud.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9262
https://github.com/hail-is/hail/pull/9262:201,Deployability,update,update,201,"CHANGELOG: Teach `hailctl dataproc` to use new `gcloud` flag names which suppresses the warnings about `--num-secondary-workers`. `hailctl` now requires at least gcloud 284.0.0. Run `gcloud components update` to update. This has been available since March, it seems high time to fix this and recommend people update gcloud.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9262
https://github.com/hail-is/hail/pull/9262:212,Deployability,update,update,212,"CHANGELOG: Teach `hailctl dataproc` to use new `gcloud` flag names which suppresses the warnings about `--num-secondary-workers`. `hailctl` now requires at least gcloud 284.0.0. Run `gcloud components update` to update. This has been available since March, it seems high time to fix this and recommend people update gcloud.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9262
https://github.com/hail-is/hail/pull/9262:309,Deployability,update,update,309,"CHANGELOG: Teach `hailctl dataproc` to use new `gcloud` flag names which suppresses the warnings about `--num-secondary-workers`. `hailctl` now requires at least gcloud 284.0.0. Run `gcloud components update` to update. This has been available since March, it seems high time to fix this and recommend people update gcloud.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9262
https://github.com/hail-is/hail/pull/9263:200,Availability,avail,available,200,"CHANGELOG: Teach `hailctl dataproc start` about `--expiration-time`. Teach `hailctl dataproc modify` about `--no-max-idle`, `no-max-age`, `--max-age`, and `--expiration-time`. These flags were always available as pass-throughs. This change adds them to the; help documentation and improves error messages in the case that an incompatible; set of arguments are provided together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9263
https://github.com/hail-is/hail/pull/9263:290,Availability,error,error,290,"CHANGELOG: Teach `hailctl dataproc start` about `--expiration-time`. Teach `hailctl dataproc modify` about `--no-max-idle`, `no-max-age`, `--max-age`, and `--expiration-time`. These flags were always available as pass-throughs. This change adds them to the; help documentation and improves error messages in the case that an incompatible; set of arguments are provided together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9263
https://github.com/hail-is/hail/pull/9263:296,Integrability,message,messages,296,"CHANGELOG: Teach `hailctl dataproc start` about `--expiration-time`. Teach `hailctl dataproc modify` about `--no-max-idle`, `no-max-age`, `--max-age`, and `--expiration-time`. These flags were always available as pass-throughs. This change adds them to the; help documentation and improves error messages in the case that an incompatible; set of arguments are provided together.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9263
https://github.com/hail-is/hail/pull/9273:55,Deployability,deploy,deploying,55,"Context: `batch/Makefile` is for manually building and deploying batch from your local machine. This is rare, and is done when we are bootstrapping a new cluster, or something goes wrong and we need to manually intervene. The service Makefiles have three main targets: `build`, `push` and `deploy`. `build` builds docker images for the service locally. `push` pushes them to the Google Container Repository. `deploy` deploys the new image to Kubernetes. The `build` target builds a new image, and BATCH_IMAGE uses docker to get the current image. `:=` means evaluate once. This means the JINJA_ENVIRONMENT had the image when `make` was invoked. So if a new image was built, deploy would deploy the wrong image. @danking @jigold I wonder if this might explain surprising behavior when you were working on the batch issue earlier today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9273
https://github.com/hail-is/hail/pull/9273:290,Deployability,deploy,deploy,290,"Context: `batch/Makefile` is for manually building and deploying batch from your local machine. This is rare, and is done when we are bootstrapping a new cluster, or something goes wrong and we need to manually intervene. The service Makefiles have three main targets: `build`, `push` and `deploy`. `build` builds docker images for the service locally. `push` pushes them to the Google Container Repository. `deploy` deploys the new image to Kubernetes. The `build` target builds a new image, and BATCH_IMAGE uses docker to get the current image. `:=` means evaluate once. This means the JINJA_ENVIRONMENT had the image when `make` was invoked. So if a new image was built, deploy would deploy the wrong image. @danking @jigold I wonder if this might explain surprising behavior when you were working on the batch issue earlier today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9273
https://github.com/hail-is/hail/pull/9273:409,Deployability,deploy,deploy,409,"Context: `batch/Makefile` is for manually building and deploying batch from your local machine. This is rare, and is done when we are bootstrapping a new cluster, or something goes wrong and we need to manually intervene. The service Makefiles have three main targets: `build`, `push` and `deploy`. `build` builds docker images for the service locally. `push` pushes them to the Google Container Repository. `deploy` deploys the new image to Kubernetes. The `build` target builds a new image, and BATCH_IMAGE uses docker to get the current image. `:=` means evaluate once. This means the JINJA_ENVIRONMENT had the image when `make` was invoked. So if a new image was built, deploy would deploy the wrong image. @danking @jigold I wonder if this might explain surprising behavior when you were working on the batch issue earlier today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9273
https://github.com/hail-is/hail/pull/9273:417,Deployability,deploy,deploys,417,"Context: `batch/Makefile` is for manually building and deploying batch from your local machine. This is rare, and is done when we are bootstrapping a new cluster, or something goes wrong and we need to manually intervene. The service Makefiles have three main targets: `build`, `push` and `deploy`. `build` builds docker images for the service locally. `push` pushes them to the Google Container Repository. `deploy` deploys the new image to Kubernetes. The `build` target builds a new image, and BATCH_IMAGE uses docker to get the current image. `:=` means evaluate once. This means the JINJA_ENVIRONMENT had the image when `make` was invoked. So if a new image was built, deploy would deploy the wrong image. @danking @jigold I wonder if this might explain surprising behavior when you were working on the batch issue earlier today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9273
https://github.com/hail-is/hail/pull/9273:674,Deployability,deploy,deploy,674,"Context: `batch/Makefile` is for manually building and deploying batch from your local machine. This is rare, and is done when we are bootstrapping a new cluster, or something goes wrong and we need to manually intervene. The service Makefiles have three main targets: `build`, `push` and `deploy`. `build` builds docker images for the service locally. `push` pushes them to the Google Container Repository. `deploy` deploys the new image to Kubernetes. The `build` target builds a new image, and BATCH_IMAGE uses docker to get the current image. `:=` means evaluate once. This means the JINJA_ENVIRONMENT had the image when `make` was invoked. So if a new image was built, deploy would deploy the wrong image. @danking @jigold I wonder if this might explain surprising behavior when you were working on the batch issue earlier today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9273
https://github.com/hail-is/hail/pull/9273:687,Deployability,deploy,deploy,687,"Context: `batch/Makefile` is for manually building and deploying batch from your local machine. This is rare, and is done when we are bootstrapping a new cluster, or something goes wrong and we need to manually intervene. The service Makefiles have three main targets: `build`, `push` and `deploy`. `build` builds docker images for the service locally. `push` pushes them to the Google Container Repository. `deploy` deploys the new image to Kubernetes. The `build` target builds a new image, and BATCH_IMAGE uses docker to get the current image. `:=` means evaluate once. This means the JINJA_ENVIRONMENT had the image when `make` was invoked. So if a new image was built, deploy would deploy the wrong image. @danking @jigold I wonder if this might explain surprising behavior when you were working on the batch issue earlier today.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9273
https://github.com/hail-is/hail/pull/9276:25,Testability,test,tests,25,This should speed up the tests and fully revert everything,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9276
https://github.com/hail-is/hail/pull/9278:8,Security,expose,exposes,8,This PR exposes `StreamGrouped` IR node in python as a `grouped(group_size)` method on `ArrayExpression`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9278
https://github.com/hail-is/hail/pull/9284:170,Testability,Benchmark,Benchmarks,170,"<img width=""922"" alt=""Screen Shot 2020-08-17 at 10 52 59 AM"" src=""https://user-images.githubusercontent.com/57302458/90410041-e1aa8400-e077-11ea-8fc0-7eabb9b974f1.png"">. Benchmarks with different tests:; <img width=""905"" alt=""Screen Shot 2020-08-19 at 9 55 34 AM"" src=""https://user-images.githubusercontent.com/57302458/90643950-29f5ad80-e202-11ea-878c-275fca3d5a5f.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9284
https://github.com/hail-is/hail/pull/9284:196,Testability,test,tests,196,"<img width=""922"" alt=""Screen Shot 2020-08-17 at 10 52 59 AM"" src=""https://user-images.githubusercontent.com/57302458/90410041-e1aa8400-e077-11ea-8fc0-7eabb9b974f1.png"">. Benchmarks with different tests:; <img width=""905"" alt=""Screen Shot 2020-08-19 at 9 55 34 AM"" src=""https://user-images.githubusercontent.com/57302458/90643950-29f5ad80-e202-11ea-878c-275fca3d5a5f.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9284
https://github.com/hail-is/hail/pull/9285:0,Security,Expose,Expose,0,Expose in front end; Currently unused,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285
https://github.com/hail-is/hail/pull/9289:82,Performance,load,loadField,82,It's wildly unsafe. It's better to scope the unsafety in; `IEmitCode.handle` for `loadField`. Also add `PNDArrayValue.shapes` to handle the previous use case of; `PBaseStructValue.apply`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9289
https://github.com/hail-is/hail/pull/9289:12,Safety,unsafe,unsafe,12,It's wildly unsafe. It's better to scope the unsafety in; `IEmitCode.handle` for `loadField`. Also add `PNDArrayValue.shapes` to handle the previous use case of; `PBaseStructValue.apply`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9289
https://github.com/hail-is/hail/pull/9289:45,Safety,unsafe,unsafety,45,It's wildly unsafe. It's better to scope the unsafety in; `IEmitCode.handle` for `loadField`. Also add `PNDArrayValue.shapes` to handle the previous use case of; `PBaseStructValue.apply`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9289
https://github.com/hail-is/hail/pull/9292:75,Testability,benchmark,benchmark,75,I'm having a lot of of trouble demonstrating that this is faster using the benchmark system. I can see a marked improvement in shuffle benchmarks using my laptop:; ```; $ hb compare /tmp/lz4-before.json /tmp/lz4-after.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; shuffle_order_by_10m_int 70.4% 47.193 33.245; ```. I also don't see any slowdown for the larger shuffles (which do get slower if we use one of the uncompressed codecs). Running benchmarks on batch have produced results where the median varies from 1% slower to 6% faster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9292
https://github.com/hail-is/hail/pull/9292:135,Testability,benchmark,benchmarks,135,I'm having a lot of of trouble demonstrating that this is faster using the benchmark system. I can see a marked improvement in shuffle benchmarks using my laptop:; ```; $ hb compare /tmp/lz4-before.json /tmp/lz4-after.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; shuffle_order_by_10m_int 70.4% 47.193 33.245; ```. I also don't see any slowdown for the larger shuffles (which do get slower if we use one of the uncompressed codecs). Running benchmarks on batch have produced results where the median varies from 1% slower to 6% faster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9292
https://github.com/hail-is/hail/pull/9292:224,Testability,Benchmark,Benchmark,224,I'm having a lot of of trouble demonstrating that this is faster using the benchmark system. I can see a marked improvement in shuffle benchmarks using my laptop:; ```; $ hb compare /tmp/lz4-before.json /tmp/lz4-after.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; shuffle_order_by_10m_int 70.4% 47.193 33.245; ```. I also don't see any slowdown for the larger shuffles (which do get slower if we use one of the uncompressed codecs). Running benchmarks on batch have produced results where the median varies from 1% slower to 6% faster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9292
https://github.com/hail-is/hail/pull/9292:473,Testability,benchmark,benchmarks,473,I'm having a lot of of trouble demonstrating that this is faster using the benchmark system. I can see a marked improvement in shuffle benchmarks using my laptop:; ```; $ hb compare /tmp/lz4-before.json /tmp/lz4-after.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; shuffle_order_by_10m_int 70.4% 47.193 33.245; ```. I also don't see any slowdown for the larger shuffles (which do get slower if we use one of the uncompressed codecs). Running benchmarks on batch have produced results where the median varies from 1% slower to 6% faster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9292
https://github.com/hail-is/hail/issues/9293:1096,Availability,avail,available,1096,"ion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2777,Availability,Error,Error,2777," 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2966,Availability,error,error,2966," File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:5141,Availability,failure,failure,5141,"eContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 9586 in stage 2.0 failed 4 times, most recent failure: Lost task 9586.3 in stage 2.0 (TID 40203, scc-q21.scc.bu.edu, executor 13): java.lang.IllegalArgumentException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at java.io.FilterOutputStream.close(FilterOutputStream.java:159); at is.hail.utils.package$.using(package.scala:603); at is.hail.io.RichContextRDDRegionValue$.writeSplitRegion(RichContextRDDRegionValue.scala:99); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:939); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:937); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupReg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:5201,Availability,failure,failure,5201,"xecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 9586 in stage 2.0 failed 4 times, most recent failure: Lost task 9586.3 in stage 2.0 (TID 40203, scc-q21.scc.bu.edu, executor 13): java.lang.IllegalArgumentException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at java.io.FilterOutputStream.close(FilterOutputStream.java:159); at is.hail.utils.package$.using(package.scala:603); at is.hail.io.RichContextRDDRegionValue$.writeSplitRegion(RichContextRDDRegionValue.scala:99); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:939); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:937); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22); at scala.col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18387,Availability,Error,Error,18387,"un(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18516,Availability,echo,echo,18516,"ion: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18750,Availability,echo,echo,18750,"hannels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; ""$@""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18773,Availability,echo,echo,18773,"hannels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; ""$@""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18920,Availability,echo,echo,18920,"hannels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; ""$@""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:776,Deployability,install,install,776,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:813,Deployability,install,install,813,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:852,Deployability,install,install,852,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:1300,Deployability,pipeline,pipelines,1300,"hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:1877,Deployability,pipeline,pipelines,1877,"oop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketEx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2067,Deployability,install,install,2067,"rsion 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2225,Deployability,install,install,2225,"64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2393,Deployability,install,install,2393,"ayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.Lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2573,Deployability,install,install,2573,"====>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2690,Deployability,install,install,2690," INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:19353,Deployability,install,install,19353,"hannels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; ""$@""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:19440,Deployability,deploy,deploy-mode,19440,"hannels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; ""$@""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:7635,Energy Efficiency,schedul,scheduler,7635,lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:7706,Energy Efficiency,schedul,scheduler,7706,la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9105,Energy Efficiency,schedul,scheduler,9105,n(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9145,Energy Efficiency,schedul,scheduler,9145,ead.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9243,Energy Efficiency,schedul,scheduler,9243,.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9340,Energy Efficiency,schedul,scheduler,9340,ket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9591,Energy Efficiency,schedul,scheduler,9591,adoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9671,Energy Efficiency,schedul,scheduler,9671,org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9776,Energy Efficiency,schedul,scheduler,9776,hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9924,Energy Efficiency,schedul,scheduler,9924,nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:10012,Energy Efficiency,schedul,scheduler,10012,eam$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:10109,Energy Efficiency,schedul,scheduler,10109,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); at is.hail.rvd.RVD.wri,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:10204,Energy Efficiency,schedul,scheduler,10204,.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:954); at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:224); at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:10367,Energy Efficiency,schedul,scheduler,10367,abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:954); at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:224); at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:41); at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:25); at is.hail.expr.ir.Interpret$.ru,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:16933,Energy Efficiency,schedul,scheduler,16933,lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:17004,Energy Efficiency,schedul,scheduler,17004,la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2142,Integrability,wrap,wrapper,2142,"_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:3327,Integrability,Wrap,WrappedArray,3327,"ixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:3348,Integrability,Wrap,WrappedArray,3348,", writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.execu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:11290,Integrability,Wrap,WrappedMatrixWriter,11290,til.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:954); at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:224); at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:41); at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:25); at is.hail.expr.ir.Interpret$.run(Interpret.scala:726); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:12828,Integrability,Wrap,WrappedArray,12828,m(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.S,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:12849,Integrability,Wrap,WrappedArray,12849,ala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.execu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:11643,Modifiability,rewrite,rewrite,11643,.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:954); at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:224); at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:41); at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:25); at is.hail.expr.ir.Interpret$.run(Interpret.scala:726); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:736,Performance,Load,Loading,736,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:2497,Performance,load,loads,2497,"dre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: SocketException: Too many open files. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(Compil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:7987,Performance,concurren,concurrent,7987,); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:8071,Performance,concurren,concurrent,8071, scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stack,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:17285,Performance,concurren,concurrent,17285,); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:17369,Performance,concurren,concurrent,17369, scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18522,Performance,Load,Loading,18522,"ion: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18547,Performance,load,load,18547,"ion: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18583,Performance,load,load,18583,"io.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18615,Performance,load,load,18615,"t sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:18640,Performance,load,load,18640,"ava:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6ef64c08b000; Error summary: SocketException: Too many open files; ```. This is the hail-submit script; ```bash; #!/bin/bash -l; module purge; echo ""Loading modules""; module load python3/3.7.7 #cj: new; module load gcc/8.3.0 #cj: new; module load spark/2.4.3; module load hail/0.2.46 #cj: new. export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/hdp/2.6.5.0-292/hadoop/lib/native/""; echo $LD_LIBRARY_PATH; echo ""Export env vars""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit \; --executor-cores 5 \; --executor-memory 40G \; --driver-memory 20g \; --driver-cores 5 \; --num-executors 40 \; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH \; --conf spark.executor.extraLibraryPath=$LD_LIBRARY_PATH \; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH \; --conf spark.yarn.appMasterEnv.PATH=$PATH \; --conf spark.yarn.jars=/share/pkg.7/spark/2.4.3/install/jars/*jar\; --jars $HAIL_HOME/backend/hail-all-spark.jar \; --master yarn \; --deploy-mode client \; --conf spark.driver.memory=20G \; --conf spark.executor.memory=40G \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/backend/hail-all-spark.jar\"" \; --conf spark.executor.extraCl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:5120,Safety,abort,aborted,5120,"eContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 9586 in stage 2.0 failed 4 times, most recent failure: Lost task 9586.3 in stage 2.0 (TID 40203, scc-q21.scc.bu.edu, executor 13): java.lang.IllegalArgumentException: Self-suppression not permitted; at java.lang.Throwable.addSuppressed(Throwable.java:1043); at java.io.FilterOutputStream.close(FilterOutputStream.java:159); at is.hail.utils.package$.using(package.scala:603); at is.hail.io.RichContextRDDRegionValue$.writeSplitRegion(RichContextRDDRegionValue.scala:99); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:939); at is.hail.rvd.RVD$$anonfun$25.apply(RVD.scala:937); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$18.apply(ContextRDD.scala:248); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22); at is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupReg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9275,Safety,abort,abortStage,9275,tive Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9372,Safety,abort,abortStage,9372,sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:9614,Safety,abort,abortStage,9614,y.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:1244,Testability,LOG,LOGGING,1244,"to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/issues/9293:1366,Testability,log,log,1366,"t/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decorator-gen-1213>"", line 2, in write; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 2524, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/share/pkg.7/hail/0.2.46/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293
https://github.com/hail-is/hail/pull/9297:526,Availability,echo,echoddd,526,"CHANGELOG: fix LocalBackend.run() succeeding when intermediate command fails. Stacked on #9219 as that PR is essentially approved, and to avoid a merge conflict. The commit in this PR is https://github.com/hail-is/hail/pull/9297/commits/cbc3bbe7f14c01d44c89995a03375d983fc14f4f. Caused by associativity of the ternary conditional ('set -e' + 'x' is the operand `a` in `a if cond else b`). Easy reproduction case on main:. ```python; def test_single_job_with_mixed_shells(self):; b = self.batch(); j = b.new_job(); j.command(f'echoddd ""hello""'); j2 = b.new_job(); j2.command(f'echo ""world""'). self.assertRaises(Exception, b.run); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297
https://github.com/hail-is/hail/pull/9297:576,Availability,echo,echo,576,"CHANGELOG: fix LocalBackend.run() succeeding when intermediate command fails. Stacked on #9219 as that PR is essentially approved, and to avoid a merge conflict. The commit in this PR is https://github.com/hail-is/hail/pull/9297/commits/cbc3bbe7f14c01d44c89995a03375d983fc14f4f. Caused by associativity of the ternary conditional ('set -e' + 'x' is the operand `a` in `a if cond else b`). Easy reproduction case on main:. ```python; def test_single_job_with_mixed_shells(self):; b = self.batch(); j = b.new_job(); j.command(f'echoddd ""hello""'); j2 = b.new_job(); j2.command(f'echo ""world""'). self.assertRaises(Exception, b.run); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297
https://github.com/hail-is/hail/pull/9297:138,Safety,avoid,avoid,138,"CHANGELOG: fix LocalBackend.run() succeeding when intermediate command fails. Stacked on #9219 as that PR is essentially approved, and to avoid a merge conflict. The commit in this PR is https://github.com/hail-is/hail/pull/9297/commits/cbc3bbe7f14c01d44c89995a03375d983fc14f4f. Caused by associativity of the ternary conditional ('set -e' + 'x' is the operand `a` in `a if cond else b`). Easy reproduction case on main:. ```python; def test_single_job_with_mixed_shells(self):; b = self.batch(); j = b.new_job(); j.command(f'echoddd ""hello""'); j2 = b.new_job(); j2.command(f'echo ""world""'). self.assertRaises(Exception, b.run); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297
https://github.com/hail-is/hail/pull/9297:597,Testability,assert,assertRaises,597,"CHANGELOG: fix LocalBackend.run() succeeding when intermediate command fails. Stacked on #9219 as that PR is essentially approved, and to avoid a merge conflict. The commit in this PR is https://github.com/hail-is/hail/pull/9297/commits/cbc3bbe7f14c01d44c89995a03375d983fc14f4f. Caused by associativity of the ternary conditional ('set -e' + 'x' is the operand `a` in `a if cond else b`). Easy reproduction case on main:. ```python; def test_single_job_with_mixed_shells(self):; b = self.batch(); j = b.new_job(); j.command(f'echoddd ""hello""'); j2 = b.new_job(); j2.command(f'echo ""world""'). self.assertRaises(Exception, b.run); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297
https://github.com/hail-is/hail/pull/9298:1047,Availability,error,error,1047,"a2c778969c1422ba. The tabix line iterator works by building a list of virtual offsets from; the index that correspond to a requested interval. We recently; discovered an issue that would lead to a runtime exception if the; following conditions held:. * An offset pair ended exactly on a block boundary.; * The block boundary was exactly the start of a line. In a blocked file with virtual offsets, there are two ways to point to; the start of every block, (previous block start offset, previous block size) or; (current block start offset, 0). Because of the way curOff was calculated in TabixLineIterator, this would; lead to a situation where curOff was the (previous block start offset, previous block size); value, causing the jump to next chunk comparision `!TbiOrd.less64(curOff, offsets(i)._2)`; to fail when it should succeed, causing an extra line to be read, which; then makes the assertion check in the next iteration. Here we revert the improvements made earlier in; 3302850eb9d93c30793dc231a2c778969c1422ba which is where this logic error; was introduced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9298
https://github.com/hail-is/hail/pull/9298:892,Testability,assert,assertion,892,"a2c778969c1422ba. The tabix line iterator works by building a list of virtual offsets from; the index that correspond to a requested interval. We recently; discovered an issue that would lead to a runtime exception if the; following conditions held:. * An offset pair ended exactly on a block boundary.; * The block boundary was exactly the start of a line. In a blocked file with virtual offsets, there are two ways to point to; the start of every block, (previous block start offset, previous block size) or; (current block start offset, 0). Because of the way curOff was calculated in TabixLineIterator, this would; lead to a situation where curOff was the (previous block start offset, previous block size); value, causing the jump to next chunk comparision `!TbiOrd.less64(curOff, offsets(i)._2)`; to fail when it should succeed, causing an extra line to be read, which; then makes the assertion check in the next iteration. Here we revert the improvements made earlier in; 3302850eb9d93c30793dc231a2c778969c1422ba which is where this logic error; was introduced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9298
https://github.com/hail-is/hail/pull/9298:1041,Testability,log,logic,1041,"a2c778969c1422ba. The tabix line iterator works by building a list of virtual offsets from; the index that correspond to a requested interval. We recently; discovered an issue that would lead to a runtime exception if the; following conditions held:. * An offset pair ended exactly on a block boundary.; * The block boundary was exactly the start of a line. In a blocked file with virtual offsets, there are two ways to point to; the start of every block, (previous block start offset, previous block size) or; (current block start offset, 0). Because of the way curOff was calculated in TabixLineIterator, this would; lead to a situation where curOff was the (previous block start offset, previous block size); value, causing the jump to next chunk comparision `!TbiOrd.less64(curOff, offsets(i)._2)`; to fail when it should succeed, causing an extra line to be read, which; then makes the assertion check in the next iteration. Here we revert the improvements made earlier in; 3302850eb9d93c30793dc231a2c778969c1422ba which is where this logic error; was introduced.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9298
https://github.com/hail-is/hail/pull/9302:272,Modifiability,variab,variables,272,"Almost every call to these methods was just casting the values to codes anyway, and using a list of values makes more sense. In the future, we are almost definitely going to want to change this so the method just directly takes in a `PTupleValue` so that we aren't making variables per dimension of the ndarray all the time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9302
https://github.com/hail-is/hail/pull/9303:279,Integrability,interface,interface,279,"This adds an `svd` method to `hl.nd`, allowing us to take singular value decomposition of local ndarrays. This is a generally useful operation, but my primary reason for adding it is to avoid the need to call numpy's SVD as a substep of Annamira's new randomized PCA method. The interface is designed to match numpy exactly. . Note that this method uses the LAPACK method `dgesdd`, which is newer, faster version of `dgesvd`. As far as I can tell, there's never a reason to use `dgesvd`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9303
https://github.com/hail-is/hail/pull/9303:186,Safety,avoid,avoid,186,"This adds an `svd` method to `hl.nd`, allowing us to take singular value decomposition of local ndarrays. This is a generally useful operation, but my primary reason for adding it is to avoid the need to call numpy's SVD as a substep of Annamira's new randomized PCA method. The interface is designed to match numpy exactly. . Note that this method uses the LAPACK method `dgesdd`, which is newer, faster version of `dgesvd`. As far as I can tell, there's never a reason to use `dgesvd`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9303
https://github.com/hail-is/hail/pull/9304:872,Testability,assert,assertion,872,"The tabix line iterator works by building a list of virtual offsets from; the index that correspond to a requested interval. We recently; discovered an issue that would lead to a runtime exception if the; following conditions held:. * An offset pair ended exactly on a block boundary.; * The block boundary was exactly the start of a line. In a blocked file with virtual offsets, there are two ways to point to; the start of every block, (previous block start offset, previous block size) or; (current block start offset, 0). Because of the way curOff is calculated in TabixLineIterator, this would; lead to a situation where curOff was the (previous block start offset, previous block size); value, causing the jump to next chunk comparision `!TbiOrd.less64(curOff, offsets(i)._2)`; to fail when it should succeed, causing an extra line to be read, which; then makes the assertion check in the next iteration. The key invariant of TabixLineIterator must be to keep bufferCursor; inside the block currently being read. We therefore make a check to see; if we need to refresh the buffer in any circumstance where we read; a line. Coupled with a reflow of control in readLine to increase; readability and fix a bug where lines greater than 64k would not be read; properly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304
https://github.com/hail-is/hail/pull/9307:243,Availability,rollback,rollback,243,"The old code in `Transaction` to exit the transaction and release the connection back to the pool looked like this:. ```; async def _aexit(self, exc_type, exc_val, exc_tb):; try:; if self.conn is not None:; try:; if exc_type:; await self.conn.rollback(); else:; await self.conn.commit(); finally:; self.conn = None; finally:; if self.conn_context_manager is not None:; try:; await aexit(self.conn_context_manager, exc_type, exc_val, exc_tb); finally:; self.conn_context_manager = None; ```. The problem was if the current coroutine was cancelled in the call to `aexit(self.conn_context_manager, ...)`, which ultimately calls aiomysql `Pool.release`, the release never happens. This was happening when database calls in `@only_active_instances` were getting cancelled when the client timed out and terminated the request. Roughly, the solution is to shield exiting the connection, and return the connection asynchronously in a background task (using `ensure_future`). FYI @danking @jigold This is a subtle bug/pattern for managing resources that we should all be aware of.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9307
https://github.com/hail-is/hail/pull/9307:58,Deployability,release,release,58,"The old code in `Transaction` to exit the transaction and release the connection back to the pool looked like this:. ```; async def _aexit(self, exc_type, exc_val, exc_tb):; try:; if self.conn is not None:; try:; if exc_type:; await self.conn.rollback(); else:; await self.conn.commit(); finally:; self.conn = None; finally:; if self.conn_context_manager is not None:; try:; await aexit(self.conn_context_manager, exc_type, exc_val, exc_tb); finally:; self.conn_context_manager = None; ```. The problem was if the current coroutine was cancelled in the call to `aexit(self.conn_context_manager, ...)`, which ultimately calls aiomysql `Pool.release`, the release never happens. This was happening when database calls in `@only_active_instances` were getting cancelled when the client timed out and terminated the request. Roughly, the solution is to shield exiting the connection, and return the connection asynchronously in a background task (using `ensure_future`). FYI @danking @jigold This is a subtle bug/pattern for managing resources that we should all be aware of.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9307
https://github.com/hail-is/hail/pull/9307:243,Deployability,rollback,rollback,243,"The old code in `Transaction` to exit the transaction and release the connection back to the pool looked like this:. ```; async def _aexit(self, exc_type, exc_val, exc_tb):; try:; if self.conn is not None:; try:; if exc_type:; await self.conn.rollback(); else:; await self.conn.commit(); finally:; self.conn = None; finally:; if self.conn_context_manager is not None:; try:; await aexit(self.conn_context_manager, exc_type, exc_val, exc_tb); finally:; self.conn_context_manager = None; ```. The problem was if the current coroutine was cancelled in the call to `aexit(self.conn_context_manager, ...)`, which ultimately calls aiomysql `Pool.release`, the release never happens. This was happening when database calls in `@only_active_instances` were getting cancelled when the client timed out and terminated the request. Roughly, the solution is to shield exiting the connection, and return the connection asynchronously in a background task (using `ensure_future`). FYI @danking @jigold This is a subtle bug/pattern for managing resources that we should all be aware of.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9307
https://github.com/hail-is/hail/pull/9307:640,Deployability,release,release,640,"The old code in `Transaction` to exit the transaction and release the connection back to the pool looked like this:. ```; async def _aexit(self, exc_type, exc_val, exc_tb):; try:; if self.conn is not None:; try:; if exc_type:; await self.conn.rollback(); else:; await self.conn.commit(); finally:; self.conn = None; finally:; if self.conn_context_manager is not None:; try:; await aexit(self.conn_context_manager, exc_type, exc_val, exc_tb); finally:; self.conn_context_manager = None; ```. The problem was if the current coroutine was cancelled in the call to `aexit(self.conn_context_manager, ...)`, which ultimately calls aiomysql `Pool.release`, the release never happens. This was happening when database calls in `@only_active_instances` were getting cancelled when the client timed out and terminated the request. Roughly, the solution is to shield exiting the connection, and return the connection asynchronously in a background task (using `ensure_future`). FYI @danking @jigold This is a subtle bug/pattern for managing resources that we should all be aware of.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9307
https://github.com/hail-is/hail/pull/9307:654,Deployability,release,release,654,"The old code in `Transaction` to exit the transaction and release the connection back to the pool looked like this:. ```; async def _aexit(self, exc_type, exc_val, exc_tb):; try:; if self.conn is not None:; try:; if exc_type:; await self.conn.rollback(); else:; await self.conn.commit(); finally:; self.conn = None; finally:; if self.conn_context_manager is not None:; try:; await aexit(self.conn_context_manager, exc_type, exc_val, exc_tb); finally:; self.conn_context_manager = None; ```. The problem was if the current coroutine was cancelled in the call to `aexit(self.conn_context_manager, ...)`, which ultimately calls aiomysql `Pool.release`, the release never happens. This was happening when database calls in `@only_active_instances` were getting cancelled when the client timed out and terminated the request. Roughly, the solution is to shield exiting the connection, and return the connection asynchronously in a background task (using `ensure_future`). FYI @danking @jigold This is a subtle bug/pattern for managing resources that we should all be aware of.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9307
https://github.com/hail-is/hail/pull/9316:287,Testability,log,logic,287,"Instead of using a fixed weighting, query the GCP /regions endpoint which includes the quota limit and usage. Use this to weight zones for new instance requests. Added a ComputeClient.list endpoint that should work with any GCE list endpoint that is paged. I'm working on improving this logic further by tracking how often instance create requests fail and then weighting zones by `capacity * p(create instance will succeed)`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9316
https://github.com/hail-is/hail/pull/9317:145,Integrability,message,messages,145,- track last 10 instance.create operations per zone; - weight probability by the success rate; - fixed bug where mark = None wouldn't return any messages,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9317
https://github.com/hail-is/hail/pull/9318:152,Integrability,wrap,wrapped,152,"Mostly doing this for visibility as we are profiling randomized PCA and dndarray stuff. . The diff makes this seem like more than it is, basically just wrapped the already existing code in method calls.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9318
https://github.com/hail-is/hail/pull/9319:10,Testability,test,tests,10,Up to 569 tests passing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9319
https://github.com/hail-is/hail/pull/9320:2236,Energy Efficiency,allocate,allocated,2236,"ame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to the leaves of the recursion. The basic rule is that any cycle of mutually recursive function calls must have at least one `call` on the cycle. Trying to keep it to just one `call` per cycle minimizes the number of closures that must be allocated. As an example, `NormalizeNames.normalizeIR` now returns `StackFrame[BaseIR]`, and `def apply(ir: BaseIR): BaseIR` calls `normalizeIR(ir, ...).run()` to actually run the traversal. The case for `Let` in `normalizeIR` is rewritten from; ```scala; case Let(name, value, body) =>; val newName = gen(); Let(newName, normalize(value), normalize(body, env.copy(eval = env.eval.bind(name, newName)))); ```; to; ```scala; case Let(name, value, body) =>; val newName = gen(); for {; newValue <- normalize(value); newBody <- normalize(body, env.copy(eval = env.eval.bind(name, newName))); } yield Let(newName, newValue, newBody); ```; or without the sugar; ```scala; case Let(name, value, body) =>; val newName = gen(); normalize(value).flatMap { newValue =>; normalize(body, env.copy(eval = env.eval.bind(name, newName))).map { newBody =>; Let(newName, newValue, newBody); }; }; ```; All recursive calls go through `normalize`, which has been rewritten to `",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9320
https://github.com/hail-is/hail/pull/9320:1890,Integrability,wrap,wraps,1890," `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to the leaves of the recursion. The basic rule is that any cycle of mutually recursive function calls must have at least one `call` on the cycle. Trying to keep it to just one `call` per cycle minimizes the number of closures that must be allocated. As an example, `NormalizeNames.normalizeIR` now returns `StackFrame[BaseIR]`, and `def apply(ir: BaseIR): BaseIR` calls `normalizeIR(ir, ...).run()` to actually run the traversal. The case for `Let` in `normalizeIR` is rewritten from; ```scala; case Let(name, value, body) =>; val newName = gen(); Let(newName, normalize(value), normalize(body, env.copy(eval = env.eval.bind(name, newName)))); ```; to; ```scala; case Let(name, value, body) =>; val newName = gen(); for {; newValue <- normalize(value); newBody <- normalize(body, env.copy(eval = env.eval.bind(name, newName))); } yield Let(newName, newValue, newBody); ```; or without the sugar; ```scala; case Let(name, value, body) =>; val newNa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9320
https://github.com/hail-is/hail/pull/9320:156,Modifiability,Rewrite,RewriteBottomUp,156,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9320
https://github.com/hail-is/hail/pull/9320:425,Modifiability,rewrite,rewrites--where,425,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9320
https://github.com/hail-is/hail/pull/9320:819,Modifiability,rewrite,rewrite,819,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9320
https://github.com/hail-is/hail/pull/9320:205,Performance,optimiz,optimizes,205,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9320
https://github.com/hail-is/hail/pull/9320:331,Performance,perform,performance,331,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9320
